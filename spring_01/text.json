[
  {
    "numpages": 13,
    "numrender": 13,
    "info": {
      "PDFFormatVersion": "1.7",
      "Language": "en-US",
      "EncryptFilterName": null,
      "IsLinearized": true,
      "IsAcroFormPresent": false,
      "IsXFAPresent": false,
      "IsCollectionPresent": false,
      "IsSignaturesPresent": false,
      "Creator": "Elsevier",
      "Custom": {
        "CrossMarkDomains[1]": "elsevier.com",
        "CrossmarkMajorVersionDate": "2010-04-23",
        "CreationDate--Text": "21st November 2023",
        "ElsevierWebPDFSpecifications": "7.0",
        "robots": "noindex",
        "doi": "10.1016/j.compbiomed.2023.107582",
        "CrossMarkDomains[2]": "sciencedirect.com",
        "CrossmarkDomainExclusive": "true"
      },
      "ModDate": "D:20231121070521Z",
      "Author": "Jie Ying",
      "Title": "Weakly supervised segmentation of uterus by scribble labeling on endometrial cancer MR images",
      "Keywords": "Endometrial cancer,Uterus segmentation,Weakly supervised,Dual branch,Pseudo label,Exponential geodesic distance",
      "CreationDate": "D:20231119083706Z",
      "Producer": "Acrobat Distiller 8.1.0 (Windows)",
      "Subject": "Computers in Biology and Medicine, 167 (2023) 107582. doi:10.1016/j.compbiomed.2023.107582"
    },
    "metadata": {
      "crossmark:crossmarkdomains": "elsevier.comsciencedirect.com",
      "crossmark:crossmarkdomainexclusive": "true",
      "crossmark:doi": "10.1016/j.compbiomed.2023.107582",
      "crossmark:majorversiondate": "2010-04-23",
      "dc:format": "application/pdf",
      "dc:identifier": "10.1016/j.compbiomed.2023.107582",
      "dc:publisher": "Elsevier Ltd",
      "dc:description": "Computers in Biology and Medicine, 167 (2023) 107582. doi:10.1016/j.compbiomed.2023.107582",
      "dc:subject": [
        "Endometrial cancer",
        "Uterus segmentation",
        "Weakly supervised",
        "Dual branch",
        "Pseudo label",
        "Exponential geodesic distance"
      ],
      "dc:title": "Weakly supervised segmentation of uterus by scribble labeling on endometrial cancer MR images",
      "dc:creator": [
        "Jie Ying",
        "Wei Huang",
        "Le Fu",
        "Haima Yang",
        "Jiangzihao Cheng"
      ],
      "jav:journal_article_version": "VoR",
      "pdf:creationdate--text": "21st November 2023",
      "pdf:producer": "Acrobat Distiller 8.1.0 (Windows)",
      "pdf:keywords": "Endometrial cancer,Uterus segmentation,Weakly supervised,Dual branch,Pseudo label,Exponential geodesic distance",
      "pdfx:creationdate--text": "21st November 2023",
      "pdfx:crossmarkdomains": "sciencedirect.comelsevier.com",
      "pdfx:crossmarkdomainexclusive": "true",
      "pdfx:crossmarkmajorversiondate": "2010-04-23",
      "zifnonpf_mgmjym6gmpf8mlnnz9erlt_.ypygm9n7ogf7ypf7ymj9o9eqn9iknm-rnd6tma": "",
      "pdfx:doi": "10.1016/j.compbiomed.2023.107582",
      "pdfx:robots": "noindex",
      "prism:aggregationtype": "journal",
      "prism:copyright": "Â© 2023 Elsevier Ltd. All rights reserved.",
      "prism:coverdate": "2023-12-01",
      "prism:coverdisplaydate": "1 December 2023",
      "prism:doi": "10.1016/j.compbiomed.2023.107582",
      "prism:issn": "0010-4825",
      "prism:pagerange": "107582",
      "prism:publicationname": "Computers in Biology and Medicine",
      "prism:startingpage": "107582",
      "prism:url": "https://doi.org/10.1016/j.compbiomed.2023.107582",
      "prism:volume": "167",
      "xmp:createdate": "2023-11-19T08:37:06",
      "xmp:creatortool": "Elsevier",
      "xmp:metadatadate": "2023-11-21T07:05:21",
      "xmp:modifydate": "2023-11-21T07:05:21",
      "xmpmm:documentid": "uuid:1b499bed-4ce8-4c0c-b682-0d58baae1cbe",
      "xmpmm:instanceid": "uuid:b6ed5266-03cb-4855-90c0-636283357f42",
      "xmprights:marked": "True"
    },
    "text": "Computers in Biology and Medicine 167 (2023) 107582\nAvailable online 20 October 2023\n0010-4825/Â© 2023 Elsevier Ltd. All rights reserved.\nContents lists available at ScienceDirect\nComputers in Biology and Medicine\njournal homepage: www.elsevier.com/locate/compbiomed\nWeakly supervised segmentation of uterus by scribble labeling on\nendometrial cancer MR images\nJie Ying \na,\nâˆ—\n, Wei Huang \na\n, Le Fu \nb,\nâˆ—\n, Haima Yang \na\n, Jiangzihao Cheng \na\na \nSchool of Optical-Electrical and Computer Engineering, University of Shanghai for Science and Technology, Shanghai, China\nb \nDepartment of Radiology, Shanghai First Maternity and Infant Hospital, Tongji University School of Medicine, Shanghai, China\nA R T I C L E I N F O\nKeywords:\nEndometrial cancer\nUterus segmentation\nWeakly supervised\nDual branch\nPseudo label\nExponential geodesic distance\nA B S T R A C T\nUterine segmentation of endometrial cancer MR images can be a valuable diagnostic tool for gynecologists.\nHowever, uterine segmentation based on deep learning relies on artificial pixel-level annotation, which is\ntime-consuming, laborious and subjective. To reduce the dependence on pixel-level annotation, a method of\nweakly supervised uterine segmentation on endometrial cancer MRI slices is proposed, which only requires\nscribble label and is enhanced by pseudo-label technology, exponential geodesic distance loss and input\ndisturbance strategy. Specifically, the limitations caused by the shortage of supervision are addressed by\ndynamically mixing the two outputs of the dual branch network to generate pseudo-labels, expanding\nsupervision information and promoting mutual supervision training. On the other hand, considering the large\ndifference of grayscale intensity between the uterus and surrounding tissues, the exponential geodesic distance\nloss is introduced to enhance the ability of the network to capture the edge of the uterus. Input disturbance\nstrategies are incorporated to adapt to the flexible and variable characteristics of the uterus and further\nimprove the segmentation performance of the network. The proposed method is evaluated on MRI images\nfrom 135 cases of endometrial cancer. Compared with other four weakly supervised segmentation methods,\nthe performance of the proposed method is the best, whose mean DI, HD\n95\n, Recall, Precision, ADP are 92.8%,\n11.632, 92.7%, 93.6%, 6.5% and increasing by 2.1%, 9.144, 0.6%, 2.4%, 2.9% respectively. The experimental\nresults demonstrate that the proposed method is more effective than other weakly supervised methods and\nachieves similar performance as those fully supervised.\n1. Introduction\nEndometrial cancer is a reproductive system malignant tumor, and\nits incidence has been yearly increasing worldwide [1,2]. Endometrial\ncancer not only has a high incidence, but also may lead to infertility,\nhysterectomy, and can be even life-threatening, so its prevention and\ntreatment are highly important clinically.\nMagnetic resonance imaging (MRI) provides high resolution for soft\ntissues, and multi-planar imaging. Therefore, MRI of the pelvic cavity\ncan be used to understand the internal anatomy of the uterine cavity,\nevaluate the range of uterine malignant tumors, evaluate the degree of\nmyometrial invasion, and distinguish the stage of endometrial cancer.\nAccurately segmenting the uterus is necessary. Hysterectomy, bi-\nlateral salpingectomy, and oophorectomy are the basic treatments of\nEC. Pelvic and paraaortic lymph node dissections are considered in\ncombination with the risk of recurrence [3,4]. Common predictive\nfactors for recurrence include age, tumor grade, lymphatic space in-\nfiltration, and International Federation of Obstetrics and Gynecology\nâˆ— \nCorresponding authors.\nE-mail addresses: yingjsh@163.com (J. Ying), fule0125@qq.com (L. Fu).\n(FIGO) staging [5]. The FIGO staging is determined by the depth of\ninvasion and spread of the tumor. Accurate segmentation of the uterus\nis beneficial for the assessment of tumor invasion depth. Zhu et al. [6]\nproposed geometric features (referred to as LS) of the uterine in the\nstudy of detecting deep myometrial invasion of endometrial cancer. The\nLS was obtained by calculating the Euclidean distance between the edge\nof the uterine body and the connected domain between the tumor and\nthe uterine cavity. Therefore, it needs accurately segment the uterine\nedge before calculating LS. In the process of EC surgical treatment,\naccurate segmentation of the uterus can help doctors remove tumors to\nthe fullest extent possible and reduce the risk of residual lesions. This\nis important for improving the integrity of surgical resection, helping\nto control the disease, and reducing the likelihood of recurrence.\nIt is usually desirable to achieve an accuracy rate of more than\n85% when segmenting the uterus by the doctorâ€™s description. However,\nhigher segmentation accuracy is more beneficial for EC evaluation and\ndiagnosis. Artificial segmentation of the uterus is a time-consuming,\nhttps://doi.org/10.1016/j.compbiomed.2023.107582\nReceived 7 May 2023; Received in revised form 28 September 2023; Accepted 15 October 2023\n\nComputers in Biology and Medicine 167 (2023) 107582\n2\nJ. Ying et al.\nlabor-intensive, and highly subjective task. Therefore, it is necessary to\nautomatically segment the uterine. However, automatically segmenting\nthe uterus is challenging. Firstly, the uterus is a dynamic organ. Its\nshape and size may change under different physiological conditions\n(e.g., The thickness and shape of the endometrium will change with\nthe menstrual cycle). Secondly, the anatomical structure of the uterus\nis relatively complex, including the endometrium, myometrium, and\nadventitia. There are also other structures surrounding the uterus,\nsuch as the fallopian tubes and ovaries [7]. Besides, the uterus of\nEC patients may deform due to the infiltration of cancerous tissue,\nmaking it difficult to accurately capture the geometric shape during the\nsegmentation process. It is worth mentioning that the distribution of\nuterine and non-uterine regions in medical images is unbalanced. This\nmay result in algorithms classifying more pixels as non-uterine regions\nwhile disregarding the details of the uterine region.\nDespite there are many existing methods to assist in the diagnosis of\nEC, only a few studies have investigated the segmentation of the uterus\nand endometrium on EC MRI [8,9]. Kurata et al. [8] found that EC\nhas high cell viability and typically shows high signal intensity on the\ndiffusion-weighted image (DWI). However, normal endometrium also\nexhibits high signal intensity on DWI. Therefore, they used multiple\nweighted images (DWI, T2-weighted image) as the networkâ€™s input to\nimprove the accuracy of the segmentation of EC. Hsiang et al. [9] used\nVGG11, VGG16 [10], and ResNet [11] as the backbone networks of\nUNet [12] to perform segmentation of the uterus and endometrium on\nT2-weighted images (T2WI) and T1-weighted images (T1WI) of EC pa-\ntients. The result shows that the UNet with VGG11 and ResNet achieved\nthe best segmentation performance on T1WI and T2WI, respectively.\nThere are also some studies on the uterus segmentation on MRI for\nother uterine diseases [13â€“15]. Chen et al. [15] proposed a semi-\nsupervised network called CTANet to segment the uterus and dangerous\norgans on uterine fibroid MRI. They connected two UNet networks\nserially to form a coarse-fine segmentation network for generating\npseudo-labels. Then, a method based on adaptive threshold is used to\ngenerate high-confidence pseudo-labels to improve segmentation accu-\nracy. Shahedi et al. [13] proposed a 3D segmentation method for the\nuterus and placenta based on user interaction. This method utilizes user\ninteraction to initialize the network, enhancing segmentation results\nand increasing the modelâ€™s reliability in clinical applications. Although\nthis network can directly segment 3D uterine regions, it requires a\nsignificant amount of processing time and consumes a large number\nof computing resources during the training process.\nThere are some drawbacks in existing methods for uterine seg-\nmentation: (1) most of these methods rely on the precise pixel-level\nannotations. However, in the process of collecting these pixel-level\nannotations, not only is professional knowledge and clinical experience\nrequired, but also a significant amount of labor is needed. Although\nlecture [15] utilizes a small number of images with pixel-level anno-\ntation and a large number of unlabeled images to train the network,\nthe process of annotating pixels remains time-consuming. (2) Due to\nthe flexible position and morphology of the uterus, segmentation algo-\nrithms are not universally applicable. But the high contrast between the\nuterine body area and surrounding tissues can improve segmentation\nperformance. Therefore, when designing algorithms, it is necessary\nto consider the characteristics of the uterine and avoid meaningless\ntraining. However, this is often overlooked by current methods.\nAccording to the characteristics of the uterus in MRI, this study pro-\nposes a weakly supervised method to segment the uterus in MRI slices\nof EC patients using scribble annotation. This method is a dual-branch\nnetwork based on the pseudo-label technique, exponential geodesic dis-\ntance map, and input rotation perturbation. Firstly, the proposed dual-\nbranch network consists of two independent encoders and a shared\ndecoder. The two independent encoders can extract more features,\nreducing the dependence of the network on a large number of data\nsets. The shared decoder is supervised by two single-branch networks\nto improve the decoding ability of the decoder. The perturbations were\nintroduced in the different depth of the two encoders. Secondly, this\narticle employs the pseudo-label technique to address the issue of weak\nsupervision signals obtained from scribbles and promote mutual super-\nvision training. In addition, considering the relatively uniform gradient\ndistribution in the uterine region on MRI and the significant gray in-\ntensity differences of surrounding tissues, we proposed an exponential\ngeodesic distance loss. It can improve the sensitivity of the network\nto the uterine edge. Due to the dynamic nature of the uterus and its\nsignificant morphological changes in MRI, input rotation disturbances\nwere introduced in this method. The combination of input rotation\ndisturbance and encoder disturbance makes the proposed network more\nrobust.\nThe main contributions of this study are summarized as follows:\n1. A method of uterine segmentation from endometrial cancer MR\nimages is proposed based on scribble supervision.\n2. A dual-branch network with perturbations at different depth was\nproposed, which can reduce the number of required training samples\nand improve the decoding ability of decoder. The pseudo-labels are\ngenerated by dynamically hybridizing the two-branch networkâ€™s two\noutput images, which can enhance the supervision signal and facilitate\nmutual supervision training.\n3. An exponential geodesic distance loss was proposed based on the\ngray characteristics of the uterine region in MR images to effectively\ncapture uterine edge information. In light of the flexible characteristics\nof the uterus, we have introduced the input rotation perturbation\nstrategy.\n4. The proposed method is evaluated on datasets of endometrial\ncancer MRI and the ACDC public dataset, and is compared to other\nadvanced methods. The results show the superiority of the proposed\nmethod.\n2. Related work\n2.1. Uterine MRI segmentation\nAlthough automatic uterus segmentation is of great significance for\nthe evaluation of endometrial cancer, due to the lack of annotated\nimage datasets, automatic uterus segmentation based on deep leaning\nremains underresearched. Niu et al. [16] proposed a uterine segmen-\ntation method based on deep learning. Specifically, the original data\nbased on Hessian matrix is first pre-processed, and then the new data\nis input to DenseUNet for training. The results showed that the dice\nsimilarity coefficient (DSC), accuracy, sensitivity and specificity were\n87.60%, 86.57%, 88.11% and 99.75%, respectively. Kurata et al. [7]\nused the improved UNet to automatically segment the uterus on dis-\neased and non-diseased MRI. The method used the common DSC and\nmean absolute distance (MAD) in the segmentation model to quanti-\ntatively evaluate the segmentation results, in addition to allowing two\nradiologists to use four kinds of scores for visual evaluation. The results\nshowed that the average DSC of the diseased and non-diseased groups\nwere 84% and 78%, respectively, and the average MAD of the two\ngroups was 18.5 and 21.4, respectively. No difference was observed in\nvisual evaluation between the two groups. Shahedi et al. [13] used a\nfull convolutional neural network for 3D segmentation of the uterus and\nplacenta, and added a humanâ€“computer interaction function auxiliary\ntraining. The average DSC of the uterus and placenta were reported\nas 92% and 82%, respectively. Although the above experiments have\nachieved good results, they all require pixel level annotation which is\ndifficult to obtain, so a method of uterine segmentation based on weak\nannotation is proposed in this paper.\n2.2. Weak labeling in medical image segmentation\nRecently, weak annotation has been emerging as a potential method\nto overcome the high cost of pixel level annotation. Typical weak\n\nComputers in Biology and Medicine 167 (2023) 107582\n3\nJ. Ying et al.\nFig. 1. Different types of weak annotations. Green dots and curves represent the uterine\nregion, and red dots and curves represent other regions.\nannotations include image level annotations [17,18], bounding box\nannotations [19,20], point annotations [21,22], and scribble anno-\ntations [23â€“26]. Wu et al. [17] proposed an image-level supervised\nmethod based on attention representation learning to segment brain\ninjuries. This method applies an independent dimensional attention\nmechanism to the class activation graph to improve the performance of\nsegmenting lesions. Zhao et al. [19] proposed a bounding box annota-\ntion method for 3D instance segmentation. This method simply needs to\nlabel the 3D bounding box for all instances, not each pixel individually.\nLaradji et al. [21] proposed a consistency loss function, which guides\nthe spatial conversion of the predicted image and the input image to\nbe consistent, in the task of segmenting Covid-2019 infected areas.\nThis method can be learned from point annotation, which is simpler\nand more convenient than obtaining pixel annotation. Lee et al. [23]\nproposed a cell segmentation framework, namely Scribble2Label, based\non scribble labeling. The core idea is to combine pseudo-labels with\nlabel filtering to generate reliable pseudo-labels. Among the different\nweak annotation methods, scribble annotation is most suitable for\nuterine segmentation in endometrial cancer patients, because it not\nonly balances the annotation cost and supervision information, but can\nalso handle irregular uterine regions (Fig. 1). Therefore, this is the\nadopted method in this study.\n2.3. Dual branch network\nAlthough UNet and its variants have achieved some success in\nthe task of medical image segmentation, this task remains a chal-\nlenging task because of the characteristics of medical images, such\nas diversity of target scales, fuzzy structure boundaries, and irregular\nshapes. Recently, the double branch network has been employed in\nliterature to address the poor segmentation performance caused by\nthese medical image characteristics [27â€“29], achieving good results.\nCheng et al. [27] designed a dual dense U-structure network (DDU-net).\nThey constructed two densely connected encoders in the network, one\nof which uses a pretrained DenseNet as a fixed feature extractor, and\nthe others have a network structure similar to the first. Both encoders\nencode the input image to extract more features. Then, the multi-\nscale semantic information learned by the two encoders is fused by\nthe densely connected convolution layer to construct a deeper decoder.\nThis method both reduces the number of parameters in the network, in\naddition to making it easier to train on small samples. Xu et al. [28] pro-\nposed the ğœ”-Net in the task of segmenting small targets. An additional\ndecoder was embedded in the UNet, additional supervision signals were\nintroduced, and more effective and robust image segmentation was\nachieved through double supervision. Experimental results show that\nthe performance of the network is better than the traditional UNet.\nDue to the small number of experimental samples in this study, a dual\nbranch network with an additional encoder is employed to extract more\nimage features.\n3. Method\nThis section describes in detail the method of weakly supervised\nuterine segmentation by scribble labeling on MR images. The first part\ndescribes the problems involved in this study and the proposed weak\nsupervised segmentation framework. The second part introduces sev-\neral core components in the network. Finally, the third part introduces\nthe loss function used adopted in the proposed method.\n3.1. Problem description and general framework\nIn this study, the segmentation task is regarded as a binary classi-\nfication problem, namely each pixel is classified as uterine region or\nbackground region. In this problem, dataset {X\nğ‘›\n, S\nğ‘›\n}\nğ‘\nğ‘›=1 \nis used to train\nthe network, where X\nğ‘› \nâˆˆ R\nğ»Ã—ğ‘Š \nis the sagittal MR image of the patient\npelvis and S\nğ‘› \nâˆˆ {0, 1, 2}\nğ»Ã—ğ‘Š \nis the corresponding scribble annotation\nimage. Three types of pixels comprise S\nğ‘›\n: 0 refers to the background\npixel of scribble, 1 refers to the uterine pixel of scribble, and 2 refers\nto the pixel without scribble (unknown category). Fig. 2 shows the\ntwo independent encoders (ğœƒ\nğ‘’1\n, ğœƒ\nğ‘’2\n) and a shared decoder (ğœƒ\nğ‘‘ \n) that are\nemployed to predict the output. The single-branch network composed\nof ğœƒ\nğ‘’1 \nand ğœƒ\nğ‘‘ \nis denoted as ğœƒ\nğ‘’1 \nâˆ’ğœƒ\nğ‘‘ \n, the single-branch network composed\nof ğœƒ\nğ‘’2 \nand ğœƒ\nğ‘‘ \nis denoted as ğœƒ\nğ‘’2 \nâˆ’ ğœƒ\nğ‘‘ \n, and the double branch network\ncomposed of ğœƒ\nğ‘’1\n, ğœƒ\nğ‘’2 \nand ğœƒ\nğ‘‘ \nis denoted as ğœƒ\nğ‘’1 \nâˆ’ ğœƒ\nğ‘‘ \nâˆ’ ğœƒ\nğ‘’2\n. It should be\nnoted that the above â€™-â€™ represents the cascade operation of the encoder\nand decoder.\nThe input sources of the entire network are X\nğ‘› \nand the corre-\nsponding S\nğ‘›\n. The network combines these two images to generate\nan exponential geodesic distance map (EGD\nğ‘›\n) offline. During training\nphase, the network randomly extracts X\nğ‘› \nfrom the training set and\nsends it into ğœƒ\nğ‘’1\n. Considering that some of uterus has an acute angle\nto the vertical direction, while others have an obtuse angle, so\nÌƒ\n ğ‘‹\nğ‘› \nis\nobtained by randomly rotating X\nğ‘› \nby angle ğ›¾ (ğ›¾ âˆˆ [0\nâ—¦\n, 180\nâ—¦\n]) and is\nsent into ğœƒ\nğ‘’2 \nto accommodate the flexible uterus morphology.\nÌƒ\n ğ‘†\nğ‘› \nand\nÌƒ\nğ¸ğºğ·\nğ‘› \nare obtained by rotating S\nğ‘› \nand EGD\nğ‘› \nby the same angle ğ›¾.\nThen, the outputs of ğœƒ\nğ‘’1 \nand ğœƒ\nğ‘’2 \nare sent to ğœƒ\nğ‘‘ \ndecoder. Afterwards, the\noutput of ğœƒ\nğ‘’1 \nâˆ’ ğœƒ\nğ‘‘ \nis rotated by angle ğ›¾ to obtain y\nğ‘›_1 \nand the predicted\nresult of ğœƒ\nğ‘’2 \nâˆ’ ğœƒ\nğ‘‘ \nis y\nğ‘›_2\n. Finally, the pseudo-label (PL\nğ‘›\n) is obtained by\nrandomly weighted summation of y\nğ‘›_1 \nand y\nğ‘›_2\n, completing a forward\npropagation. The red, green and blue dashed lines in Fig. 2 respectively\nindicate the calculation process of the pseudo-label loss (L\nğ‘ƒ ğ¿1\n, L\nğ‘ƒ ğ¿2\n),\nthe exponential geodesic distance loss (L\nğ¸ğºğ·1\n, L\nğ¸ğºğ·2\n) and the scribble\nsupervision loss (L\nğ‘ğ‘ğ‘’1\n, L\nğ‘ğ‘ğ‘’2\n). Specially, they are calculated by PL\nğ‘›\n,\nÌƒ\nğ¸ğºğ·\nğ‘›\n,\nÌƒ\n ğ‘†\nğ‘›\n, y\nğ‘›_1 \nand y\nğ‘›_2 \nduring the back propagation process. Then,\nthe calculation of the total loss (L\nğ‘¡ğ‘œğ‘¡ğ‘ğ‘™ \n) can be seen in the lower part\nof Fig. 2. Firstly, the L\nğ‘ğ‘ğ‘’1\n, L\nğ‘ğ‘ğ‘’2\n, L\nğ¸ğºğ·1 \nand L\nğ¸ğºğ·2 \nare added together,\nand then the total loss is obtained by weighted summation of the sum\nof the previous four losses and the sum of L\nğ‘ƒ ğ¿1 \nand L\nğ‘ƒ ğ¿2\n. Finally, L\nğ‘¡ğ‘œğ‘¡ğ‘ğ‘™\nis used to update the parameters of the network.\nIn the test phase, only the trained ğœƒ\nğ‘’2 \nâˆ’ ğœƒ\nğ‘‘ \nnetwork was used to\npredict the uterine region of the MR image without rotating the input\nimage.\n3.2. Network core components\n3.2.1. Improved double branch network\nFig. 3 shows the overall structure of the dual branch network used\nin this study. Fig. 4 shows the network structure of Conv Block (CB),\nUp Block (UB), E1 Down Block (E1-DB) and E2 Down Block (E2-DB) in\nthis structure.\n\nComputers in Biology and Medicine 167 (2023) 107582\n4\nJ. Ying et al.\nFig. 2. General framework of medical image segmentation model for scribble supervision.\nAn additional encoder (ğœƒ\nğ‘’2\n) is embedded in the conventional\nUNet [12] network (ğœƒ\nğ‘’1 \nâˆ’ ğœƒ\nğ‘‘ \n) (Fig. 3). The structures of ğœƒ\nğ‘’2 \nand ğœƒ\nğ‘’1\nare exactly same, but the zeroing probability (p) of the dropout layer\nin ğœƒ\nğ‘’2 \nis different from that in ğœƒ\nğ‘’1 \nto achieve the purpose of disturbance.\nSpecifically, ğœƒ\nğ‘’1 \nincludes a CB, whose p and the number of output\nchannels (OC) are 0.05, 16, respectively, and four E1-DB with OC and\np increasing with depth. The structure of ğœƒ\nğ‘’2 \nis the same as that of ğœƒ\nğ‘’1\n,\nexcept that E2-DB is used instead of E1-DB. Finally, ğœƒ\nğ‘‘ \nconsists of four\nUB with p = 0 and OC decreasing with depth, a 3 Ã— 3 convolution layer\nwith OC = 2 and a softmax activation function.\nâ€˜â€˜Conv Block (OC, p)â€™â€™ in Fig. 4 represents a CB with an OC number\nof output channels and a zeroing probability of dropout layer of p.\nâ€˜â€˜Up Block kâ€™â€™ represents the ğ‘˜th UB in ğœƒ\nğ‘‘ \n. â€˜â€˜E1 Down Block jâ€™â€™ and\nâ€˜â€˜E2 Down Block jâ€™â€™ represent the ğ‘—th E1-DB and E2-DB in ğœƒ\nğ‘’1 \nand ğœƒ\nğ‘’2\n,\nrespectively. â€˜â€˜Conv Block (OC, p)â€™â€™ comprises the BN, ReLu, 3 Ã— 3\nconvolution with variable OC and p variable dropout layers (Fig. 4).\nâ€˜â€˜Up Block kâ€™â€™ comprises the 2 Ã— 2 transposed convolution, concat and\nCB with p = 0 and OC varying with k layers. The structures of â€˜â€˜E1 Down\nBlock jâ€™â€™ and â€˜â€˜E2 Down Block jâ€™â€™ are the same, both of which contain a\n2 Ã— 2 maximum pool layer and CB with p and OC varying with j. The\nonly difference is that p in â€˜â€˜E2 Down Block jâ€™â€™ is twice the p in the same\nposition in â€˜â€˜E1 Down Block jâ€™â€™.\nUsing such a dual branch network has the following advantages:\n(1) both ğœƒ\nğ‘’1 \nand ğœƒ\nğ‘’2 \ntry to encode the input image and extract more\nimage features. This way the number of parameters in the network\nis reduced and fewer samples are required for training; (2) The ğœƒ\nğ‘’1 \nâˆ’\nğœƒ\nğ‘‘ \nâˆ’ ğœƒ\nğ‘’2 \nnetwork structure can not only generate pseudo-labels through\ntwo prediction results without training two networks and expand the\nsupervision signal, but also enable the decoder to benefit from two\nseparate supervisions and improve the decoding ability of the network;\n(3) The operation of using different p in ğœƒ\nğ‘’1 \nand ğœƒ\nğ‘’2 \nis equivalent to\nadding disturbance into the network, which can boost the learning\nability of the network and enhance the robustness of the model.\n3.2.2. Dynamic mixing to generate pseudo-labels\nBased on the dual branch network, y\nğ‘›_1 \nand y\nğ‘›_2 \nare dynamically\nmixed to generate PL\nğ‘›\n. The specific mixing methods are\nğ‘ƒ ğ¿\nğ‘› \n= ğ‘ğ‘Ÿğ‘”ğ‘šğ‘ğ‘¥[ğ›¼ğ‘¦\nğ‘›_1 \n+ (1 âˆ’ ğ›¼)ğ‘¦\nğ‘›_2\n] (1)\nwhere ğ›¼ is the weight coefficient randomly generated in each iteration,\nranging in [0, 1]. For the value of ğ›¼, two extreme cases can be\nconsidered. The first case is ğ›¼ = 0, where PL\nğ‘› \n= y\nğ‘›_2\n, and single branch\nnetwork ğœƒ\nğ‘’2\nâˆ’ğœƒ\nğ‘‘ \nsupervises the training of single branch network ğœƒ\nğ‘’1\nâˆ’ğœƒ\nğ‘‘ \n;\nThe second case is ğ›¼ = 1, where PL\nğ‘› \n= y\nğ‘›_1 \nand single branch network\nğœƒ\nğ‘’1 \nâˆ’ ğœƒ\nğ‘‘ \nsupervises the training of single branch network ğœƒ\nğ‘’2 \nâˆ’ ğœƒ\nğ‘‘ \n. These\ntwo extreme cases show that parameter ğ›¼ not only actively updates the\npseudo-label in each iteration, and alleviates the inherent disadvantage\nof keeping the prediction results unchanged without updating the\nnetwork parameters when using the pseudo label strategy [30], but\nalso promotes the mutual supervision and training of the two single\nbranch networks. On the other hand, when the pseudo-label provided\nby dynamic mixing propagates a low number of labeled pixels to non-\nlabeled pixels, the supervising signal is amplified from a low number\nof pixels to the entire image.\n3.2.3. Exponential geodesic distance map\nIn MRI, the grayscale intensity in the uterine region is lower than\nthe gray intensity outside, forming a notable difference between the\ntwo. Based on the above characteristics of uterus in MRI and the\ngeodesic distance that image gradient information is taken into ac-\ncount, the X\nğ‘› \nand corresponding S\nğ‘› \nare used to generate EGD\nğ‘› \n[31,32]\n\nComputers in Biology and Medicine 167 (2023) 107582\n5\nJ. Ying et al.\nFig. 3. Dual branch network structure diagram.\nFig. 4. Structure diagram of each module. Conv(3 Ã— 3,OC) indicates that the convolution kernel size is 3 Ã— 3 and the number of output channels is OC. Dropout(p) indicates the\ndropout layer with a zero return probability of p. Up_Conv(2 Ã— 2, 256/2k) uses a deconvolution kernel size of 2 Ã— 2 and the number of output channels is 256/2k. Maxpooling(2 Ã— 2)\nindicates that the maximum pooled window size is 2 Ã— 2.\n\nComputers in Biology and Medicine 167 (2023) 107582\n6\nJ. Ying et al.\nFig. 5. Different prompts generated according to the scribble label in the uterine area\nand the original image.\nfor network supervision. The formulae are\nğ¸ğºğ·\nğ‘›\n(ğ‘–, ğ‘†, ğ¼) = min\nğ‘—âˆˆğ‘† \nğ‘’\nâˆ’ğœ†ğ‘€ğ‘€ğ‘[ğ·\nğ‘”ğ‘’ğ‘œ\n(ğ‘–,ğ‘—,ğ¼)] \n(2)\nğ·\nğ‘”ğ‘’ğ‘œ\n(ğ‘–, ğ‘—, ğ¼) = min\nğ‘âˆˆğ‘ƒ\nğ‘–,ğ‘— \nâˆ«\n1\n0\nâ€–âˆ‡ğ¼(ğ‘(ğ‘¥)) â‹… ğ‘¢(ğ‘¥)â€–ğ‘‘ğ‘¥ (3)\nwhere I is X\nğ‘›\n. S is the scribble pixel set of uterus. i, j are the pixels in I.\nP\nğ‘–,ğ‘— \nis the set of all paths between pixels i and j. p is a feasible path and\nit is parameterized by ğ‘¥ âˆˆ [0, 1]. u(x) = p\nâ€²\n(x)/||p\nâ€²\n(x)|| is the unit direc-\ntion tangent to the path direction. MMN [â‹…] is the minimum-maximum\nnormalization and ğœ† is a parameter to be adjusted. In this paper, ğœ† = 25\nis concluded to be the optimal value, which can not only clearly display\nthe outline of the uterus, but also remove the interference from other\npixels. Compared with the Euclidean distance, the geodesic distance\nconsiders the spatial position information in addition to the intensity\ndifference between pixels, providing more information about the region\nof interest (ROI). Fig. 5 shows an example of using scribble annotation\nto obtain hint diagrams. It can be concluded that EGD is output in the\nform of a probability diagram, which can better distinguish the uterine\nregion from other regions, compared with other prompt diagrams.\nFor the multi-target segmentation task, the exponential geodesic\ndistance map is generated separately from each target and original\nimage, then the exponential geodesic distance maps are combined by\nkeeping the maximum pixel value. The specific formula is\nğ¸ğºğ·\nğ‘›\n(ğ‘–, ğ‘†, ğ¼) = max\nğ‘âˆˆğ¶ \nğ¸ğºğ·\nğ‘ \n(ğ‘–, ğ‘†, ğ¼) (4)\nWhere EGD\nğ‘ \n(i, S, I ) is the exponential geodesic distance graph of\ncategory c, and C is the set of segmented target categories.\n3.2.4. Input rotation disturbance\nThe flexible position of the uterus in the pelvic cavity challenges\nthe segmentation. For example, some uterine have an acute angle with\nthe vertical direction, while others have an obtuse angle. Therefore,\nin order to overcome the challenge brought by this uterine position,\nthe input image X\nğ‘› \nis rotated at any angle and then sent in ğœƒ\nğ‘’2\n, X\nğ‘›\nis directly sent in ğœƒ\nğ‘’1\n, forming input disturbance. Then the output of\nğœƒ\nğ‘’1 \nâˆ’ ğœƒ\nğ‘‘ \n, scribble labels S\nğ‘› \nand exponential geodesic distance map EGD\nğ‘›\nare rotated by the same angle, which is useful for the generation of\npseudo labels and the calculation of the loss function. According to the\nexperimental results in 4.1.2, the input rotation disturbance is indeed\nconducive to the segmentation of the uterus.\n3.3. Loss function\n3.3.1. Supervision loss\nAs most weakly supervised segmentation methods based on scribble\nlabeling [16,21], pixels without scribble labeling (pixels with pixel\nvalue of 2) are ignored, and partial cross entropy (PCE) is adopted to\ndefine scribble loss L\nğ‘ğ‘ğ‘’\n, defined as\nğ¿\nğ‘ğ‘ğ‘’\n(ğ‘†\nğ‘›\n, ğ‘¦\nğ‘›\n) = âˆ’ \n1\n|ğ›º\nğ‘ \n|\nğ¶\nâˆ‘\nğ‘=1\nâˆ‘\n(ğ‘–,ğ‘—)âˆˆğ›º\nğ‘ \nÌƒ\nğ‘†\nğ‘\nğ‘› \n(ğ‘–, ğ‘—) log ğ‘¦\nğ‘\nğ‘›\n(ğ‘–, ğ‘—) (5)\nwhere ğ›º\nğ‘  \nis the set of scribble pixels for the input image X\nğ‘›\n, and (i, j)\ndenotes the coordinates of pixels on image X\nğ‘›\n.\nÌƒ\n ğ‘†\nğ‘\nğ‘› \n(i, j) is the probability\nthat pixel (i, j) in S\nğ‘› \nbelongs to category c after one-hot coding, and y\nğ‘\nğ‘›\n(i,\nj) denotes the prediction probability that pixel (i, j) belongs to category\nc.\nThe network was made sensitive to uterine boundary information\nby adding EGD loss to the supervision loss. This loss is only calculated\nby using the probability map of channel 0 in the output of the dual\nbranch network and EGD\nğ‘›\n. The specific definitions are\nğ¿\nğ¸ğºğ·\n(ğ¸ğºğ·\nğ‘›\n, ğ‘¦\nğ‘›\n) = \n1\nğ» Ã— ğ‘Š\nğ»\nâˆ‘\nâ„=1\nğ‘Š\nâˆ‘\nğ‘¤=1\n[(ğ¸ğºğ·\nğ‘› \nâˆ’ ğ‘¦\nğ‘›\n(0))\n2\n]\n(â„,ğ‘¤) \n(6)\nwhere y\nğ‘›\n(0) is the probability map of channel 0 for y\nğ‘›\n, and H and W\nare the length and width of the image, respectively.\n3.3.2. Pseudo-label loss\nWhen using L\nğ‘ğ‘ğ‘’\n, the majority of the pixels that are conducive to net-\nwork training but not included in scribble labels are ignored, resulting\nin weak supervision signals. To address this problem, a common way\nis to combine pseudo-labels with label filtering technology [23,25] to\ngenerate reliable pseudo-label supervised network training. Inspired by\nthis method, the proposed method uses the pseudo label loss\nğ¿\nğ‘ƒ ğ¿\n(ğ‘ƒ ğ¿\nğ‘›\n, ğ‘¦\nğ‘›\n) = ğ¿\nğ‘‘ğ‘–ğ‘ğ‘’\n(ğ‘ƒ ğ¿\nğ‘›\n, ğ‘ğ‘Ÿğ‘”ğ‘šğ‘ğ‘¥(ğ‘¦\nğ‘›\n)) (7)\nwhere PL\nğ‘› \nis the pseudo label generated by dynamic mixing, and y\nğ‘›\ndenotes the prediction of the network. L\nğ‘‘ğ‘–ğ‘ğ‘’ \nis the dice loss widely used\nin the field of segmentation. This loss can also be replaced by other loss\nfunctions. Finally, the argmax function is used to reduce the number of\nchannels in y\nğ‘› \nand facilitate the calculation of loss.\n3.3.3. Total loss\nThe proposed weakly supervised segmentation network is trained by\nminimizing the weighted combination of L\nğ‘ğ‘ğ‘’\n, L\nğ¸ğºğ· \nand L\nğ‘ƒ ğ¿\n. Therefore,\nthe combined objective function can be written as\nğ¿\nğ‘¡ğ‘œğ‘¡ğ‘ğ‘™ \n= 0.5(ğ¿\nğ‘ğ‘ğ‘’ \n+ ğ¿\nğ¸ğ·ğº \n) + ğœ‡ğ¿\nğ‘ƒ ğ¿ \n(8)\nğ¿\nğ‘ğ‘ğ‘’ \n= 0.5(ğ¿\nğ‘ğ‘ğ‘’\n(\nÌƒ\nğ‘†\nğ‘›\n, ğ‘¦\nğ‘›_1\n) + ğ¿\nğ‘ğ‘ğ‘’\n(\nÌƒ\nğ‘†\nğ‘›\n, ğ‘¦\nğ‘›_2\n)) (9)\nğ¿\nğ¸ğºğ· \n= 0.5(ğ¿\nğ¸ğºğ·\n(\nÌƒ\n ğ¸ğºğ·\nğ‘›\n, ğ‘¦\nğ‘›_1\n(0)) + ğ¿\nğ¸ğºğ·\n(\nÌƒ\nğ¸ğºğ·\nğ‘›\n, ğ‘¦\nğ‘›_2\n(0))) (10)\nğ¿\nğ‘ƒ ğ¿ \n= 0.5(ğ¿\nğ‘ƒ ğ¿\n(ğ‘ƒ ğ¿\nğ‘›\n, ğ‘¦\nğ‘›_1\n) + ğ¿\nğ‘ƒ ğ¿\n(ğ‘ƒ ğ¿\nğ‘›\n, ğ‘¦\nğ‘›_2\n)) (11)\nIn Eq. (8), ğœ‡ is the weight of the balance between the supervision loss\nand the pseudo-label loss.\n4. Experiments and results\n4.1. Datasets\n4.1.1. MRI datasets of endometrial cancer patients\nOur institutional review board approved this retrospective study.\nThe requirement for written informed consent was waived by the\nreview board.\n\nComputers in Biology and Medicine 167 (2023) 107582\n7\nJ. Ying et al.\nThis dataset contains 135 patients with endometrial cancer who\nunderwent MRI examination in Shanghai First Maternal and Child\nHealth Hospital from February 2016 to January 2019. Their ages\nranged from 33 to 73 years old, with an average of 57 years old.\nExcluding slices without uterus, there are 473 two-dimensional slices in\nthis dataset. An experienced radiologist manually segmented the uterus\non the slice of the sagittal T2 weighted image (the image resolution\nwas 512 Ã— 512 pixels). This segmentation result is considered the gold\nstandard. Then, the green scribble is randomly drawn in the area where\nthe doctor manually segmented the uterus and is considered as the\nscribble annotation of the uterus, the red closed scribble containing the\nuterus was considered as the scribble annotation of the background in\nthe area outside the full annotation of the uterus and the areas without\nscribble are converted to black. Finally, the pixel in the black areas,\non the green scribble and red scribble are respectively converted into\npixels with grayscale values of 2, 1, and 0 to generate a grayscale image\nas the final scribble label.\n4.1.2. ACDC datasets\nThe method proposed by the automatic cardiac diagnosis challenge\n(ACDC) dataset test is employed in this paper for the multi-objective\nsegmentation task. This dataset contains 100 MRI examinations with\ndifferent TEM channel resolutions and magnetic field strengths (1.5T\nand 3T). Each examination consists of cardiac MRI at end diastolic (ED)\nand end systolic (ES). Fine pixel level annotation is provided for the\nright ventricle (RV), left ventricle (LV) and myocardium (Myo) on each\nslice. Valvano et al. [33] provided hand drawn scribble information\nbased on pixel level annotation of RV, Myo, LV and background.\n4.2. Experimental setup\nThe performance of the proposed method in the uterine segmen-\ntation of endometrial cancer MR images is compared to four com-\nmon weak supervision methods and a fully supervised segmentation\nmethod, namely scribble2label (S2L) [23], uncertainty perception self-\nintegration and transformation consistent model (USTM) [25], partial\nUNet (P-UNet) [34], dual branch network with one encoder and two\ndecoders (EDD) [24] and UNet [12]. The core idea of S2L is the com-\nbination of pseudo-labels with label filtering technology to generate\nreliable labels. USTM uses the uncertain perception teacher model and\ntransformation consistency strategy to filter unreliable labels. P-UNet\ncalculates the cross-entropy loss of pixels with scribble annotation, but\ndoes not introduce other losses. EDD dynamically combines the two\noutputs of the dual branch network of one encoder and two decoders\nto generate the pseudo-labels. The double branch network used in\nthis study, the number of disturbances and the loss function are quite\ndifferent from EDD. In particular, EDD only adds disturbances to the\nnetwork, but this study also adds disturbances to the network input\nand the exponential geodesic distance loss is added to the loss function.\nFor fair comparison, various methods are not only tested on the same\ndatasets, but also have the same backbone network and parameter\nsettings (batchsize, learning rate, optimization function, max iteration\ntimes, etc.) in training. In addition, UNet is trained using fully labeled\nimages.\nThe program is implemented with pytorch [35] and run on NVIDIA\nGeforce 1080ti GPU. In the pre-processing stage, the gray intensity of\neach input image is normalized to 0â€“1, and the data is augmented by\nrandom noise, random rotation and random flip operations. Finally, the\nnew data is input into the network for training. The SGD optimizer is\nused to minimize the total loss function L\nğ‘¡ğ‘œğ‘¡ğ‘ğ‘™ \n, and its parameters mo-\nmentum and weight_decay are set to 0.9 and 0.001, respectively. The\nlearning rate online is adjusted by multiple learning rate strategy (the\ninitial learning rate was 0.01) [36]. Batchsize, maximum number of\niterations, and ğœ‡ are set to 4, 60,000, and 0.5, respectively. During the\ntest and validation, only the single branch network ğœƒ\nğ‘’2 \nâˆ’ ğœƒ\nğ‘‘ \nprediction\nis used, and the input image does not undergo any image enhancement\noperation. The uterus has complex anatomy, for example, some of\nthe uterus bends forward, while others bend backward resulting in\na marked change in the outline of the uterus. Moreover, the depth\nof invasion of the uterus vary for each patient, leading to significant\ndifferences in the morphology of the endometrium. In addition, there\nare many other organs around the uterus (such as the small intestine,\nbladder, and ovaries). These characteristics of the uterus can make\nit difficult for the network to accurately segment the uterus, so the\nnumber of iteration is high. The training time was around 8 h on the\nMRI datasets of endometrial cancer patients and the testing time was\naround 20 ms per slice.\nFive measurement indicators are used as performance measures,\nnamely the 95th percentile Hausdorff distance (HD\n95\n), dice coefficient\n(DI), recall (RE), precision (PRE) and area difference percentage (ADP).\nBoth HD\n95 \nand DI are set similarity measurement functions, which are\nusually used to calculate the similarity of two samples, but DI is more\nsensitive to the internal segmentation, while HD\n95 \nis more sensitive to\nthe boundary of the segmentation. Their definitions are\nğ»ğ·\n95\n(ğ‘ƒ , ğ‘„) = ğ‘šğ‘ğ‘¥{max\nğ‘âˆˆğ‘ƒ \nmin\nğ‘âˆˆğ‘„ \nğ‘‘(ğ‘, ğ‘), max\nğ‘âˆˆğ‘„ \nmin\nğ‘âˆˆğ‘ƒ \nğ‘‘(ğ‘, ğ‘)} \n(12)\nğ·ğ¼ = \n2ğ‘‡ ğ‘ƒ\nğ¹ ğ‘ƒ + 2ğ‘‡ ğ‘ƒ + ğ¹ ğ‘ \n(13)\nwhere P and Q represent the surface points of the output and gold\nstandard map, respectively. The d(â‹…, â‹…) denotes the Euclidean distance\nbetween two points. TP, TN, FP and FN respectively represent the\nnumber of true positive, true negative, false positive and false negative\npixels in the prediction. The other three metrics are\nğ‘…ğ¸ = \nğ‘‡ ğ‘ƒ\nğ‘‡ ğ‘ƒ + ğ¹ ğ‘ \n, ğ‘ƒ ğ‘…ğ¸ = \nğ‘‡ ğ‘ƒ\nğ‘‡ ğ‘ƒ + ğ¹ ğ‘ƒ \n, ğ´ğ·ğ‘ƒ = \n| ğ‘†\n1 \nâˆ’ ğ‘†\n2\n|\nğ‘†\n1\n(14)\nwhere S\n1 \nand S\n2 \nrepresent the areas of the uterine region in the true\nlabel and predicted map, respectively. It can be seen from the equation\nthat ADP smaller means better performance.\nFor more stable and persuasive segmentation results, we conduct a\n5-fold cross-validation to train and test our proposed method. At each\ntime 1 fold (27 patients) is used for testing while the other 4 folds (total\n108 patients) are divided into a training set (98 patients) for training\nand a validation set (10 patients) for validation. In the end, we calculate\nits mean result according to the 5-fold testing respectively. The fol-\nlowing results are all calculated mean of the 5-fold cross-validation. In\norder to select the optimal parameters to test, we validate the network\nby feeding the validation set every 200 iterations. The validation set\naverage DI and the networkâ€™s weights have been saved. In the testing\nphase, the weights with the highest average DI on the validation set are\nused for testing. Besides, the hyper-parameters are artificially adjusted\nbased on the validation results.\n4.3. Experimental results\nThis part presents the qualitative and quantitative comparison re-\nsults of the proposed method and other methods on the endometrial\ncancer MRI dataset.\n4.3.1. Qualitative results\nFig. 6 shows the segmentation results of the proposed method and\nthe four benchmark weak supervision methods (i.e., EDD, P-UNet,\nS2L, USTM) on the same datasets. The results show that P-UNet (in\nthe second column), which only uses scribble annotation and partial\ncross entropy loss, has a large area of false positive pixels, marking\nother organs and tissues as uterine regional tissues, and failing to\nproduce accurate segmentation results. Although there is no isolated\nfalse positive in the segmentation results of EDD (in the first column),\nthere are cases of multiple segmentation or missing segmentation when\nat the edge of the uterus. S2L (in the third column) and USTM (in\nthe fourth column) using label filtering technology did not have a\nlarge area of false positive like P-UNet, but both had a small area\n\nComputers in Biology and Medicine 167 (2023) 107582\n8\nJ. Ying et al.\nFig. 6. Results of segmentation of uterine region on the test set by five weak supervision methods (four benchmarks and the proposed) and the gold standard, where red area\nrepresents the segmentation results, the blue arrow indicates the area where the segmentation is incorrect.\nof false positives, and could not accurately capture the edge of the\nuterus. Comparison to the benchmark weakly supervised segmentation\nmethods, the segmentation results of the our method (in the fifth\ncolumn) and the gold standard (in the sixth column) have a higher\noverlap rate. Specifically, this paper uses the ğœƒ\nğ‘’1 \nâˆ’ ğœƒ\nğ‘‘ \nâˆ’ ğœƒ\nğ‘’2 \ndual branch\nnetwork structure and dynamic hybrid strategy to supervise and train\neach other, so that no false positives appear in the segmentation results.\nFurthermore, the exponential geodesic distance loss is included in the\ntotal loss, which improves the ability of the network to capture the\nuterine edges. In addition, the effectiveness of this method is due to the\ninput rotation disturbance, which can help the network better adapt to\nthe changing characteristics of the uterus. In general, the results shown\nin Fig. 6 can reflect the effectiveness of the proposed method.\n4.3.2. Quantitative results\nTable 1 lists the quantitative results of uterine segmentation on test\nset, and Fig. 7 shows the change of DI and HD\n95 \nwith the number of\niterations in validation set.\nAs shown in Table 1, the proposed method is superior to other\nmethods in all indicators, and P-UNet had the worst performance in all\ninspected indicators. In terms of DI and HD\n95\n, P-UNet is at least 9.8%\nand 129.834 lower from other methods. This is mainly because P-UNet\nonly uses scribble supervision with weak supervision ability. Although\nS2L is slightly inferior to USTM and EDD in RE, the performance of S2L\non PRE is 2.4% and 1.8% higher than USTM and EDD respectively,\nand the performance of S2L on the other three indicators is between\nUSTM and EDD. In particular, the proposed method achieved the best\nperformance in all indicators, and its index values are DI = 92.8%, HD\n95\n= 11.632, RE = 92.7%, PRE = 93.6% and ADP = 6.5%, which are 2.1%,\n9.144, 0.6%, 2.4% and 2.9% better than the optimal indexes in other\nmethods. The indexes DI and HD\n95 \nfully show that the proposed method\nis closer to the gold standard both in the interior and at the edge of the\nsegmented uterus. In addition, the performance of the proposed method\nis equivalent to that of the fully supervised U-Net model.\nThe curve of the average DI and HD\n95 \nof the validation set during\nthe training process is shown in Fig. 7. Fig. 7(a) shows that the pro-\nposed method is superior to the other four weak supervision methods\nin terms of both the speed of DI convergence and the final convergence\nresult. Specifically, the maximum number of iterations is required for\nP-UNet to reach DI convergence, and the final convergence value is\nalso the lowest. Because both S2L and USTM adopt the idea of filter\nlabel, the final DI convergence results are similar, but USTM needed a\nmuch higher number of iterations than S2L to reach convergence. The\ndivergence degree and convergence results of DI in the training process\nof EDD, which also adopts the pseudo-label idea, are slightly inferior\nto the proposed method. This is possibly because the dual branch\nnetwork of two encoders and one decoder extracts more image features\nconducive to segmenting the uterine, and the exponential geodesic\ndistance map also provides more edge information. Fig. 7(b) also shows\nthat the performance of the proposed method is also the best on HD\n95\n.\n4.4. Ablation experiment\n4.4.1. Effect of weight value ğœ‡\nThe ğœ‡ in the loss function has an important impact on the prediction\nresults, so ablation experiments with different ğœ‡ values (ğœ‡ set as 0.01,\n0.1, 0.2, 0.3, 0.5, 1.0) are conducted. The experimental results on test\nset are shown in Fig. 8. The results show that when ğœ‡ is not greater\nthan 0.5, the larger the ğœ‡, the better the performance of DI, HD\n95 \nand\nPRE, until the performance is the best when ğœ‡ = 0.5. However, when ğœ‡\nis 1.0, the performance of these four indicators will decrease compared\nto when ğœ‡ = 0.5, so the value of ğœ‡ in this paper is chosen to be 0.5. The\nhigher the proportion of pseudo-label loss in the total loss, the better\nthe segmentation effect, but when the proportion surpasses 50%, the\nsegmentation performance begins to decline. The specific quantitative\nresults are shown in Table 2.\n\nComputers in Biology and Medicine 167 (2023) 107582\n9\nJ. Ying et al.\nTable 1\nQuantitative results of six methods for uterine segmentation on the EC dataset. This table lists the average value Â± standard\ndeviation of the five evaluation indexes in the test set. The optimal results are highlighted in bold.\nLabel Methods DI HD\n95 \nRE PRE ADP\nScribble P-UNet 0.792 (Â±0.116) 170.632 (Â±68.643) 0.872 (Â±0.097) 0.756 (Â±0.174) 0.220 (Â±0.231)\nUSTM 0.890 (Â±0.077) 40.798 (Â±55.481) 0.906 (Â±0.068) 0.888 (Â±0.106) 0.099 (Â±0.150)\nEDD 0.907 (Â±0.052) 20.776 (Â±36.496) 0.921 (Â±0.051) 0.894 (Â±0.059) 0.094 (Â±0.081)\nS2L 0.893 (Â±0.074) 20.874 (Â±33.626) 0.885 (Â±0.062) 0.912 (Â±0.101) 0.095 (Â±0.060)\nProposed 0.928 (Â±0.039)\n0.928 (Â±0.039)\n0.928 (Â±0.039) 11.632 (Â±12.218)\n11.632 (Â±12.218)\n11.632 (Â±12.218) 0.927 (Â±0.048)\n0.927 (Â±0.048)\n0.927 (Â±0.048) 0.936 (Â±0.045)\n0.936 (Â±0.045)\n0.936 (Â±0.045) 0.065 (Â±0.050)\n0.065 (Â±0.050)\n0.065 (Â±0.050)\nFull UNet 0.959 (Â±0.033) 9.211 (Â±6.824) 0.958 (Â±0.057) 0.961 (Â±0.039) 0.044 (Â±0.057)\nFig. 7. Curve of average DI and HD\n95 \nin the validation set during training, (a) represents DI change curve, (b) represents HD\n95 \nchange curve.\nTable 2\nQuantitative results of influence of different weights ğœ‡ on the output. This table lists the average value Â± standard deviation\nof the five evaluation indexes in the test set. The optimal results are highlighted in bold.\nğœ‡ DI HD\n95 \nRE PRE ADP\n0.001 0.788 (Â±0.151) 134.192 (Â±76.186) 0.941 (Â±0.062) 0.708 (Â±0.187) 0.442 (Â±0.494)\n0.1 0.906 (Â±0.053) 33.825 (Â±47.126) 0.949 (Â±0.044)\n0.949 (Â±0.044)\n0.949 (Â±0.044) 0.880 (Â±0.083) 0.114 (Â±0.110)\n0.2 0.920 (Â±0.043) 16.873 (Â±17.022) 0.933 (Â±0.039) 0.915 (Â±0.068) 0.070 (Â±0.054)\n0.3 0.923 (Â±0.030) 14.110 (Â±14.913) 0.935 (Â±0.034) 0.918 (Â±0.062) 0.069 (Â±0.058)\n0.5 (Proposed) 0.928 (Â±0.039)\n0.928 (Â±0.039)\n0.928 (Â±0.039) 11.632 (Â±12.218)\n11.632 (Â±12.218)\n11.632 (Â±12.218) 0.927 (Â±0.048) 0.936 (Â±0.045)\n0.936 (Â±0.045)\n0.936 (Â±0.045) 0.065 (Â±0.050)\n0.065 (Â±0.050)\n0.065 (Â±0.050)\n1.0 0.922 (Â±0.034) 12.478 (Â±9.825) 0.926 (Â±0.052) 0.929 (Â±0.058) 0.068 (Â±0.055)\nFig. 8. Indicators with parameters ğœ‡ ablation curve in the test.\n4.4.2. Effect of different input disturbances on segmentation performance\nThis paper studies the influence of different input disturbances. The\nexperimental results on test set are listed in Table 3. It shows that\ncompared with no disturbance, DI and HD\n95 \nwith random rotation\nincrease by 1% and 1.686, respectively, and the performance of random\nflip is also slightly improved (DI increases from 91.8% to 92.3%).\nFig. 9 displays the confusion matrix of segmentation results on test\nset under three kinds of disturbances. â€˜â€˜Rotateâ€™â€™ has a higher accuracy\nin segmenting the background and uterine region than â€˜â€˜Withoutâ€™â€™ and\nâ€˜â€˜Flipâ€™â€™, and has lower false segmentation and missing segmentation\nrates.\nFig. 10 shows the prediction results of uterine segmentation from\ndifferent angles under different input disturbances on test set. When the\nmodel does not add any disturbance, the uterine segmentation results\nfrom some angles are better (such as the first and second columns),\nbut the segmentation results from other angles are worse (such as the\nthird and fourth cases). After adding flip and rotate disturbances, the\nsegmentation performance at all angles is improved. It is worth noting\nthat the rotate disturbance can segment the uterus at any angle very\nwell.\n4.4.3. Necessity of exponential geodesic distance loss and pseudo-label loss\nThe necessity of exponential geodesic distance loss and pseudo\nlabel-loss is also studied in this paper. The results on test set are shown\nin Table 4. Compared with â€˜â€˜PCEâ€™â€™, the DI of â€˜â€˜PCE+EGDâ€™â€™ increased from\n75.4% to 83.1%, and HD\n95 \ndecreased from 126.346 to 54.114. The DI\nand HD\n95 \nof â€˜â€˜PCE + PLâ€™â€™ are 91.4% and 18.174 respectively, which are\n16% and 108.172 higher than â€˜â€˜PCEâ€™â€™, respectively. The results show\nthat both the exponential geodesic distance loss and the pseudo-label\nloss are conducive to the prediction of the network. Their combination\nyields even higher performance, reaching 92.8% and 11.632 on DI and\nHD\n95 \nrespectively, which is 17.4% and 11.714 higher than using â€˜â€˜PCEâ€™â€™\nonly.\nThe curve of the average DI and HD\n95 \nin the validation set with the\nnumber of iterations under the supervision of different loss functions\nis shown in Fig. 11. The results show that since â€˜â€˜PCE+PLâ€™â€™ introduces\npseudo-label loss and expands the supervision information, its average\nDI convergence speed and convergence result are higher than â€˜â€˜PCEâ€™â€™.\n\nComputers in Biology and Medicine 167 (2023) 107582\n10\nJ. Ying et al.\nTable 3\nQuantitative results of influence of different disturbances on output. This table lists the average value Â± standard deviation\nof the five evaluation indexes in the test set. The optimal results are highlighted in bold. â€˜â€˜Withoutâ€™â€™ means no disturbance,\nâ€˜â€˜Flipâ€™â€™ means random flip disturbance, â€˜â€˜Rotateâ€™â€™ means random rotation disturbance.\nDisturbance DI HD\n95 \nRE PRE ADP\nWithout 0.918 (Â±0.043) 13.318 (Â±12.432) 0.919 (Â±0.045) 0.929 (Â±0.043) 0.078 (Â±0.064)\nFlip 0.923 (Â±0.041) 12.096 (Â±11.012) 0.923 (Â±0.051) 0.932 (Â±0.047) 0.068 (Â±0.054)\nRotate (Proposed) 0.928 (Â±0.039)\n0.928 (Â±0.039)\n0.928 (Â±0.039) 11.632 (Â±12.218)\n11.632 (Â±12.218)\n11.632 (Â±12.218) 0.927 (Â±0.048)\n0.927 (Â±0.048)\n0.927 (Â±0.048) 0.936 (Â±0.045)\n0.936 (Â±0.045)\n0.936 (Â±0.045) 0.065 (Â±0.050)\n0.065 (Â±0.050)\n0.065 (Â±0.050)\nFig. 9. Confusion matrix of segmentation results under three input disturbances. â€˜â€˜Withoutâ€™â€™ means no disturbance, â€˜â€˜Flipâ€™â€™ means random flip disturbance, â€˜â€˜Rotateâ€™â€™ means random\nrotation disturbance.\nFig. 10. Segmentation results of different input disturbances on the test set. The red curve represents the gold standard of uterine contour, and the green curve represents the\npredicted uterine contour.\nSimilarly, â€˜â€˜PCE+EGDâ€™â€™ introduces the exponential geodesic distance\nloss, which makes the model more sensitive to the edge of the uterus,\nresulting in the convergence result of â€˜â€˜PCE+EGDâ€™â€™ on HD\n95 \nbeing much\nlower than that of â€˜â€˜PCEâ€™â€™. When three loss functions are used simulta-\nneously, the convergence speed and convergence results of average DI\nand HD\n95 \nare further improved.\nThe prediction results on test set under the supervision of different\nloss functions are shown in Fig. 12. Many false positive areas appear\nwhen only scribble is used to monitor the network, but the addition\nof EGD loss minimized them. Although â€˜â€˜PCE+PLâ€™â€™ can remove isolated\nfalse positives, there will be mis-segmentation at the edge of the uterus.\nThe use of the proposed loss function can not only eliminate the false\npositives, but also capture the edge information of the uterus, yielding a\nsegmentation result closer to the gold standard. Therefore, the addition\nof pseudo-label loss and exponential geodesic distance loss to the total\nloss function is necessary.\n4.5. Comparison to other studies on uterine segmentation\nThe experimental results of this study are compared to other stud-\nies on uterine segmentation, and the specific quantitative results are\nshown in Table 5. Since this study focuses on uterine segmentation\n\nComputers in Biology and Medicine 167 (2023) 107582\n11\nJ. Ying et al.\nTable 4\nThe results of the necessary ablation experiment for geodesic distance loss and pseudo label loss. This table lists the average\nvalue Â± standard deviation of the five evaluation indexes in the test set. The optimal results are highlighted in bold. â€˜â€˜PCEâ€™â€™\nmeans the loss of scribble supervision only,â€˜â€˜PCE+PLâ€™â€™ means the total loss is composed of scribble supervision loss and\npseudo-label loss, â€˜â€˜PCE+EGDâ€™â€™ means the total loss is composed of scribble supervision loss and exponential geodesic distance\nloss, â€˜â€˜PCE+PL+EGD (Proposed)â€™â€™ means the proposed loss, which is composed of scribble supervision loss, pseudo label loss,\nand exponential geodesic distance loss.\nSetting DI HD\n95 \nRE PRE ADP\nPCE 0.754 (Â±0.128) 126.346 (Â±64.817) 0.880 (Â±0.139) 0.812 (Â±0.142) 0.206 (Â±0.212)\nPCE+PL 0.914 (Â±0.081) 18.174 (Â±25.125) 0.904 (Â±0.108) 0.917 (Â±0.073) 0.070 (Â±0.053)\nPCE+EGD 0.831 (Â±0.094) 54.114 (Â±66.482) 0.899 (Â±0.097) 0.894 (Â±0.112) 0.154 (Â±0.142)\nPCE+PL+EGD (Proposed) 0.928 (Â±0.039)\n0.928 (Â±0.039)\n0.928 (Â±0.039) 11.632 (Â±12.218)\n11.632 (Â±12.218)\n11.632 (Â±12.218) 0.927 (Â±0.048)\n0.927 (Â±0.048)\n0.927 (Â±0.048) 0.936 (Â±0.045)\n0.936 (Â±0.045)\n0.936 (Â±0.045) 0.065 (Â±0.050)\n0.065 (Â±0.050)\n0.065 (Â±0.050)\nFig. 11. Curve of average DI and HD\n95 \nin the validation set under different total loss functions, (a) shows the DI change curve, (b) shows the HD\n95 \nchange curve.\nFig. 12. Visualization of segmentation results of different loss functions on the test set, where red represents the uterine segmented region.\nTable 5\nQuantitative results of uterine segmentation in various studies. The average values of\nfive evaluation indicators in the test set are listed, with â€™-â€™ indicating that this indicator\nwas not used in the study. The optimal results are highlighted in bold.\nMethod DI HD\n95 \nRE PRE\nDenseUNet [16] 0.876 â€“ 0.881 0.866\nImprovedUNet [7] 0.860 â€“ â€“ â€“\nUNet-Resnet34 [37] 0.954\n0.954\n0.954 â€“ 0.969\n0.969\n0.969 0.878\nProposed 0.928 11.632 0.927 0.936\n0.936\n0.936\nfor endometrial cancer T2 weighted images, the table only lists the\nindicators used in various studies to segment the uterus of endometrial\ncancer T2 weighted images. Our method is superior to the other three\nfully supervised methods in terms of PRE, but DI and RE are lower\nthan UNet-Resnet34. This difference may be due to the fully supervised\ntraining of UNet-Resnet34 and the large number of training samples\n(1440 T2WI slices). It is worth pointing out that our method only\nneeds scribble labeling, which simplifies the annotation process, saves\nlabeling cost, and requires fewer training samples, which is suitable for\nsmall sample learning.\n4.6. Experimental results on ACDC dataset\nUterine segmentation is a single object segmentation task. In order\nto prove that the proposed method is also suitable for multi-object\nsegmentation, experiments are carried out on the ACDC public dataset,\nand compared with other weakly supervised segmentation methods.\nThe specific quantitative and qualitative experimental results on test\nset are shown in Table 6 and Fig. 13, respectively.\nTable 6 shows that the proposed method achieves the best perfor-\nmance for both mean DI and HD\n95 \nin segmenting RV, Myo and LV\norgans on the ACDC dataset. In terms of DI, it has the best performance\nwhen segmenting LV, reaching 93.9%, which is 2.6% higher than the\nsecond place.\nAccording to the visual results displayed in Fig. 13, the other\nfour weak supervision methods are more accurate in segmenting large\ntargets (the first and second rows) than in segmenting small targets (the\nthird and fourth rows), but the proposed method (the fifth column)\nperforms well in the segmentation of both large and small targets.\nThis achievement is entirely due to the fact that the ğœƒ\nğ‘’1 \nâˆ’ ğœƒ\nğ‘‘ \nâˆ’ ğœƒ\nğ‘’2\ndual branch network can extract more features of ROI regions, and the\nloss of exponential geodesic distance makes the network more sensitive\n\nComputers in Biology and Medicine 167 (2023) 107582\n12\nJ. Ying et al.\nFig. 13. Results of LV, RV and Myo segmentation in the ACDC dataset by five weak supervision methods on the test set. The LV region is represented in red, Myo region in\ngreen, and the RV region in blue.\nTable 6\nQuantitative results of five weakly supervised segmentation methods on the ACDC dataset. This table lists the average value\nÂ± standard deviation of the two evaluation indexes in the test set. The optimal results are highlighted in bold.\nMethod RV Myo LV\nDI HD\n95 \nDI HD\n95 \nDI HD\n95\nP-UNet [34] 0.625 (Â±0.160) 187.200 (Â±35.200) 0.668 (Â±0.095) 165.100 (Â±34.400) 0.776 (Â±0.156) 167.700 (Â±55.000)\nUSTM [25] 0.815 (Â±0.115) 54.700 (Â±65.700) 0.756 (Â±0.081) 112.200 (Â±54.100) 0.785 (Â±0.162) 139.600 (Â±57.700)\nS2L [23] 0.833 (Â±0.103) 14.600 (Â±30.900) 0.806 (Â±0.069) 37.100 (Â±49.400) 0.856 (Â±0.121) 65.200 (Â±65.100)\nEDD [24] 0.861 (Â±0.096) 7.900 (Â±12.500) 0.842 (Â±0.054) 9.700 (Â±23.200) 0.913 (Â±0.082) 12.100 (Â±27.200)\nProposed 0.878 (Â±0.076)\n0.878 (Â±0.076)\n0.878 (Â±0.076) 7.331 (Â±10.582)\n7.331 (Â±10.582)\n7.331 (Â±10.582) 0.861 (Â±0.056)\n0.861 (Â±0.056)\n0.861 (Â±0.056) 7.653 (Â±16.546)\n7.653 (Â±16.546)\n7.653 (Â±16.546) 0.939 (Â±0.067)\n0.939 (Â±0.067)\n0.939 (Â±0.067) 9.475 (Â±11.327)\n9.475 (Â±11.327)\n9.475 (Â±11.327)\nto edge information. In conclusion, the proposed weakly supervised\nsegmentation method not only presents excellent performance in single\ntarget segmentation task, but is also fully applicable to multi-target\nsegmentation.\n5. Conclusion\nThis paper presents a method for uterine segmentation of endome-\ntrial cancer MR images with weakly supervised segmentation. The\nentire framework is constructed by a double branch network, and\noptimized by the weighted combination of supervision loss and scribble\nloss. Specifically, a random rotation input disturbance is introduced to\nthe double branch network of two encoders and one decoder, the out-\nputs of the double branch network are dynamically mixed to generate\npseudo-labels, and the exponential geodesic distance loss is introduced\ninto the loss function. The results show that uterine segmentation based\non scribble supervision can be more accurate by adopting the measures\ndescribed above. Compared with other optimal scribble supervision\nmethod, the proposed method improves the DI by 2.1%. This sig-\nnificantly impacts the diagnosis of the depth of myometrial invasion\nin endometrial cancer. Through rough experimental calculation, we\ndetermined that the number of pixels in the uterine region of the\nground truth and the predicted map in one EC slice is approximately\n9700 and 9800, respectively. If the DI drops by 2.1%, the number of\npixels that overlap between the uterine region of the ground truth and\nthe predicted map is reduced by about 204. Furthermore, if these 204\npixels are clustered together and appear at the edge of the uterus, it will\nseriously affect the calculation of the LS [6]. This can lead to errors in\ndiagnosing the depth of myometrial invasion and subsequently impact\nthe patientâ€™s treatment.\nAlthough the our method outperforms other weakly supervised\nmethods, due to the complex anatomical structure and flexible mor-\nphology of the uterus, the accuracy of uterine segmentation is not\nhigh. Therefore, it cannot be applied in clinical practice. In order to\nimprove the segmentation performance, future studies will use MRI\nwith different sequences, such as T1WI and DWI. Compared with single-\nsequence images, multi-sequence images can not only help to extract\ncomplementary features from different views, but also improve the\ndiscrimination ability of the network. On the other hand, more useful\nmonitoring information and more disturbance strategies in the network\nwill be explored, and the segmentation of uterus based on scribble\nannotation will be studied directly on the three-dimensional image,\nrather than on the two-dimensional slice. In addition, this method will\nbe applied to other challenging medical image segmentation tasks.\n\nComputers in Biology and Medicine 167 (2023) 107582\n13\nJ. Ying et al.\nDeclaration of competing interest\nThere is no conflict of interests in this work.\nAcknowledgments\nThis work was supported by the grants from the Shanghai Science\nand Technology Innovation Action Plan, China (No. 22S31903700)\nand grants from Shanghai Hospital Development Center-United Imag-\ning Joint Research & Development Plan, China (No. 2022SKLY-12).\nWe would like to thank MogoEdit (www.mogoedit.com) for English\nlanguage editing.\nReferences\n[1] J. Sun, Z.H. Wang, T.T. Zhao, S.N. Tao, G.P. Ye, W.P. Hu, Clinical significance\nof endometrial cytology in patients with endometrial disease, J. Qiqihar Med.\nColl. 40 (2019) 696â€“698.\n[2] J. Lortet-Tieulent, J. Ferlay, F. Bray, A. Jemal, International patterns and trends\nin endometrial cancer incidence, 1978â€“2013, J. Natl. Cancer Inst. 110 (4) (2018)\n354â€“361.\n[3] J.A. Frost, K.E. Webster, A. Bryant, J. Morrison, Lymphadenectomy for the\nmanagement of endometrial cancer, Cochrane Database Syst. Rev. (10) (2017).\n[4] G. Bogani, S.C. Dowdy, W.A. Cliby, F. Ghezzi, D. Rossetti, A. Mariani, Role of\npelvic and para-aortic lymphadenectomy in endometrial cancer: current evidence,\nJ. Obstet. Gynaecol. Res. 40 (2) (2014) 301â€“311.\n[5] P. Morice, A. Leary, C. Creutzberg, N. Abu-Rustum, E. Darai, Endometrial cancer,\nLancet 387 (10023) (2016) 1094â€“1108.\n[6] X.L. Zhu, J. Ying, H.M. Yang, L. Fu, B.Y. Li, B. Jiang, Detection of deep\nmyometrial invasion in endometrial cancer MR imaging based on multi-feature\nfusion and probabilistic support vector machine ensemble, Comput. Biol. Med.\n134 (2021) 104487.\n[7] Y. Kurata, M. Nishio, A. Kido, K. Fujimoto, M. Yakami, H. Isoda, K. Togashi,\nAutomatic segmentation of the uterus on MRI using a convolutional neural\nnetwork, Comput. Biol. Med. 114 (2019) 103438.\n[8] Y. Kurata, M. Nishio, Y. Moribata, A. Kido, Y. Himoto, S. Otani, K. Fujimoto, M.\nYakami, S. Minamiguchi, M. Mandai, et al., Automatic segmentation of uterine\nendometrial cancer on multi-sequence MRI using a convolutional neural network,\nSci. Rep. 11 (1) (2021) 14440.\n[9] H.-C. Dong, H.-K. Dong, M.-H. Yu, Y.-H. Lin, C.-C. Chang, Using deep learning\nwith convolutional neural network approach to identify the invasion depth of\nendometrial cancer in myometrium using MR images: a pilot study, Int. J.\nEnviron. Res. Public Health 17 (16) (2020) 5993.\n[10] K. Simonyan, A. Zisserman, Very deep convolutional networks for large-scale\nimage recognition, 2014, arXiv preprint arXiv:1409.1556.\n[11] K. He, X. Zhang, S. Ren, J. Sun, Deep residual learning for image recognition, in:\nProceedings of the IEEE Conference on Computer Vision and Pattern Recognition,\n2016, pp. 770â€“778.\n[12] O. Ronneberger, P. Fischer, T. Brox, U-net: Convolutional networks for biomed-\nical image segmentation, in: Int. Conf. Med. Image Comput. Comput.-Assist.\nInterv, Springer, 2015, pp. 234â€“241.\n[13] M. Shahedi, J.D. Dormer, A.D. TT, Q.N. Do, Y. Xi, M.A. Lewis, A.J. Madhu-\nranthakam, D.M. Twickler, B. Fei, Segmentation of uterus and placenta in MR\nimages using a fully convolutional neural network, in: Int. Soc. Opt. Eng., Vol.\n11314, SPIE, 2020, pp. 411â€“418.\n[14] C. Zhang, H. Shu, G. Yang, F. Li, Y. Wen, Q. Zhang, J.-L. Dillenseger, J.-L.\nCoatrieux, HIFUNet: multi-class segmentation of uterine regions from MR images\nusing global convolutional networks for HIFU surgery planning, IEEE Trans. Med.\nImaging 39 (11) (2020) 3309â€“3320.\n[15] C. Zhang, G. Yang, F. Li, Y. Wen, Y. Yao, H. Shu, A. Simon, J.-L. Dillenseger,\nJ.-L. Coatrieux, CTANet: Confidence-based threshold adaption network for semi-\nsupervised segmentation of uterine regions from MR images for HIFU treatment,\nIRBM 44 (3) (2023) 100747.\n[16] Y.Y. Zhang, L.P. Ying, H. Li, W.B. Chen, H.C. Miao, N. Bao, et al., Uterine\nmagnetic resonance image segmentation based on deep learning, J. Phys.: Conf.\nSer. 1861 (1) (2021) 012067.\n[17] K. Wu, B.W. Du, M. Luo, H.K. Wen, Y.R. Shen, J.F. Feng, Weakly supervised\nbrain lesion segmentation via attentional representation learning, in: Int. Conf.\nMed. Image Comput. Comput.-Assist. Interv, Springer, 2019, pp. 211â€“219.\n[18] Y. Zhang, Y.F. Li, Y.Y. Kong, J.S. Wu, J. Yang, H.Z. Shu, G. Coatrieux, GSCFN:\nA graph self-construction and fusion network for semi-supervised brain tissue\nsegmentation in MRI, Neurocomputing 455 (2021) 23â€“37.\n[19] Z. Zhao, L. Yang, H. Zheng, I.H. Guldner, S.Y. Zhang, D.Z. Chen, Deep learning\nbased instance segmentation in 3D biomedical images using weak annotation,\nin: Int. Conf. Med. Image Comput. Comput.-Assist. Interv, Springer, 2018, pp.\n352â€“360.\n[20] M. Rajchl, M.C. Lee, O. Oktay, K. Kamnitsas, J. Passerat-Palmbach, W. Bai, M.\nDamodaram, M.A. Rutherford, J.V. Hajnal, B. Kainz, et al., Deepcut: Object seg-\nmentation from bounding box annotations using convolutional neural networks,\nIEEE Trans. Med. Imaging 36 (2) (2016) 674â€“683.\n[21] I. Laradji, P. Rodriguez, O. Manas, K. Lensink, M. Law, L. Kurzman, W. Parker,\nD. Vazquez, D. Nowrouzezahrai, A weakly supervised consistency-based learning\nmethod for covid-19 segmentation in ct images, in: Proc. IEEE Winter Conf. Appl.\nComput. Vis, 2021, pp. 2453â€“2462.\n[22] D.J. Matuszewski, I.-M. Sintorn, Minimal annotation training for segmentation\nof microscopy images, in: Proc. IEEE 15th Int. Symp. Biomed. Imag, IEEE, 2018,\npp. 387â€“390.\n[23] H. Lee, W.-K. Jeong, Scribble2label: Scribble-supervised cell segmentation via\nself-generating pseudo-labels with consistency, in: Int. Conf. Med. Image Comput.\nComput.-Assist. Interv, Springer, 2020, pp. 14â€“23.\n[24] X.D. Luo, M.H. Hu, W.J. Liao, S.W. Zhai, T. Song, G.T. Wang, S.T. Zhang,\nScribble-supervised medical image segmentation via dual-branch network and\ndynamically mixed pseudo labels supervision, in: Int. Conf. Med. Image Comput.\nComput.-Assist. Interv, Springer, 2022, pp. 528â€“538.\n[25] X.M. Liu, Q. Yuan, Y.Z. Gao, K.L. He, S. Wang, X. Tang, J.S. Tang, D.G. Shen,\nWeakly supervised segmentation of COVID19 infection with scribble annotation\non CT images, Pattern Recognit. 122 (2022) 108341.\n[26] Y.B. Can, K. Chaitanya, B. Mustafa, L.M. Koch, E. Konukoglu, C.F. Baumgartner,\nLearning to segment medical images with scribble-supervision alone, in: 4th Int.\nWork. Deep Learn. Med. Image Anal. (DLMIA)/8th Int. Workshop on Mult. Learn.\nClin. Decis. Support, ML-CDS, Springer, 2018, pp. 236â€“244.\n[27] J.L. Cheng, S.W. Tian, L. Yu, S.J. Liu, C.Q. Wang, Y. Ren, H.C. Lu, M. Zhu,\nDDU-Net: A dual dense U-structure network for medical image segmentation,\nAppl. Soft. Comput. 126 (2022) 109297.\n[28] Z.H. Xu, S.J. Liu, D. Yuan, L. Wang, J.Y. Chen, T. Lukasiewicz, Z.G. Fu, R. Zhang,\nğœ”-net: Dual supervised medical image segmentation with multi-dimensional self-\nattention and diversely-connected multi-scale convolution, Neurocomputing 500\n(2022) 177â€“190.\n[29] J. Dolz, C. Desrosiers, I.B. Ayed, Teach me to segment with mixed supervision:\nConfident students become masters, in: Proc. Int. Conf. Inf. Process. Med. Imag,\nSpringer, 2021, pp. 517â€“529.\n[30] X.Y. Huo, L.X. Xie, J.Z. He, Z.J. Yang, W.G. Zhou, H.Q. Li, Q. Tian, Atso: Asyn-\nchronous teacher-student optimization for semi-supervised image segmentation,\nin: Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit, 2021, pp. 1235â€“1244.\n[31] X.D. Luo, G.T. Wang, T. Song, J.Y. Zhang, M. Aertsen, J. Deprest, S. Ourselin,\nT. Vercauteren, S.T. Zhang, MIDeepSeg: Minimally interactive segmentation of\nunseen objects from medical images using deep learning, Med. Image. Anal. 72\n(2021) 102102.\n[32] G.T. Wang, M.A. Zuluaga, W.Q. Li, R. Pratt, P.A. Patel, M. Aertsen, T. Doel, A.L.\nDavid, J. Deprest, S. Ourselin, et al., DeepIGeoS: a deep interactive geodesic\nframework for medical image segmentation, IEEE Trans. Pattern Anal. 41 (7)\n(2018) 1559â€“1572.\n[33] G. Valvano, A. Leo, S.A. Tsaftaris, Learning to segment from scribbles using\nmulti-scale adversarial attention gates, IEEE Trans. Med. Imaging 40 (8) (2021)\n1990â€“2001.\n[34] M. Tang, A. Djelouah, F. Perazzi, Y. Boykov, C. Schroers, Normalized cut loss\nfor weakly-supervised cnn segmentation, in: Proc. IEEE/CVF Conf. Comput. Vis.\nPattern Recognit, 2018, pp. 1818â€“1827.\n[35] A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan, T. Killeen,\nZ.M. Lin, N. Gimelshein, L. Antiga, et al., Pytorch: An imperative style,\nhigh-performance deep learning library, Adv. Neural. Inf. Process. Syst. 32\n(2019).\n[36] X.D. Luo, W.J. Liao, J.N. Chen, T. Song, Y.N. Chen, S.C. Zhang, N.Y. Chen,\nG.T. Wang, S.T. Zhang, Efficient semi-supervised gross target volume of nasopha-\nryngeal carcinoma segmentation via uncertainty rectified pyramid consistency,\nin: Int. Conf. Med. Image Comput. Comput.-Assist. Interv, Springer, 2021, pp.\n318â€“329.\n[37] H. Dong, H. Dong, M. Yu, Y.H. Lin, C. Chang, Using deep learning with convo-\nlutional neural network approach to identify the invasion depth of endometrial\ncancer in myometrium using MR images: a pilot study, Int. J. Environ. Res.\nPublic Health 17 (16) (2020) 5993.",
    "version": "5.3.31"
  },
  {
    "numpages": 10,
    "numrender": 10,
    "info": {
      "PDFFormatVersion": "1.7",
      "Language": "en-US",
      "EncryptFilterName": null,
      "IsLinearized": true,
      "IsAcroFormPresent": false,
      "IsXFAPresent": false,
      "IsCollectionPresent": false,
      "IsSignaturesPresent": false,
      "Creator": "Elsevier",
      "Custom": {
        "CrossMarkDomains[1]": "elsevier.com",
        "CrossmarkMajorVersionDate": "2010-04-23",
        "CreationDate--Text": "27th January 2024",
        "ElsevierWebPDFSpecifications": "7.0",
        "robots": "noindex",
        "doi": "10.1016/j.compbiomed.2023.107913",
        "CrossMarkDomains[2]": "sciencedirect.com",
        "CrossmarkDomainExclusive": "true"
      },
      "ModDate": "D:20240127151534Z",
      "Author": "Juan Wang",
      "Title": "Weakly supervised image segmentation beyond tight bounding box annotations",
      "Keywords": "Weakly supervised image segmentation,Bounding box,Multiple instance learning,Polar transformation,Deep neural networks",
      "CreationDate": "D:20240127133235Z",
      "Producer": "Acrobat Distiller 8.1.0 (Windows)",
      "Subject": "Computers in Biology and Medicine, 169 (2024) 107913. doi:10.1016/j.compbiomed.2023.107913"
    },
    "metadata": {
      "crossmark:crossmarkdomains": "elsevier.comsciencedirect.com",
      "crossmark:crossmarkdomainexclusive": "true",
      "crossmark:doi": "10.1016/j.compbiomed.2023.107913",
      "crossmark:majorversiondate": "2010-04-23",
      "dc:format": "application/pdf",
      "dc:identifier": "10.1016/j.compbiomed.2023.107913",
      "dc:publisher": "Elsevier Ltd",
      "dc:description": "Computers in Biology and Medicine, 169 (2024) 107913. doi:10.1016/j.compbiomed.2023.107913",
      "dc:subject": [
        "Weakly supervised image segmentation",
        "Bounding box",
        "Multiple instance learning",
        "Polar transformation",
        "Deep neural networks"
      ],
      "dc:title": "Weakly supervised image segmentation beyond tight bounding box annotations",
      "dc:creator": ["Juan Wang", "Bin Xia"],
      "jav:journal_article_version": "VoR",
      "pdf:creationdate--text": "27th January 2024",
      "pdf:producer": "Acrobat Distiller 8.1.0 (Windows)",
      "pdf:keywords": "Weakly supervised image segmentation,Bounding box,Multiple instance learning,Polar transformation,Deep neural networks",
      "pdfx:creationdate--text": "27th January 2024",
      "pdfx:crossmarkdomains": "sciencedirect.comelsevier.com",
      "pdfx:crossmarkdomainexclusive": "true",
      "pdfx:crossmarkmajorversiondate": "2010-04-23",
      "lmfnsmdunymb.zwugnwermcnnnwn-lwj-y9qgodr_ztf.mtipntyro9eqn9iknm-snm2tma": "",
      "pdfx:doi": "10.1016/j.compbiomed.2023.107913",
      "pdfx:robots": "noindex",
      "prism:aggregationtype": "journal",
      "prism:copyright": "Â© 2023 Elsevier Ltd. All rights reserved.",
      "prism:coverdate": "2024-02-01",
      "prism:coverdisplaydate": "1 February 2024",
      "prism:doi": "10.1016/j.compbiomed.2023.107913",
      "prism:issn": "0010-4825",
      "prism:pagerange": "107913",
      "prism:publicationname": "Computers in Biology and Medicine",
      "prism:startingpage": "107913",
      "prism:url": "https://doi.org/10.1016/j.compbiomed.2023.107913",
      "prism:volume": "169",
      "xmp:createdate": "2024-01-27T13:32:35",
      "xmp:creatortool": "Elsevier",
      "xmp:metadatadate": "2024-01-27T15:15:34",
      "xmp:modifydate": "2024-01-27T15:15:34",
      "xmpmm:documentid": "uuid:1b499bed-4ce8-4c0c-b682-0d58baae1cbe",
      "xmpmm:instanceid": "uuid:b6ed5266-03cb-4855-90c0-636283357f42",
      "xmprights:marked": "True"
    },
    "text": "Computers in Biology and Medicine 169 (2024) 107913\nAvailable online 29 December 2023\n0010-4825/Â© 2023 Elsevier Ltd. All rights reserved.\nContents lists available at ScienceDirect\nComputers in Biology and Medicine\njournal homepage: www.elsevier.com/locate/compbiomed\nWeakly supervised image segmentation beyond tight bounding box\nannotations\nJuan Wang \na,\nâˆ—\n, Bin Xia \nb\na \nHorizon Med Innovation Inc., 23421 South Pointe Dr., Laguna Hills, CA 92653, USA\nb \nShenzhen SiBright Co. Ltd., Tinwe Industrial Park, No. 6 Liufang Rd., Shenzhen, Guangdong 518052, China\nA R T I C L E I N F O\nKeywords:\nWeakly supervised image segmentation\nBounding box\nMultiple instance learning\nPolar transformation\nDeep neural networks\nA B S T R A C T\nWeakly supervised image segmentation approaches in the literature usually achieve high segmentation\nperformance using tight bounding box supervision and decrease the performance greatly when supervised\nby loose bounding boxes. However, compared with loose bounding box, it is much more difficult to acquire\ntight bounding box due to its strict requirements on the precise locations of the four sides of the box. To\nresolve this issue, this study investigates whether it is possible to maintain good segmentation performance\nwhen loose bounding boxes are used as supervision. For this purpose, this work extends our previous parallel\ntransformation based multiple instance learning (MIL) for tight bounding box supervision by integrating an MIL\nstrategy based on polar transformation to assist image segmentation. The proposed polar transformation based\nMIL formulation works for both tight and loose bounding boxes, in which a positive bag is defined as pixels in\na polar line of a bounding box with one endpoint located inside the object enclosed by the box and the other\nendpoint located at one of the four sides of the box. Moreover, a weighted smooth maximum approximation\nis introduced to incorporate the observation that pixels closer to the origin of the polar transformation are\nmore likely to belong to the object in the box. The proposed approach was evaluated on two public datasets\nusing dice coefficient when bounding boxes at different precision levels were considered in the experiments.\nThe results demonstrate that the proposed approach achieves state-of-the-art performance for bounding boxes\nat all precision levels and is robust to mild and moderate errors in the loose bounding box annotations. The\ncodes are available at https://github.com/wangjuan313/wsis-beyond-tightBB.\n1. Introduction\nImage segmentation is the process of partitioning a digital image\ninto multiple image segments such that pixels in an image segment\nshare certain characteristics and are assigned the same category label.\nIt has been widely studied in all kinds of applications [1] since 1965.\nIn recent years, with the development of the deep learning in medical\nimage analysis [2â€“5], deep neural networks (DNNs) have been used to\nsuccessfully tackle a variety of image segmentation problems in a fully-\nsupervised manner [6â€“8]. However, it is labor-intensive and expensive\nto collect large-scale dataset with precise pixel-wise annotations for\nDNN training, thus limiting the value of the image segmentation in real\napplications. This is especially true in medical image analysis due to the\ndifficulty of the segmentation problems and the difficulty in recruiting\nqualified annotators.\nTo resolve the problem mentioned above, great interests have been\nmade in the literature to develop weakly supervised image segmen-\ntation (WSIS), the purpose of which is to substitute costly pixel-wise\nâˆ— \nCorresponding author.\nE-mail addresses: wangjuan313@gmail.com (J. Wang), b.xia@sibionics.com (B. Xia).\nURL: https://scholar.google.com/citations?user=iD_8RSwAAAAJ&hl=en (J. Wang).\nannotations into cost-effective annotations as supervision for image\nsegmentation. Several types of annotations have been investigated,\nincluding image labels [9,10], points [11], scribbles [12], and bounding\nboxes [13,14]. This work considers bounding boxes as supervision\nfor image segmentation. In the literature, some efforts have been\nmade to develop WSIS adopting bounding box supervision. For ex-\nample, Rajchl et al. [13] designed an iterative optimization approach\nfor image segmentation, in which a neural network classifier was\ntrained using bounding box supervision. Hsu et al. [15] exploited mask\nR-CNN to simultaneously conduct object detection and image segmen-\ntation, in which the bounding box tightness prior was formulated as\nmultiple instance learning (MIL) for image segmentation. Kervadec\net al. [14] imposed a set of constraints on the network output by\nleveraging the tightness prior of bounding boxes as supervision for\nimage segmentation.\nDepending on the relationship between an object and its bounding\nbox annotation, as shown in Fig. 1, bounding box annotations can\nhttps://doi.org/10.1016/j.compbiomed.2023.107913\nReceived 10 March 2023; Received in revised form 21 November 2023; Accepted 24 December 2023\n\nComputers in Biology and Medicine 169 (2024) 107913\n2\nJ. Wang and B. Xia\nFig. 1. Demonstration of tight (red color) and loose (green color) bounding boxes for\nthe object â€˜â€˜sheepâ€™â€™.\nbe divided into two categories: tight bounding box and loose bounding\nbox. The tight bounding box has its four sides touching the object,\nthus the size of the tight bounding box is same as the size of the\nobject; in contrast, the size of the loose bounding box is larger than\nthe size of the object, thus at least one side of the loose bounding\nbox does not touch the object. Compared with the loose bounding\nboxes, the tight bounding boxes have strict requirements on the precise\nlocations of the four sides of the bounding box annotations, thus it is\nmuch more difficult and time-consuming to annotate tight bounding\nboxes. However, most methods in the literature, if not all, achieve high\nsegmentation performance using tight bounding box supervision, and\ndecrease the performance greatly when supervised by loose bounding\nboxes [14]. Such inconsistency in the difficulty of annotation acquisi-\ntion and the segmentation performance for tight and loose bounding\nboxes poses a problem in bounding box supervision. To conquer this\nissue, in this paper we investigate whether it is possible to maintain\ngood segmentation performance when loose bounding boxes are used\nas supervision.\nRecently, in our previous study we developed a WSIS approach us-\ning tight bounding box supervision by exploiting the properties of tight\nbounding boxes for MIL formulation [16] and achieved state-of-the-art\nsegmentation performance [16,17]. Building on our previous success\nin [16], this study extends tight bounding box supervision to loose\nbounding box supervision for image segmentation. For this purpose, we\npropose an MIL formulation based on polar transformation of the image\nregion in the bounding box to assist the approach in [16] for image seg-\nmentation, and develop a weighted smooth maximum approximation to\nincorporate the observation that pixels closer to the origin of the polar\ntransformation are more likely to belong to the object in the bounding\nbox. In the end, the segmentation assistance is conducted by combining\nthe loss derived from the polar transformation based MIL into the loss\nin [16]. The polar transformation based MIL strategy developed in this\nstudy is valid for both tight and loose bounding boxes, contributing\non the good segmentation performance of the proposed approach for\nboth tight and loose bounding box supervision. In the experiments, the\nproposed approach is evaluated by two public datasets when both tight\nbounding boxes and loose bounding boxes at different precision levels\nare used as supervision. The results demonstrate that the proposed\napproach outperforms several existing methods in all precision levels\nof bounding boxes; and more importantly, it is robust to mild and\nmoderate errors in the loose bounding box annotations.\nIn summary, the contributions of this study are as follows:\n1. First, we develop a WSIS approach beyond tight bounding box\nsupervision. It achieves state-of-the-art performance for bound-\ning boxes at all precision levels and is robust to mild and\nmoderate errors in the loose bounding box annotations.\n2. Second, we propose an MIL strategy based on polar transforma-\ntion of the image regions of the bounding boxes to incorporate\nthe bounding box supervision into the network output to as-\nsist the image segmentation. The proposed polar transformation\nbased MIL formulation works for both tight and loose bounding\nboxes, contributing on the good segmentation performance of\nthe proposed approach for both tight and loose bounding box\nsupervision.\n3. Third, a weighted smooth maximum approximation is intro-\nduced for the bag prediction in the proposed polar transforma-\ntion based MIL to incorporate the observation that pixels closer\nto the origin of the polar transformation are more likely to\nbelong to the object in the box.\n4. Finally, the proposed approach is evaluated on two public\ndatasets when bounding boxes at different precision levels are\nused as supervision. The results demonstrate the effectiveness of\nthe proposed approach for image segmentation in all precision\nlevels of bounding boxes.\n2. Preliminaries\n2.1. Problem descriptions\nThis study investigates the use of bounding boxes as supervision for\nweakly supervised image segmentation (WSIS). That is, for each object\nin the training set, a bounding box annotation is provided to supervise\nthe model training.\n2.1.1. Bounding box and object\nTo avoid any ambiguity, the definitions of bounding box and object\nare first introduced as follows:\nBounding box is an imaginary rectangle enclosed a thing of interest\nin an image, which has been widely used in object detection. For an\nobject, its bounding box annotation encloses the whole object in the\nbox such that it does not overlap with the region outside its bounding\nbox. Depending on the relationship between an object and its bounding\nbox annotation, bounding box annotations can be divided into two\ncategories: one is tight bounding box, and the other is loose bounding\nbox. Tight bounding box is the smallest rectangle enclosing the whole\nobject, thus the object must touch the four sides of its bounding box.\nIn the end, the size of the object is same as the size of its tight bounding\nbox. In contrary, loose bounding box is outside of the object, thus at least\none side of the loose bounding box does not touch the object. In the\nend, the size of the object is smaller than the size of its loose bounding\nbox.\nIn a bounding box, two types of lines are considered in this study.\nFor convenience, they are named as crossing line and polar line. Cross-\ning line of a bounding box is defined as a line with its two endpoints\nlocated on the opposite sides of the box. Polar line of a bounding box\nis defined as a line with one endpoint (denoted as point ğ‘‚) located\non a pixel belonging to the object in the bounding box and the other\nendpoint located at one of the four sides of the bounding box. As\nexamples, Fig. 2 demonstrates examples of crossing lines (left column)\nand polar lines (right column) for both tight (upper row) and loose\n(lower row) bounding boxes.\nWithout loss of generality, object considered in this study is defined\nas a thing which covers a connected region in an image. It is important\nto notice that an object does not include any disjointed parts of a thing.\nIf there are multiple disjointed parts in a thing, each part is treated as\nan independent object, thus a bounding box is annotated for each part.\n2.1.2. WSIS using bounding box supervision\nFor ease of development, we first introduce the fully supervised\nimage segmentation (FSIS). Suppose ğˆ denotes an input image and\nğ˜ âˆˆ {1, 2, â€¦ , ğ¶} is its corresponding pixel-level category label for\nğ¶ categories under consideration, the image segmentation task is to\nobtain the prediction of ğ˜, denoted as ğ, for the input image ğˆ. For the\nproblem of FSIS, the pixel-wise category label ğ˜ is available for each\nimage ğˆ in the training set for model optimization.\nHowever, in the problem of WSIS using bounding box supervision,\nthe pixel-level category label ğ˜ is unavailable; instead, it provides the\nbounding box label ğ as supervision for model training. In this study,\n\nComputers in Biology and Medicine 169 (2024) 107913\n3\nJ. Wang and B. Xia\nFig. 2. Demonstration of crossing and polar lines for the object â€˜â€˜sheepâ€™â€™. In these\nplots, crossing and polar lines are marked by blue dashed lines and bounding boxes are\ndenoted as red rectangles, in which the tight and loose bounding boxes are shown in the\nupper and lower rows, respectively. The left column shows examples of crossing lines\nand the right column shows examples of polar lines, in which points ğ‘‚ are indicated\nby green dots.\nthe bounding box label ğ is denoted as ğ = {ğ›\nğ‘š\n, ğ‘¦\nğ‘š\n}, ğ‘š = 1, 2, â€¦ , ğ‘€,\nin which ğ‘€ is the number of bounding box annotations, ğ›\nğ‘š \nis a 4-\ndimensional vector denoting the top left and bottom right points of the\nmth bounding box, and ğ‘¦\nğ‘š \nâˆˆ {1, 2, â€¦ , ğ¶} is the category label of the\nobject in the mth bounding box.\nThis study considers a specific type of deep neural networks (DNNs)\nfor image segmentation, such as UNet [6] and FCN [7]. This type of\nDNNs is able to output pixel-wise prediction for the input image. Due\nto the possible overlaps of objects of different categories in an image,\nwhich is especially true in medical images, this study formulates the\nimage segmentation problem as a multi-label classification problem.\nThat is, for a location ğ‘˜ in the input image, it outputs a vector ğ©\nğ‘˜ \n=\n[ğ‘\nğ‘˜1\n, ğ‘\nğ‘˜2\n, â€¦ , ğ‘\nğ‘˜ğ¶ \n], one element for a category; each element is converted\nto the range of [0, 1] using the sigmoid function.\n2.1.3. Multiple instance learning\nMultiple instance learning (MIL) is a form of weakly supervised\nlearning in which training samples are arranged in sets, called bags, and\na category label is provided for the entire bag [18]. In MIL, supervision\nis only provided for bags, and the labels of individual samples in the\nbags are not provided. For image segmentation, training samples are\nindividual pixels of images in the training set, thus a bag consists of a\nset of different individual pixels of an image.\nIn MIL, a bag is positive if it has at least one positive sample, and a\nbag is negative if all of its individual samples are negative. Therefore,\nfor a category ğ‘, the pixel with highest prediction in a positive bag tends\nto belong to category ğ‘, while even the pixel with highest prediction\nin a negative bag does not in category ğ‘. Based on this observation,\nsuppose ğ‘\nğ‘˜ğ‘ \nis the network output of the kth pixel in bag ğ‘ for category\nğ‘, the bag prediction ğ‘ƒ\nğ‘ \n(ğ‘) of bag ğ‘ for category ğ‘ can be defined as\nğ‘ƒ\nğ‘ \n(ğ‘) = \nğ‘›âˆ’1\nmax\nğ‘˜=0 \nğ‘\nğ‘˜ğ‘ \n(1)\nwhere ğ‘› is the number of pixels in the bag ğ‘.\n2.2. MIL baseline\nThis study considers the MIL baseline approach which employs tight\nbounding boxes for supervision.\nFig. 3. Demonstration of positive and negative bags for MIL baseline with the tight\nbounding box annotation. The tight bounding box is indicated by the red rectangle for\nthe object â€˜â€˜sheepâ€™â€™. The examples of positive and negative bags are marked by blue\nand green colors, respectively.\n2.2.1. Positive and negative bags\nIn an input image ğˆ, for an object of category ğ‘ and its tight\nbounding box, it can be easily noted that any vertical and horizontal\ncrossing line in the tight bounding box has at least one pixel belonging\nto the object in the box, hence pixels on a vertical or horizontal crossing\nline of the tight bounding box compose a positive bag for category ğ‘.\nFurthermore, pixels on a vertical or horizontal line of the image that\ndo not overlap with any bounding boxes of category ğ‘ in the image\ndo not belong to category ğ‘, hence pixels on a vertical or horizontal\nline of the image that does not overlap with any bounding boxes of\ncategory ğ‘ constitute a negative bag for category ğ‘. Based on these\nobservations, for a category ğ‘, the MIL baseline approach considers all\nof the horizontal and vertical crossing lines of the tight bounding boxes\nof category ğ‘ as positive bags, and all of the horizontal and vertical lines\nof the image that do not overlap any bounding boxes of category ğ‘ in\nthe image as negative bags [15]. As examples, the positive and negative\nbags for MIL baseline approach are demonstrated in Fig. 3.\n2.2.2. MIL baseline loss\nTo optimize the various parameters associated with network, MIL\nbaseline loss with two terms is employed [15]. For a category ğ‘, suppose\nits positive and negative bags in the training set are denoted as îˆ®\n+\nğ‘ \nand\nîˆ®\nâˆ’\nğ‘ \n, respectively, then MIL baseline loss îˆ¸\nğ‘ \nis\nîˆ¸\nğ‘ \n= ğœ™\nğ‘ \n(ğ; îˆ®\n+\nğ‘ \n, îˆ®\nâˆ’\nğ‘ \n) + ğœ†ğœ‘\nğ‘ \n(ğ) (2)\nwhere ğœ™\nğ‘ \nis the unary loss, ğœ‘\nğ‘ \nis the pairwise loss, and ğœ† is a constant\nvalue controlling the trade off between the unary loss and the pairwise\nloss.\nThe unary loss ğœ™\nğ‘ \nis defined as:\nğœ™\nğ‘ \n= âˆ’ \n1\n|îˆ®\n+\nğ‘ \n| + |îˆ®\nâˆ’\nğ‘ \n|\nâ›\nâœ\nâœ\nâ\nâˆ‘\nğ‘âˆˆîˆ®\n+\nğ‘\nlog ğ‘ƒ\nğ‘ \n(ğ‘) + \nâˆ‘\nğ‘âˆˆîˆ®\nâˆ’\nğ‘\nlog(1 âˆ’ ğ‘ƒ\nğ‘ \n(ğ‘))\nâ\nâŸ\nâŸ\nâ \n(3)\nwhere |îˆ®| is the cardinality of îˆ®. Mathematically, the unary loss is a\nbinary cross entropy loss for the bag prediction ğ‘ƒ\nğ‘ \n(ğ‘). It gets minimum\nwhen the bag prediction ğ‘ƒ\nğ‘ \n(ğ‘) is 1 for positive bags and 0 for negative\nbags. More importantly, the unary loss adaptively selects one pixel\nper bag based on the network prediction for optimization, yielding an\nadaptive sampling effect on the training samples during training.\nHowever, using the unary loss alone is prone to segment merely\nthe discriminative parts of an object rather than the whole object. To\nresolve this problem, the pairwise loss is introduced as follows:\nğœ‘\nğ‘ \n= \n1\n|ğœ€|\nâˆ‘\n(ğ‘˜,ğ‘˜\nâ€²\n)âˆˆğœ€\n(\nğ‘\nğ‘˜ğ‘ \nâˆ’ ğ‘\nğ‘˜\nâ€²\nğ‘\n)\n2 \n(4)\nwhere ğœ€ is the set containing all neighboring pixel pairs. Comple-\nmentary to the unary loss, the pairwise loss enforces the piece-wise\nsmoothness in the network prediction.\nIn the end, for all ğ¶ categories, the MIL baseline loss îˆ¸ is\nîˆ¸ =\nğ¶\nâˆ‘\nğ‘=1\nîˆ¸\nğ‘ \n(5)\n\nComputers in Biology and Medicine 169 (2024) 107913\n4\nJ. Wang and B. Xia\n3. Methods\nAs noted in the introduction, tight bounding boxes are difficult to\nacquire due to the strong constraint posed on the precise locations of\nthe four sides of the bounding box annotations. To deal with this issue,\nthis study extends our previous approach on WSIS using tight bounding\nbox supervision [16], named as parallel transformation based MIL in\nthis study, by incorporating a polar transformation based MIL, which\nworks for both tight and loose bounding boxes, to assist the image\nsegmentation. In the end, the total loss îˆ¸ for network optimization is\nas follows:\nîˆ¸ =\nğ¶\nâˆ‘\nğ‘=1\nğœ™\nğ‘ğ‘\nğ‘ \n(ğ; îˆ®\nğ‘ğ‘+\nğ‘ \n, îˆ®\nğ‘ğ‘âˆ’\nğ‘ \n) + ğœ™\nğ‘ğ‘œ\nğ‘ \n(ğ; îˆ®\nğ‘ğ‘œ+\nğ‘ \n, îˆ®\nğ‘ğ‘œâˆ’\nğ‘ \n) + ğœ†ğœ‘\nğ‘ \n(ğ) (6)\nwhere ğœ™\nğ‘ğ‘\nğ‘ \nis the unary loss derived from the parallel transformation\nbased MIL for its positive bags îˆ®\nğ‘ğ‘+\nğ‘ \nand negative bags îˆ®\nğ‘ğ‘âˆ’\nğ‘ \n(will be\ndescribed in Section 3.1), ğœ™\nğ‘ğ‘œ\nğ‘ \nis the unary loss obtained from the polar\ntransformation based MIL for its positive bags îˆ®\nğ‘ğ‘œ+\nğ‘ \nand negative bags\nîˆ®\nğ‘ğ‘œâˆ’\nğ‘ \n(will be introduced in Section 3.2), and ğœ‘\nğ‘ \nis the pairwise loss\ndefined in Eq. (4).\n3.1. Parallel transformation based MIL using tight bounding box supervision\nThe method described in this section was first introduced in [16].\nMore details are provided in this study regarding to efficient calculation\nof the positive bag prediction.\n3.1.1. Positive bags îˆ®\nğ‘ğ‘+\nğ‘\nOne issue associated with the positive bag definition in MIL baseline\nis that for an object of height ğ» pixels and width ğ‘Š pixels, it yields\nonly ğ»+ğ‘Š positive bags, the value of which is much smaller than\nthe size of the object, hence limiting the selected positive samples\nduring training and resulting in a bottleneck for the segmentation\nperformance. Noticed that in an input image ğˆ, for an object of category\nğ‘ and its tight bounding box, any parallel crossing line of the tight\nbounding box also has at least one pixel belonging to the object in the\nbox, this study generalizes the positive bag definition by considering\na parallel crossing line of the tight bounding box as a positive bag\nfor category ğ‘. A parallel crossing line of a bounding box can be\nparameterized by an angle ğœƒ\nâ€² \nâˆˆ (âˆ’90\nâ—¦\n, 90\nâ—¦\n) with respect to the edges\nof the box where its two endpoints located. For an angle ğœƒ\nâ€²\n, two sets\nof parallel crossing lines can be obtained from the bounding box, one\ncrosses up and bottom edges of the box, and the other crosses left\nand right edges. In the end, the positive bags îˆ®\nğ‘ğ‘+\nğ‘ \nfor category ğ‘ are\nall parallel crossing lines of the tight bounding boxes of the objects\nof category ğ‘ on a set of different angles. As demonstration, Fig. 4\nshows examples of positive bags obtained from two different angles,\nwhere those indicated by purple dashed lines are with ğœƒ\nâ€² \n= 25\nâ—¦\n, and\nthose marked by blue dashed lines have ğœƒ\nâ€² \n= 0\nâ—¦\n. More importantly,\nby comparing Figs. 3 and 4, the positive bags in MIL baseline are a\nsubset and special cases of the positive bags îˆ®\nğ‘ğ‘+\nğ‘ \nwith ğœƒ\nâ€² \n= 0\nâ—¦\n. In the\nexperiments, this study presets the set of angles as ğœƒ\nâ€² \nâˆˆ (ğ‘, ğ‘, ğ‘ ), denoting\nevenly spaced angle values within interval (ğ‘, ğ‘) with step ğ‘ .\nIn implementation, it is inefficient to directly calculate the bag\nprediction ğ‘ƒ\nğ‘ \n(ğ‘) of positive bags îˆ®\nğ‘ğ‘+\nğ‘ \nfrom parallel crossing lines in the\ninput image. To facilitate it, we propose to transform the parallel cross-\ning lines with angle ğœƒ\nâ€² \nin the input image into vertical or horizontal\nlines by rotating the input image by angle ğœƒ\nâ€²\n. In this study, this process\nof obtaining parallel crossing lines is named as parallel transformation\nto emphasize that the transformation is targeted at crossing lines of\nbounding boxes. As examples, Fig. 5(a) shows the process of parallel\ntransformation, in which examples of parallel crossing lines with angle\nğœƒ\nâ€² \n= 25\nâ—¦ \nin the upper image are transformed into horizontal and vertical\nlines in the lower image.\nHowever, for efficient calculation of the bag prediction, the parallel\ntransformation of the input image alone is not enough. As shown in\nFig. 4. Demonstration of positive bags in parallel transformation based MIL with tight\nbounding box annotation. In this plot, the tight bounding box of the object â€˜â€˜sheepâ€™â€™ is\nindicated by the red rectangle; examples of positive bags from two different angles are\nprovided, in which those with ğœƒ\nâ€² \n= 25\nâ—¦ \nare marked by purple dashed lines and those\nwith ğœƒ\nâ€² \n= 0\nâ—¦ \nare given by blue dashed lines.\nFig. 5. Demonstration of parallel transformation, in which the images and their results\nof parallel transformation are provided in the upper and lower rows, respectively. In\nthese plots, examples of paralleled crossing lines with angle ğœƒ\nâ€² \n= 25\nâ—¦ \nare marked by\npurple dashed lines. (a) input image, in which the tight bounding box of â€˜â€˜sheepâ€™â€™ is\nmarked by red rectangle, (b) the box-mask image of the tight bounding box in (a).\nFig. 5(a), the vertical lines in the rotated image are no longer aligned\nat the same starting and ending points along the horizontal direction;\nand the same problem also exists for the horizontal lines in the rotated\nimages. Therefore, an indicator has to be provided for each pixel in the\nrotated image to determine whether it is in a positive bag or not. For\nthis purpose, we construct a box-mask image for each tight bounding\nbox in the input image. The box-mask image has same size as the input\nimage, its value is set to be 1 for the pixels in the box and 0 for those\noutside the box. Afterwards, the same parallel transformation is applied\nto the box-mask image to determine whether each pixel is in a positive\nbag. In the rotated box-mask image, the pixels with value 1 in a vertical\nor horizontal line corresponds to a parallel crossing line, thus consisting\nof a positive bag. As examples, in Figs. 5(b), we also show the parallel\ntransformation of the box-mask image of Figs. 5(a). From Fig. 5(b), the\nwhite pixels along each horizontal or vertical line (denoted by purple\ndashed line) consist of a positive bag in the rotated box-mask image.\nFinally, to further speed up the calculation, for a tight bounding box\nof an object, the parallel transformation is only applied to the cropped\nregion of the input image around the box and its corresponding box-\nmask image, in which a small margin is added to four sides of the box\nto avoid information loss during rotation.\n3.1.2. Negative bags îˆ®\nğ‘ğ‘âˆ’\nğ‘\nSimilar as the positive bag definition, the negative bag definition\nin MIL baseline also has the problem of limited samples. Notice in an\n\nComputers in Biology and Medicine 169 (2024) 107913\n5\nJ. Wang and B. Xia\nFig. 6. Demonstration of positive bags in polar transformation based MIL for (a) tight\nand (b) loose bounding boxes of the object â€˜â€˜sheepâ€™â€™. In each plot, the bounding box is\nmarked by the red rectangle, the point ğ‘‚ is denoted by the green dot, and examples\nof positive bags are indicated by blue dashed lines.\ninput image ğˆ, for a category ğ‘, any individual pixels outside of any\nbounding boxes of category ğ‘ in the image do not belong to category ğ‘,\nwe define a negative bag for category ğ‘ as an individual pixel outside\nany bounding boxes of category ğ‘. In the end, negative bags îˆ®\nğ‘ğ‘âˆ’\nğ‘ \nfor\ncategory ğ‘ consist of all of individual pixels outside all of the bounding\nboxes of category ğ‘. This definition greatly increases the number of\nnegative bags for training, and forces the network to learn every pixel\noutside bounding boxes.\n3.1.3. Unary loss ğœ™\nğ‘ğ‘\nğ‘\nThe parallel transformed based MIL formulation above will in-\nevitably lead to imbalance between positive and negative bags. To\neliminate this issue, we borrow the concept of focal loss [19] and define\nthe unary loss as follows:\nğœ™\nğ‘ğ‘\nğ‘ \n= âˆ’ \n1\nğ‘\n+\nâ›\nâœ\nâœ\nâ\nâˆ‘\nğ‘âˆˆîˆ®\nğ‘ğ‘+\nğ‘\nğ›½ \n(\n1 âˆ’ ğ‘ƒ\nğ‘ \n(ğ‘)\n)\nğ›¾ \nlog ğ‘ƒ\nğ‘ \n(ğ‘)+\nâˆ‘\nğ‘âˆˆîˆ®\nğ‘ğ‘âˆ’\nğ‘\n(1 âˆ’ ğ›½)ğ‘ƒ\nğ‘ \n(ğ‘)\nğ›¾ \nlog(1 âˆ’ ğ‘ƒ\nğ‘ \n(ğ‘))\nâ\nâŸ\nâŸ\nâ \n(7)\nwhere ğ‘\n+ \n= max(1, |îˆ®\nğ‘ğ‘+\nğ‘ \n|), ğ›½ âˆˆ [0, 1] is the weighting factor, and\nğ›¾ â‰¥ 0 is the focusing parameter. Mathematically, the unary loss ğœ™\nğ‘ğ‘\nğ‘\nis focal loss for bag prediction ğ‘ƒ\nğ‘ \n(ğ‘), it gets minimum when ğ‘ƒ\nğ‘ \n(ğ‘) is 1\nfor positive bags and 0 for negative bags.\n3.2. Polar transformation based MIL using tight or loose bounding box\nsupervision\nThis study proposes polar transformation based MIL to assist the\nparallel transformation based MIL for image segmentation. The pro-\nposed polar transformation based MIL works for both tight and loose\nbounding boxes, contributing on the good segmentation performance\nof the proposed approach for both tight and loose bounding box super-\nvision. Its details are provided in this section.\n3.2.1. Positive bags îˆ®\nğ‘ğ‘œ+\nğ‘\nTo extend the positive bag definition beyond the tight bounding box\nsupervision, we consider the polar line of the bounding box. For an\nobject of category ğ‘, any polar line of its bounding box has at least\none pixel belonging to category ğ‘, thus this study considers pixels in\na polar line of the bounding box as a positive bag for category ğ‘. This\ndefinition does not employ any prior information of the bounding box,\nthus is valid for both tight and loose bounding boxes of the object. In\nthe end, the positive bags îˆ®\nğ‘ğ‘œ+\nğ‘ \nfor category ğ‘ are defined as all of the\npolar lines of the bounding boxes of the objects with category ğ‘. As\nexamples, Fig. 6 shows examples of positive bags for both tight and\nloose bounding boxes.\nMathematically, the bag prediction ğ‘ƒ\nğ‘ \n(ğ‘) of positive bags of an\nobject from the corresponding polar lines can be efficiently obtained by\nFig. 7. Demonstration of polar transformation for examples of positive bags, in\nwhich the images and their polar images are provided in the upper and lower rows,\nrespectively. (a) The cropped region of the object â€˜â€˜sheepâ€™â€™ from tight bounding box,\n(b) the box-mask image of (a), (c) the cropped region of the object â€˜â€˜sheepâ€™â€™ from a\nloose bounding box, (d) the box-mask image of (c).\napplying polar transformation to the image. The polar transformation\nof an image transfers the image from the Cartesian coordinate system\nto the polar coordinate system, providing a pixel-wise representation\nin the polar coordinate system. In this study, it transfers a polar line\nof the bounding box of an object in an image into a horizontal line in\nthe polar image. Suppose (ğ‘¢, ğ‘£) is the Cartesian coordinate of a pixel\nin the Cartesian coordinate domain with respect to an origin ğ‘‚\nğ‘ \nand\na radius ğ‘…\nğ‘\n, and the polar coordinate is (ğ‘Ÿ, ğœƒ), where ğ‘Ÿ > 0 and ğœƒ âˆˆ\n[0, 2ğœ‹) are the radial and angular coordinates, respectively. The polar\ntransformation maps the pixel (ğ‘¢, ğ‘£) in the Cartesian coordinate plane to\nthe corresponding pixel (ğ‘Ÿ, ğœƒ) in the polar coordinate plane as follows:\nğ‘Ÿ = \nâˆš\nğ‘¢\n2 \n+ ğ‘£\n2\nğœƒ = tan\nâˆ’1\n(ğ‘£âˆ•ğ‘¢) \n(8)\nIn polar transformation, the size of the polar image in the polar coordi-\nnate plane is preset by user during experiments. Suppose it is ğ‘\nğ‘Ÿ \nÃ— ğ‘\nğœƒ \n,\nwhere ğ‘\nğ‘Ÿ \nis the dimension of the polar axis and ğ‘\nğœƒ \nis the dimension\nof the angle axis.\nIn this study, the polar transformation is applied to the cropped\nregion of the input image around the bounding box. A small margin\nis added to four sides of the bounding box for region cropping to\navoid possible information loss during transformation. For the cropped\nregion, during polar transformation, the origin ğ‘‚\nğ‘ \nis set as the point\nğ‘‚ defined in Section 2.1.1 and the radius ğ‘…\nğ‘ \nis set as half length of\nthe diagonal line of the bounding box ğ‘…, which is the radius of the\nminimum circle enclosing the bounding box. With such settings, in\npolar transformation of the cropped region, the radial coordinate ğ‘Ÿ is\nevenly distributed in [0, ğ‘…] with step ğ‘…âˆ•ğ‘\nğ‘Ÿ\n, and the angular coordinate\nğœƒ is evenly distributed in [0, 2ğœ‹) with step 2ğœ‹âˆ•ğ‘\nğœƒ \n. As demonstration,\nFigs. 7(a) and (c) show polar transformation for examples of positive\nbags from tight and loose bounding boxes, respectively.\nSimilarly, as shown in Figs. 7(a) and (c), the horizontal lines in\na polar image are no longer aligned at the ending points along the\nvertical direction. To determine whether each pixel in the polar image\nis in a positive bag or not, we also introduce the box-mask image of\nthe cropped region and applied the same polar transformation to the\nbox-mask image as indicator of pixels in positive bags. As examples,\nFigs. 7(b) and (d) show the box-mask images and its corresponding\npolar images for the cropped regions in Figs. 7(a) and (c), respectively.\nIn the lower row of Figs. 7(b) and (d), the white pixels along a\nhorizontal line (denoted by dashed blue lines) consist of a positive bag.\n\nComputers in Biology and Medicine 169 (2024) 107913\n6\nJ. Wang and B. Xia\nFinally, for bounding box annotation, the origin ğ‘‚ is unknown and\nshould be determined during experiments. Based on the fact that the\norigin ğ‘‚ is inside the object in the bounding box, it is selected as\nthe pixel with maximum network output among all of the pixels in\nthe bounding box during training in the experiments. Such design is\nintuitive since the pixel with highest prediction are more likely belong\nto the object.\n3.2.2. Negative bags îˆ®\nğ‘ğ‘œâˆ’\nğ‘\nThis study employs the same negative bag definition as in that in\nSection 3.1.2 for the polar transformation based MIL, therefore, we\nhave îˆ®\nğ‘ğ‘œâˆ’\nğ‘ \n= îˆ®\nğ‘ğ‘âˆ’\nğ‘ \n.\n3.2.3. Unary loss ğœ™\nğ‘ğ‘œ\nğ‘\nSimilar as unary loss ğœ™\nğ‘ğ‘\nğ‘ \ndefined in Section 3.1.3, the unary loss ğœ™\nğ‘ğ‘œ\nğ‘\nfor polar transformation based MIL is\nğœ™\nğ‘ğ‘œ\nğ‘ \n= âˆ’ \n1\nğ‘\n+\nâ›\nâœ\nâœ\nâ\nâˆ‘\nğ‘âˆˆîˆ®\nğ‘ğ‘œ+\nğ‘\nğ›½ \n(\n1 âˆ’ ğ‘ƒ\nğ‘ \n(ğ‘)\n)\nğ›¾ \nlog ğ‘ƒ\nğ‘ \n(ğ‘)+\nâˆ‘\nğ‘âˆˆîˆ®\nğ‘ğ‘œâˆ’\nğ‘\n(1 âˆ’ ğ›½)ğ‘ƒ\nğ‘ \n(ğ‘)\nğ›¾ \nlog(1 âˆ’ ğ‘ƒ\nğ‘ \n(ğ‘))\nâ\nâŸ\nâŸ\nâ \n(9)\nwhere ğ‘\n+ \n= max(1, |îˆ®\nğ‘ğ‘œ+\nğ‘ \n|).\n3.3. Smooth maximum approximation\nIn both unary losses ğœ™\nğ‘ğ‘\nğ‘ \nand ğœ™\nğ‘ğ‘œ\nğ‘ \n, the bag prediction ğ‘ƒ\nğ‘ \n(ğ‘) =\nmax\nğ‘›âˆ’1\nğ‘˜=0 \nğ‘\nğ‘˜ğ‘ \nis the maximum prediction value of pixels in a bag. There-\nfore, the derivative ğœ•ğ‘ƒ\nğ‘ \nâˆ•ğœ•ğ‘\nğ‘˜ğ‘ \nis discontinuous, leading to numerical\ninstability. To conquer this issue, we introduce a technique called\nsmooth maximum approximation to replace the maximum function by\nits smooth maximum approximation [20]. In this study, we consider\ntwo variants of smooth maximum approximation for ğ‘ƒ\nğ‘ \n(ğ‘) as follows:\n(1) ğ›¼-softmax function:\nğ‘†\nğ›¼ \n(ğ‘) =\nâˆ‘\nğ‘›âˆ’1\nğ‘˜=0 \nğ‘\nğ‘˜ğ‘ \nğ‘’\nğ›¼ğ‘\nğ‘˜ğ‘\nâˆ‘\nğ‘›âˆ’1\nğ‘˜=0 \nğ‘’\nğ›¼ğ‘\nğ‘˜ğ‘\n(10)\nwhere ğ›¼ > 0 is a constant. The higher ğ›¼ value denotes the closer\napproximation of ğ‘†\nğ›¼ \n(ğ‘) to ğ‘ƒ\nğ‘ \n(ğ‘).\n(2) ğ›¼-quasimax function:\nğ‘„\nğ›¼ \n(ğ‘) = \n1\nğ›¼ \nlog\n(\nğ‘›âˆ’1\nâˆ‘\nğ‘˜=0\nğ‘’\nğ›¼ğ‘\nğ‘˜ğ‘\n)\nâˆ’ \nlog ğ‘›\nğ›¼ \n(11)\nwhere ğ›¼ > 0 is a constant. The higher ğ›¼ value also denotes closer the\napproximation of ğ‘„\nğ›¼ \n(ğ‘) to ğ‘ƒ\nğ‘ \n(ğ‘). It can be easily proved that ğ‘„\nğ›¼ \n(ğ‘) â‰¤\nğ‘ƒ\nğ‘ \n(ğ‘) always holds.\nFor image segmentation problem, the smooth maximum approxi-\nmation has an extra advantage as follows: different from the maximum\nfunction ğ‘ƒ\nğ‘ \nwith ğœ•ğ‘ƒ\nğ‘ \nâˆ•ğœ•ğ‘\nğ‘˜ğ‘ \n> 0 at only one pixel, the smooth maximum\napproximation has ğœ•ğ‘†\nğ›¼ \nâˆ•ğœ•ğ‘\nğ‘˜ğ‘ \n> 0 and ğœ•ğ‘„\nğ›¼ \nâˆ•ğœ•ğ‘\nğ‘˜ğ‘ \n> 0 for all ğ‘\nğ‘˜ğ‘ \n, thus it is\nable to learn all pixels together in a bag for model optimization. More-\nover, a positive bag usually has more than one positive pixel in real\nsegmentation problem, thus this property is beneficial as well consid-\nering this fact. Therefore, besides the advantage in numerical stability,\nthe smooth maximum approximation is also helpful for performance\nimprovement.\n3.3.1. Weighted smooth maximum approximation for îˆ®\nğ‘ğ‘œ+\nğ‘\nFor the pixels in a polar line, the origin ğ‘‚ is inside the object and\nthe pixels closer to the origin ğ‘‚ are more likely belonging to the object.\nTo incorporate this fact in optimization, a weight is introduced to the\nsmooth maximum approximation for positive bags îˆ®\nğ‘ğ‘œ+\nğ‘ \n. In particular,\na weight ğ‘¤\nğ‘˜ \nis assigned to prediction ğ‘\nğ‘˜ğ‘ \nof each pixel in the positive\nbag, yielding weighted smooth maximum approximation. The weight\nğ‘¤\nğ‘˜ \nis defined as follows:\nğ‘¤\nğ‘˜ \n= ğ‘’\nâˆ’ğ‘˜\n2\nâˆ•(2ğœ\n2\n) \n(12)\nwhere ğœ = (ğ‘\nğ‘Ÿ \nâˆ’ 1)âˆ•\nâˆš\nâˆ’2 log ğ‘¤\nğ‘šğ‘–ğ‘› \nand ğ‘¤\nğ‘šğ‘–ğ‘› \nis a preset parameter for the\nminimum weight of the pixel in positive bags of an object.\n4. Experiments\n4.1. Datasets\nFor performance evaluation, two public datasets, which are widely\nused in the literature for medical image segmentation, were considered.\nOne is PROMISE12 dataset [21] for prostate segmentation, and the\nother is ATLAS dataset [22] for brain lesion segmentation.\nPROMISE12: The PROMISE12 dataset was released in MICCAI 2012\ngrand challenge [21]. It consists of transversal T2-weighted MR im-\nages and their pixel-wise annotations from 50 patients, including both\nbenign and prostate cancer cases. The MR images were acquired at\ndifferent centers with multiple MRI vendors and different scanning\nprotocols. The dataset was divided into two non-overlapping subsets,\none subset with 40 patients for training and the other with 10 patients\nfor validation.\nATLAS: The ATLAS dataset was developed by University of South-\nern California [22]. It consists of 229 T1-weighted MR images and\ntheir pixel-wise annotations from 220 patients, acquired from different\ncohorts and different scanners. The dataset was divided into two non-\noverlapping subsets, one subset with 203 images from 195 patients for\ntraining and the other with 26 images from 25 patients for validation.\nFor fairness of comparison, for both datasets, the images in the train-\ning and validation subsets are exactly same as those in studies [14,16].\nMoreover, same as studies [14,16], this study reports the segmentation\nperformance for the validation subset in the results.\n4.2. Performance evaluation\nThis study employs dice coefficient to evaluate the performance of\nthe proposed approach for image segmentation. The dice coefficient has\nbeen widely used as a standard performance metric in medical image\nsegmentation. It is in the range of [0, 1]. The higher dice coefficient\nrepresents better segmentation performance. In this study, the dice\ncoefficient is calculated based on 3D MR images by stacking predictions\nof the corresponding 2D slices together.\n4.3. Bounding box settings\nTo evaluate the performance of the proposed approach supervised\nby bounding boxes at different precision levels, this study considers\nboth tight and loose bounding boxes in the experiments. The loose\nbounding box of an object is obtained by adding a margin (denoted as\nğ‘š) on each side of its corresponding tight bounding box. Specifically,\nthe bounding boxes at the following four different precision levels are\ninvestigated: (1) tight bounding boxes (denoted as ğ‘š=0), (2) loose\nbounding boxes obtained by adding 5 pixels to each side of the cor-\nresponding tight bounding boxes (denoted as ğ‘š=5), (3) loose bounding\nboxes which add 10 pixels to each side of the corresponding tight bond-\ning boxes (denoted as ğ‘š=10), and (4) loose bounding boxes acquired by\nadding random number of pixels, generated from uniform distribution\nin the range of [0, 10], to each side of the corresponding tight bounding\nboxes (denoted as ğ‘šâˆ¼ğ‘ˆ (0, 10)). In the experiments, ğ‘š=0, ğ‘š=5, and ğ‘š=10\nare used to quantitatively investigate the effect of precision levels of\nthe bounding boxes on the segmentation performance, and ğ‘šâˆ¼ğ‘ˆ (0, 10)\nstimulates bounding boxes acquired in real annotation task, in which\nthe margins provided by annotators are usually different and random\namong different objects.\n\nComputers in Biology and Medicine 169 (2024) 107913\n7\nJ. Wang and B. Xia\nTable 1\nMean and standard deviation (in bracket) of the MARD values for the bounding boxes\nat four different precision levels for PROMISE12 and ATLAS datasets.\nBounding box settings PROMISE12 ATLAS\nğ‘š=0 0.00% (0.00%) 0.00% (0.00%)\nğ‘š=5 22.28% (10.62%) 122.22% (96.68%)\nğ‘š=10 44.56% (21.23%) 244.44% (193.36%)\nğ‘šâˆ¼ğ‘ˆ (0, 10) 22.23% (12.90%) 122.67% (109.41%)\nTo measure the precision of a bounding box annotation, mean abso-\nlute relative difference (MARD) is introduced. This study defines MARD\nas the average of the absolute errors in height and width between the\nobject and its bounding box as follows:\nğ‘€ğ´ğ‘…ğ· = \n1\n2 \nÃ—\n( \nğ‘š\nğ‘¥\n1 \n+ ğ‘š\nğ‘¥\n2\nğ‘¤ \n+ \nğ‘š\nğ‘¦\n1 \n+ ğ‘š\nğ‘¦\n2\nâ„\n)\n(13)\nwhere ğ‘¤ and â„ are width and height of the object, respectively; ğ‘š\nğ‘¥\n1 \n,\nğ‘š\nğ‘¥\n2 \n, ğ‘š\nğ‘¦\n1 \n, and ğ‘š\nğ‘¦\n2 \nare the margin added to the left, right, up, and down\nsides of the tight bounding box.\nIn Table 1, the mean and standard deviation of the MARD values\nfor the bounding boxes at four different precision levels are provided\nfor PROMISE12 dataset, in which the results are calculated based on\nall of the bounding boxes in the training subset. As can be seen, mean\nMARD values are close to 22% for ğ‘š=5 and ğ‘šâˆ¼ğ‘ˆ (0, 10), indicating that\nthe bounding boxes are accurate and there is only mild error in the\nbounding box annotations. For ğ‘š=10, mean MARD value increases into\n44.56%, indicating the sizes of the bounding boxes are almost 1.5 times\nof the sizes of the objects on average, a moderate error in the bounding\nbox annotations.\nTable 1 also lists the mean and standard deviation of the MARD\nvalues for ATLAS dataset. It can be seen, for ğ‘š=5 and ğ‘šâˆ¼ğ‘ˆ (0, 10), the\nmean MARD values are close to 122.5%, suggesting a severe error in\nbounding box annotations. Lastly, for ğ‘š=10, the mean MARD value is\n244.44%. It represents that the sizes of the bounding boxes are almost\n2.5 times larger than the sizes of the objects on average, indicating an\nextremely severe error in bounding box annotations.\n4.4. Methods for comparison\nTo demonstrate the overall performance of the proposed approach\nfor image segmentation, this study considers the following methods for\ncomparison:\n(1) Fully supervised image segmentation (FSIS): FSIS employs pixel-\nwise annotations as supervision for image segmentation. It can be\ntreated as the upper bound of the segmentation performance for WSIS\ndue to the use of fully supervised learning based on costly pixel-wise\nannotations.\n(2) MIL baseline: It is a WSIS approach supervised by tight bounding\nboxes. It is described in Section 2.2.\n(3) Deep cut [13]: It is a WSIS approach using bounding box super-\nvision for image segmentation. It trains neural network classifier in an\niterative optimization way.\n(4) Bounding box tightness prior (denoted as BBTP) [15]: It is a\nWSIS method supervised by tight bounding box. It employs mask R-\nCNN for simultaneous object detection and image segmentation, and\nthe bounding box tightness prior was enforced by MIL for image\nsegmentation.\n(5) Global constraint [14]: It is a WSIS approach adopting tight\nbounding boxes as supervision. It imposes a set of constraints on the\nnetwork outputs based on the tightness prior of bounding boxes for\nimage segmentation.\n(6) Parallel transformation based MIL (denoted as PA): It is a WSIS\napproach with tight bounding box supervision, which was developed\nin [16]. This study also describes it in Section 3.1 for completeness. The\nloss of this approach is îˆ¸ = \nâˆ‘\nğ¶\nğ‘=1 \nğœ™\nğ‘ğ‘\nğ‘ \n(ğ; îˆ®\nğ‘ğ‘+\nğ‘ \n, îˆ®\nğ‘ğ‘âˆ’\nğ‘ \n) + ğœ†ğœ‘\nğ‘ \n(ğ). Besides\nTable 2\nSummary of the methods for comparison.\nMethods Supervision Properties of supervision\nFSIS masks pixel-wise\nDeep cut [13] bounding boxes tight/loose\nBBTP [15] bounding boxes tight\nGlobal constraint [14] bounding boxes tight\nMIL baseline bounding boxes tight\nPA [16] bounding boxes tight\nPO bounding boxes tight/loose\nProposed approach bounding boxes tight/loose\nbeing an existing method for comparison, this approach also serves\nas an ablation study of the proposed approach which removes the\ncomponent of polar transformation based MIL.\n(7) Polar transformation based MIL (denoted as PO): It is a WSIS\napproach supervised by bounding boxes. It is described in Section 3.2,\noptimized by the loss îˆ¸ = \nâˆ‘\nğ¶\nğ‘=1 \nğœ™\nğ‘ğ‘œ\nğ‘ \n(ğ; îˆ®\nğ‘ğ‘œ+\nğ‘ \n, îˆ®\nğ‘ğ‘œâˆ’\nğ‘ \n) + ğœ†ğœ‘\nğ‘ \n(ğ) during\ntraining. This method is an ablation study of the proposed approach\nafter removing the component of parallel transformation based MIL.\nOverall, the summary of the methods for comparison is listed in\nTable 2. For fairness of comparison, the network structures used in\na comparison study for all methods but BBTP are same. As for the\nBBTP approach, the network structure in the original paper [15] was\nconsidered.\n4.5. Implementation details\n4.5.1. Experimental setups\nIn this study, all experiments were implemented using PyTorch and\nthe experimental codes are available at https://github.com/wangjuan3\n13/wsis-beyond-tightBB.Image segmentation was conducted on the 2D\nslices of MR images. As indicated below, most experimental setups were\nset to be same as those in [14,16] for fairness of comparison.\nFor the PROMISE12 dataset, all images were resized to 256 Ã— 256\npixels. A residual version of UNet [6] was employed for segmentation.\nThe models were trained by Adam optimizer [23] with parameters as\nfollows: batch size = 16, initial learning rate = 10\nâˆ’4\n, ğ›½\n1 \n= 0.9, and\nğ›½\n2 \n= 0.99. An off-line data augmentation procedure was performed\nto the images in the training set, and the following operations were\nconsidered: (1) mirroring, (2) flipping, and (3) rotation.\nFor the ATLAS dataset, all images were resized to 208 Ã— 256 pixels.\nENet [24] was used as backbone for image segmentation. The models\nwere trained by Adam optimizer with following parameters: batch size\n= 80, initial learning rate = 5 Ã— 10\nâˆ’4\n, ğ›½\n1 \n= 0.9, and ğ›½\n2 \n= 0.99. No\naugmentation was conducted during training.\n4.5.2. Hyperparameters\nThe weight ğœ† of the pairwise loss ğœ‘\nğ‘ \n(ğ) was set as ğœ† = 10 based\non experience in all experiments. The parameters in the unary losses\nğœ™\nğ‘ğ‘\nğ‘ \nand ğœ™\nğ‘ğ‘œ\nğ‘ \nwere set as ğ›½ = 0.25 and ğ›¾ = 2 according to the focal\nloss [19]. For parallel transformation based MIL, the parameters ğœƒ\nâ€² \nfor\nparallel crossing lines and ğ›¼ for smooth maximum approximation were\nobtained by grid search with the following values: ğ›¼ âˆˆ {4, 6, 8} and ğœƒ\nâ€² \nâˆˆ\n{(âˆ’40\nâ—¦\n, 40\nâ—¦\n, 10\nâ—¦\n), (âˆ’40\nâ—¦\n, 40\nâ—¦\n, 20\nâ—¦\n), (âˆ’60\nâ—¦\n, 60\nâ—¦\n, 30\nâ—¦\n)}. For polar transforma-\ntion based MIL, the parameters ğ‘\nğ‘Ÿ \nand ğ‘\nğœƒ \nin polar transformation and\nğ‘¤\nğ‘šğ‘–ğ‘› \nand ğ›¼ in weighted smooth maximum approximation were obtained\nby grid search as follows: ğ‘\nğ‘Ÿ \nâˆˆ {10, 20, 30, 40}, ğ‘\nğœƒ \nâˆˆ {60, 90, 120},\nğ‘¤\nğ‘šğ‘–ğ‘› \nâˆˆ {0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8}, and ğ›¼ âˆˆ {0.5, 1, 2}.\n5. Results\n5.1. Performance comparison for PROMISE12 dataset\nTable 3 gives dice coefficients of the proposed approach supervised\nby bounding boxes at different precision levels for PROMISE12 dataset.\n\nComputers in Biology and Medicine 169 (2024) 107913\n8\nJ. Wang and B. Xia\nTable 3\nComparison of dice coefficients among different methods for the PROMISE12 dataset when bounding boxes at different precision levels are\nconsidered, in which the standard deviation of dice coefficients among different MR images is reported in the bracket. NA denotes that the\nresult is not applicable and the symbol â€˜â€˜-â€™â€™ indicates unavailable result.\nMethods ğ‘š = 0 ğ‘š = 5 ğ‘š = 10 ğ‘š âˆ¼ ğ‘ˆ (0, 10)\nFSIS 0.894 (0.021) NA NA NA\nDeep cut [13] 0.827 (0.085) â€“ 0.684 (0.069) â€“\nBBTP [15] 0.838 (0.168) 0.756 (0.151) 0.637 (0.154) 0.746 (0.145)\nGlobal constraint [14] 0.835 (0.032) â€“ 0.778 (0.047) â€“\nMIL baseline 0.859 (0.038) 0.840 (0.046) 0.795 (0.023) 0.832 (0.047)\nPA (ğ›¼-softmax) [16] 0.878 (0.031) 0.868 (0.033) 0.862 (0.041) 0.869 (0.043)\nPA (ğ›¼-quasimax) [16] 0.880 (0.024) 0.871 (0.030) 0.856 (0.039) 0.875 (0.031)\nPO (ğ›¼-softmax) 0.867 (0.022) 0.858 (0.034) 0.843 (0.035) 0.858 (0.032)\nPO (ğ›¼-quasimax) 0.871 (0.019) 0.859 (0.030) 0.841 (0.038) 0.860 (0.021)\nProposed approach (ğ›¼-softmax) 0.887 (0.027) 0.882 (0.023) 0.875 (0.034) 0.874 (0.026)\nProposed approach (ğ›¼-quasimax) 0.887 (0.017) 0.880 (0.029) 0.869 (0.026) 0.876 (0.033)\nTable 4\nComparison of dice coefficients among different methods for the ATLAS dataset when bounding boxes at different precision levels are considered.\nMethods ğ‘š = 0 ğ‘š = 5 ğ‘š = 10 ğ‘š âˆ¼ ğ‘ˆ (0, 10)\nFSIS 0.512 (0.292) NA NA NA\nDeep cut [13] 0.375 (0.246) â€“ â€“ â€“\nBBTP [15] 0.467 (0.320) 0.388 (0.278) 0.290 (0.231) 0.335 (0.274)\nGlobal constraint [14] 0.474 (0.245) â€“ â€“ â€“\nMIL baseline 0.408 (0.249) 0.395 (0.240) 0.357 (0.223) 0.379 (0.237)\nPA (ğ›¼-softmax) [16] 0.494 (0.236) 0.451 (0.248) 0.400 (0.254) 0.456 (0.269)\nPA (ğ›¼-quasimax) [16] 0.488 (0.240) 0.448 (0.250) 0.412 (0.241) 0.437 (0.267)\nPO (ğ›¼-softmax) 0.463 (0.241) 0.417 (0.225) 0.373 (0.220) 0.432 (0.215)\nPO (ğ›¼-quasimax) 0.470 (0.221) 0.407 (0.241) 0.381 (0.230) 0.437 (0.218)\nProposed approach (ğ›¼-softmax) 0.491 (0.233) 0.464 (0.251) 0.418 (0.246) 0.487 (0.246)\nProposed approach (ğ›¼-quasimax) 0.503 (0.245) 0.462 (0.265) 0.409 (0.236) 0.464 (0.267)\nTwo models are considered for each level, one employing ğ›¼-softmax\nfunction and the other adopting ğ›¼-quasimax function. As can be seen,\nthe proposed approach has only minor decrease in dice coefficients\nfor ğ‘š=5, ğ‘š=10, and ğ‘šâˆ¼ğ‘ˆ (0, 10) when compared with ğ‘š=0, indicating\nthat the proposed approach is robust to minor and moderate errors in\nbounding box annotations.\nFor comparison, we also report results of the MIL baseline in Ta-\nble 3. It gets dice coefficient of 0.859 for ğ‘š=0, 0.840 for ğ‘š=5, 0.795\nfor ğ‘š=10, and 0.832 for ğ‘šâˆ¼ğ‘ˆ (0, 10). These values are much lower than\ntheir counterparts of the proposed approach for all precision levels.\nMoreover, the results of PA and PO approaches are listed in Table 3\nas well. For both approaches, two models are considered for each\nprecision level, one using ğ›¼-softmax function and the other adopting\nğ›¼-quasimax function. As can be seen, both approaches have lower dice\ncoefficients when compared with the proposed approach at different\nprecision levels. More importantly, comparing with both of these two\napproaches, the proposed approach has greater performance improve-\nments when ğ‘š increases from ğ‘š=0 to ğ‘š=5 and ğ‘š=10. These results\nsuggest that both parallel transformation based MIL and polar trans-\nformation based MIL are effective in the proposed approach, and they\nare especially helpful when the error in the bounding box annotations\nis larger.\nFurthermore, Table 3 also provides the results of Deep cut, Global\nconstraint, and BBTP approaches, in which those of Deep cut and\nGlobal constraint are cited from study [14]. The results demonstrate\nthat the proposed approach outperforms all of these three methods at\na large margin for bounding boxes at different precision levels.\nLastly, FSIS gets dice coefficient of 0.894, the upper bound of\nperformance for WSIS. As can be seen, the proposed approach achieves\nperformance close to FSIS for ğ‘š=0 and ğ‘š=5, and slightly lower perfor-\nmance for ğ‘š=10 and ğ‘šâˆ¼ğ‘ˆ (0, 10).\n5.2. Performance comparison for ATLAS dataset\nTable 4 reports dice coefficients of the proposed approach super-\nvised by bounding boxes at different precision levels for the ATLAS\ndataset. As can be noted, the proposed approach gets decreased per-\nformance for ğ‘š=5, ğ‘š=10, and ğ‘šâˆ¼ğ‘ˆ (0, 10) when compared with ğ‘š=0.\nIn particular, for ğ‘š=10, the proposed approach has dice coefficient\nof 0.418 (a 14.87% reduction in performance) when using ğ›¼-softmax\nfunction and 0.409 (a 18.69% reduction) when employing ğ›¼-quasimax\nfunction. These results show that severe or very severe errors in bound-\ning boxes could decrease the segmentation performance greatly for the\nproposed approach.\nFor comparison, Table 4 also shows results of the MIL baseline. It\ngets much lower dice coefficients when compared with the proposed\napproach at all precision levels.\nIn Table 4, we also report the results of PA and PA approaches. Both\napproaches get much lower dice coefficients when compared with the\nproposed approach at different precision levels. These results certify\nthe effectiveness of both parallel transformation based MIL and polar\ntransformation based MIL in the proposed approach.\nMoreover, Table 4 also lists the results of Deep cut and Global\nconstraint approaches reported in study [14] and the results of BBTP.\nFor Deep cut and Global constraint approaches, only the results for ğ‘š=0\nare available. The dice coefficients of these three methods are much\nlower than those of the proposed approach.\nFinally, FSIS achieves dice coefficient of 0.512, which is close to the\nresults of the proposed approach for ğ‘š=0, and much higher for ğ‘š=5,\nğ‘š=10, and ğ‘šâˆ¼ğ‘ˆ (0, 10).\n5.3. Visualization of the origin in the polar transformation\nIn the proposed approach, the origin ğ‘‚ in the polar transformation\nwas determined during training. To verify that the such automatic\nselection process indeed yields valid origin ğ‘‚ (i.e. it is located inside\nthe object in the bounding box), we show the selected origins of several\nexample images in Fig. 8. In these plots, the models of the proposed\napproach with ğ›¼-softmax function obtained at the end of each epoch\nare considered for origin selection. The selected origins are marked by\nred plus signs and the pixel-wise ground truths of segmentation are\nindicated by blue color. In Fig. 8, bounding boxes at two different\nprecision levels are considered: ğ‘š=0 denoting the use of accurate\n\nComputers in Biology and Medicine 169 (2024) 107913\n9\nJ. Wang and B. Xia\nFig. 8. Selected origins in the polar transformation, where the selected origins are\ndenoted by red plus signs and the pixel-wise ground truths of segmentation are marked\nby blue color.\nbounding boxes and ğ‘šâˆ¼ğ‘ˆ (0, 10) indicating the use of simulated real\nbounding box annotations. From Fig. 8, all selected origins are located\nin the object, verifying that the proposed approach is able to select\norigins correctly during training.\n6. Conclusion\nThis study investigates whether it is possible to maintain good seg-\nmentation performance for loose bounding box supervision. Extending\nthe previous parallel transformation based MIL, it developed an MIL\nstrategy based on polar transformation to assist image segmentation.\nMoreover, a weighted smooth maximum approximation was intro-\nduced to incorporate the observation that pixels closer to the origin\nof the polar transformation are more likely to belong to the object\nin the bounding box. In the experiments, the proposed approach was\nevaluated on two public datasets using dice coefficient for bounding\nboxes at different precision levels. The results demonstrate the superior\nperformance of the proposed approach for bounding boxes at differ-\nent precision levels and the robustness of the proposed approach for\nbounding boxes with mild and moderate errors.\nCRediT authorship contribution statement\nJuan Wang: Conceptualization, Formal analysis, Investigation,\nMethodology, Project administration, Validation, Visualization,\nWriting â€“ original draft, Writing â€“ review & editing. Bin Xia:\nConceptualization, Investigation, Project administration, Supervision,\nWriting â€“ review & editing.\nDeclaration of competing interest\nThe authors declare that they have no known competing financial\ninterests or personal relationships that could have appeared to\ninfluence the work reported in this paper.\nAcknowledgment\nThe authors would like to thank Xiaochun Pan from Shenzhen\nSiBright Co., Ltd. for conducting experiments of the BBTP approach.\nReferences\n[1] S. Minaee, Y.Y. Boykov, F. Porikli, A.J. Plaza, N. Kehtarnavaz, D. Terzopoulos,\nImage segmentation using deep learning: A survey, IEEE Trans. Pattern Anal.\nMach. Intell. (2021).\n[2] J. Wang, H. Ding, F.A. Bidgoli, B. Zhou, C. Iribarren, S. Molloi, P. Baldi, Detecting\ncardiovascular disease from mammograms with deep learning, IEEE Trans. Med.\nImaging 36 (2017) 1172â€“1181.\n[3] A. Esteva, B. Kuprel, R.A. Novoa, J. Ko, S.M. Swetter, H.M. Blau, S. Thrun,\nDermatologist-level classification of skin cancer with deep neural networks,\nNature 542 (2017) 115â€“118.\n[4] J. Wang, Y. Yang, A context-sensitive deep learning approach for mi-\ncrocalcification detection in mammograms, Pattern Recognit. 78 (2018)\n12â€“22.\n[5] J. Wang, Y. Bai, B. Xia, Simultaneous diagnosis of severity and features of\ndiabetic retinopathy in fundus photography using deep learning, IEEE J. Biomed.\nHealth Inf. 24 (2020) 3397â€“3407.\n[6] O. Ronneberger, P. Fischer, T. Brox, U-net: Convolutional networks for\nbiomedical image segmentation, in: International Conference on Medical Image\nComputing and Computer-Assisted Intervention, Springer, 2015, pp. 234â€“241.\n[7] J. Long, E. Shelhamer, T. Darrell, Fully convolutional networks for semantic\nsegmentation, in: Proceedings of the IEEE Conference on Computer Vision and\nPattern Recognition, 2015, pp. 3431â€“3440.\n[8] L.C. Chen, Y. Zhu, G. Papandreou, F. Schroff, H. Adam, Encoder-decoder with\natrous separable convolution for semantic image segmentation, in: Proceedings\nof the European Conference on Computer Vision, 2018, pp. 801â€“818.\n[9] J. Ahn, S. Cho, S. Kwak, Weakly supervised learning of instance segmentation\nwith inter-pixel relations, in: Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition, 2019, pp. 2209â€“2218.\n[10] Y. Wang, J. Zhang, M. Kan, S. Shan, X. Chen, Self-supervised equivariant atten-\ntion mechanism for weakly supervised semantic segmentation, in: Proceedings\nof the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2020,\npp. 12275â€“12284.\n[11] A. Bearman, O. Russakovsky, V. Ferrari, L. Fei-Fei, Whatâ€™s the point: Semantic\nsegmentation with point supervision, in: Proceedings of the European Conference\non Computer Vision, Springer, 2016, pp. 549â€“565.\n[12] D. Lin, J. Dai, J. Jia, K. He, J. Sun, Scribblesup: Scribble-supervised convolutional\nnetworks for semantic segmentation, in: Proceedings of the IEEE Conference on\nComputer Vision and Pattern Recognition, 2016, pp. 3159â€“3167.\n[13] M. Rajchl, M.C. Lee, O. Oktay, K. Kamnitsas, J. Passerat-Palmbach, W. Bai, M.\nDamodaram, M.A. Rutherford, J.V. Hajnal, B. Kainz, et al., Deepcut: Object seg-\nmentation from bounding box annotations using convolutional neural networks,\nIEEE Trans. Med. Imaging 36 (2016) 674â€“683.\n[14] H. Kervadec, J. Dolz, S. Wang, E. Granger, I.B. Ayed, Bounding boxes for weakly\nsupervised segmentation: Global constraints get close to full supervision, in:\nMedical Imaging with Deep Learning, PMLR, 2020, pp. 365â€“381.\n[15] C.C. Hsu, K.J. Hsu, C.C. Tsai, Y.Y. Lin, Y.Y. Chuang, Weakly supervised instance\nsegmentation using the bounding box tightness prior, Adv. Neural Inf. Process.\nSyst. 32 (2019) 6586â€“6597.\n[16] J. Wang, B. Xia, Bounding box tightness prior for weakly supervised image\nsegmentation, in: International Conference on Medical Image Computing and\nComputer-Assisted Intervention, Springer, 2021, pp. 526â€“536.\n[17] J. Wang, B. Xia, CDRNet: Accurate cup-to-disc ratio measurement with\ntight bounding box supervision in fundus photography using deep learning,\nMultimedia Tools Appl. (2022) 1â€“23.\n[18] M.A. Carbonneau, V. Cheplygina, E. Granger, G. Gagnon, Multiple instance\nlearning: A survey of problem characteristics and applications, Pattern Recognit.\n77 (2018) 329â€“353.\n[19] T.Y. Ross, G. DollÃ¡r, Focal loss for dense object detection, in: Proceedings of\nthe IEEE Conference on Computer Vision and Pattern Recognition, 2017, pp.\n2980â€“2988.\n[20] M. Lange, D. ZÃ¼hlke, O. Holz, T. Villmann, S.G. Mittweida, Applications of\nlp-norms and their smooth approximations for gradient based learning vector\nquantization, in: ESANN, 2014, pp. 271â€“276.\n\nComputers in Biology and Medicine 169 (2024) 107913\n10\nJ. Wang and B. Xia\n[21] G. Litjens, R. Toth, W. van de Ven, C. Hoeks, S. Kerkstra, B. van Ginneken,\nG. Vincent, G. Guillard, N. Birbeck, J. Zhang, et al., Evaluation of prostate\nsegmentation algorithms for mri: The promise12 challenge, Med. Image Anal.\n18 (2014) 359â€“373.\n[22] S.L. Liew, J.M. Anglin, N.W. Banks, M. Sondag, K.L. Ito, H. Kim, J. Chan, J. Ito,\nC. Jung, N. Khoshab, et al., A large, open source dataset of stroke anatomical\nbrain images and manual lesion segmentations, Sci. Data 5 (2018) 1â€“11.\n[23] D.P. Kingma, J. Ba, Adam: A method for stochastic optimization, 2014, arXiv\npreprint arXiv:14126980.\n[24] A. Paszke, A. Chaurasia, S. Kim, E. Culurciello, Enet: A deep neural network\narchitecture for real-time semantic segmentation, 2016, arXiv preprint arXiv:\n1606.02147.",
    "version": "5.3.31"
  },
  {
    "numpages": 14,
    "numrender": 14,
    "info": {
      "PDFFormatVersion": "1.7",
      "Language": "en-US",
      "EncryptFilterName": null,
      "IsLinearized": true,
      "IsAcroFormPresent": false,
      "IsXFAPresent": false,
      "IsCollectionPresent": false,
      "IsSignaturesPresent": false,
      "Creator": "Elsevier",
      "Custom": {
        "CrossMarkDomains[1]": "elsevier.com",
        "CrossmarkMajorVersionDate": "2010-04-23",
        "CreationDate--Text": "21st February 2024",
        "ElsevierWebPDFSpecifications": "7.0",
        "robots": "noindex",
        "doi": "10.1016/j.compbiomed.2024.107988",
        "CrossMarkDomains[2]": "sciencedirect.com",
        "CrossmarkDomainExclusive": "true"
      },
      "ModDate": "D:20240221015228Z",
      "Author": "Zhixun Li",
      "Title": "SG-MIAN: Self-guided multiple information aggregation network for image-level weakly supervised skin lesion segmentation",
      "Keywords": "Image-level label,Weakly supervised segmentation,Skin lesion,Deep learning",
      "CreationDate": "D:20240221014327Z",
      "Producer": "Acrobat Distiller 8.1.0 (Windows)",
      "Subject": "Computers in Biology and Medicine, 170 (2024) 107988. doi:10.1016/j.compbiomed.2024.107988"
    },
    "metadata": {
      "crossmark:crossmarkdomains": "elsevier.comsciencedirect.com",
      "crossmark:crossmarkdomainexclusive": "true",
      "crossmark:doi": "10.1016/j.compbiomed.2024.107988",
      "crossmark:majorversiondate": "2010-04-23",
      "dc:format": "application/pdf",
      "dc:identifier": "10.1016/j.compbiomed.2024.107988",
      "dc:publisher": "Elsevier Ltd",
      "dc:description": "Computers in Biology and Medicine, 170 (2024) 107988. doi:10.1016/j.compbiomed.2024.107988",
      "dc:subject": [
        "Image-level label",
        "Weakly supervised segmentation",
        "Skin lesion",
        "Deep learning"
      ],
      "dc:title": "SG-MIAN: Self-guided multiple information aggregation network for image-level weakly supervised skin lesion segmentation",
      "dc:creator": [
        "Zhixun Li",
        "Nan Zhang",
        "Huiling Gong",
        "Ruiyun Qiu",
        "Wei Zhang"
      ],
      "jav:journal_article_version": "VoR",
      "pdf:creationdate--text": "21st February 2024",
      "pdf:producer": "Acrobat Distiller 8.1.0 (Windows)",
      "pdf:keywords": "Image-level label,Weakly supervised segmentation,Skin lesion,Deep learning",
      "pdfx:creationdate--text": "21st February 2024",
      "pdfx:crossmarkdomains": "sciencedirect.comelsevier.com",
      "pdfx:crossmarkdomainexclusive": "true",
      "pdfx:crossmarkmajorversiondate": "2010-04-23",
      "6miuqnwqomm6jytugn9r7zsnnypuolwj9ypignwj8mmmongelowz9o9eqn9iknm-sn9ytma": "",
      "pdfx:doi": "10.1016/j.compbiomed.2024.107988",
      "pdfx:robots": "noindex",
      "prism:aggregationtype": "journal",
      "prism:copyright": "Â© 2024 Elsevier Ltd. All rights reserved.",
      "prism:coverdate": "2024-03-01",
      "prism:coverdisplaydate": "1 March 2024",
      "prism:doi": "10.1016/j.compbiomed.2024.107988",
      "prism:issn": "0010-4825",
      "prism:pagerange": "107988",
      "prism:publicationname": "Computers in Biology and Medicine",
      "prism:startingpage": "107988",
      "prism:url": "https://doi.org/10.1016/j.compbiomed.2024.107988",
      "prism:volume": "170",
      "xmp:createdate": "2024-02-21T01:43:27",
      "xmp:creatortool": "Elsevier",
      "xmp:metadatadate": "2024-02-21T01:52:28",
      "xmp:modifydate": "2024-02-21T01:52:28",
      "xmpmm:documentid": "uuid:1b499bed-4ce8-4c0c-b682-0d58baae1cbe",
      "xmpmm:instanceid": "uuid:b6ed5266-03cb-4855-90c0-636283357f42",
      "xmprights:marked": "True"
    },
    "text": "Computers in Biology and Medicine 170 (2024) 107988\nAvailable online 15 January 2024\n0010-4825/Â© 2024 Elsevier Ltd. All rights reserved.\nContents lists available at ScienceDirect\nComputers in Biology and Medicine\njournal homepage: www.elsevier.com/locate/compbiomed\nSG-MIAN: Self-guided multiple information aggregation network for\nimage-level weakly supervised skin lesion segmentation\nZhixun Li, Nan Zhang, Huiling Gong, Ruiyun Qiu \nâˆ—\n, Wei Zhang\nSchool of Mathematics and Computer Sciences, Nanchang University, Nanchang, China\nA R T I C L E I N F O\nKeywords:\nImage-level label\nWeakly supervised segmentation\nSkin lesion\nDeep learning\nA B S T R A C T\nNowadays, skin disease is becoming one of the most malignant diseases that threaten peopleâ€™s health. Computer\naided diagnosis based on deep learning has become a widely used technology to assist medical professionals\nin diagnosis, and segmentation of lesion areas is one of the most important steps in it. However, traditional\nmedical image segmentation methods rely on numerous pixel-level labels for fully supervised training, and\nsuch labeling process is time-consuming and requires professional competence. In order to reduce the costs of\npixel-level labeling, we proposed a method only using image-level label to segment skin lesion areas. Due to\nthe lack of lesionâ€™s spatial and intensity information in image-level labels, and the wide distribution range of\nirregular shape and different texture on skin lesions, the algorithm must pay great attention to the automatic\nlesion localization and perception of lesion boundary. In this paper, we proposed a Self-Guided Multiple\nInformation Aggregation Network (SG-MIAN). Our backbone network MIAN utilizes the Multiple Spatial\nPerceptron (MSP) solely using classification information as guidance to discriminate the key classification\nfeatures of lesion areas, and thereby performing more accurate localization and activation of lesion areas.\nAdditionally, adjunct to MSP, we also proposed an Auxiliary Activation Structure (AAS) and two auxiliary loss\nfunctions to further self-guided boundary correction, achieving the goal of accurate boundary activation. To\nverify the effectiveness of the proposed method, we conducted extensive experiments using the HAM10000\ndataset and the ğ‘ƒ ğ»\n2\ndataset, which demonstrated superior performance compared to most existing weakly\nsupervised segmentation methods.\n1. Introduction\nWith the increasing mortality of skin cancer annually, it is imper-\native to develop an efficient and accurate diagnosis tool [1]. Recent\nstudies have shown that deep learning technology provides a new\nbreakthrough in skin cancer diagnosis. Deep learning algorithms pos-\nsess powerful image recognition and analysis capabilities that can\nautomatically extract valuable information from skin cancer images. By\nidentifying these features, the accuracy and efficiency of skin cancer\ndiagnosis have been significantly improved [2].\nSkin lesion segmentation plays a crucial role in computer-aided\ndiagnosis, and it is still a challenging task. Although significant progress\nhas been made in skin lesion segmentation using fully supervised meth-\nods [3â€“10], there are many unresolved issues. The primary challenge\nis the requirement with a large amount of pixel-level labeled data\nfor network training, which demands significant human and profes-\nsional resources. Additionally, skin lesion images often contain nu-\nmerous interfering factors, such as hairs and contaminants, which\nâˆ— \nCorresponding author.\nE-mail address: qryun1228@qq.com (R. Qiu).\nalso pose significant challenges to the precise labeling as well. Fortu-\nnately, image-level weakly supervised segmentation methods provide\nus with an effective pathway to reduce labeling costs. They only\nrequire image-level labels to train segmentation models. The difference\nof segmentation schemes between image-level annotations and pixel-\nlevel annotations is illustrated in Fig. 1. Therefore, studying a weakly\nsupervised based segmentation method has important and practical sig-\nnificance, thus opening up new avenues for computer-aided diagnosis\non skin lesions.\nThe segmentation methods using image-level labels have been\nwidely studied for natural images in recent years. Typically, weakly\nsupervised segmentation and fully supervised segmentation both in-\nvolve two stages: target localization and edge segmentation. Under\nthe weak supervision, target localization mainly relies on training a\nnetwork for target classification, and then generating a class activation\nmap (CAM) for segmentation. Based on the CAM, a threshold is applied\nto segment the finally appropriate result. However, many studies have\nhttps://doi.org/10.1016/j.compbiomed.2024.107988\nReceived 5 October 2023; Received in revised form 11 December 2023; Accepted 13 January 2024\n\nComputers in Biology and Medicine 170 (2024) 107988\n2\nZ. Li et al.\nFig. 1. The difference of segmentation schemes between image-level annotations and pixel-level annotations: (a) illustrates the fully supervised segmentation process using pixel-level\nlabels, which is trained using pixel-level labels that require labor-intensive labeling. (b) illustrates the weakly supervised segmentation process using image-level labels, where the\npixel-level labels used in (a) are replaced by easily accessible image-level labels (classification labels). Zoom in to view more clearly.\npointed out that although convolutional neural networks (CNNs) per-\nform well on localization tasks, they do not have the ability to explicitly\nperceive targetâ€™s boundaries [11]. Therefore, directly thresholding the\nactivation regions generated by CAM cannot ensure segmentation\naccuracy. To address this issue, many methods locate the target by\nfinding a seed area and then use various methods to discriminate\ntheir boundaries [12â€“15]. And other methods have been proposed to\nfirstly accurately locate the seed area, and then additional areas are\nsupplemented using complementary approaches, ultimately activating\nthe whole target area [16â€“20]. Furthermore, some researchers have\nalso proposed methods that use consistency between scales in network\ntraining to learn the complete boundary information [21]. In order to\novercome the issue of rough activation in CNN, Gao et al. [22], Xu\net al. [23] have proposed weakly supervised segmentation methods that\nleverage the long-distance information perception ability based on the\nTransformer architecture.\nDespite the significant progress has been made by weakly supervised\nsegmentation methods in the field of natural images, there are still\nmany challenges and issues in the application of skin lesion images [20,\n24â€“29]. Firstly, skin lesion images often accompany by diverse shapes\nand complex textures, and they are often obscured by hairs or contam-\ninants, which make them difficult to effectively and accurately activate\nthe lesion areas under weak labels. In other words, these methods\nmay create bias or error in the location or area of the activation\nregion. Secondly, the boundaries of skin lesions are often blurry, yet\nthe activation intensity of gradient regions is a key step for subsequent\nthreshold segmentation. To achieve superior results, it is necessary\nto ensure high-class activation in the edge regions. However, existing\nmethods have many shortcomings in terms of localization or edge\nactivation.\nTo address the aforementioned issues, we propose a novel Self-\nGuided Multiple Information Aggregation Network (SG-MIAN) for skin\nlesion segmentation tasks, which only requires image-level labels but\ncan achieve accurate segmentation. SG-MIAN can capture more com-\nprehensive pixel-level correlation information using Multiple Spatial\nPerceptron (MSP) at several scales through the classification features,\nand aggregate these features to accurately enhance the lesion areas and\nobtain not bad preliminary activation map of the lesion. Additionally,\nwe propose an Auxiliary Activation Structure (AAS) to extract shallow\ntexture information from the image to compensate the deep spatial\nfeatures on deeper layers. Finally, we use an Activation Guided Loss\n(AGL) and a Boundary Control Loss (BCL) to collaboratively fix the\nactivation area on lesion boundary for eventually obtaining accurate\nactivation of the whole lesion area. The key contributions of our\nmethod are summarized below:\n1. We propose a novel Self-Guided Multiple Information Aggrega-\ntion Network (SG-MIAN) based solely on image-level labels for weakly\nsupervised segmentation of skin cancer lesions. Our method only uti-\nlizes classification information from image labels to guide the net-\nwork training for accurate segmentation of skin lesions. The proposed\nnetwork can significantly reduce the annotation costs of skin cancer\ndatasets by doctors.\n2. In terms of lesion localization, we propose a Multiple Spatial\nPerceptron (MSP) that can deeply mine classification information and\naggregate contextual relationships between distant pixels and neigh-\nboring pixels through the Global Information Aggregation Unit (GIAU)\nand the Local Information Aggregation Unit (LIAU). The combination of\nthese two units enhances the classification networkâ€™s ability to discrim-\ninate lesionâ€™s key features, thereby enabling more accurate localization\nand activation of lesion areas.\n3. In terms of lesion boundary activation, we propose an Auxil-\niary Activation Structure (AAS) and two auxiliary loss functions. The\nauxiliary activation structure further extracts boundary information\nfrom shallow features adjunct to MSP. And the Activation Guided Loss\n(AGL) and Boundary Control Loss (BCL) can enable mutual learning\nand adaptive correction of edge feature information at different scales,\nmaking precise activation of lesion boundaries.\n4. To verify the effectiveness of the proposed method, we con-\nducted experiments on the HAM10000 [30,31] and ğ‘ƒ ğ»\n2 \n[32] datasets.\nExperimental results demonstrate that the proposed method has signif-\nicant advantages in skin lesion segmentation tasks compared to other\nmethods. Additionally, we conducted extensive ablation experiments to\nvalidate the effectiveness of the proposed modules.\nIn the following sections, Section 2 reviews the related work. Sec-\ntion 3 presents the proposed method in detail. Sections 4 and 5 describe\n\nComputers in Biology and Medicine 170 (2024) 107988\n3\nZ. Li et al.\nthe experimental results and discussion. In Section 6, we conclude our\nmethod and future works.\n2. Related work\n2.1. Fully supervised medical image segmentation\nWith the continuous development of deep learning technology,\nimage processing methods based on deep learning have been receiving\nincreasing attention, especially in the field of medical imaging. Among\nthese methods, the Full Convolutional Network (FCN) is one of the early\ndeep learning-based segmentation approaches that demonstrates the\nability of deep learning networks to perform end-to-end segmentation\nat different scales [33]. Inspired by FCN, the U-Net has made great im-\nprovements by introducing a symmetric encoderâ€“decoder architecture\nfor effective small-sample medical image feature extraction, and skip\nconnections are used to connect the encoder and decoder to mitigate\ninformation loss caused by downsampling [34]. Owing to its unique\nstructure, U-Net has demonstrated excellent performance in medical\nimage segmentation.\nMany subsequent medical image segmentation methods have been\ndeveloped based on U-Net, such as U-Net++, U-Net3+, Attention U-Net,\nRes-U-Net, R2U-Net, and TransUNet [35â€“40]. These methods mainly\nfocus on improving the skip connections of the U-Net structure, and\nsome also modify the encoderâ€“decoder blocks. For example, TransUNet\nreplaces the encoder structure with a multi-head attention (MHSA)\nmodule [40]. PraNet is based on the deep feature gradual acquisition\nof the U-Netâ€™s U-shape structure [41]. KiU-Net draws from U-Netâ€™s\nskip connections in its network design [42], and DoubleU-Net directly\nimproves and combines double U-Net structures [43].\nIn addition, the proposal of Vision Transformer (ViT) has real-\nized the application of Transformer architecture in visual tasks [44],\nand SETR based on ViT has shown promising results when it ap-\nplied to segmentation tasks [45]. TransUNet, which mentioned ear-\nlier, brought Transformer architecture to medical segmentation tasks\nand also achieved good segmentation results. Other methods such as\nTransFuse, MCTrans, and MedT have applied ViT to medical image\nsegmentation tasks as well [46â€“48].\nAlthough many medical image segmentation methods have achieved\ngood segmentation of lesions, they mainly adopt fully supervised\ntraining methods. These methods require numerous pixel-level an-\nnotations, which is difficult to obtain and leads to high annotation\ncosts. Moreover, the medical image data is professional, which further\nincreases the difficulty of annotation and severely limits the pop-\nularization of medical image segmentation applications. Even with\nsemi-supervised segmentation methods, their effects are still limited by\naccurate annotations.\n2.2. Image-level weakly supervised medical image segmentation\nBefore the advent of the class activation map (CAM) algorithm,\nresearchers had tried various weakly supervised segmentation schemes\nbased on image-level labels. For example, the STC method is a simple\nto complex framework that progressively trains convolutional neural\nnetworks (CNNS) through three stages so that they can segment targets\nbased only on the image-level label [49]. Besides, Chen et al. [50]\nprovided a novel semi-supervised solution to overcome the challenges\nof fully supervised dataset labeling. Currently, most of the existing\nweakly supervised segmentation methods rely on the use of CAM [11].\nHowever, since feature mapping cannot locate features based on spatial\ninformation, CNN network can only locate targets roughly based on\nimage-level labels. For example, traditional classification networks such\nas VGG and ResNet often lack spatial perception and are unable to\nactivate complete lesions [51,52]. In order to solve this problem,\nsubsequent researchers have proposed various solutions. The erasing\nmethods, they try to activate sub-significant areas and merging them\nto the significant area gradually, finally to activate the complete area,\nsuch as AE-PSL, ACoL and AMR [16,17,19]. These methods focus on\nroughly activating the complete area, but they cannot precisely activate\nthe lesion boundaries. Another methods have been proposed to obtain\nthe highly confident seed area through the classification information,\nand gradually expand them, making lesion area fully activated to the\nend [12â€“15]. It is noticed that the SIPE method obtains the seed area\nwith high confidence through IoU, and then uses ISCAM to guide\nthe expansion processing. This method provides a new perspective for\nsubsequent research, but seed expansion is still a challenging task,\nbecause it is impossible to determine whether the expanded pixels are\ncorrect, and it may be misled by noise.\nAfter the proposal of Vision Transformer (ViT), researchers have\nexplored a series of weakly supervised segmentation methods based on\nit [22,23,25]. These methods have validated the effectiveness of Trans-\nformer architecture in segmentation tasks and further demonstrated\nthe tremendous potential of it in weakly supervised segmentation.\nHowever, most of the methods based on ViT face the problem of class\nco-occurrence phenomena. In order to solve this problem, MCTformer\nwas proposed [23]. In addition, DANet proposed a new idea to let\nthe network detect the activation areas of lesions under different con-\nstraints, and then achieve the complete activation of lesions [18]. Wang\net al. [24] found that different scales of input images can lead to\ndifferences in activation ranges of lesions, so they proposed SEAM to\ninput two different scales of images into a coupled network for mutual\nguidance, and ultimately learn the complete activation of lesions. ASDT\nborrowed the idea of knowledge distillation and created an alternative\nstudent branch to transfer segmentation features and localization fea-\ntures to learn the response of lesions under different branches [20].\nL2G trained both the global network and local network on the complete\nimage and chunked image, and then used the extracted local saliency\nimage to supervise the global network [21].\nThese weakly supervised methods are almost based on pre-trained\nVGG, ResNet, and ViT as the backbone networks. However, traditional\nCNN networks lack the ability to perceive boundary features of lesions,\nwhile ViT lacks sensitivity to class differences. Therefore, these meth-\nods have to use complex external techniques to compensate for the\nshortcomings of their weak supervision segmentation capabilities.\n2.3. Skin lesion image segmentation\nThe task of skin lesion segmentation is still a challenging task.\nThey mainly face problems such as similar textures, blurred boundaries,\nand contaminant occlusion, which can directly lead to a decrease\nin segmentation performance. Many researchers have designed and\nproposed many innovative methods specifically for skin lesion char-\nacteristics. Schaefer et al. [53] found that preprocessing steps such\nas color information and image contrast enhancement would play\nkey roles in improving lesion segmentation accuracy. Peruch et al.\n[3] proposed a unique threshold method that simulates the cognitive\nprocess of dermatologists to achieve lesion segmentation. This method\nprovides a new perspective and idea for skin lesion segmentation. Jafari\net al. [4] successfully extracted the contextual information of skin\nlesions using different scales of sliding windows. This method signif-\nicantly improves the detection accuracy of lesions, but it also faces the\nproblem of high computational complexity. Yuan et al. [5] proposed\na fully automated method for lesion segmentation using an encoderâ€“\ndecoder network structure with 19 layers of DCNN. To address the\nproblem of class imbalance, they proposed a new loss function based on\nJaccard distance. Li et al. [6] proposed an innovative DDNet based on\nRefineNet by fusing local and global contextual information to obtain\nhigh-resolution predictive areas. Ã–ztÃ¼rk and Ã–zkaya [54] proposed an\nimproved FCN architecture. They introduced residual structures into\nthe FCN architecture to better utilize spatial information to support\nlesion edge recognition. Xie et al. [55] designed a trinary branch\nstructure method that extracts robust features with detailed spatial\n\nComputers in Biology and Medicine 170 (2024) 107988\n4\nZ. Li et al.\nFig. 2. The flowchart of the proposed Self-Guided Multiple Information Aggregation Network (SG-MIAN). Zoom in to view more clearly.\ninformation by fusing the outputs of each branch to achieve accurate\nlesion boundary extraction.\nSo far, the fully supervised methods are still widely used in skin\nlesion segmentation, and weakly supervised methods face a great chal-\nlenge because it is carried out under the supervision of image-level\nlabels and lacks spatial and intensity information.\n3. Methodology\nIn this section, we extensively present the proposed Self-Guided\nMultiple Information Aggregation Network (SG-MIAN) for segmenting\nskin lesion images only using image-level labels. This network mainly\nconsists of the multiple information aggregation network (MIAN) as\nthe backbone network and the auxiliary activation structure (AAS),\nas well as three loss functions including classification loss, activation\nguidance loss (AGL), and boundary control loss (ACL). The flowchart\nof the proposed SG-MIAN is shown in Fig. 2.\n3.1. Multiple information aggregation network\nThe overall architecture of the backbone network MIAN consists\nof four blocks, each of which is composed of several multiple spatial\nperceptrons (MSPs) and convolution layers. The specific structure of\nMSP is shown in Fig. 3. In each block, we use a patch embedding layer\nconsisting of convolutions to achieve a 2Ã— downsampling of the feature\nmaps. After being processed by the patch embedding layer, we use\nseveral MSPs to extract features from the inputs and ensure consistent\ninputâ€“output between each MSP layers. Inside MSP, it includes Local\nInformation Aggregation Unit (LIAU) and Global Information Aggrega-\ntion Unit (GIAU), which can fully perceive and accurately perceive the\nrelationship between their neighboring and global pixels. In addition,\nwe also perform feature fusion through Feature Integration Unit (FIU).\nFinally, a classifier is adopted by consisting of convolution layers\nto provide classification information, and following the classification\nlayer, the feature maps are fed into a Global Average Pooling (GAP)\nlayer to obtain classification scores. Regarding input images, inspired\nby the SEAM [24], the transformed image is achieved from the original\nimage ğ‘‹ âˆˆ ğ‘…\nğ»Ã—ğ‘Š Ã—3 \nto ğ‘‹\nâ€² \nâˆˆ ğ‘… \nğ»\n2 \nÃ— \nğ‘Š\n2 \nÃ—3 \nand both of them are parallelly\nfed into the network for training. The transformation is denoted by\nğ‘‹\nâ€² \n= ğ´(ğ‘‹).\n3.1.1. Global information aggregation unit (GIAU)\nTo enable aggregation of global pixel-level contextual relationships\nfor perceiving complete boundary, the GIAU is designed based on\nTransformer architecture. Before the attention structure, we add max\npooling for sparse sampling, and perform a TransConv layer after\nmulti layer perceptron (MLP) to recover original scale. This unit is\nlightweight yet effective in perception of lesions. The sparse multi-head\nattention and the output ğºğ‘‹ of GIAU are defined by the following\nformulas:\nğ‘„, ğ¾, ğ‘‰ = ğ¿ğ‘–ğ‘›ğ‘’ğ‘ğ‘Ÿ(ğ‘€ğ‘ğ‘¥ğ‘ğ‘œğ‘œğ‘™\nğ‘Ÿ\n(â‹…))\nğ¹\nğ´ğ‘¡ğ‘¡ğ‘’ğ‘› \n= ğ‘†ğ‘œğ‘“ ğ‘¡ğ‘šğ‘ğ‘¥( \nğ‘„ğ¾\nğ‘‡\nâˆš\nğ‘‘\nğ‘˜\n)ğ‘‰\nğºğ‘‹ = ğ‘‡ ğ‘Ÿğ‘ğ‘›ğ‘ ğ¶ğ‘œğ‘›ğ‘£(ğ‘€ğ¿ğ‘ƒ (ğ¹\nğ´ğ‘¡ğ‘¡ğ‘’ğ‘›\n))\n(1)\nwhere the (â‹…) denotes their preorder input. The ğ‘Ÿ is the sparse rate,\nwhich controls the degree of sparsity in multi-head attention and ğ‘Ÿ = 1\ncorresponds to the original multi-head attention. The adoption of sparse\nstructure not only reduces the computational complexity of the original\nmulti-head attention but also allows for more aggregation effect around\neach pixel, making the semantic representation of each pixel more\nprominent.\n3.1.2. Local information aggregation unit (LIAU)\nLIAU is a unit based on spatial feature aggregation, which ef-\nfectively captures local spatial features by searching for correlated\ninformation among neighboring pixels within a small region. This unit\ncan effectively capture the boundary details of lesions and control the\nactivation area to be closer to the actual shape of the lesion itself. The\noutput ğ¿ğ‘‹ of LIAU is defined as follows:\nğ¿ğ‘‹ = ğ¶ğ‘œğ‘›ğ‘£(ğ¹ (ğ¶ğ‘œğ‘›ğ‘£(â‹…))) + (â‹…) \n(2)\nwhere ğ¹ (â‹…) = ğ·ğ‘Š ğ¶ğ‘œğ‘›ğ‘£\nğ‘Ÿ1\n(â‹…) + ğ·ğ‘Š ğ¶ğ‘œğ‘›ğ‘£\nğ‘Ÿ3\n(â‹…) + ğ·ğ‘Š ğ¶ğ‘œğ‘›ğ‘£\nğ‘Ÿ5\n(â‹…). The (â‹…)\ndenotes their preorder input. In the LIAU, BatchNorm and GeLu are\n\nComputers in Biology and Medicine 170 (2024) 107988\n5\nZ. Li et al.\nFig. 3. The structure of MSP. MSP is consist of LIAU, GIAU and FIU. Zoom in to view more clearly.\nFig. 4. The structure of the Auxiliary Activation Structure (AAS). Zoom in to view more clearly.\nomitted, and in the DWConv convolution layers, ğ‘Ÿ\nğ‘– \nrepresents the\ndilation rate ğ‘–. By using the DWConv convolution layer with multi-scale\nreceptive fields, we can effectively aggregate pixel information from the\nsurrounding area, thereby helping us identify lesion boundaries more\naccurately. Additionally, this unit introduces a skip connection similar\nto the classical residual network, resulting in enhancing the ability of\ngradient cross-layer propagation.\n3.1.3. Feature integration unit (FIU)\nWhen both the GIAU and LIAU units are trained together in the\nnetwork, they may generate some conflicting feature maps. Therefore,\nwe need to design an effective fusion mechanism to address this issue.\nFIU is thus defined as follows:\nğ¹ ğ¼ğ‘ˆ (ğ¿ğ‘‹, ğºğ‘‹) = ğ¶ğ‘œğ‘›ğ‘£(ğ¹ (ğ¶ğ‘œğ‘›ğ‘£(ğ¿ğ‘‹ + ğºğ‘‹))) + (ğ¿ğ‘‹ + ğºğ‘‹) \n(3)\nwhere ğ¹ (â‹…) = ğ·ğ‘Š ğ¶ğ‘œğ‘›ğ‘£(â‹…) + (â‹…), (â‹…) denotes their preorder input. ğ¿ğ‘‹ and\nğºğ‘‹ represent the feature maps output by LIAU and GIAU, respectively.\nSimilar to LIAU, we use DWConv convolution to fuse feature maps and\nemploy skip connections to enhance the ability of gradient propagation\nacross layers.\n3.2. Auxiliary activation structure\nThrough the MIAN, it is possible to conduct preliminary activation\nof lesion areas. Although the proposed backbone network can achieve\ncomplete activation of lesions, its accuracy still needs to be improved.\nWe have found that this is mainly caused by the process of hierarchical\ndownsampling, the original texture information of the input image is\nlost, making it difficult for some lesion areas to be effectively activated.\nTo address this issue, an auxiliary activate structure is proposed and\nadded to extract shallow texture information to further enhance the\nperception ability of lesion areas, thereby strengthening the effect of\nthe MIAN with preliminary activation. The specific structural details\nare shown in Fig. 4. This structure can be represented by the following\nequation:\nğ¹\nğ‘“ ğ‘¢ğ‘ \n(ğ‘‹\nğ‘–\n) = ğ¶ğ‘œğ‘›ğ‘£(\n3\nâˆ‘\nğ‘–=1\nğ¶ğ‘œğ‘›ğ‘£(ğ‘‹\nğ‘–\n))\nğ¹ \nâ€²\n(ğ‘‹\n4\n, ğ‘‹\nğ‘“ ğ‘¢ğ‘ \n) = ğ¶ğ‘œğ‘›ğ‘£(ğ¶ğ‘ğ‘¡(ğ¶ğ‘œğ‘›ğ‘£(ğ‘‹\n4\n), ğ‘‹\nğ‘“ ğ‘¢ğ‘ \n))\n(4)\nIn AAS, we have omitted operations such as BatchNorm, ReLU, and\nupsampling. ğ‘‹\nğ‘“ ğ‘¢ğ‘  \nis the output of ğ¹\nğ‘“ ğ‘¢ğ‘ \n(ğ‘‹\nğ‘–\n) and serves as the input to\nğ¹ \nâ€²\n(ğ‘‹\n4\n, ğ‘‹\nğ‘“ ğ‘¢ğ‘ \n). Our MIAN has four blocks in total, and the preliminary\nactivation map of the lesion area is obtained through the fourth block.\nTherefore, we extract shallow texture information from the first three\nblocks to enrich the details of the activation map in the fourth block.\nSpecifically, as shown in Fig. 4, we added three convolutional layers\nto compress the feature maps of the first three blocks and extract\nmulti-scale feature information. Then, the multi-scale feature maps\nare adjusted to the same size and added together as fused features.\n\nComputers in Biology and Medicine 170 (2024) 107988\n6\nZ. Li et al.\nSimilarly, the feature maps for the fourth block are compressed in terms\nof channels and adjusted to the same size as the fused features. Because\nthe fused features contain rich texture information from the first three\nblocks as well as some preceding lost spatial information, they can\nprovide very richer shallow texture feature information to optimize and\nfurther refine the activation map in the fourth block, making it closer\nto the actual shape of the lesion.\n3.3. Loss functions\nIn our method, the loss function is divided to three components:\nthe classification loss, the activation guidance loss (AGL), and the\nboundary control loss (BCL). Among them, the classification loss plays\na key role in locating the lesion area, while the AGL guides the\nAAS to activate the lesion more accurately by utilizing preliminary\nactivation maps between scales. Additionally, the BCL is used to impose\nimplicit constraints on the activation maps to compensate for the lost\ninformation in boundary perception between scales. Consequently, our\nmethod can more effectively achieve precise localization and activation\nof lesion areas.\nğ‘‡ â„ğ‘’ ğ‘ğ‘™ğ‘ğ‘ ğ‘ ğ‘–ğ‘“ ğ‘–ğ‘ğ‘ğ‘¡ğ‘–ğ‘œğ‘› ğ‘™ğ‘œğ‘ ğ‘ . Using image-level labels for training can\nenable lesion localization and enhance the perception of differences\nbetween classes. We extract features using the four-block MSPs, and\nclassify the features from the last block to generate class activation\nmaps. The predicted scores ğ‘  are then obtained using Global Average\nPooling (GAP). Therefore, the binary cross-entropy loss function is\nemployed here to calculate the loss for network training, and it is\ndefined as follows.\nğ‘™\nğ‘ğ‘™ \n= âˆ’ \n1\nğ¶\nğ¶\nâˆ‘\nğ‘=1\n[ğ‘™\nğ‘ \nlog( \n1\n1 + ğ‘’\nâˆ’ğ‘ \nğ‘ \n) + (1 âˆ’ ğ‘™\nğ‘ \n) log( \nğ‘’\nâˆ’ğ‘ \nğ‘\n1 + ğ‘’\nâˆ’ğ‘ \nğ‘ \n)] (5)\nwhere ğ‘ \nğ‘ \nand ğ‘™\nğ‘ \nrepresent the predicted score and ground truth label\nof class ğ‘. As we are dealing with a dual-branch architecture similar\nto SEAM, we need to compute the classification loss function for both\nbranches. The mix classification loss is obtained by the predicted score\nğ‘ \nğ‘œ\nğ‘ \nof the original image and the predicted score ğ‘ \nğ‘¡\nğ‘ \nof the transformed\nimage as shown below:\nîˆ¸\nğ‘ğ‘™ğ‘  \n= ğ‘™\nğ‘ğ‘™ \n(ğ‘ \nğ‘œ\nğ‘ \n, ğ‘™\nğ‘ \n) + ğ‘™\nğ‘ğ‘™ \n(ğ‘ \nğ‘¡\nğ‘ \n, ğ‘™\nğ‘ \n) (6)\nğ‘‡ â„ğ‘’ ğ‘ğ‘ğ‘¡ğ‘–ğ‘£ğ‘ğ‘¡ğ‘–ğ‘œğ‘› ğ‘”ğ‘¢ğ‘–ğ‘‘ğ‘ğ‘›ğ‘ğ‘’ ğ‘™ğ‘œğ‘ ğ‘ . To avoid AAS from neglecting the\nboundary detail while fusing shallow feature information, we designed\nAGL to provide guidance clues for the secondary activation maps ğ¹ \nâ€² \nâˆˆ\nğ‘…\nğ»Ã—ğ‘Š Ã—(ğ¶+1) \nobtained by AAS. Here, we also calculated the AGL for\nimages after scale transformation. Variables with superscript ğ‘œ repre-\nsent those from the original image, while variables with superscript ğ‘¡\nrepresent those from the image after scale transformation. The AGL is\ndefined as:\nîˆ¸\nğ‘ğ‘”ğ‘™ \n= \n1\n2 \nâ€–ğ¹ \nğ‘œ \nâˆ’ ğ¹ \nğ‘œ\nâ€²\nâ€–\n1 \n+ \n1\n2 \nâ€–ğ¹ \nğ‘¡ \nâˆ’ ğ¹ \nğ‘¡\nâ€²\nâ€–\n1 \n(7)\nThe activation map transmits precise localization and boundary\ninformation to the secondary activation map ğ¹ \nâ€² \nthrough AGL. The\nsecondary activation map, which already contains rich texture infor-\nmation, accurately suppresses the non-lesional area after receiving the\nguidance information. This function helps address the issue of scatter-\ning in the secondary activation map, which tends to focus on texture\nfeatures. As a result, the activation of the lesion area becomes more\naccurate and complete.\nğ‘‡ â„ğ‘’ ğ‘ğ‘œğ‘¢ğ‘›ğ‘‘ğ‘ğ‘Ÿğ‘¦ ğ‘ğ‘œğ‘›ğ‘¡ğ‘Ÿğ‘œğ‘™ ğ‘™ğ‘œğ‘ ğ‘ . By intervening with AGL, the network\nis able to achieve good activation effects on the lesion. However, in\norder to extract latent semantic features from the network structure\nfor self-guided training, we designed the BCL. BCL learns from two\nbranches and obtains the final feature map for mutual learning, thereby\nextracting the features that cannot be obtained at respective scale. The\nBCL is defined as follows:\nîˆ¸\nğ‘ğ‘ğ‘™ \n= \n1\n2 \nâ€–ğ´(ğ¹ \nğ‘œ\n) âˆ’ ğ¹ \nğ‘¡\nâ€–\n1 \n+ \n1\n2 \nâ€–ğ´(ğ¹ \nğ‘œ\nâ€²\n) âˆ’ ğ¹ \nğ‘¡\nâ€²\nâ€–\n1 \n(8)\nwhere ğ´(â‹…) represents the scale transformation.\nIn summary, the total loss function of the proposed SG-MIAN is\ndefined as follows:\nîˆ¸ = îˆ¸\nğ‘ğ‘™ğ‘  \n+ ğ›¼îˆ¸\nğ‘ğ‘”ğ‘™ \n+ ğ›½îˆ¸\nğ‘ğ‘ğ‘™ \n(9)\nwhere ğ›¼ and ğ›½ are hyperparameters, which are both set to 1 in the\nexperiments. The classification loss is used for SG-MIAN preliminary\nlocalization and activation map, the activation guidance loss is used\nfor providing clues to guide AAS through preliminary activation map,\nthe boundary control loss is used to control the edge to prevent over-\nactivation issue.\n4. Experimental results and analysis\nIn this section, we conducted comparisons between our method\nand other weakly supervised segmentation methods to assess their\nperformances. In addition, we performed extensive ablation experi-\nments on various structures proposed in our method to evaluate their\neffectiveness.\n4.1. Datasets and preprocessing\nWe assessed the generalization performance of our method by con-\nducting experiments on the HAM10000 [30,31] and ğ‘ƒ ğ»\n2 \n[32] datasets.\nFor the HAM10000 dataset, this dataset is specifically designed for\nneural network training to automatically diagnose skin lesions. It con-\nsists of 10015 images of different ethnicities with 7 categories, includ-\ning elanoma (MEL, 11.1%), nevus (NV, 66.9%), basal cell carcinoma\n(BCC, 5.1%), actinic keratosis or Bowenâ€™s disease (AKIEC 3.3%), benign\nkeratosis (sun spots, seborrheic keratosis, or lichen planus kerato-\nsis, BKL, 11%), dermatofibroma (DF, 1.1%), and vasculopathy (VASC,\n1.4%). This dataset is provided for the ISIC2018 challenge for task\n3. However, since the original dataset was used as a classification\ntask dataset in ISIC2018, it does not include pixel-level labels. To\nevaluate the performance of each method on this dataset, we used\nPhilipp Tschandlâ€™s developed pixel-level labels as segmentation masks.\nThe segmentation masks for HAM10000 were originally obtained using\na pre-trained FCN segmentation model and then reviewed by dermatol-\nogists. The FIJI tool was eventually used to correct any inaccuracies in\nthe automatic segmentation.\nFor the ğ‘ƒ ğ»\n2 \ndataset, this dataset was obtained from Dermatology\nDepartment of Hospital Pedro Hispano in Matosinhos, Portugal. It\nconsists of 200 dermoscopic images of melanocytic lesions, captured\nusing the Tuebinger Mole Analyzer system at a magnification of 20\ntimes. These image pixels are 768 Ã— 560. There are 80 ordinary nevi,\n80 atypical nevi, and 40 melanomas in the images. During training,\nwe divided them into 3 categories based on the lesion type. The ğ‘ƒ ğ»\n2\ndataset comes with pixel-level labels.\nTo ensure fair and consistent preprocessing, we applied the same\npreprocessing steps to both the HAM10000 and ğ‘ƒ ğ»\n2 \ndatasets. Specif-\nically, we divided the images in both datasets into a training set,\nvalidation set, and test set with a 60%:20%:20% ratio. For HAM10000\ndataset, its training set contains 6009 images, validation set and test\nset contain 2003 images, respectively. For ğ‘ƒ ğ»\n2 \ndataset, its training set\ncontains 160 images, validation set and test set containing 40 images,\nrespectively.\nAdditionally, due to the small number of images in the ğ‘ƒ ğ»\n2 \ndataset,\nwe augmented the 160 training images to obtain 1280 images by 8\ntimes through data augmentation. Specifically, we achieved this by\nhorizontally flipping the original images and rotating them and original\nimages clockwise by 90\nâ—¦\n, 180\nâ—¦\n, and 270\nâ—¦\n. This processing can help\nincrease the modelâ€™s generalization ability and prevent overfitting.\nFinally, to ensure data normalization, we resized all images based\non their short edges to (h, 224) or (224, w), and then center-cropped\nthem to make them in (224, 224) pixels. These preprocessing opera-\ntions were applied to both the HAM10000 and ğ‘ƒ ğ»\n2 \ndatasets.\nThe preprocessed images were then directly input into the network\nfor training without any additional preprocessing.\n\nComputers in Biology and Medicine 170 (2024) 107988\n7\nZ. Li et al.\n4.2. Evaluation metrics\nThe performance of this work is evaluated using three metrics:\nIoU, Dice, and CPA. These metrics are used to measure the similarity\nbetween the lesion area and the ground truth, as well as the pixel\naccuracy. To represent the evaluations of the entire dataset, we average\nthe metrics across different classes. These metrics allow for a fair and\nreasonable evaluation of the performance. The metrics are calculated\nas follows:\nğ¼ğ‘œğ‘ˆ = \nğ‘‡ ğ‘ƒ\nğ‘‡ ğ‘ƒ + ğ¹ ğ‘ƒ + ğ¹ ğ‘ \n(10)\nğ·ğ‘–ğ‘ğ‘’ = \n2ğ‘‡ ğ‘ƒ\n2ğ‘‡ ğ‘ƒ + ğ¹ ğ‘ƒ + ğ¹ ğ‘ \n(11)\nğ¶ğ‘ƒ ğ´ = \nğ‘‡ ğ‘ƒ + ğ‘‡ ğ‘\nğ‘‡ ğ‘ƒ + ğ¹ ğ‘ \n(12)\nwhere TP, TN, FP, and FN stand for true positive, true negative, false\npositive, and false negative, respectively. True positive represents the\nnumber of pixels that both the actual and predicted classes are positive.\nTrue negative represents the number of pixels that both the actual and\npredicted classes are negative. False positive represents the number\nof pixels that the actual class is negative, but the predicted class is\npositive. False negative represents the number of pixels that the actual\nclass is positive, but the predicted class is negative. MPA metric in this\npaper is the mean value of CPA in all classes.\n4.3. Implementation details\nIn pre-processing step, all images are resized to a size of 224 Ã— 224,\nand are applied the scaling technique on four different scales [0.5, 1,\n1.5, 2]. Then we averaged the results obtained from these four scales\nas the final results. All of our experiments were conducted on NVIDIA\nRTX 3060 (12G) graphics card. The proposed method is implemented\nby using Pytorch and an Adam optimizer with a learning rate of 5e-4.\nThe batch size is set to 16.\n4.4. Comparative experiments\nIn this section, we compare the proposed method with other weakly\nsupervised segmentation methods, including ACoL [17], SEAM [24],\nAMR [19], TS-CAM [22], MCTformer [23], AFA [25], and SIPE [15].\nAll of these methods rely solely on image-level labels for supervision.\nAdditionally, the segmentation results generated by all methods are\nnot subject to any post-processing tools. CAM is used to extract the\nsegmentation results, with a threshold range of 0.1â€“0.9. All evaluations\nare conducted using the same metrics.\nAccording to the results presented in Table 1, our method out-\nperformed other methods in terms of mIoU and Dice with a score\nof 72.09% and 81.47%. These results demonstrate that our method\nis capable of achieving superior segmentation performance on the\nHAM10000 dataset. Specifically, because skin cancer lesions are often\nthe massive areas in the image, anti-erasing methods cannot perform\nwell on this dataset, such as ACoL and AMR methods. This is because\nthese methods supplement the preliminary activation area by find-\ning the sub-significant activation area step by step, so it is difficult\nto adapt to the segmentation task in such scenarios. On the other\nhand, the Transformer-based methods can accurately get lesion bound-\nary information due to its advantages of long distance information\nperception. However, for some data with unclear boundaries and seri-\nous occlusions, their performance may not be satisfactory. Moreover,\nthese methods also have some shortcomings in achieving fine edge\ninformation, which will have a certain impact on the overall metric\nvalues. Among the comparative methods, TS-CAM performed the best,\nachieving an overall mIoU score of 67.51%. Conversely, MCTformer\nand AFA were found to be more severely affected by occlusions in the\nexperiments. SEAM addresses the local activation problem in CNNs by\nTable 1\nThe quantitative comparison of the methods on the dataset HAM10000.\nMethod Year mIoU Dice MPA\nACoL 2018 46.45 60.66 54.96\nSEAM 2020 69.72 78.88 78.79\nAMR 2021 51.06 65.83 62.88\nTS-CAM 2021 67.51 79.21 91.67\nMCTformer 2022 54.01 64.43 65.04\nAFA 2022 52.84 64.79 77.34\nOurs 2023 74.33 83.10 86.92\ninputting images of different scales into the network for activation.\nAdditionally, it used the PCM (Pixel Correlation Matrix) structure to\nextract the lesion boundary simultaneously. In comparative experi-\nments, SEAM performed the second best. However, SEAM is sensitive\nto redundant elements and tends to over-activating for images with\nunclear boundaries.\nBy using MSP, our method can extract the relationship between the\nnear and far pixels of the lesion boundary and construct a preliminary\nactivation map through these multi-layer structures. Compared with\nordinary CNN and ViT based networks, our activation maps contain\nmore semantic information, which can provide more valuable features\nfor subsequent processing. In addition, with the guiding role of AAS and\nits ability to integrate multi-layer structural features, we can achieve\nprecise activation of the lesion. In summary, our structure is very\nhelpful in finding pixels that are closely related to the lesion, as well\nas can reduce the influence of interference factors such as hairs on the\nresults, thus improving the segmentation performance. So the proposed\nmethod can solve the shortcomings of the previous method skillfully\nand achieves excellent results.\nMeanwhile, we present the IoU values for different classes of images\nin Table 2. The proposed method achieves best on MEL, NV, BKL and\nDF, SEAM performs best on AKIEC and VASC, and TS-CAM achieves\nbest on BCC. While SEAM and TS-CAM have achieved the best results\non other classes, our method is also very close to the best values in these\nclasses, indicating that our method has excellent weakly supervised\nsegmentation performance across all classes.\nIn addition, we show the segmentation results of different algo-\nrithms for five types of lesions in Fig. 5.\nThe first image represents the simplest image in the dataset, char-\nacterized by clear and simple boundaries, regular lesion shapes, and\nmarked color differences between lesions and normal skin. For these\ntypes of images, all methods perform fairly well, but ours can perform\nthe best.\nThe second image is an image with complex lesion boundary, which\nis often characterized by clear boundary, large color difference between\nthe lesion and normal skin, but irregular shape. Transform-based meth-\nods are good for segmentation of such images because they are sensitive\nto boundary. Our method can also efficiently perceive the boundary, so\nit can perform best on these images.\nThe third image represents an image with a small lesion. This\nimage often feature small lesion area that has a significant impact on\nnumerical results. SEAM and our method have the best accuracy for this\ntype of images. The BCL in our method allows us to accurately obtain\nboundary of small mass.\nThe fourth image shows an image that is difficult to recognize. This\nimage has very blurred boundary with no distinct edge and is similar\nin color to normal skin. For such image, our method can use AAS to\nfuse the features extracted from different perceptual layers to improve\nthe detection ability of lesion texture, and then combine the initial\nboundary information perception ability of the backbone network to\nachieve good activation. Other methods have trouble in recognizing\nthis image, either by activating too widely or by being too cautious,\nand some even have trouble in locating lesion.\nThe fifth image has significant hair occlusion, which is one of the\nchallenges found in the skin lesion image dataset. AMR, MCTformer\n\nComputers in Biology and Medicine 170 (2024) 107988\n8\nZ. Li et al.\nFig. 5. Segmentation results of the methods on the dataset HAM10000. Zoom in to view more clearly.\nTable 2\nThe quantitative comparison of the mIoU of the methods on different classes on the dataset HAM10000.\nMethod Year MEL NV BCC AKIEC BKL DF VASC\nACoL 2018 44.64 51.05 28.18 41.13 36.69 40.36 23.29\nSEAM 2020 71.19 71.10 52.63 55.62 69.17 56.94 62.96\nAMR 2021 52.82 47.36 47.35 49.38 46.38 58.12 41.92\nTS-CAM 2021 69.49 68.31 55.92 53.68 66.84 61.82 58.74\nMCTformer 2022 52.01 57.74 33.26 38.48 50.05 42.34 41.32\nAFA 2022 52.66 55.94 26.18 39.16 49.35 33.66 51.49\nOurs 2023 73.76 77.96 51.63 48.54 70.45 68.33 55.33\nTable 3\nThe quantitative comparison of the methods on the dataset ğ‘ƒ ğ»\n2 \n.\nMethod Year mIoU Dice MPA\nACoL 2018 40.26 52.51 47.62\nSEAM 2020 62.80 75.10 72.43\nAMR 2021 49.80 64.01 62.56\nTS-CAM 2021 59.50 70.97 72.58\nMCTformer 2022 60.60 71.84 76.36\nAFA 2022 53.79 66.03 63.80\nOurs 2023 76.30 85.36 89.93\nand AFA are greatly affected by this problem, while the segmenta-\ntion results by other methods are relatively stable. Among them, the\nproposed method has the boundary closest to the ground truth.\nFurthermore, we have conducted experiments on the ğ‘ƒ ğ»\n2 \ndataset,\nand the results are presented in Table 3. According to the results, our\nmethod has also achieved the best performance in terms of the mIoU,\nreaching 76.3%. Meanwhile, the proposed method has also achieved a\nDice of 85.36%. These data all validate that our method performs better\nthan other methods on the ğ‘ƒ ğ»\n2 \ndataset for weakly supervised segmen-\ntation tasks. Similar to the comparative experiments on the HAM10000\ndataset, the ACoL and AMR methods based on anti-erasing have not\nperformed well on fine-grained boundary segmentation. However, the\nTransformer-based methods have performed well. On the ğ‘ƒ ğ»\n2 \ndataset,\nSEAM still ranks behind ours, achieving an mIoU of 62.8% and a Dice\nof 75.1%.\nThe results in Table 4 provide a comparison of the mIoU values of\ndifferent methods on the ğ‘ƒ ğ»\n2 \ndataset for different classes. It can be\nTable 4\nThe quantitative comparison of the mIoU of the methods on different classes on the\ndataset ğ‘ƒ ğ»\n2 \n.\nMethod Year Common Nevi Atypical Melanomas\nACoL 2018 42.37 32.73 68.07\nSEAM 2020 50.84 47.42 68.46\nAMR 2021 37.76 42.40 60.50\nTS-CAM 2021 63.23 62.91 42.72\nMCTformer 2022 68.71 57.81 48.86\nAFA 2022 48.41 53.63 66.48\nOurs 2023 73.15 77.34 75.08\nobserved that for the Common Nevi and Atypical classes, Transformer-\nbased methods TS-CAM and MCTformer achieved an mIoU around\n60% on second level. However, for the Melanomas class, CNN-based\nmethods have shown excellent performance, particularly ACoL and\nSEAM, with mIoU values reaching around 68%. These results indicate\nthat different architectures have varying abilities to perceive different\nclasses. Our method can effectively combine the architectures of Trans-\nformer and CNN architectures, harnessing the advantages of both to\nobtain the best performance.\nAs shown in Fig. 6, on the ğ‘ƒ ğ»\n2 \ndataset, we have selected three\nimages to demonstrate the qualitative performance of various meth-\nods. Experimental results show that the AMR method performs the\nmost unstable, while the ACoL method lacks perceptive capabilities\nin boundary detection and can only achieve good segmentation for\nblob-like lesions. Other methods have shown quietly stable perfor-\nmance and can segment image boundaries accurately. However, from\n\nComputers in Biology and Medicine 170 (2024) 107988\n9\nZ. Li et al.\nFig. 6. Segmentation results of the methods on dataset ğ‘ƒ ğ»\n2 \n. Zoom in to view more clearly.\nFig. 7. Class Activation Maps obtained from each module on the dataset HAM10000. Zoom in to view more clearly.\na comprehensive perspective, our method is superior in terms of detail\nperception.\n4.5. Ablation study\nIn this section, we conducted many ablation experiments to verify\nthe effectiveness of the proposed method. Firstly, the ablation exper-\niments of different modules in the network are carried out to verify\nthe effectiveness of each module. Secondly, we compare the spatial\nperception of MIAN with other methods. Thirdly, to verify the experi-\nmental results more strictly, we analyze the segmentation performance\nvalues at different segmentation thresholds. Fourthly, we evaluated\nthe effectiveness of two loss functions AGL and BGL. To verify their\neffectiveness, we added them separately to the network and observed\ntheir impact. Finally, we compare the results of image transformation\nat different scales.\n4.5.1. Effectiveness of each module\nIn the section of effectiveness validity of modules, the results are\npresented in Table 5. Firstly, when training the dataset only using\nMIAN with classification loss, we achieved a mIoU of 62.66% and a\nDice of 74.23%. These scores indicate that our backbone network has\ngood spatial perception ability and can achieve good activation of the\nlesion only using image-level label. And then, when we added AAS to\nthe backbone network for training, the mIoU increased by 1.87% to\n64.53%, and the Dice increased by 1.97% to 76.20%. These scores show\nthat AAS can extract feature information from the first three blocks\nof the network and combine it with the last block, resulting in finer\nTable 5\nThe quantitative comparison of the effectiveness of each module on the\ndataset HAM10000.\nMIAN AAS AGL BCL mIoU Dice\nâœ“ 62.66 74.23\nâœ“ âœ“ 64.53 76.20\nâœ“ âœ“ âœ“ 70.03 79.36\nâœ“ âœ“ âœ“ âœ“ 74.33 83.10\nactivation of the lesion area. Next, we added AGL on the structure\naforementioned, the mIoU and Dice increased by 5.5% and 3.16% to\n70.03% and 79.36%, respectively. These results indicate that using the\npreliminary activation map of the backbone network to guide AAS can\neffectively improve activation effectiveness. Finally, adding BCL can\nsolve the problem of excessive activation caused by AGL, increasing\nthe mIoU and Dice to 74.33% and 83.10%, with increases of 4.3% and\n3.74%, achieving the best results.\nIn addition, we demonstrate the CAMs obtained from each module\non the dataset HAM10000 in Fig. 7. We can see that from this figure\nalthough the MIAN can only achieve preliminary activation of the\nlesion area, it has strong spatial perception ability and can respond\nto most of the lesion area. After adding AAS, the activation of the\nlesion is more complete, but due to the low confidence, there are many\nuncertain areas in the image. Many images produce obvious activation\nin the lesion area, but mostly the wrong response. After adding AGL, the\nconfidence of the network increases significantly, indicating that AGL\ncan effectively reduce the response to the error area and improve the\nperception of the lesion area. When BCL is added to form the complete\n\nComputers in Biology and Medicine 170 (2024) 107988\n10\nZ. Li et al.\nFig. 8. Class Activation Maps of different backbone networks on the dataset HAM10000. Zoom in to view more clearly.\nTable 6\nThe quantitative comparison of the spatial perception ability to perceive\nboundary information on the dataset HAM10000.\nMethod Year mIoU Dice MPA\nVGG16 2014 60.57 72.83 79.91\nResNet50 2016 44.75 57.18 58.51\nU-Net 2016 30.83 43.55 30.96\nAtt-UNet 2018 30.96 43.71 31.05\nViT 2020 31.19 44.60 37.74\nCMT 2022 39.79 54.20 57.14\nEdgeViT 2022 31.36 44.19 31.70\nMIAN (ours) 2023 64.66 75.90 77.85\nstructure of our method, the response of BCL to the boundary is more\naccurate.\n4.5.2. Comparison with CNN and transformer based methods\nSince the final segmentation performance depends on the ability of\nthe network to generate a good preliminary activation map, therefore\nwe compared our backbone network MIAN with several widely used\nclassification networks, including VGG16, ResNet50, ViT, CMT, and\nEdgeViT. In the experiments, we trained the networks using only image-\nlevel labels and extracted the last layer feature map to obtain the class\nactivation maps. We also fairly set different thresholds from 0.1 to 0.9\nto get their best segmentation result for each network.\nThe results are shown in the Table 6, we can see that our backbone\nnetwork has good basic spatial perception. Only the activation map\nobtained by the backbone network with classification loss is trained,\nand the threshold value is extracted from the segmentation results,\nmIoU can reach 64.66%, which is the highest among all comparative\nmethods. Our segmentation performance is also relatively excellent\ncompared to the fully weakly supervised segmentation method.\nAnd, among these widely used baselines, VGG16 has the best mIoU\nat 60.57%. However, due to the lack of good spatial perception ability,\nit is difficult to accurately extract the lesion boundary, and only the\nrough shape of the lesion can be activated. Furthermore, since most\nof the lesions in the dataset are mass shapes, the recognition effect of\nVGG16 network is relatively ideal. ResNet50 also achieved good results,\nwith a mIoU of 44.75%. The activation effect of some images is better\nthan those of VGG16, but some images cannot generate good activation\nimages.\nAdditionally, for the U-Net-like methods, we found that the mIoU\nand Dice of U-Net and Att-UNet are significantly lower than our net-\nwork. We believe that this is mainly because U-Net and Att-UNet are\ndesigned for fully supervised segmentation tasks, so they pay more\nattention to pixel-level classification and are particularly focused on\ntexture information extraction. However, this ability is not applicable\nin weakly supervised segmentation tasks due to the lack of pixel-level\nboundary constraints in them. And the lack would result in deficiency\nof boundary detection ability.\nFor the ViT-based approaches, their results are not satisfactory due\nto the direct reconstruction of the final patch embedding to obtain\nthe activation map. But ViT network has strong perception ability of\nboundary information, and its activation boundary is more accurate\nthan CNN network. Therefore, CMT and EdgeViT are relatively bad by\nthe addition of specific spatial perception structures and designed for\nclassification tasks. Although their values are not high, the activation\nmaps also show some spatial awareness, with some images activating\nthe area closer to its actual shape. Furthermore, it should be noted\nthat although ViT-based methods require a large amount of data for\ntraining, many methods have implemented lightweight treatments to\nmake them work well on smaller datasets.\nAccording to the analysis of Fig. 8, in general, CNN network can\nachieve better activation for massive lesion. Although it may not be\nable to accurately identify boundaries, it has excellent localization abil-\nity. Transformer-based methods can perceive the entire area, but may\nlack local information perception, so their localization ability is slightly\nworse than CNN. Our backbone network combines the advantages of\nTransformer and CNN to not only locate the lesion area very accurately,\nbut also extract the lesion boundary.\n4.5.3. Influence of number of modules on time and performance\nIn our proposed SG-MIAN, our backbone network MIAN is divided\ninto four stages, with two MSP modules in each stage, making a total\nof 8 MSP modules in the entire MIAN with a depth of [2,2,2,2]. We\nset the BatchSize to 16. For the HAM10000 dataset, the training and\nvalidation time for each epoch is approximately 3 min and 34 s; for\nthe ğ‘ƒ ğ»\n2 \ndataset, the training and validation time for each epoch is\napproximately 1 min and 10 s. This training process was completed on\nan NVIDIA RTX 3060 graphics card with 12G of memory.\nTo explore the influence between the number of modules and the\nrunning time and performance, we conducted this ablation study on\nthe HAM10000 dataset with different depths. In the experiment, we\nunified the BatchSize to 16 and maintained the four stages of the\nbackbone network (i.e., each stage contains at least one MSP module).\nWe gradually increased the network depth to verify the impact of\ndifferent numbers of MSP blocks. The lowest depth was [1,1,1,1] with\n4 MSPs, while the highest depth was [3,3,3,3] with 12 MSPs.\nAccording to the experimental results in Table 7, we can find that\nas the network depth gradually increases, the network complexity in-\ncreases, leading to an increase in network computation and the training\n\nComputers in Biology and Medicine 170 (2024) 107988\n11\nZ. Li et al.\nFig. 9. The line graphs of mIoU and Dice of the methods with different thresholds on the HAM10000. Zoom in to view more clearly.\nFig. 10. The bar graph of mIoU and Dice obtained at different scales of transformation\non the dataset HAM10000. Zoom in to view more clearly.\nTable 7\nThe quantitative comparison of the influence of number of modules on time and\nperformance on the dataset HAM10000.\nDepth Time per epoch (min) mIoU Dice MPA\n1,1,1,1 2.38 71.01 80.37 84.66\n1,1,1,2 2.57 71.51 80.92 85.12\n1,1,2,2 2.75 71.87 81.05 86.79\n1,2,2,2 3.03 72.07 81.32 85.38\n2,2,2,2 3.57 74.33 83.10 86.92\n2,2,2,3 3.72 71.60 80.71 87.41\n2,2,3,3 3.93 72.71 81.81 87.59\n2,3,3,3 4.20 72.86 81.99 85.75\n3,3,3,3 4.46 70.29 79.54 84.69\ntime per epoch. Deeper networks mean higher time costs. At the same\ntime, as the depth increases, the performance of the network structure\ngradually improves at first, with mIoU and Dice scores increasing.\nHowever, when the depth reaches [2,2,2,2], the performance is the\nbest. After that, as the depth increases, the time cost increases, but\nthe performance no longer improves, remaining at around 70%â€“72%.\nWe believe that the MSP is already a relatively complex structure, and\ncontinuously stacking MSP modules will quickly increase network com-\nplexity. When network complexity reaches a certain level, overfitting\nwill occur.\n4.5.4. Influence of thresholding value on segmentation performance\nIn the experiments, we observed that the average activation val-\nues of activation maps of different methods are different. If a fixed\nthreshold is used, the segmentation results will be unfair. Therefore,\nin order to ensure the fairness of the experiment, we calculated the\nsegmentation results of each method within the threshold range of 0.1\nto 0.9, and selected the optimal result as the final threshold of each\nnetwork. In this section, we give the evaluation scores of our methodâ€™s\nTable 8\nThe quantitative comparison of different thresholds on\nthe class activation maps on the dataset HAM10000.\nThreshold mIoU Dice\n0.1 62.57 73.57\n0.2 66.16 76.53\n0.3 68.92 78.81\n0.4 71.35 80.78\n0.5 73.27 82.31\n0.6 74.33 83.10\n0.7 73.51 82.37\n0.8 68.10 78.00\n0.9 48.45 61.17\nTable 9\nThe quantitative comparison of transformations at\ndifferent scales on the dataset HAM10000.\nScale mIoU Dice\n0.3 68.15 78.16\n0.5 74.33 83.10\n0.7 71.72 81.12\n0.9 70.56 80.73\nsegmentation results under different thresholds and compare them with\nother methods.\nAs shown in the Table 8, our method obtained the best segmentation\nresults when the threshold value is set to 0.6, mIoU was 74.33%, Dice\nwas 83.10%. In addition, it can be seen that except for the low mIoU\nwhen the threshold is set to 0.9, the mIoU values under other thresh-\nolds are distributed between 62% and 75%, and the score difference\nbetween the two intervals is about 0.2. This shows that our method\nhas good performance of lesion area localization, and generally high\nconfidence in lesion activation. We plot the values of other methods and\nour method in Fig. 9. The optimal threshold result for most methods is\nset to from 0.3 to 0.5, while our threshold result reaches the best at\n0.6, outperforming all other methods.\n4.5.5. Influence of different transformation scales of the input image\nIn order to explore the influence of different transformation scales\nof the input image on the activation result, we tested different trans-\nformation scales. The scale values are set to [0.3,0.5,0.7,0.9]. Since the\noriginal size of the input image is 224 Ã— 224, when the scale value\nis wet to 0.1, the feature map of the fourth block is too small and\nfeature extraction becomes ineffective, so we did not test the scale value\nof 0.1. According to the results shown in the Table 9, it can be seen\nthat the result is best when the transformation scale is at 0.5, and the\nscore is lowest when the scale is at 0.3. Both 0.7 and 0.9 scores are\nabove 0.3 but below 0.5, while mIoU and Dice are above 70% and\n80%, respectively. From this perspective, input images that are too\nsmall are more difficult to extract efficient features and have a greater\nnegative impact on the results. Reducing to half the original size is a\nsuitable transformation scale, which can better extract features, obtain\n\nComputers in Biology and Medicine 170 (2024) 107988\n12\nZ. Li et al.\nFig. 11. The class activation maps obtained at different scales of transformation on the dataset HAM10000. Zoom in to view more clearly.\nbetter activation results, and achieve more ideal segmentation results.\nIn Fig. 10, we output the results as a bar graph for better observation. In\nFig. 11, training results are presented at different transformation scales.\nIt can be seen that the boundary confidence is relatively low when the\nscale is at 0.3, while the scales of 0.7 and 0.9 appear to over-shrink,\nresulting in more cautious boundary determination in the first image\nand internal holes in the second image. Overall, it works best when the\nscale is at 0.5.\n5. Discussion\nWeakly supervised image segmentation is an important research di-\nrection in the field of computer vision, which has received widespread\nattention. Because in weakly supervised segmentation, we usually do\nnot have the pixel-level labels, so we cannot directly train the model for\npixel-level classification. Image level weakly supervised segmentation\nis often based on the combination of shallow feature and deep feature,\nits core idea is to classify the pixels in the image to achieve the class\nactivation map. Nevertheless, an effective CAM require a reasonable\ncombination of shallow feature and deep features. Shallow features\ncan provide rich local information and help to improve the accuracy\nof segmentation. These features are usually derived from lower-level\nConvolutional Neural Networks, which extract local features of the\nimage, such as color and texture, through a series of convolutional and\npooling operations. These features are very useful for distinguishing\nbetween different categories of targets, especially in the case of weak\nsupervision, and can be effectively utilized for segmentation. How-\never, the use of shallow features alone is not enough. This requires\nus to use the deep features to improve the discriminant ability of\nthe model. Deep features can provide more global information, as\nwell as greater discrimination. These features are often derived from\nhigher-level layers. How to efficiently map shallow features to higher\nlevels to obtain more global information through a series of operations\nis a key step. This global information can help model segmentation\nmore effectively, especially in some scenes with similar structures or\ncomplex environments. Therefore, in weakly supervised segmentation,\nwe improve the performance of the method by combining shallow\nfeatures and deep features through the MIAN and a correction by AAS\nand two auxiliary loss functions. Finally, our method can improve the\nsegmentation performance.\nAdditionally, another one of the main challenges in weakly super-\nvised segmentation is the impact of co-occurrence phenomenon. The\nco-occurrence phenomenon is mainly the co-occurrence or correlation\nof specific pixels or objects in the images in the dataset, which brings\ndifficulties to the task of image segmentation. Co-occurrence phenom-\nena include class (object) co-occurrence and pixel co-occurrence. Class\n(object) co-occurrence means that pixels belong to the same object in\nthe image area. For example, abdominal CT or MRI images may contain\nmultiple targets such as stomach, gallbladder and kidney at the same\ntime. On the other hand, pixel co-occurrence means that different lesion\nobjects in the image area have similar features. In MRI images, the\nsignal strength of the soft tissues in different organs tends to be similar.\nIn fully supervised pixel-level label segmentation, the phenomenon\nof co-occurrence is relatively easy to deal with because the consis-\ntency within regions and the differences between regions are easily\ndistinguishable at the pixel level. However, in image-level label based\nimage segmentation, the co-occurrence phenomenon is difficult to deal\nwith, mainly because the image-level label cannot obtain pixel-level\nspatial information. Traditional pixel-level labeling methods can use\npixel context information to deal with co-occurrence, but this infor-\nmation cannot be used for image-level labeling. Co-occurrence plays\nan important role in weakly supervised image segmentation, and its\ninfluence cannot be ignored. Future research should focus on finding\nmore effective methods to solve the co-occurrence phenomenon, to\nfurther develop weakly supervised image segmentation methods based\non image-level labels.\n6. Conclusion and future works\nIn this paper, we proposed a novel weakly supervised segmentation\nmethod specifically for the skin lesion tasks. By training using only\nimage-level labels, our method successfully achieved ideal pixel-level\nsegmentation results. We have successfully addressed the problem of\nweakly supervised methods performing poorly in skin lesion datasets\ndue to the occlusions and the lack of a clear boundary between lesion\nand normal skin. Experimental results on the HAM10000 dataset and\nHAM10000 dataset as well as extensive ablation experiments validate\nthe significant advantages of the proposed method. In the future,\nwe will extend this algorithm to other medical image datasets to\ndesign a more general image-level weakly supervised medical image\nsegmentation algorithm, especially for image data with co-occurrence\nproblems.\nCRediT authorship contribution statement\nZhixun Li: Conceptualization, Methodology, Supervision, Writing\nâ€“ review & editing. Nan Zhang: Methodology, Software, Writing â€“\noriginal draft. Huiling Gong: Validation, Visualization. Ruiyun Qiu:\nFormal analysis, Methodology, Writing â€“ review & editing. Wei Zhang:\nData curation, Resources, Validation.\n\nComputers in Biology and Medicine 170 (2024) 107988\n13\nZ. Li et al.\nDeclaration of competing interest\nThe authors declare that they have no known competing finan-\ncial interests or personal relationships that could have appeared to\ninfluence the work reported in this paper.\nAcknowledgment\nThis work has been supported in part by the National Natural\nScience Foundation of China 81960325.\nReferences\n[1] S. Chatterjee, D. Dey, S. Munshi, Integration of morphological preprocessing and\nfractal based feature extraction with recursive feature elimination for skin lesion\ntypes classification, Comput. Methods Programs Biomed. 178 (2019) 201â€“218,\nhttp://dx.doi.org/10.1016/j.cmpb.2019.06.018, URL https://www.sciencedirect.\ncom/science/article/pii/S016926071930238X.\n[2] M.K. Hasan, M.A. Ahamad, C.H. Yap, G. Yang, A survey, review, and fu-\nture trends of skin lesion segmentation and classification, Comput. Biol. Med.\n155 (2023) 106624, http://dx.doi.org/10.1016/j.compbiomed.2023.106624, URL\nhttps://www.sciencedirect.com/science/article/pii/S0010482523000896.\n[3] F. Peruch, F. Bogo, M. Bonazza, V.-M. Cappelleri, E. Peserico, Simpler, faster,\nmore accurate melanocytic lesion segmentation through meds, IEEE Trans.\nBiomed. Eng. 61 (2) (2013) 557â€“565.\n[4] M.H. Jafari, N. Karimi, E. Nasr-Esfahani, S. Samavi, S.M.R. Soroushmehr, K.\nWard, K. Najarian, Skin lesion segmentation in clinical images using deep\nlearning, in: 2016 23rd International Conference on Pattern Recognition, ICPR,\nIEEE, 2016, pp. 337â€“342.\n[5] Y. Yuan, M. Chao, Y.-C. Lo, Automatic skin lesion segmentation using deep fully\nconvolutional networks with jaccard distance, IEEE Trans. Med. Imaging 36 (9)\n(2017) 1876â€“1886.\n[6] H. Li, X. He, Z. Yu, F. Zhou, J.-Z. Cheng, L. Huang, T. Wang, B. Lei, Skin\nlesion segmentation via dense connected deconvolutional network, in: 2018 24th\nInternational Conference on Pattern Recognition, ICPR, IEEE, 2018, pp. 671â€“675.\n[7] L. Ren, A.A. Heidari, Z. Cai, Q. Shao, G. Liang, H.-L. Chen, Z. Pan, Gaussian\nkernel probability-driven slime mould algorithm with new movement mechanism\nfor multi-level image segmentation, Measurement 192 (2022) 110884.\n[8] J. Xia, Z. Cai, A.A. Heidari, Y. Ye, H. Chen, Z. Pan, Enhanced moth-flame\noptimizer with quasi-reflection and refraction learning with application to image\nsegmentation and medical diagnosis, Curr. Bioinform. 18 (2) (2023) 109â€“142.\n[9] L. Liu, D. Zhao, F. Yu, A.A. Heidari, J. Ru, H. Chen, M. Mafarja, H. Turabieh,\nZ. Pan, Performance optimization of differential evolution with slime mould\nalgorithm for multilevel breast cancer image segmentation, Comput. Biol. Med.\n138 (2021) 104910.\n[10] A.G. Hussien, A.A. Heidari, X. Ye, G. Liang, H. Chen, Z. Pan, Boosting whale\noptimization with evolution strategy and Gaussian random walks: An image\nsegmentation method, Eng. Comput. 39 (3) (2023) 1935â€“1979.\n[11] B. Zhou, A. Khosla, A. Lapedriza, A. Oliva, A. Torralba, Learning deep features for\ndiscriminative localization, in: Proceedings of the IEEE Conference on Computer\nVision and Pattern Recognition, 2016, pp. 2921â€“2929.\n[12] A. Kolesnikov, C.H. Lampert, Seed, expand and constrain: Three principles\nfor weakly-supervised image segmentation, in: Computer Visionâ€“ECCV 2016:\n14th European Conference, Amsterdam, the Netherlands, October 11â€“14, 2016,\nProceedings, Part IV 14, Springer, 2016, pp. 695â€“711.\n[13] S. Joon Oh, R. Benenson, A. Khoreva, Z. Akata, M. Fritz, B. Schiele, Exploiting\nsaliency for object segmentation from image level labels, in: Proceedings of\nthe IEEE Conference on Computer Vision and Pattern Recognition, 2017, pp.\n4410â€“4419.\n[14] Z. Huang, X. Wang, J. Wang, W. Liu, J. Wang, Weakly-supervised semantic\nsegmentation network with deep seeded region growing, in: Proceedings of\nthe IEEE Conference on Computer Vision and Pattern Recognition, 2018, pp.\n7014â€“7023.\n[15] Q. Chen, L. Yang, J.-H. Lai, X. Xie, Self-supervised image-specific prototype\nexploration for weakly supervised semantic segmentation, in: Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern Recognition, 2022, pp.\n4288â€“4298.\n[16] Y. Wei, J. Feng, X. Liang, M.-M. Cheng, Y. Zhao, S. Yan, Object region\nmining with adversarial erasing: A simple classification to semantic segmentation\napproach, in: Proceedings of the IEEE Conference on Computer Vision and\nPattern Recognition, 2017, pp. 1568â€“1576.\n[17] X. Zhang, Y. Wei, J. Feng, Y. Yang, T.S. Huang, Adversarial complementary\nlearning for weakly supervised object localization, in: Proceedings of the IEEE\nConference on Computer Vision and Pattern Recognition, 2018, pp. 1325â€“1334.\n[18] H. Xue, C. Liu, F. Wan, J. Jiao, X. Ji, Q. Ye, Danet: Divergent activation\nfor weakly supervised object localization, in: Proceedings of the IEEE/CVF\nInternational Conference on Computer Vision, 2019, pp. 6589â€“6598.\n[19] J. Qin, J. Wu, X. Xiao, L. Li, X. Wang, Activation modulation and recalibration\nscheme for weakly supervised semantic segmentation, in: Proceedings of the\nAAAI Conference on Artificial Intelligence, Vol. 36, 2022, pp. 2117â€“2125.\n[20] D. Zhang, W. Zeng, G. Guo, C. Fang, L. Cheng, M.-M. Cheng, J. Han, Weakly\nsupervised semantic segmentation via alternative self-dual teaching, 2021, arXiv\npreprint arXiv:2112.09459, arXiv:2112.09459.\n[21] P.-T. Jiang, Y. Yang, Q. Hou, Y. Wei, L2g: A simple local-to-global knowledge\ntransfer framework for weakly supervised semantic segmentation, in: Proceedings\nof the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2022,\npp. 16886â€“16896.\n[22] W. Gao, F. Wan, X. Pan, Z. Peng, Q. Tian, Z. Han, B. Zhou, Q. Ye, Ts-cam:\nToken semantic coupled attention map for weakly supervised object localization,\nin: Proceedings of the IEEE/CVF International Conference on Computer Vision,\n2021, pp. 2886â€“2895.\n[23] L. Xu, W. Ouyang, M. Bennamoun, F. Boussaid, D. Xu, Multi-class token\ntransformer for weakly supervised semantic segmentation, in: Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern Recognition, 2022, pp.\n4310â€“4319.\n[24] Y. Wang, J. Zhang, M. Kan, S. Shan, X. Chen, Self-supervised equivariant atten-\ntion mechanism for weakly supervised semantic segmentation, in: Proceedings\nof the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2020,\npp. 12275â€“12284.\n[25] L. Ru, Y. Zhan, B. Yu, B. Du, Learning affinity from attention: End-to-end\nweakly-supervised semantic segmentation with transformers, in: Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2022,\npp. 16846â€“16855.\n[26] B. Qi, G. Zhao, X. Wei, C. Du, C. Pan, Y. Yu, J. Li, GREN: graph-regularized\nembedding network for weakly-supervised disease localization in X-Ray images,\nIEEE J. Biomed. Health Inf. 26 (10) (2022) 5142â€“5153, http://dx.doi.org/10.\n1109/JBHI.2022.3193108.\n[27] G. Patel, J. Dolz, Weakly supervised segmentation with cross-modality\nequivariant constraints, Med. Image Anal. 77 (2022) 102374.\n[28] M. Yu, M. Han, X. Li, X. Wei, H. Jiang, H. Chen, R. Yu, Adaptive soft erasure\nwith edge self-attention for weakly supervised semantic segmentation: Thyroid\nultrasound image case study, Comput. Biol. Med. 144 (2022) 105347, http:\n//dx.doi.org/10.1016/j.compbiomed.2022.105347.\n[29] C. Han, J. Lin, J. Mai, Y. Wang, Q. Zhang, B. Zhao, X. Chen, X. Pan, Z. Shi,\nZ. Xu, et al., Multi-layer pseudo-supervision for histopathology tissue semantic\nsegmentation using patch-level classification labels, Med. Image Anal. 80 (2022)\n102487.\n[30] P. Tschandl, C. Rinner, Z. Apalla, G. Argenziano, N. Codella, A. Halpern, M.\nJanda, A. Lallas, C. Longo, J. Malvehy, et al., Humanâ€“computer collaboration\nfor skin cancer recognition, Nat. Med. 26 (8) (2020) 1229â€“1234.\n[31] P. Tschandl, C. Rosendahl, H. Kittler, The HAM10000 dataset, a large collection\nof multi-source dermatoscopic images of common pigmented skin lesions, Sci.\nData 5 (1) (2018) 1â€“9.\n[32] T. MendonÃ§a, P.M. Ferreira, J.S. Marques, A.R. Marcal, J. Rozeira, PH 2-a\ndermoscopic image database for research and benchmarking, in: 2013 35th\nAnnual International Conference of the IEEE Engineering in Medicine and Biology\nSociety, EMBC, IEEE, 2013, pp. 5437â€“5440.\n[33] J. Long, E. Shelhamer, T. Darrell, Fully convolutional networks for semantic\nsegmentation, in: Proceedings of the IEEE Conference on Computer Vision and\nPattern Recognition, 2015, pp. 3431â€“3440.\n[34] O. Ronneberger, P. Fischer, T. Brox, U-net: Convolutional networks for biomedi-\ncal image segmentation, in: Medical Image Computing and Computer-Assisted\nInterventionâ€“MICCAI 2015: 18th International Conference, Munich, Germany,\nOctober 5-9, 2015, Proceedings, Part III 18, Springer, 2015, pp. 234â€“241.\n[35] Z. Zhou, M.M.R. Siddiquee, N. Tajbakhsh, J. Liang, UNet++: redesigning skip\nconnections to exploit multiscale features in image segmentation, IEEE Trans.\nMed. Imaging 39 (6) (2020) 1856â€“1867, http://dx.doi.org/10.1109/TMI.2019.\n2959609.\n[36] H. Huang, L. Lin, R. Tong, H. Hu, Q. Zhang, Y. Iwamoto, X. Han, Y.-W. Chen,\nJ. Wu, UNet 3+: A full-scale connected unet for medical image segmentation,\nin: ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech\nand Signal Processing (ICASSP), IEEE, Barcelona, Spain, 2020, pp. 1055â€“1059,\nhttp://dx.doi.org/10.1109/ICASSP40776.2020.9053405.\n[37] O. Oktay, J. Schlemper, L.L. Folgoc, M. Lee, M. Heinrich, K. Misawa, K. Mori,\nS. McDonagh, N.Y. Hammerla, B. Kainz, et al., Attention u-net: Learning where\nto look for the pancreas, 2018, arXiv preprint arXiv:1804.03999.\n[38] X. Xiao, S. Lian, Z. Luo, S. Li, Weighted res-UNet for high-quality retina vessel\nsegmentation, in: 2018 9th International Conference on Information Technology\nin Medicine and Education (ITME), 2018, http://dx.doi.org/10.1109/ITME.2018.\n00080.\n[39] M.Z. Alom, C. Yakopcic, M. Hasan, T.M. Taha, V.K. Asari, Recurrent residual\nU-Net for medical image segmentation, J. Med. Imaging 6 (1) (2019) 1, http:\n//dx.doi.org/10.1117/1.JMI.6.1.014006.\n[40] J. Chen, Y. Lu, Q. Yu, X. Luo, E. Adeli, Y. Wang, L. Lu, A.L. Yuille, Y. Zhou,\nTransunet: Transformers make strong encoders for medical image segmentation,\n2021, arXiv preprint arXiv:2102.04306.\n\nComputers in Biology and Medicine 170 (2024) 107988\n14\nZ. Li et al.\n[41] D.-P. Fan, G.-P. Ji, T. Zhou, G. Chen, H. Fu, J. Shen, L. Shao, Pranet: Parallel\nreverse attention network for polyp segmentation, in: International Conference on\nMedical Image Computing and Computer-Assisted Intervention, Springer, 2020,\npp. 263â€“273.\n[42] J.M. Jose, V. Sindagi, I. Hacihaliloglu, V.M. Patel, KiU-Net: Towards accurate\nsegmentation of biomedical images using over-complete representations, 2020.\n[43] D. Jha, M.A. Riegler, D. Johansen, P. Halvorsen, H.D. Johansen, Doubleu-\nnet: A deep convolutional neural network for medical image segmentation, in:\n2020 IEEE 33rd International Symposium on Computer-Based Medical Systems\n(CBMS), IEEE, 2020, pp. 558â€“564.\n[44] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner,\nM. Dehghani, M. Minderer, G. Heigold, S. Gelly, et al., An image is worth\n16x16 words: Transformers for image recognition at scale, 2020, arXiv preprint\narXiv:2010.11929.\n[45] S. Zheng, J. Lu, H. Zhao, X. Zhu, Z. Luo, Y. Wang, Y. Fu, J. Feng, T. Xiang, P.H.\nTorr, Rethinking semantic segmentation from a sequence-to-sequence perspective\nwith transformers, in: Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, 2021, pp. 6881â€“6890.\n[46] Y. Zhang, H. Liu, Q. Hu, Transfuse: Fusing transformers and cnns for medical\nimage segmentation, in: Medical Image Computing and Computer Assisted\nInterventionâ€“MICCAI 2021: 24th International Conference, Strasbourg, France,\nSeptember 27â€“October 1, 2021, Proceedings, Part I 24, Springer, 2021, pp.\n14â€“24.\n[47] Y. Ji, R. Zhang, H. Wang, Z. Li, L. Wu, S. Zhang, P. Luo, Multi-compound\ntransformer for accurate biomedical image segmentation, in: Medical Image\nComputing and Computer Assisted Interventionâ€“MICCAI 2021: 24th International\nConference, Strasbourg, France, September 27â€“October 1, 2021, Proceedings,\nPart I 24, Springer, 2021, pp. 326â€“336.\n[48] J.M.J. Valanarasu, P. Oza, I. Hacihaliloglu, V.M. Patel, Medical transformer:\nGated axial-attention for medical image segmentation, in: Medical Image Com-\nputing and Computer Assisted Interventionâ€“MICCAI 2021: 24th International\nConference, Strasbourg, France, September 27â€“October 1, 2021, Proceedings,\nPart I 24, Springer, 2021, pp. 36â€“46.\n[49] Y. Wei, X. Liang, Y. Chen, X. Shen, M.-M. Cheng, J. Feng, Y. Zhao, S. Yan, Stc:\nA simple to complex framework for weakly-supervised semantic segmentation,\nIEEE Trans. Pattern Anal. Mach. Intell. 39 (11) (2016) 2314â€“2320.\n[50] G. Chen, J. Ru, Y. Zhou, I. Rekik, Z. Pan, X. Liu, Y. Lin, B. Lu, J. Shi,\nMTANS: Multi-scale mean teacher combined adversarial network with shape-\naware embedding for semi-supervised brain lesion segmentation, NeuroImage\n244 (2021) 118568.\n[51] K. Simonyan, A. Zisserman, Very deep convolutional networks for large-scale\nimage recognition, 2014, arXiv preprint arXiv:1409.1556.\n[52] K. He, X. Zhang, S. Ren, J. Sun, Deep residual learning for image recognition, in:\nProceedings of the IEEE Conference on Computer Vision and Pattern Recognition,\n2016, pp. 770â€“778.\n[53] G. Schaefer, M.I. Rajab, M.E. Celebi, H. Iyatomi, Colour and contrast enhance-\nment for improved skin lesion segmentation, Comput. Med. Imaging Graph. 35\n(2) (2011) 99â€“104.\n[54] Å. Ã–ztÃ¼rk, U. Ã–zkaya, Skin lesion segmentation with improved convolutional\nneural network, J. Digital Imaging 33 (2020) 958â€“970.\n[55] F. Xie, J. Yang, J. Liu, Z. Jiang, Y. Zheng, Y. Wang, Skin lesion segmentation\nusing high-resolution convolutional neural network, Comput. Methods Programs\nBiomed. 186 (2020) 105241.",
    "version": "5.3.31"
  },
  {
    "numpages": 14,
    "numrender": 14,
    "info": {
      "PDFFormatVersion": "1.7",
      "Language": "en-US",
      "EncryptFilterName": null,
      "IsLinearized": true,
      "IsAcroFormPresent": false,
      "IsXFAPresent": false,
      "IsCollectionPresent": false,
      "IsSignaturesPresent": false,
      "Creator": "Elsevier",
      "Custom": {
        "CrossMarkDomains[1]": "elsevier.com",
        "CrossmarkMajorVersionDate": "2010-04-23",
        "CreationDate--Text": "15th March 2024",
        "ElsevierWebPDFSpecifications": "7.0",
        "robots": "noindex",
        "doi": "10.1016/j.compbiomed.2024.108228",
        "CrossMarkDomains[2]": "sciencedirect.com",
        "CrossmarkDomainExclusive": "true"
      },
      "ModDate": "D:20240315190754Z",
      "Author": "Zhuo Kuang",
      "Title": "Weakly supervised learning for multi-class medical image segmentation via feature decomposition",
      "Keywords": "Medical image segmentation,Weakly supervision,Semantic affinity,Multi-class segmentation",
      "CreationDate": "D:20240315154901Z",
      "Producer": "Acrobat Distiller 8.1.0 (Windows)",
      "Subject": "Computers in Biology and Medicine, 171 (2024) 108228. doi:10.1016/j.compbiomed.2024.108228"
    },
    "metadata": {
      "crossmark:crossmarkdomains": "elsevier.comsciencedirect.com",
      "crossmark:crossmarkdomainexclusive": "true",
      "crossmark:doi": "10.1016/j.compbiomed.2024.108228",
      "crossmark:majorversiondate": "2010-04-23",
      "dc:format": "application/pdf",
      "dc:identifier": "10.1016/j.compbiomed.2024.108228",
      "dc:publisher": "Elsevier Ltd",
      "dc:description": "Computers in Biology and Medicine, 171 (2024) 108228. doi:10.1016/j.compbiomed.2024.108228",
      "dc:subject": [
        "Medical image segmentation",
        "Weakly supervision",
        "Semantic affinity",
        "Multi-class segmentation"
      ],
      "dc:title": "Weakly supervised learning for multi-class medical image segmentation via feature decomposition",
      "dc:creator": ["Zhuo Kuang", "Zengqiang Yan", "Li Yu"],
      "jav:journal_article_version": "VoR",
      "pdf:creationdate--text": "15th March 2024",
      "pdf:producer": "Acrobat Distiller 8.1.0 (Windows)",
      "pdf:keywords": "Medical image segmentation,Weakly supervision,Semantic affinity,Multi-class segmentation",
      "pdfx:creationdate--text": "15th March 2024",
      "pdfx:crossmarkdomains": "sciencedirect.comelsevier.com",
      "pdfx:crossmarkdomainexclusive": "true",
      "pdfx:crossmarkmajorversiondate": "2010-04-23",
      "hioniy9arnpj_zwugnt_9nsnnowv-lt6pmt-gntf8y9v.y.n8zwqko9eqn9iknm-smdmtma": "",
      "pdfx:doi": "10.1016/j.compbiomed.2024.108228",
      "pdfx:robots": "noindex",
      "prism:aggregationtype": "journal",
      "prism:copyright": "Â© 2024 Elsevier Ltd. All rights reserved.",
      "prism:coverdate": "2024-03-01",
      "prism:coverdisplaydate": "1 March 2024",
      "prism:doi": "10.1016/j.compbiomed.2024.108228",
      "prism:issn": "0010-4825",
      "prism:pagerange": "108228",
      "prism:publicationname": "Computers in Biology and Medicine",
      "prism:startingpage": "108228",
      "prism:url": "https://doi.org/10.1016/j.compbiomed.2024.108228",
      "prism:volume": "171",
      "xmp:createdate": "2024-03-15T15:49:01",
      "xmp:creatortool": "Elsevier",
      "xmp:metadatadate": "2024-03-15T19:07:54",
      "xmp:modifydate": "2024-03-15T19:07:54",
      "xmpmm:documentid": "uuid:1b499bed-4ce8-4c0c-b682-0d58baae1cbe",
      "xmpmm:instanceid": "uuid:b6ed5266-03cb-4855-90c0-636283357f42",
      "xmprights:marked": "True"
    },
    "text": "Computers in Biology and Medicine 171 (2024) 108228\nAvailable online 28 February 2024\n0010-4825/Â© 2024 Elsevier Ltd. All rights reserved.\nContents lists available at ScienceDirect\nComputers in Biology and Medicine\njournal homepage: www.elsevier.com/locate/compbiomed\nWeakly supervised learning for multi-class medical image segmentation via\nfeature decomposition\nZhuo Kuang, Zengqiang Yan, Li Yu \nâˆ—\nSchool of Electronic Information and Communications, Huazhong University of Science and Technology, Wuhan, 430074, China\nA R T I C L E I N F O\nKeywords:\nMedical image segmentation\nWeakly supervision\nSemantic affinity\nMulti-class segmentation\nA B S T R A C T\nWeakly supervised learning with image-level labels, releasing deep learning from highly labor-intensive\npixel-wise annotation, has gained great attention for medical image segmentation. However, existing weakly\nsupervised methods are mainly designed for single-class segmentation while leaving multi-class medical image\nsegmentation rarely-explored. Different from natural images, label symbiosis, together with location adjacency,\nare much more common in medical images, making it more challenging for multi-class segmentation. In this\npaper, we propose a novel weakly supervised learning method for multi-class medical image segmentation\nwith image-level labels. In terms of the multi-class classification backbone, a multi-level classification network\nencoding multi-scale features is proposed to produce binary predictions, together with the corresponding\nCAMs, of each class separately. To address the above issues (i.e., label symbiosis and location adjacency),\na feature decomposition module based on semantic affinity is first proposed to learn both class-independent\nand class-dependent features by maximizing the inter-class feature distance. Through a cross-guidance loss\nto jointly utilize the above features, label symbiosis is largely alleviated. In terms of location adjacency,\na mutually exclusive loss is constructed to minimize the overlap among regions corresponding to different\nclasses. Experimental results on three datasets demonstrate the superior performance of the proposed weakly-\nsupervised framework for both single-class and multi-class medical image segmentation. We believe the analysis\nin this paper would shed new light on future work for multi-class medical image segmentation. The source\ncode of this paper is publicly available at https://github.com/HustAlexander/MCWSS.\n1. Introduction\nMedical image segmentation is important and beneficial for clinical\ndiagnosis and treatment. Despite the great success of deep learning\nin medical image segmentation [1â€“3], relying on a large amount of\npixel-wise annotated data usually makes it less feasible in clinical appli-\ncations. Comparatively, weakly supervised learning based on low-cost\nannotations, e.g. slice-level labels, bounding box [4], points annota-\ntions [5], has recently attracted extensive attention for medical image\nsegmentation [6â€“8].\nAmong the various weak annotations mentioned above, slice-level\nlabels, as the most lightweight type of annotation, are more readily\navailable for weakly supervised learning. In general, the feature maps\nare fed to a classifier to generate the classification activation maps\n(CAMs) for localization and then segmentation [9] as shown in Fig. 1.\nHere, CAMs reflect the activation regions of the image corresponding\nto the given label for correct classification.\nCurrently, most weakly supervised medical image segmentation\nmethods focus on binary segmentation (i.e., single-class CAMs refine-\nment). Some methods introduce prior knowledge, like fixed organ\nâˆ— \nCorresponding author.\nE-mail addresses: kuangzhuo@hust.edu.cn (Z. Kuang), z_yan@hust.edu.cn (Z. Yan), hustlyu@hust.edu.cn (L. Yu).\nstructures/shapes, for refinement. Kervadec et al. [6] proposed to\napply size constraints to the segmentation results of target organs,\nwhile Li et al. [10] decomposed each whole breast ultrasound image\ninto four anatomical regions and restricted the produced CAMs to the\ncorresponding regions. Based on the gray characteristics of the uterine\nregion in MR images, Ying et al. [11] proposed an exponential geodesic\ndistance loss to effectively refine the uterine edge in CAMs. These\nmethods focused on the segmentation of a certain type of target and\nachieved good performance. Some other approaches are designed for\nmore general segmentation tasks. Focusing on boundary activation, Li\net al. [12] proposed a boundary Control Loss to enable mutual learning\nand adaptive correction of edge feature information at different scales.\nWang et al. [13] utilized unsupervised segmentation to refine the\nCAMs, significantly reducing false positives. Kuang et al. [14] sepa-\nrated false detections in CAMs by uncertainty estimation and obtained\npseudo pixel-level labels closer to the ground truth. Xiang et al. [15]\naddressed label noise by a self-ensemble method under the threshold of\nuncertainty. Wu et al. [16] proposed to use the attention along different\nhttps://doi.org/10.1016/j.compbiomed.2024.108228\nReceived 17 October 2023; Received in revised form 19 February 2024; Accepted 25 February 2024\n\nComputers in Biology and Medicine 171 (2024) 108228\n2\nZ. Kuang et al.\nFig. 1. Illustration of weakly supervised medical image segmentation based on image-/slice-level labels. ğ‘@ğ‘Š âˆ— ğ» represents the feature maps that have ğ‘ channels and the\nwidth and height are W and H respectively.\ndimensions to refine the CAMs and achieve comparable performance\nwith fully supervised methods. In addition, methods based on self-\nsupervision and contrastive learning were also proposed for weakly\nsupervised segmentation. Laradji et al. [17] transformed the input\nimages and constructed a consistency loss to align their outputs, im-\nproving the localization robustness of CAMs. Similarly, Patel et al. [18]\napplied contrastive learning to align the CAMs generated from differ-\nent modalities, exploring inter-modality information. Liu et al. [19]\nproposed a self-supervised learning strategy to learn structural and\ntextual information in normal images, mitigating the effects of limited\nsupervision.\nCompared to single-class problems, multi-class problems are usu-\nally more difficult, facing the challenges, such as multi-class imbal-\nance [20] and edge classes [21]. Likewise, multi-class medical image\nsegmentation with slice-level labels is more challenging and meaning-\nful than single-class segmentation [16,18]. Though more information\nis available under multi-class settings, serious label symbiosis and\nlocation adjacency would hinder the deployment of existing single-\nclass frameworks for multi-class segmentation, resulting in significant\nperformance degradation [18]. The most relevant work is proposed by\nChen et al. [22], where an anatomy-causality-based CAM to distinguish\nthe abdominal organs, which highly relies on the fixed positions of\norgans and could hardly handle more complex cases like tumor or\nlesion segmentation.\nIn this paper, based on the above analysis, we propose a new\nand more general weakly-supervised framework for multi-class medical\nimage segmentation with slice-level labels. In which, a multi-level\nclassification network is constructed as the backbone to generate high-\nquality CAMs where each class is treated separately through binary\nclassification. To address label symbiosis, a semantic affinity-aware\nfeature decomposition module is proposed to separate the features\nfor different classes while maintaining the effectiveness of the com-\nmon/shared features. Then, the above features are jointly utilized to\nrefine the CAMs corresponding to each class. Finally, a mutually exclu-\nsive loss is penalized to minimize the overlap of the CAMs belonging to\ndifferent classes. Experimental results on brain tumor and spontaneous\nhemorrhage segmentation demonstrate the superior performance of\nthe proposed weakly supervised learning framework against existing\napproaches. The main contributions are summarized as follows:\n1. A novel weakly-supervised framework for multi-class medical\nimage segmentation with slice-level labels outperforming exist-\ning weakly-supervised segmentation frameworks.\n2. Semantic affinity-aware feature decomposition to address label\nsymbiosis. It enables the model to extract class-independent and\nclass-dependent features simultaneously, maximizing the feature\ndistance among classes to alleviate cross-class interference.\n3. A mutually exclusive loss to separate adjacent multi-class ac-\ntivation regions. It is to penalize the overlap between multi-\nclass activation regions, achieving compact and distinguishable\nmulti-class segmentation.\nThe paper is organized as follows. Section 2 analyzes the main chal-\nlenges in multi-class medical image segmentation. Section 3 presents\ndetails of the proposed semantic affinity-aware weakly-supervised\nframework for multi-class medical image segmentation. In Section 4,\nwe evaluate the effectiveness of the proposed framework through\nmultiple comparison experiments. Section 5 provides ablation studies\nand Section 6 concludes this paper.\n2. Problem analysis\nIn this section, we analyze in detail the main challenges behind\nmulti-class medical image segmentation.\n2.1. Label symbiosis\nGiven the tumor core and peritumoral edema in Fig. 2, the two\ntissues coexist on most slices. Denoting the classification labels of\nslices across the whole dataset as ğ‘Œ\nğ‘‡ ğ¶ \n= [ğ‘¦\n0\nğ‘‡ ğ¶ \n, ğ‘¦\n1\nğ‘‡ ğ¶ \n, ğ‘¦\n2\nğ‘‡ ğ¶ \n, â€¦ , ğ‘¦\nğ¼\nğ‘‡ ğ¶ \n] and\nğ‘Œ\nğ¸ğ· \n= [ğ‘¦\n0\nğ¸ğ·\n, ğ‘¦\n1\nğ¸ğ·\n, ğ‘¦\n2\nğ¸ğ·\n, â€¦ , ğ‘¦\nğ¼\nğ¸ğ·\n] where ğ¼ is the total number of slices,\nğ‘¦\nğ¼\nğ‘‡ ğ¶ \nand ğ‘¦\nğ¼\nğ¸ğ· \nrepresent if the ğ¼th slice contains the tumor core or edema\nseparately. The relationship between ğ‘Œ\nğ‘‡ ğ¶ \nand ğ‘Œ\nğ¸ğ· \ncan be formulated\nas:\n((1 âˆ’ ğ‘Œ\nğ‘‡ ğ¶ \n) âŠ™ ğ‘Œ\nğ¸ğ· \n+ (1 âˆ’ ğ‘Œ\nğ¸ğ·\n) âŠ™ ğ‘Œ\nğ‘‡ ğ¶ \n)âˆ•ğ¼ â‰ˆ 0. (1)\nIt means the slices only containing one of the sub-tumors account for\na small portion of the entire dataset. As a result, for a specific loss\nfunction like cross-entropy (CE), it can hardly tell whether losses are\nbrought by the misclassification of ğ‘Œ\nğ‘‡ ğ¶ \nor ğ‘Œ\nğ¸ğ· \nas follows:\nîˆ¸\nğ¶ğ¸ \n(\nÌ‚\nğ‘Œ\nğ‘‡ ğ¶ \n, ğ‘Œ\nğ¸ğ·\n) â‰ˆ îˆ¸\nğ¶ğ¸ \n(\nÌ‚\nğ‘Œ\nğ‘‡ ğ¶ \n, ğ‘Œ\nğ‘‡ ğ¶ \n),\nîˆ¸\nğ¶ğ¸ \n(\nÌ‚\nğ‘Œ\nğ¸ğ·\n, ğ‘Œ\nğ‘‡ ğ¶ \n) â‰ˆ îˆ¸\nğ¶ğ¸ \n(\nÌ‚\nğ‘Œ\nğ¸ğ·\n, ğ‘Œ\nğ¸ğ·\n), \n(2)\nwhere\nÌ‚\n ğ‘Œ is the predicted score to classify the slice to ğ‘Œ . In this\ncase, the classification problem degenerates from multi-class to single-\nclass, making classifiers struggle to learn class-dependent features for\ndifferent classes. Consequently, the activation regions corresponding to\nthe two classes are inseparable in the produced CAMs, as shown in the\nlast two rows of Fig. 2 where the â€˜â€˜segmentedâ€™â€™ regions are almost the\nsame for both classes. Therefore, how to learn class-dependent features\nfor classes based on symbiotic labels is crucial for weakly supervised\nmulti-class medical image segmentation.\n\nComputers in Biology and Medicine 171 (2024) 108228\n3\nZ. Kuang et al.\nFig. 2. Examples of label symbiosis. Here, FLAIR images are sampled from one scan of the BraTS dataset, where the colors yellow and green represent the tumor core and the\nperitumoral edema respectively. The last two rows are the CAMs results of the tumor core and the peritumoral edema respectively, from the basic CAMs generation networks as\nshown in Fig. 1.\nFig. 3. An example of location adjacency. From left to right: the origin FLAIR image, the annotated FLAIR image, and the CAMs corresponding to the tumor core and the\nperitumoral edema respectively. Here, the colors yellow and green are assigned to the tumor core and the peritumoral edema respectively. The â€˜â€˜overlapped regionsâ€™â€™ between the\nCAMs are marked by dotted circles.\n2.2. Location adjacency\nIn multi-class tumor/lesion segmentation, regions belonging to dif-\nferent classes can be spatially close as shown in Fig. 3. Nevertheless,\nCAMs based on the slice-level labels have been proven insensitive to\nboundaries [23]. Taking tumor core and peritumoral edema in Fig. 3 for\ninstance, due to the low contrast of the adjacent tissues, their activated\nregions in CAMs are inevitably â€˜â€˜overlappedâ€™â€™ (i.e. high responses for\nboth classes).\nAs discussed above, due to label symbiosis, multi-class classifi-\ncation would become single-class classification (i.e. background vs.\nforeground) during training. Those overlapped activation regions cor-\nrespond to the highly discriminative features for foreground and back-\nground classification. As CAMs tend to only activate the most discrim-\ninative regions during training [24], it will be more challenging to\nseparate the overlapped activation regions for multi-class segmentation.\n3. Methodology\nThe key idea behind the proposed weakly supervised learning\nframework is to learn class-dependent features to address label symbio-\nsis and produce separable CAMs of different labels through additional\nregularization. The former part is achieved by a semantic affinity-aware\n\nComputers in Biology and Medicine 171 (2024) 108228\n4\nZ. Kuang et al.\nFig. 4. Overview of the proposed semantic affinity-aware weakly supervised framework with slice-level labels for multi-class medical image segmentation.\nfeature decomposition module to fully utilize the slices with a single\nclass (i.e. without label symbiosis), while the latter part is realized by\na mutually exclusive loss. In the following, we describe in detail the\ndesign of the proposed framework as illustrated in Fig. 4.\n3.1. CAMs generation\nProducing high-quality CAMs is the basis for weakly supervised\nmedical image segmentation. Inspired by [25], a multi-level classifi-\ncation network (MLCN) is constructed to fuse multi-scale features for\nhigh-quality CAMs, especially in the boundary regions as shown in\nFig. 4. Given the multi-class classification task in this paper, each class\nis assigned a separate classification (CF) head for binary classification\n(i.e. either background or the target class) and CAMs generation as\nsegmentation. Through 1 Ã— 1 convolutional bottleneck layers, feature\nmaps of two different levels are squeezed along the channel dimension\nto generate the corresponding CAMs, denoted as ğ¶ğ´ğ‘€\nğ‘™\nğ‘› \nwhere ğ‘™ âˆˆ ğ¿,\nğ¿ represents the feature maps produced by conv3 and conv5 in the\nproposed MLCN, and ğ‘› represents the ğ‘›th class. Then, the correspond-\ning classification score\nÌ‚\n ğ‘Œ \nğ‘™\nğ‘› \nis derived from ğ¶ğ´ğ‘€\nğ‘™\nğ‘› \nthrough the global\naverage pooling (GAP) function [26]. The classification loss of all CF\nheads is defined as:\nîˆ¸\nğ‘ğ‘™ğ‘ğ‘ ğ‘  \n= \nâˆ‘\nğ‘›,ğ¿\nîˆ¸\nğ¶ğ¸ \n(\nÌ‚\nğ‘Œ \nğ‘™\nğ‘› \n, ğ‘Œ\nğ‘›\n), (3)\nwhere îˆ¸\nğ¶ğ¸ \nis the cross entropy loss,\nÌ‚\n ğ‘Œ \nğ‘™\nğ‘› \nis the prediction of the ğ‘›th class,\nand ğ‘Œ\nğ‘› \nrepresents the ground truth of the ğ‘›th class.\nTo suppress irrelevant false positives in the background regions of\nCAMs, a CAM loss îˆ¸\nğ‘ğ‘ğ‘š \nis defined as\nîˆ¸\nğ‘ğ‘ğ‘š \n= \nâˆ‘\nğ‘›,ğ‘™\n(min(ğ¶ğ´ğ‘€\nğ‘™\nğ‘›\n[0], ğ¶ğ´ğ‘€\nğ‘™\nğ‘›\n[1]) + ğ‘Œ\nğ‘› \nâˆ— ğ¶ğ´ğ‘€\nğ‘™\nğ‘›\n[0]\n+ (1 âˆ’ ğ‘Œ\nğ‘›\n) âˆ— ğ¶ğ´ğ‘€\nğ‘™\nğ‘›\n[1]), (4)\nwhere ğ¶ğ´ğ‘€\nğ‘™\nğ‘› \nrepresents the CAMs corresponding to the ğ‘›th class,\nproduced based on the ğ‘™th feature maps (e.g. ğ‘“ \nğ‘ğ‘œğ‘›ğ‘£3 \nor ğ‘“ \nğ‘ğ‘œğ‘›ğ‘£5 \nin Fig. 4),\nindexes [0] and [1] indicate the foreground and background CAMs re-\nspectively. Classification is implemented through global average pool-\ning which calculates the average values of all the pixels in ğ¶ğ´ğ‘€\nğ‘™\nğ‘›\n[0]\nand ğ¶ğ´ğ‘€\nğ‘™\nğ‘›\n[1] corresponding to the predictions of positive and negative\nclasses respectively. Through îˆ¸\nğ‘ğ‘™ğ‘ğ‘ ğ‘ \n, ğ¶ğ´ğ‘€\nğ‘™\nğ‘›\n[1] tends to contain more\nforeground pixels for correct classification while ğ¶ğ´ğ‘€\nğ‘™\nğ‘›\n[0] would con-\ntain more background pixels. However, there may exist pixels existing\nin both ğ¶ğ´ğ‘€\nğ‘™\nğ‘›\n[0] and ğ¶ğ´ğ‘€\nğ‘™\nğ‘›\n[1] (i.e., false positives) without affect-\ning classification, which is vital for foreground segmentation based\non ğ¶ğ´ğ‘€\nğ‘™\nğ‘›\n[1]. To address this, min(ğ¶ğ´ğ‘€\nğ‘™\nğ‘›\n[0], ğ¶ğ´ğ‘€\nğ‘™\nğ‘›\n[1]) is penalized\nto minimize the overlap between ğ¶ğ´ğ‘€\nğ‘™\nğ‘›\n[0] and ğ¶ğ´ğ‘€\nğ‘™\nğ‘›\n[1], and thus\npixels are encouraged to be localized in either ğ¶ğ´ğ‘€\nğ‘™\nğ‘›\n[0] or ğ¶ğ´ğ‘€\nğ‘™\nğ‘›\n[1]\ninstead of both. In this way, ğ¶ğ´ğ‘€\nğ‘™\nğ‘›\n[1] is more likely to contain â€˜â€˜realâ€™â€™\nforeground pixels. Furthermore, to ensure those pixels have higher\nvalues in ğ¶ğ´ğ‘€\nğ‘™\nğ‘›\n[1], for any positive slice, îˆ¸\nğ‘ğ‘ğ‘š \nwould minimize the L1\nnorm of ğ¶ğ´ğ‘€\nğ‘™\nğ‘›\n[0]. Thus, îˆ¸\nğ‘ğ‘ğ‘š \nwould train the model to focus only on\nthose â€˜â€˜realâ€™â€™ meaningful regions, to avoid false positives.\nFor the ğ‘›th class, the final CAM is calculated as:\nğ¶ğ´ğ‘€\nğ‘› \n= \nâˆ\nğ‘™\nÌƒ\nğ¶ğ´ğ‘€\nğ‘™\nğ‘›\n, (5)\nwhere\nÌƒ\n ğ¶ğ´ğ‘€\nğ‘™\nğ‘› \nis the resized ğ¶ğ´ğ‘€\nğ‘™\nğ‘› \nto the same resolution of the input\nslice.\n3.2. Feature decomposition module\nAs mentioned in Section 2.1, the key to solving label symbiosis is to\nlearn class-dependent features, thus maximizing the distance between\nclasses. However, relatively limited single-class slices make it difficult\nfor effective feature learning. In this section, inspired by [24,27], we\ndevelop a feature decomposition module to generate extra samples with\nunique information for each class at the feature level based on semantic\naffinity.\nAs shown in Fig. 4, except for the slice as input, a reference slice\nis selected to generate the co-attention maps. Given the labels of the\ninput slice as ğ‘Œ\nğ‘– \n= {ğ‘Œ\n0,ğ‘–\n, ğ‘Œ\n1,ğ‘–\n...ğ‘Œ\nğ‘›,ğ‘–\n} and the reference slice as ğ‘Œ\nğ‘Ÿ \n=\n{ğ‘Œ\n0,ğ‘Ÿ\n, ğ‘Œ\n1,ğ‘Ÿ\n...ğ‘Œ\nğ‘›,ğ‘Ÿ\n}, they satisfy the following requirement\nâˆ‘\nğ‘›\n(ğ‘Œ\nğ‘›,ğ‘– \nâŠ™ ğ‘Œ\nğ‘›,ğ‘Ÿ\n) â‰¤ 1. (6)\nIn other words, ğ‘Œ\nğ‘– \nand ğ‘Œ\nğ‘Ÿ \nhave at most one shared/common class label.\nDuring training, the two slices are fed to the same MLCN separately,\nand the two sets of feature maps are extracted accordingly. Denote the\nğ‘™th feature maps of any input slice as ğ‘“\nğ‘–\nğ‘™ \nand the reference slice as\nğ‘“\nğ‘Ÿ\nğ‘™ \n, where ğ‘“\nğ‘–\nğ‘™ \n, ğ‘“\nğ‘Ÿ\nğ‘™ \nâˆˆ R\nğ¶Ã—ğ»Ã—ğ‘Š \nand ğ‘™ âˆˆ ğ¿, ğ¿ represents the collection\nof features with different scales (e.g. ğ¿ represents the feature maps\nproduced by conv3 and conv5 in the proposed MLCN). Through the\nfeature decomposition module (FDM), these feature maps are used to\ngenerate co-attention maps based on their semantic affinity, which\nhighlights the same semantics across the input and the reference slices.\nFirstly, the semantic affinity between the features from different\nslices is computed as an affinity matrix, according to\nğ‘ƒ \nğ‘™ \n= (\nÌ„\nğ‘“ \nğ‘™\nğ‘– \n)\nğ‘‡ \nâ‹… ğ‘Š\nğ‘ƒ \nğ‘™ \nâ‹…\nÌ„\n ğ‘“ \nğ‘™\nğ‘Ÿ \nâˆˆ R\nğ»ğ‘Š Ã—ğ»ğ‘Š \n, (7)\n\nComputers in Biology and Medicine 171 (2024) 108228\n5\nZ. Kuang et al.\nwhere ğ‘™ âˆˆ ğ¿,\nÌ„\n ğ‘“ \nğ‘™\nğ‘– \nâˆˆ R\nğ¶Ã—ğ»ğ‘Š \nand\nÌ„\n ğ‘“ \nğ‘™\nğ‘Ÿ \nâˆˆ R\nğ¶Ã—ğ»ğ‘Š \nare the flattened feature\nmaps of ğ‘“ \nğ‘™\nğ‘– \nand ğ‘“ \nğ‘™\nğ‘Ÿ \n, and ğ‘Š\nğ‘ƒ \nğ‘™ \nâˆˆ R\nğ¶Ã—ğ¶ \nis a learnable fully connected\nlayer for the ğ‘™th set of feature maps. The affinity matrix ğ‘ƒ \nğ‘™ \nmeasures\nthe similarity between each pair of values in\nÌ„\n ğ‘“ \nğ‘™\nğ‘– \nand\nÌ„\n ğ‘“ \nğ‘™\nğ‘Ÿ \n. For instance,\nğ‘ƒ \nğ‘™ \n[ğ‘, ğ‘] represents the similarity between\nÌ„\n ğ‘“ \nğ‘™\nğ‘– \n[âˆ¶, ğ‘] and\nÌ„\n ğ‘“ \nğ‘™\nğ‘Ÿ \n[âˆ¶, ğ‘]. Then, co-\nattention maps for the input slice are derived from the affinity matrix\nğ‘ƒ \nğ‘™ \nby\nğ´\nğ‘™ \n= ğœ(ğ‘ƒ \nğ‘™ \n) âˆˆ R\nğ»ğ‘Š Ã—ğ»ğ‘Š \n, (8)\nwhere ğœ is the softmax function performed column-wisely. According to\nEq. (8), the co-attention maps would focus on the regions corresponding\nto the shared semantics between the two slices. With the co-attention\nmaps ğ´\nğ‘™ \n, we decompose the flattened input feature maps\nÌ„\n ğ‘“ \nğ‘™\nğ‘– \nby:\nÌ„\nğ‘“ \nğ‘™\nğ›© \n=\nÌ„\n ğ‘“ \nğ‘™\nğ‘– \nğ´\nğ‘™ \n. (9)\nIn this way, features in\nÌ„\n ğ‘“ \nğ‘™\nğ‘– \nare decomposed into two parts, features\ncontributing to the shared semantics between the two slices and other\nfeatures. In\nÌ„\n ğ‘“ \nğ‘™\nğ›©\n, the former features are retained while the latter features\nare suppressed. Therefore, labels corresponding to the decomposed\nfeature maps are defined as\nğ‘Œ\nğ›© \n= ğ‘Œ\nğ‘–\nâ‹‚ \nğ‘Œ\nğ‘Ÿ\n. (10)\nAccording to Eq. (6), there exists at most one shared class between ğ‘Œ\nğ‘–\nand ğ‘Œ\nğ‘Ÿ\n, making the decomposed feature maps in Eq. (9) class-dependent\n(i.e. the shared class).\nAs discussed in Section 2.1, label symbiosis degenerates the multi-\nclass classification task into single-class, resulting in the limited inter-\nclass distance and less-discriminative class-dependent features. Conse-\nquently, CAMs belonging to different classes are inseparable. Utilizing\nthe feature decomposition module, the networks are compelled to learn\nfeatures of at most one class each time, thereby enlarging the inter-\nclass distance and enriching class-dependent features. To this end, the\nchallenge of label symbiosis is effectively mitigated.\nIn [27], co-attention maps were only trained through a classification\ntask without interactions, lacking sufficient constraints to produce high-\nquality co-attention maps, which in turn limits the original features\nfor performance improvement. In the proposed feature decomposi-\ntion module, extra constraints and unique complementary mechanisms\nare presented to better optimize feature learning based on semantic\naffinity. As shown in Fig. 4, the original feature maps from the in-\nput slice and the decomposed feature maps are simultaneously fed\ninto two differently-initialized classification heads for CAMs generation\naccording to\nğ¶ğ´ğ‘€\nğ‘™\nğ‘– \n= {ğ¶ğ´ğ‘€\n0,ğ‘–\nğ‘™ \n, â€¦ ğ¶ğ´ğ‘€\nğ‘›,ğ‘–\nğ‘™ \n} = Ï\nğ‘–\n(ğ‘“ \nğ‘™\nğ‘– \n),\nğ¶ğ´ğ‘€\nğ‘™\nğ›© \n= {ğ¶ğ´ğ‘€\n0,ğ›©\nğ‘™ \n, â€¦ ğ¶ğ´ğ‘€\nğ‘›,ğ›©\nğ‘™ \n} = Ï\nğ›©\n(ğ‘“ \nğ‘™\nğ›©\n), \n(11)\nwhere Ï\nğ‘– \nand Ï\nğ›© \nare the bottleneck networks as shown in Fig. 4,\nğ¶ğ´ğ‘€\nğ‘›,ğ‘–\nğ‘™ \nrepresents the CAMs of class ğ‘› produced by the original ğ‘™th\nfeature maps of the input slice, and ğ¶ğ´ğ‘€\nğ‘›,ğ›©\nğ‘™ \nrepresents the CAMs of\nclass ğ‘› produced by the decomposed ğ‘™th feature maps.\nAs discussed in Section 2.1, label symbiosis would emphasize more\non class-independent features. In other words, ğ‘“\nğ‘– \ncan effectively dis-\ntinguish the foreground from the background while ignoring the dif-\nferences between classes. As a result, ğ¶ğ´ğ‘€\nğ‘– \ncontains fewer false pos-\nitives belonging to the background but more false detections among\ndifferent foreground classes. In contrast, ğ¶ğ´ğ‘€\nğ›©\n, produced based on\nclass-dependent features, can better capture the class-specific regions\nbut may suffer from more false negatives. Therefore, we propose a\ncross-guidance loss defined as\nîˆ¸\nğ¶ğº \n= âˆ’ğ‘Œ\nğ›© \nâˆ— ğ‘™ğ‘œğ‘”(ğ‘šğ‘–ğ‘›(ğ¶ğ´ğ‘€\nğ›©\n[1], ğ¶ğ´ğ‘€\nğ‘–\n[1])). (12)\nIn this way, ğ¶ğ´ğ‘€\nğ‘– \nand ğ¶ğ´ğ‘€\nğ›© \ncould complement each other during\ntraining.\n3.3. Overlapping suppression\nIn medical image segmentation, each pixel must be assigned to only\none specific class. To achieve this, we construct a pixel-wise exclusive\nloss defined as\nîˆ¸\nğ‘’ğ‘ \n= \nâˆ‘\nğ‘›\n1\narg max\nğ‘›\nâ€²\n(\nÌ„\nğ¶ğ´ğ‘€\nğ‘›\nâ€²\n,ğ‘–\n[1]) â‰  ğ‘› \nâˆ—\nÌ„\n ğ¶ğ´ğ‘€\nğ‘›,ğ‘–\n[1]\nâˆ’ \nâˆ‘\nğ‘›\n1\narg max\nğ‘›\nâ€²\n(\nÌ„\nğ¶ğ´ğ‘€\nğ‘›\nâ€²\n,ğ‘–\n[1]) == ğ‘› \nâˆ—\nÌ„\n ğ¶ğ´ğ‘€\nğ‘›,ğ‘–\n[1], (13)\nwhere\nÌ„\n ğ¶ğ´ğ‘€\nğ‘›,ğ‘– \n= \nâˆ\nğ‘™ \nğ¶ğ´ğ‘€\nğ‘™\nğ‘›,ğ‘– \nâˆˆ R\n2Ã—ğ»ğ‘Š \nand\nÌ„\n ğ¶ğ´ğ‘€\nğ‘›,ğ‘–\n[1] âˆˆ R\nğ»ğ‘Š \nis the\nCAM corresponding to foreground. For each pixel, only the highest\nresponse in one certain CAM would be preserved while its values in\nother CAMs are suppressed. Thereby, the overlapping problem will be\nalleviated.\nAs discussed in Section 2.2, overlapped activation regions are highly\ndiscriminative for classification tasks, due to label symbiosis. Further-\nmore, CAMs tend to only activate the most discriminative regions, as\nthe overlapped activation regions would further hinder the activation\nof non-overlapping areas. To better refine CAMs for segmentation, we\nconstruct a mutually exclusive loss defined as\nîˆ¸\nğ‘šğ‘’ \n= îˆ¸\nğ‘’ğ‘ \n+ \nâˆ‘\nğ‘›\nîˆ¸\nğ¶ğ¸ \n(ğºğ´ğ‘ƒ (ğ¶ğ´ğ‘€\nğ‘›,ğ‘– \nâŠ™ \nâˆ‘\nğ‘›\nâ€²\nâ‰ ğ‘›\n(1 âˆ’ ğ¶ğ´ğ‘€\nğ‘›\nâ€²\n,ğ‘–\n[1])), ğ‘Œ\nğ‘›,ğ‘–\n), (14)\nwhere ğºğ´ğ‘ƒ represents the global average pooling. The first item îˆ¸\nğ‘’ğ‘\nin îˆ¸\nğ‘šğ‘’ \nis to suppress the regions activated in the foreground CAMs but\nbelonging to other classes, thereby reducing region overlap. Following\nthe removal of these overlapped activation regions, the second item in\nîˆ¸\nğ‘’ğ‘ \npenalizes the networks to output the correct classification results\nwith the rest parts of the CAMs. In this way, CAMs are forced to focus\nmore on non-overlapping areas and learn other less-discriminative\nregions for complete detection.\n3.4. Overall losses\nAlgorithm 1 The training process of the proposed framework\nInput: Dataset X; slice-level labels for ğ‘› classes ğ‘Œ = ğ‘Œ\n0\n, ğ‘Œ\n1\n, ..., ğ‘Œ\nğ‘›\n;\ntrade-off hyper parameters\nOutput: Optimized encoder E, bottleneck layers Ï\nğ›© \nand Ï\nğ‘–\n.\n1: Initialize E, Ï\nğ›©\n, Ï\nğ‘– \nand the FDM module ğœƒ;\n2: while in iteration number do\n3: Randomly select a batch of input slices ğ‘‹\nğ‘– \n= {ğ‘¥\nğ‘–\n0\n, ğ‘¥\nğ‘–\n0\n, ..., ğ‘¥\nğ‘–\nğµ \n}\nand the reference slices ğ‘‹\nğ‘Ÿ \n= {ğ‘¥\nğ‘Ÿ\n0\n, ğ‘¥\nğ‘Ÿ\n0\n, ..., ğ‘¥\nğ‘Ÿ\nğµ \n}, for each\ninput-reference slice pair \nâˆ‘\nğ‘›\n(ğ‘Œ \nğ‘\nğ‘›,ğ‘– \nâŠ™ ğ‘Œ \nğ‘\nğ‘›,ğ‘Ÿ\n) â‰¤ 1;\n4: Extract the feature maps ğ‘“\nğ‘– \nand ğ‘“\nğ‘Ÿ \nfrom the input and reference\nslices through the encoder E, ğ‘“\nğ‘– \n= ğ¸(ğ‘‹\nğ‘–\n), ğ‘“\nğ‘Ÿ \n= ğ¸(ğ‘‹\nğ‘Ÿ\n);\n5: Generate the decomposed feature maps ğœƒ(ğ‘“\nğ‘–\n) through the FDM\nmodule;\n6: Generate the CAMs for the input slices and decomposed feature\nmaps ğ¶ğ´ğ‘€\nğ‘– \n= Ï\nğ‘–\n(ğ‘“\nğ‘–\n), ğ¶ğ´ğ‘€\nğœƒ \n= Ï\nğ‘–\n(ğœƒ(ğ‘“\nğ‘–\n)) and the estimated class\nscore\nÌ‚\n ğ‘Œ\nğ‘– \n= ğºğ´ğ‘ƒ (ğ¶ğ´ğ‘€\nğ‘–\n),\nÌ‚\n ğ‘Œ\nğœƒ \n= ğºğ´ğ‘ƒ (ğ¶ğ´ğ‘€\nğœƒ \n);\n7: Calculate the slice-level labels for the decomposed feature maps\nğ‘Œ \nğ‘\nğ‘›,ğœƒ \n= ğ‘Œ \nğ‘\nğ‘›,ğ‘– \nâŠ™ ğ‘Œ \nğ‘\nğ‘›,ğ‘Ÿ \n;\n8: Calculate the Overall loss îˆ¸ = îˆ¸\nğ¶ğ¶ \n+ ğ›¼îˆ¸\nğ‘€ğ¸ \n+ ğ›½îˆ¸\nğ¶ğº \nand optimize\nthe E, Ï\nğ›©\n, Ï\nğ‘– \nand ğœƒ;\n9: end while\n10: return Optimized encoder E, bottleneck layers Ï\nğ›© \nand Ï\nğ‘–\n.\nThe training process of the proposed framework is summarized as\nthe pseudo-code as shown in Algorithm 1. During training, a total of\nfour outputs are penalized, including ğ¶ğ´ğ‘€\nğ‘–\n, ğ¶ğ´ğ‘€\nğ›©\n,\nÌ‚\n ğ‘Œ\nğ‘–\n, and ğ‘Œ\nğ›©\n. The\nfirstly-adopted loss functions are îˆ¸\nğ‘ğ‘™ğ‘ğ‘ ğ‘  \nand îˆ¸\nğ‘ğ‘ğ‘š \nfor classification and\nCAMs refinement respectively, combined as\n\nComputers in Biology and Medicine 171 (2024) 108228\n6\nZ. Kuang et al.\nîˆ¸\nğ¶ğ¶ \n= îˆ¸\nğ‘ğ‘ğ‘š\n(ğ¶ğ´ğ‘€\nğ‘–\n, ğ‘Œ\nğ‘–\n) + îˆ¸\nğ‘ğ‘ğ‘š\n(ğ¶ğ´ğ‘€\nğ›©\n, ğ‘Œ\nğ›©\n) + îˆ¸\nğ‘ğ‘™ğ‘ğ‘ ğ‘ \n(\nÌ‚\nğ‘Œ\nğ‘–\n, ğ‘Œ\nğ‘–\n) + îˆ¸\nğ‘ğ‘™ğ‘ğ‘ ğ‘ \n(\nÌ‚\nğ‘Œ\nğ›©\n, ğ‘Œ\nğ›©\n).\n(15)\nThen, the mutually exclusive loss îˆ¸\nğ‘šğ‘’ \nis applied to ğ¶ğ´ğ‘€\nğ‘– \nand ğ¶ğ´ğ‘€\nğ›©\nto separate multi-class regions in CAMs, combined as\nîˆ¸\nğ‘€ğ¸ \n= îˆ¸\nğ‘šğ‘’\n(ğ¶ğ´ğ‘€\nğ‘–\n, ğ‘Œ\nğ‘–\n) + îˆ¸\nğ‘šğ‘’\n(ğ¶ğ´ğ‘€\nğ›©\n, ğ‘Œ\nğ›©\n). (16)\nFinally, the cross-guidance loss îˆ¸\nğ¶ğº \nin Eq. (12) is applied to integrate\nthe CAMs produced by the class-independent and the class-dependent\nfeatures. The overall loss is formulated as\nîˆ¸ = îˆ¸\nğ¶ğ¶ \n+ ğ›¼îˆ¸\nğ‘€ğ¸ \n+ ğ›½îˆ¸\nğ¶ğº \n, (17)\nwhere ğ›¼ and ğ›½ are the trade-off hyper-parameters (default as 0.1 and\n0.1 in our experiments to promise the stability of the backbone during\ntraining).\n4. Evaluation\n4.1. Dataset and preprocessing\nThe publicly-available BraTS 2019 datasets [28â€“30] for brain tumor\nsegmentation and ISLES 2015 [31] dataset for ischemic stroke lesion\nsegmentation are selected for evaluation.\nThe BraTS 2019 dataset consists of MR images including T1, T1CE,\nT2, and FLAIR volumes, as well as pixel-wise annotations for the GD-\nenhancing tumor (ET), the peritumoral edema (ED), the necrotic and\nnon-enhancing tumor core (NET). It contains 335 brain tumor patients\nfrom 19 different contributors for training and 125 patients for testing\n(without labels). The training set is divided into 234 patients for train-\ning and 101 patients for validating locally, with High-Grade Glioma\n(HGG) and Low-Grade Glioma (LGG) patients distributed equally in\nboth data splits. The official segmentation evaluation is conducted on\nthe whole tumor (i.e. WT = ET + ED + NET), the tumor core (i.e. TC =\nET + NET), and ET separately. For weakly supervised learning, only the\nslice-level classification labels of ED and TC are available for training.\nIn addition to the segmentation maps of ED and TC produced by slice-\nlevel labels, the segmentation maps of WT are generated by fusing the\nresults of ED and TC for evaluation.\nThe ISLES 2015 dataset consists of MR images including TTP, CBV,\nCBF, Tmax, DWI, T2, and T1c volumes, as well as pixel-wise anno-\ntations for the infarct core (IC), and the surrounding tissue that can\nbe potentially rescued (the â€˜â€˜penumbraâ€™â€™, PB). It contains 30 ischemic\nstroke patients for training and 20 patients without labels for testing.\nThe training set is divided into 20 patients for training and 10 patients\nfor validating locally. Similar to BraTS 2019, on the ISLES2015 dataset,\nwe conduct segmentation evaluation on the whole stroke lesion (i.e. WS\n= IC + PB), IC, and PB separately.\nFor data preprocessing, all 3D MRI scans are regarded as a collection\nof 2D slices/images with the same resolution as 240 Ã— 240 pixels. The\nslice-level label of each slice is derived from the corresponding pixel-\nwise annotations. For BraTS2019, 2D slices are sampled with a step\nof three from 3D MRI scans, producing a total of 9368 2D slices for\ntraining where 679 slices only contain ED and 19 slices only contain\nTC. For ISLES2015, in total 1499 2D slices are obtained, among which\n117 slices only contain PB and 7 slices only contain IC.\n4.2. Implementation details\nAll the networks were trained using an Adam optimizer with an\ninitial learning rate of 1eâˆ’5 and a batch size of 32. All methods were\nimplemented within the PyTorch framework and trained on Nvidia GTX\n3090 Ti GPUs for 100 epochs. Quantitative results of the online testing\nset are obtained by uploading the segmentation results generated by\nthe best models on the local validation set to the official websites of the\nBraTS and ISLES challenges. It should be noted that for the proposed\nmethod, the produced CAMs were directly used for pixel-wise evalua-\ntion without retraining (i.e., regarding those CAMs as pseudo pixel-wise\nannotations for fully supervised learning). Instead, the foreground and\nbackground CAMs, denoted as ğ¶ğ´ğ‘€\nğ‘™\nğ‘› \n= {ğ¶ğ´ğ‘€\nğ‘™\nğ‘›\n[0], ğ¶ğ´ğ‘€\nğ‘™\nğ‘›\n[1]}, were\nfirst jointly normalized by Min-Max normalization, and then the seg-\nmentation maps were obtained from the normalized foreground CAMs\nğ‘\nğ‘“ \nwith a fixed threshold of 0.5.\n4.3. Settings for comparison\nTo validate the benefits of different components, combinations of\ndifferent loss functions and modules are used for comparison, including:\n1. îˆ¸\nğ‘ \nğ¶ğ¶ \n: using îˆ¸\nğ‘ğ‘ğ‘š \nand îˆ¸\nğ‘ğ‘™ğ‘ğ‘ ğ‘  \nto train the multi-level classification\nnetworks with slice-level labels for each class separately.It is to\nbenchmark the performance of weakly supervised medical image\nsegmentation with single-class slice-level labels.\n2. îˆ¸\nğ¶ğ¶ \n: adopting îˆ¸\nğ‘ğ‘ğ‘š \nand îˆ¸\nğ‘ğ‘™ğ‘ğ‘ ğ‘  \nto train MLCN with multi-class\nlabels. It is to show the baseline performance of multi-class\nweakly supervised medical image segmentation with slice-level\nlabels.\n3. îˆ¸\nğ¶ğ¶ \n+ ğ¹ ğ·ğ‘€: introducing feature decomposition module for\nfeature decomposition. It is to validate the effectiveness of FDM\nin addressing label symbiosis.\n4. îˆ¸\nğ¶ğ¶ \n+ ğ¹ ğ·ğ‘€ + îˆ¸\nğ¶ğº \n: using îˆ¸\nğ¶ğº \nfor guiding FDM. It is to validate\nthe effectiveness of cross-guidance between CAMs.\n5. îˆ¸\nğ¶ğ¶ \n+ ğ¹ ğ·ğ‘€ + îˆ¸\nğ¶ğº \n+ îˆ¸\nğ‘€ğ¸ \n: the complete framework.\n4.4. Metrics\nThree widely-used metrics are adopted for evaluation, including\nsensitivity (SE), specificity (SP), and Dice coefficient (DC), defined as:\nğ‘†ğ‘ƒ = \nğ‘‡ ğ‘\nğ‘‡ ğ‘ + ğ¹ ğ‘ƒ \n, ğ·ğ¶ = \n2 âˆ— ğ‘‡ ğ‘ƒ\n2 âˆ— ğ‘‡ ğ‘ƒ + ğ¹ ğ‘ƒ + ğ¹ ğ‘ \n, ğ‘†ğ¸ = \nğ‘‡ ğ‘ƒ\nğ‘‡ ğ‘ƒ + ğ¹ ğ‘ \n,\nğ¼ğ‘‚ğ‘ˆ = \nğ‘‡ ğ‘ƒ\nğ‘‡ ğ‘ƒ + ğ¹ ğ‘ƒ + ğ¹ ğ‘ \n, (18)\nwhere TP, TN, FP, and FN refer to true positives, true negatives,\nfalse positives, and false negatives respectively. Specially, mean and\nstandard deviation (SD) are used to summarize the distribution of\nevaluation metrics, and all the metrics are estimated beyond the 95%\nconfidence level.\n4.5. Results analysis\nExemplar qualitative results of brain tumor and stroke lesion seg-\nmentation results are shown in Figs. 5 and 6 separately. When training\nwith single-class labels via îˆ¸\nğ‘ \nğ¶ğ¶ \n, there exist considerable false positives\ndue to the lack of interactions among multiple classes. Given multi-\nclass labels for joint training by classification loss only, îˆ¸\nğ¶ğ¶ \n, false\npositives are largely alleviated. However, as discussed in Section 2.1,\nlabel symbiosis would hinder the classifier from learning class-specific\nfeatures for different classes, making their segmentation maps highly\noverlapped.\nWith the help of the feature decomposition module, better class-\ndependent features are learned for each class, leading to slightly better\nboundary preservation. Compared to îˆ¸\nğ¶ğ¶ \n, introducing FDM can effec-\ntively reduce false negatives as shown in Fig. 5, but as discussed in\nSection 3.2, the class-dependent learning brings more false negatives as\nshown in Fig. 6. Combining with the cross-guidance loss îˆ¸\nğ¶ğº \n, the fea-\ntures learned through multi-class and single-class labels are integrated,\nremoving false positives and false negatives at the same time. However,\ndue to the location adjacency of tumors and lesions, their segmentation\nmaps are badly overlapped.\nComparatively, with the mutually exclusive loss îˆ¸\nğ‘€ğ¸ \n, regions be-\nlonging to different classes are effectively separated as shown in Figs. 5\nand 6. According to îˆ¸\nğ‘€ğ¸ \n, each pixel in CAMs is forced to be classified\n\nComputers in Biology and Medicine 171 (2024) 108228\n7\nZ. Kuang et al.\nFig. 5. Qualitative results under different settings on brain tumor segmentation of the BraTS 2019 dataset. From left to right: the raw FLAIR slides, the brain tumor segmentation\nmaps generated by îˆ¸\nğ‘ \nğ¶ğ¶ \n, îˆ¸\nğ¶ğ¶ \n, îˆ¸\nğ¶ğ¶ \n+ ğ¹ ğ·ğ‘€, îˆ¸\nğ¶ğ¶ \n+ ğ¹ ğ·ğ‘€ + îˆ¸\nğ¶ğº \n, and îˆ¸\nğ¶ğ¶ \n+ ğ¹ ğ·ğ‘€ + îˆ¸\nğ¶ğº \n+ îˆ¸\nğ‘€ğ¸ \n(i.e., the proposed framework) respectively, and the ground true annotations. The\ntumor core is colored yellow while the peritumoral edema is green.These examples show the proposed frameworks could better differentiate the tumor core and the edema rather\nthan segmenting them as the same and could recall more target regions.\ninto one specific class, thus minimizing the overlap between multi-\nclass CAMs. Furthermore, penalized by îˆ¸\nğ‘€ğ¸ \n, the overlapping features\nare suppressed, and the classifier would seek the highly discriminative\nclass-dependent features for each class, instead of focusing on the\nfeatures for foreground and background classification as discussed in\nSection 2. As a result, the segmentation maps by the complete frame-\nwork have fewer false negatives with better boundary preservation,\nleading to the best overall segmentation performance.\nQuantitative comparison results are summarized in Tables 1, 2, 3\nand 4. For both the datasets, the complete framework achieves the best\nsegmentation performance consistently on the local and the official test\nset for the different tumor tissues and lesions. With average DC scores of\n0.72 for the whole tumor; 0.51 for the tumor core; 0.49 for the edema;\n0.65 for the whole stroke lesion; 0.42 for the infarct core, and 0.62 for\nthe penumbra. In general, the quantitative results in Tables 1 to 4 are\nconsistent with the qualitative results in Figs. 5 and 6, demonstrating\nthe stability of the proposed weakly supervised learning framework.\nIn terms of the benefits from different components, training with\nsingle-class labels separately via îˆ¸\nğ‘ \nğ¶ğ¶ \nis better than training with multi-\nclass labels via îˆ¸\nğ¶ğ¶ \nin most cases, validating the statement that weakly\nmulti-class medical image segmentation is more challenging in [16,\n18]. After applying feature decomposition through îˆ¸\nğ¶ğ¶ \n+ ğ¹ ğ·ğ‘€, non-\nnegligible performance improvements are achieved, showing its ef-\nfectiveness in addressing label symbiosis. However, the performance\nimprovements brought by FDM are unstable, resulting in an aver-\nage increase of 12.09% in DC for WS segmentation locally, while an\naverage decrease of 2.81% in DC for IC segmentation on the ISLES\n2015 dataset. Introducing the cross guidance loss îˆ¸\nğ¶ğº \neffectively in-\ntegrates the class-dependent and -independent features and obtains\na more robust segmentation performance. It should be noticed that,\n\nComputers in Biology and Medicine 171 (2024) 108228\n8\nZ. Kuang et al.\nFig. 6. Qualitative results under different settings on stroke lesion segmentation of the ISLES 2015 dataset. From left to right: the raw DWI slides, the stroke lesion segmentation\nmaps generated by îˆ¸\nğ‘ \nğ¶ğ¶ \n, îˆ¸\nğ¶ğ¶ \n, îˆ¸\nğ¶ğ¶ \n+ ğ¹ ğ·ğ‘€, îˆ¸\nğ¶ğ¶ \n+ ğ¹ ğ·ğ‘€ + îˆ¸\nğ¶ğº \n, and îˆ¸\nğ¶ğ¶ \n+ ğ¹ ğ·ğ‘€ + îˆ¸\nğ¶ğº \n+ îˆ¸\nğ‘€ğ¸ \n(i.e., the proposed framework) respectively, and the ground true annotations. The\ntumor core is colored yellow while the peritumoral edema is green. These examples show the proposed frameworks could largely reduce the overlapping between the infarct core\nand the penumbra and could obtain more complete boundaries of the penumbra.\nin FDM, the slices with single classes are selected as references for\ncross guidance. From the data augmentation perspective, those slices\nare augmented via resampling. To better testify the effectiveness of\nFDM, experimental results obtained by îˆ¸\nğ¶ğ¶ \nand resampling are used\nfor comparison. On the BraTS dataset, though îˆ¸\nğ¶ğ¶ \n+ ğ‘…ğ‘’ğ‘ ğ‘ğ‘šğ‘ğ‘™ğ‘–ğ‘›ğ‘” im-\nproves the overall segmentation performance, the improvements are\nrelatively limited compared to the feature decomposition module with\ncross guidance loss, îˆ¸\nğ¶ğ¶ \n+ ğ¹ ğ·ğ‘€ + îˆ¸\nğ¶ğº \n. On the smaller ISLES dataset,\nsimple resampling gets an effective performance improvement for TC\nsegmentation, while a remarkable reduction for WS and PB segmenta-\ntion. The results of îˆ¸\nğ¶ğ¶ \n+ ğ‘…ğ‘’ğ‘ ğ‘ğ‘šğ‘ğ‘™ğ‘–ğ‘›ğ‘” demonstrate the value of FDM.\nFurther, the mutual exclusive loss îˆ¸\nğ‘€ğ¸ \nsignificantly suppresses the\noverlaps between different tumor tissues and lesions, bringing full-scale\nsegmentation performance improvements. Finally, the proposed frame-\nwork (e.g., îˆ¸\nğ¶ğ¶ \n+ ğ¹ ğ·ğ‘€ + îˆ¸\nğ¶ğº \n+ îˆ¸\nğ‘€ğ¸ \n) achieves the best segmentation\nresults, leading to an average increase of 4.04% and 2.49% in DC\nand mIOU respectively, compared to îˆ¸\nğ¶ğ¶ \n+ ğ¹ ğ·ğ‘€ + îˆ¸\nğ¶ğº \n. The mIOU\nboxplots shown in Fig. 7 further illustrate the proposed framework\ngets a robust improvement for the segmentation performance on both\nthe datasets, compared to other 5 methods. More importantly, the\ncomplete framework outperforms îˆ¸\nğ‘ \nğ¶ğ¶ \nwith a large margin (i.e., an\naverage increase of 8.73% and 8.65% in DC and mIOU on the BraTS\n2019 dataset, an average increase of 4.57% and 5.51% in DC and\nmIOU on the ISLES 2015 dataset), demonstrating the value of utilizing\nmulti-class slice-level labels for medical image segmentation.\n\nComputers in Biology and Medicine 171 (2024) 108228\n9\nZ. Kuang et al.\nTable 1\nQuantitative segmentation results on the local validation set of BraTS 2019.\nMethods îˆ¸\nğ‘ \nğ¶ğ¶ \nîˆ¸\nğ¶ğ¶ \nîˆ¸\nğ¶ğ¶ \n+ ğ‘…ğ‘’ğ‘ ğ‘ğ‘šğ‘ğ‘™ğ‘–ğ‘›ğ‘” îˆ¸\nğ¶ğ¶ \n+ ğ¹ ğ·ğ‘€ îˆ¸\nğ¶ğ¶ \n+ ğ¹ ğ·ğ‘€ + îˆ¸\nğ¶ğº \nîˆ¸\nğ¶ğ¶ \n+ ğ¹ ğ·ğ‘€ + îˆ¸\nğ¶ğº \n+ îˆ¸\nğ‘€ğ¸\nWT\nSE (%) 54.96 Â± 15.64 50.01 Â± 14.67 45.99 Â± 13.16 59.18 Â± 14.17 62.94 Â± 15.04 68.54 Â± 15.95\nSP (%) 99.83 Â± 0.12 99.84 Â± 0.11 99.89 Â± 0.11 99.82 Â± 0.16 99.84 Â± 0.13 99.81 Â± 0.13\nDC (%) 61.19 Â± 14.45 57.84 Â± 15.90 56.30 Â± 14.30 64.49 Â± 15.26 68.13 Â± 14.48 71.68 Â± 12.45\nIOU (%) 45.59 Â± 14.48 42.49 Â± 14.79 40.45 Â± 13.44 49.42 Â± 15.09 53.53 Â± 14.98 57.23 Â± 13.70\nTC\nSE (%) 55.29 Â± 25.54 49.88 Â± 21.12 47.68 Â± 20.49 78.47 Â± 18.66 80.30 Â± 18.85 73.59 Â± 19.92\nSP (%) 99.60 Â± 0.28 99.83 Â± 0.15 99.88 Â± 0.11 99.59 Â± 0.28 99.56 Â± 0.30 99.65 Â± 0.24\nDC (%) 38.72 Â± 21.19 45.67 Â± 17.92 47.76 Â± 17.64 49.92 Â± 20.59 49.64 Â± 20.80 50.01 Â± 19.84\nIOU (%) 26.61 Â± 17.08 31.28 Â± 14.28 32.69 Â± 14.69 35.73 Â± 18.35 35.59 Â± 18.71 36.01 Â± 17.74\nED\nSE (%) 49.29 Â± 18.19 30.98 Â± 15.16 29.12 Â± 14.12 43.32 Â± 17.70 48.95 Â± 19.23 59.21 Â± 19.77\nSP (%) 99.57 Â± 0.28 99.59 Â± 0.25 99.66 Â± 0.23 99.55 Â± 0.29 99.54 Â± 0.30 99.51 Â± 0.33\nDC (%) 43.36 Â± 18.85 30.90 Â± 17.60 31.09 Â± 17.15 39.34 Â± 18.83 43.04 Â± 20.00 48.64 Â± 18.76\nIOU (%) 29.63 Â± 15.60 19.53 Â± 12.48 19.63 Â± 12.22 26.17 Â± 14.63 29.67 Â± 15.98 34.11 Â± 15.87\nmIOU 33.79 Â± 10.69 31.13 Â± 9.69 31.16 Â± 9.92 37.20 Â± 9.92 39.47 Â± 9.88 42.44 Â± 9.24\nTable 2\nQuantitative segmentation results on the online test set of BraTS 2019.\nMethods îˆ¸\nğ‘ \nğ¶ğ¶ \nîˆ¸\nğ¶ğ¶ \nîˆ¸\nğ¶ğ¶ \n+ ğ‘…ğ‘’ğ‘ ğ‘ğ‘šğ‘ğ‘™ğ‘–ğ‘›ğ‘” îˆ¸\nğ¶ğ¶ \n+ ğ¹ ğ·ğ‘€ îˆ¸\nğ¶ğ¶ \n+ ğ¹ ğ·ğ‘€ + îˆ¸\nğ¶ğº \nîˆ¸\nğ¶ğ¶ \n+ ğ¹ ğ·ğ‘€ + îˆ¸\nğ¶ğº \n+ îˆ¸\nğ‘€ğ¸\nWT\nSE (%) 65.21 Â± 16.00 55.45 Â± 15.78 51.63 Â± 14.50 64.04 Â± 14.71 67.62 Â± 15.49 74.69 Â± 16.13\nSP (%) 98.75 Â± 0.69 98.23 Â± 1.28 99.17 Â± 0.60 98.55 Â± 1.02 98.88 Â± 0.76 98.59 Â± 0.09\nDC (%) 66.63 Â± 15.11 55.82 Â± 18.05 58.94 Â± 16.12 63.90 Â± 17.06 68.62 Â± 16.89 72.71 Â± 14.62\nTC\nSE (%) 58.17 Â± 25.71 48.54 Â± 18.50 43.70 Â± 29.95 77.59 Â± 18.41 80.17 Â± 18.51 72.05 Â± 19.10\nSP (%) 99.02 Â± 0.93 98.94 Â± 0.91 99.22 Â± 0.66 97.68 Â± 1.52 97.48 Â± 1.70 97.96 Â± 1.39\nDC (%) 43.17 Â± 21.93 46.29 Â± 17.73 47.66 Â± 17.36 53.50 Â± 18.68 53.68 Â± 19.50 53.71 Â± 16.45\nTable 3\nQuantitative segmentation results on the local validation set of ISLES 2015 dataset.\nMethods îˆ¸\nğ‘ \nğ¶ğ¶ \nîˆ¸\nğ¶ğ¶ \nîˆ¸\nğ¶ğ¶ \n+ ğ‘…ğ‘’ğ‘ ğ‘ğ‘šğ‘ğ‘™ğ‘–ğ‘›ğ‘” îˆ¸\nğ¶ğ¶ \n+ ğ¹ ğ·ğ‘€ îˆ¸\nğ¶ğ¶ \n+ ğ¹ ğ·ğ‘€ + îˆ¸\nğ¶ğº \nîˆ¸\nğ¶ğ¶ \n+ ğ¹ ğ·ğ‘€ + îˆ¸\nğ¶ğº \n+ îˆ¸\nğ‘€ğ¸\nWS\nSE (%) 52.09 Â± 10.26 46.00 Â± 11.14 44.00 Â± 11.18 63.97 Â± 10.73 71.79 Â± 13.93 69.60 Â± 14.27\nSP (%) 99.70 Â± 0.08 99.57 Â± 0.23 99.44 Â± 0.20 99.36 Â± 0.17 98.97 Â± 0.41 99.25 Â± 0.17\nDC (%) 62.79 Â± 8.86 55.83 Â± 10.42 52.24 Â± 12.15 67.92 Â± 8.91 68.48 Â± 8.29 69.18 Â± 11.76\nIOU (%) 46.41 Â± 9.45 39.49 Â± 9.92 36.41 Â± 11.17 51.84 Â± 9.95 52.83 Â± 8.86 54.03 Â± 13.47\nIC\nSE (%) 48.64 Â± 18.87 82.64 Â± 9.17 55.84 Â± 13.06 31.70 Â± 21.57 70.57 Â± 18.12 60.11 Â± 15.73\nSP (%) 99.68 Â± 0.17 99.03 Â± 0.39 99.65 Â± 0.14 99.85 Â± 0.09 99.30 Â± 0.37 99.66 Â± 0.18\nDC (%) 38.09 Â± 9.96 34.32 Â± 9.89 38.37 Â± 6.47 31.51 Â± 15.72 38.26 Â± 10.53 42.55 Â± 9.76\nIOU (%) 23.93 Â± 7.16 19.85 Â± 10.71 24.03 Â± 5.45 21.04 Â± 7.39 24.11 Â± 8.08 27.49 Â± 7.74\nPB\nSE (%) 51.15 Â± 11.73 43.41 Â± 12.08 39.01 Â± 11.98 57.34 Â± 9.81 68.94 Â± 14.40 67.70 Â± 15.21\nSP (%) 99.45 Â± 0.21 99.36 Â± 0.28 99.27 Â± 0.26 99.19 Â± 0.23 98.68 Â± 0.59 98.90 Â± 0.37\nDC (%) 57.29 Â± 9.48 49.41 Â± 11.53 44.66 Â± 13.56 59.40 Â± 8.60 61.11 Â± 8.06 61.75 Â± 11.63\nIOU (%) 40.80 Â± 9.73 33.95 Â± 10.67 29.74 Â± 11.73 42.68 Â± 8.90 44.90 Â± 8.36 45.91 Â± 12.53\nmIOU 37.03 Â± 6.65 30.96 Â± 6.50 29.93 Â± 8.01 38.51 Â± 9.92 40.54 Â± 6.90 42.54 Â± 9.86\nTable 4\nQuantitative segmentation results on the online test set of ISLES 2015 for whole stroke lesion segmentation.\nMethods îˆ¸\nğ‘ \nğ¶ğ¶ \nîˆ¸\nğ¶ğ¶ \nîˆ¸\nğ¶ğ¶ \n+ ğ‘…ğ‘’ğ‘ ğ‘ğ‘šğ‘ğ‘™ğ‘–ğ‘›ğ‘” îˆ¸\nğ¶ğ¶ \n+ ğ¹ ğ·ğ‘€ îˆ¸\nğ¶ğ¶ \n+ ğ¹ ğ·ğ‘€ + îˆ¸\nğ¶ğº \nîˆ¸\nğ¶ğ¶ \n+ ğ¹ ğ·ğ‘€ + îˆ¸\nğ¶ğº \n+ îˆ¸\nğ‘€ğ¸\nRE (%) 52 Â± 12 47 Â± 11 45 Â± 11 69 Â± 20 64Â±16 64 Â± 15\nPR (%) 66 Â± 17 60 Â± 19 52 Â± 18 53 Â± 11 57 Â± 17 57 Â± 18\nDC (%) 57 Â± 13 52 Â± 14 47 Â± 12 58 Â± 14 59 Â± 15 60 Â± 16\nThe online test for ISLES 2015 adopts recall (RE), precision (PR), and dice (DC) to validate the segmentation performance.\n5. Discussion\n5.1. Comparison with related methods\nIn this section, the state-of-the-art slice-level weakly supervised\nmethods via cross-modality equivariant constraints (CMEC) [18] for\nthe brain tumor segmentation on Brats2019 and via attentional CAM\n(ACAM) [16] for stroke lesion segmentation on ISLES2017 (an ex-\ntended version of ISLES2015) are chosen for comparison separately. As\ndiscussed in Section 1, existing weakly supervised medical image seg-\nmentation methods, including the above two methods, focus on single-\nclass slice-level labels. For a fair comparison, we apply the feature\ndecomposition module for single-class medical image segmentation\n(i.e. whole tumor segmentation and whole stroke lesion segmentation).\nIn [13,15], four-fold cross-validation was adopted but the data splits\nwere unknown. Thus, in this paper, four-fold cross-validation was\nconducted with random data splits. In addition, we re-implement two\nweakly-supervised segmentation methods which achieved superior per-\nformance on tumor segmentation in histopathology images, WSISS [8],\nand geographic atrophy lesions in SD-OCT images MS-CAM [33], to\nmake further comparison.\nQuantitative comparison results are stated in Table 5. Compared\nto the unstable segmentation performance of applying classification\nloss only, îˆ¸\nğ‘ \nğ¶ğ¶ \n, introducing a feature decomposing module effectively\nstabilizes the segmentation performance, with an average increase of\n12.37% in DC. Across all four folds, the proposed approach consistently\noutperforms the state-of-the-art weakly-supervised brain tumor seg-\nmentation method in [18], leading to performance improvements rang-\ning from 3.82% to 5.51% in DC. As for the stroke lesion segmentation,\nour method outperforms the method in [16] with an average increase\nof 3.51% in DC. Furthermore, the segmentation performance of the\n\nComputers in Biology and Medicine 171 (2024) 108228\n10\nZ. Kuang et al.\nFig. 7. The mIOU boxplots of segmentation performance of different methods on BraTS 2019 dataset and ISLES 2015 dataset.\nFig. 8. Qualitative results under different settings on SICH and PHE segmentation. From left to right: the raw CT slides, the SICH and PHE segmentation maps generated by îˆ¸\nğ‘ \nğ¶ğ¶ \n,\nîˆ¸\nğ¶ğ¶ \n, îˆ¸\nğ¶ğ¶ \n+ ğ¹ ğ·ğ‘€, îˆ¸\nğ¶ğ¶ \n+ ğ¹ ğ·ğ‘€ + îˆ¸\nğ¶ğº \n, and îˆ¸\nğ¶ğ¶ \n+ ğ¹ ğ·ğ‘€ + îˆ¸\nğ¶ğº \n+ îˆ¸\nğ‘€ğ¸ \n(i.e., the proposed framework) respectively, and the ground true annotations. Here, SICH and PHE are colored\nblue and red. These examples show the proposed frameworks could better distinguish the hemorrhage from the edema and obtain more precise boundaries for both the edema\nand the hemorrhage.\nproposed framework is more stable across the folds compared to MS-\nCAM and MSISS, the DC variances of the proposed framework are only\n1.12% and 4.68% on the two datasets separately. The above compar-\nison results further validate the effectiveness of the proposed method\nfor both single-class and multi-class medical image segmentation.\n5.2. Analysis of the robustness\nTo further validate the robustness of the proposed method, we\nconduct an additional evaluation on the training set and visualize the\nresults on different datasets as line charts in Fig. 9. It should be noted\nthat only slice-level labels were used for optimization during training.\nOn the BraTS19 dataset, all six methods achieve stable segmentation\nperformance across the three datasets, among which training the model\nby single-class classification loss îˆ¸\nğ‘ \nğ¶ğ¶ \nhas the largest performance vari-\nations, i.e., 5.44% in DC. Comparatively, all approaches encounter\nperformance variations across the training, the validation, and the\ntest sets on the ISLES15 dataset due to its relatively limited samples.\nSpecifically, îˆ¸\nğ‘ \nğ¶ğ¶ \nencounters an average of 15.38% decrease in DC\ntested on the training and validation sets. Introducing multi-class clas-\nsification loss îˆ¸\nğ¶ğ¶ \neffectively reduces the performance gap to 3.83%\nin DC. In general, the proposed multi-class weakly-supervised frame-\nwork achieves better and more consistent segmentation performance\ncompared to the single-class approach îˆ¸\nğ‘ \nğ¶ğ¶ \n.\n\nComputers in Biology and Medicine 171 (2024) 108228\n11\nZ. Kuang et al.\nTable 5\nQuantitative comparison results, DC (%) of whole tumor and whole stroke lesion segmentation on the BraTS 2019 and ISLES 2017 dataset.\nLesion Methods Folds\n1 2 3 4 Avg.\nWT (BraTS19)\nWSISS 52.55 Â± 14.13 56.68 Â± 14.15 59.26 Â± 15.37 51.69 Â± 13.92 55.04 Â± 4.21\nMS-CAM 64.58 Â± 18.15 62.65 Â± 15.59 61.42 Â± 20.32 58.57 Â± 18.34 61.80 Â± 3.23\nSEAM [32]\na\nN/A \n56.14 Â± 11.93\nCMEC [18] 62.40 Â± 14.32\nîˆ¸\nğ‘ \nğ¶ğ¶ \n46.17 Â± 17.15 55.68 Â± 16.15 58.38 Â± 17.26 59.59 Â± 14.90 54.96 Â± 8.78\nîˆ¸\nğ‘ \nğ¶ğ¶ \n+ ğ¹ ğ·ğ‘€ + îˆ¸\nğ¶ğº \n67.91 Â± 11.16 67.67 Â± 13.79 67.54 Â± 14.98 66.22 Â± 14.18 67.33 Â± 1.12\nWS (ISLES17)\nWSISS 21.10 Â± 21.05 32.20 Â± 19.29 32.23 Â± 27.65 24.23 Â± 21.06 27.44 Â± 6.34\nMS-CAM 20.23 Â± 18.44 29.24 Â± 14.55 31.06 Â± 21.50 20.07 Â± 18.73 25.15 Â± 5.91\nACAM [16] N/A 33.81\nîˆ¸\nğ‘ \nğ¶ğ¶ \n29.64 Â± 13.51 32.52 Â± 22.31 40.25 Â± 23.46 31.30 Â± 20.87 33.42 Â± 6.82\nîˆ¸\nğ‘ \nğ¶ğ¶ \n+ ğ¹ ğ·ğ‘€ + îˆ¸\nğ¶ğº \n32.64 Â± 13.09 35.48 Â± 19.69 42.63 Â± 21.67 38.53 Â± 18.51 37.32 Â± 4.68\na \nResults re-implemented and reported by [18].\nFig. 9. DC(%) comparison of the whole tumor (WT) and the whole stroke lesion segmentation among the training set, local validation set, and online test set of the BraTS 2019\n(i.e. left figure) and ISLES 2015 dataset (i.e. right figure). It should be noted that only slice-level labels were used for optimization during training.\n5.3. Analysis of the ğ›¼ and ğ›½ value\nIn general, both ğ›¼ and ğ›½ are set to be less than 1, to emphasize more\nclassification through îˆ¸\nğ¶ğ¶ \n. To evaluate the potential effect of ğ›¼ and ğ›½\non segmentation, quantitative DC results of three-fold cross-validation\non the ISLES 2015 dataset under different settings are summarized in\nTable 6. Compared to ğ›½, the segmentation performance is less sensitive\nto ğ›¼. With the same settings of ğ›½, the maximal performance gap is\nless than 1 in DC under varying ğ›¼. Comparatively, ğ›½ is slightly more\ndominating than ğ›¼. With the same ğ›¼, decreasing ğ›½ from 0.5 to 0.1\nachieves an average increase of 1.26% in DC across the three classes.\nMeanwhile, further decreasing ğ›¼ and ğ›½ to be smaller than 0.1 can hardly\nimprove the overall performance. Therefore, in this paper, both ğ›¼ and\nğ›½ are set as 0.1 by default. When applying the proposed framework to\nother datasets, to determine the best settings of ğ›¼ and ğ›½, we recommend\nfirst tuning ğ›½ by a small step size such as 0.1 given a fixed ğ›¼. Once ğ›½\nis determined, ğ›¼ can be tuned based on a larger step size such as 0.5.\n5.4. Extension on SICH and prostate segmentation\nTo further validate the effectiveness of the proposed weakly-\nsupervised framework, we conduct additional experiments on locally-\ncollected dataset in [14,34] where spontaneous hemorrhage (SICH) and\nperihematomal edema (PHE) in each CT slice have both pixel-wise\nannotations and slice-level labels. Under weakly supervised learning,\npixel-wise annotations are only used for evaluation. In our experiments,\nthe SICH dataset is randomly divided into three parts, including the\ntraining set with 70 CT scans, the validation set with 15 scans, and the\ntest set with 30 CT scans.\nTable 6\nAverage DC (%) comparison among the 3-fold validation on the ISLES 2015 dataset.\nğ›¼ ğ›½ WS IC PB\n0.5 0.5 62.58 Â± 8.60 39.17 Â± 3.37 53.03 Â± 7.67\n0.5 0.1 64.76 Â± 7.42 39.40 Â± 3.14 55.19 Â± 6.55\n0.5 0.05 65.54 Â± 7.64 39.83 Â± 2.72 55.46 Â± 6.29\n0.1 0.5 63.13 Â± 8.04 40.25 Â± 2.34 53.70 Â± 7.25\n0.1 0.1 65.21 Â± 7.97 39.84 Â± 2.71 56.19 Â± 5.56\n0.1 0.05 65.85 Â± 8.21 39.71 Â± 2.51 56.59 Â± 5.79\n0.05 0.5 64.45 Â± 7.18 40.17 Â± 2.66 54.97 Â± 7.47\n0.05 0.1 65.82 Â± 8.30 39.64 Â± 3.73 56.79 Â± 6.12\n0.05 0.05 65.70 Â± 7.46 39.84 Â± 2.68 56.69 Â± 5.35\nExemplar qualitative results are shown in Fig. 8. Due to label\nsymbiosis and location adjacency, the segmentation maps of SICH and\nPHE are largely overlapped, failing to distinguish different classes.\nComparatively, the proposed framework (i.e. îˆ¸\nğ¶ğ¶ \n+ğ¹ ğ·ğ‘€ +îˆ¸\nğ¸ğ‘† \n+îˆ¸\nğ‘€ğ¸ \n)\ncan effectively extract class-specific features, making the activation\nregions corresponding to different classes in CAMs more separable.\nIn addition, with the help of the mutually exclusive loss îˆ¸\nğ‘€ğ¸ \n, the\nsegmentation maps of both SICH and PHE become more complete with\nbetter boundary preservation.\nQuantitative results are summarized in Table 7. Compared to train-\ning with single-class labels via îˆ¸\nğ‘ \nğ¶ğ¶ \n, training with multi-class labels by\nîˆ¸\nğ¶ğ¶ \nbrings an average increase of 3.22% in DC for SICH and an average\ndecrease of 5.56% for PHE. Introducing the feature decomposition\nmodule significantly improves the segmentation performance for SICH\nwhile the improvements for PHE are limited. With the help of the cross-\nguidance loss îˆ¸\nğ¶ğº \n, the segmentation performance of PHE is further\n\nComputers in Biology and Medicine 171 (2024) 108228\n12\nZ. Kuang et al.\nTable 7\nQuantitative comparison results on SICH and PHE segmentation.\nMethods SICH PHE\nSE (%) SP (%) DC (%) SE (%) SP (%) DC (%)\nîˆ¸\nğ‘ \nğ¶ğ¶ \n97.35 Â± 10.83 99.61 Â± 0.22 57.41 Â± 19.53 59.18 Â± 25.08 99.30 Â± 0.44 34.21 Â± 8.16\nîˆ¸\nğ¶ğ¶ \n61.33 Â± 20.00 99.88 Â± 0.11 54.19 Â± 18.13 34.92 Â± 15.73 99.54 Â± 0.40 28.65 Â± 11.35\nîˆ¸\nğ¶ğ¶ \n+ ğ¹ ğ·ğ‘€ 69.76 Â± 20.06 99.95 Â± 0.03 70.76 Â± 7.73 49.73 Â± 15.63 99.48 Â± 0.38 34.22 Â± 6.46\nîˆ¸\nğ¶ğ¶ \n+ ğ¹ ğ·ğ‘€ + îˆ¸\nğ¶ğº \n69.89 Â± 19.55 99.97 Â± 0.02 74.33 Â± 14.27 51.36 Â± 14.73 99.50 Â± 0.62 36.19 Â± 4.20\nîˆ¸\nğ¶ğ¶ \n+ ğ¹ ğ·ğ‘€ + îˆ¸\nğ¶ğº \n+ îˆ¸\nğ‘€ğ¸ \n77.74 Â± 22.19 99.96 Â± 0.02 75.85 Â± 12.72 65.44 Â± 11.27 99.26 Â± 0.62 41.24 Â± 6.84\nTable 8\nQuantitative comparison results on prostate peripheral zone and central gland segmentation.\nMethods Peripheral zone Central gland\nSE (%) SP (%) DC (%) SE (%) SP (%) DC (%)\nîˆ¸\nğ‘ \nğ¶ğ¶ \n48.76 Â± 20.04 98.23 Â± 1.43 24.04 Â± 13.55 70.66 Â± 14.16 98.72 Â± 0.40 55.40 Â± 12.42\nîˆ¸\nğ¶ğ¶ \n26.79 Â± 13.31 99.01 Â± 0.74 19.89 Â± 13.48 51.76 Â± 16.16 99.52 Â± 0.25 58.28 Â± 13.05\nîˆ¸\nğ¶ğ¶ \n+ ğ¹ ğ·ğ‘€ 63.73 Â± 15.01 97.88 Â± 0.96 24.50 Â± 12.26 78.71 Â± 8.67 99.04 Â± 0.51 69.96 Â± 8.44\nîˆ¸\nğ¶ğ¶ \n+ ğ¹ ğ·ğ‘€ + îˆ¸\nğ¶ğº \n35.94 Â± 20.83 98.46 Â± 0.84 18.66 Â± 12.98 69.37 Â± 8.74 99.32 Â± 0.35 68.53 Â± 5.40\nîˆ¸\nğ¶ğ¶ \n+ ğ¹ ğ·ğ‘€ + îˆ¸\nğ¶ğº \n+ îˆ¸\nğ‘€ğ¸ \n62.30 Â± 20.24 98.38 Â± 0.99 28.63 Â± 15.52 71.55 Â± 12.91 99.35 Â± 0.39 69.90 Â± 8.30\nFig. 10. Qualitative results under different settings on prostate peripheral zone and central gland segmentation. From left to right: the raw T2 slides, the peripheral zone and\ncentral gland segmentation maps generated by îˆ¸\nğ‘ \nğ¶ğ¶ \n, îˆ¸\nğ¶ğ¶ \n, îˆ¸\nğ¶ğ¶ \n+ ğ¹ ğ·ğ‘€, îˆ¸\nğ¶ğ¶ \n+ ğ¹ ğ·ğ‘€ + îˆ¸\nğ¶ğº \n, and îˆ¸\nğ¶ğ¶ \n+ ğ¹ ğ·ğ‘€ + îˆ¸\nğ¶ğº \n+ îˆ¸\nğ‘€ğ¸ \n(i.e., the proposed framework) respectively, and the\nground true annotations. Here, the peripheral zone and central gland are colored cyanâ€“blue and brown. These examples show the proposed frameworks could significantly improve\nthe prostate central gland segmentation performance.\nimproved. Comparatively, after including the mutually exclusive loss\nîˆ¸\nğ‘€ğ¸ \n, the complete framework achieves average increases of 21.66%\nfor SICH and 7.02% for PHE in DC. Though îˆ¸\nğ‘ \nğ¶ğ¶ \nachieves much better\nperformance in SE for SICH segmentation, it is at the cost of lower\nSP. Considering the highly imbalanced ratio between foreground and\nbackground, the overall segmentation performance of îˆ¸\nğ‘ \nğ¶ğ¶ \nis much\nworse. The above experimental results demonstrate the robustness\nof the proposed weakly supervised learning framework for various\nsegmentation tasks.\n5.5. Limitation and future work\nAs described in Section 3.2, the reference slices with one or fewer\nclasses are important for feature decomposition, and thus the number of\nreference slices is closely related to the final segmentation performance.\nAs summarized in Tables 1 and 2, the tumor core and edema would\nbenefit more from the proposed framework than the infarct core and\npenumbra, as both infarct core and penumbra encounter serious label\nsymbiosis and have fewer reference slices for feature decomposition.\nFurther, we conduct additional experiments on a more extreme\ndataset, namely the MSD-Prostate dataset [35] for prostate segmen-\ntation (including peripheral zone and central gland) on ADC and T2\nimages. The MSD-Prostate dataset is randomly divided into a training\nset with 16 MR scans, a validation set with 8 MR scans, and a test set\nwith 8 MR scans. In the MSD-Prostate dataset, label symbiosis between\nperipheral zone and central gland is more severe. As summarized in\nTable 8, compared to training with single-class labels via îˆ¸\nğ‘ \nğ¶ğ¶ \nand\nmulti-class labels by îˆ¸\nğ¶ğ¶ \ndirectly, the proposed framework could sig-\nnificantly improve the DC performance of central gland segmentation\nby over 10%, but for the segmentation of peripheral zone, the DC\nimprovement is only around 6%. It is consistent with the qualitative\nresults in Fig. 10. In general, the proposed framework significantly\nalleviates the over-segmentation problem (the first row) and the under-\nsegmentation problem (the second row) of central gland, but still\nencounters a severe over-segmentation problem of peripheral zone.\n\nComputers in Biology and Medicine 171 (2024) 108228\n13\nZ. Kuang et al.\nTo address this, we would explore to further distance the features\nof different classes through the traditional machine learning methods,\nsuch as the K-nearest neighbors algorithm, which has been used to\nobtain inter-class information and improve multi-class classification\nin [36]. In addition, patch-level supervision deserves more exploration\nto make inter-class regions more distinguishable, inspired by [37]\nwhere images were cropped into patches through class-specific CAMs\nand re-supervised by the same classification head of the backbone.\nBesides, as îˆ¸\nğ‘’ğ‘ \nonly preserve the highest value in the foreground\nCAMs among all classes, the efficiency of îˆ¸\nğ‘šğ‘’ \nhinges on the CAMs\nassigning the highest value for each class correctly. In this case, some\nincorrect activation in CAMs would be exacerbated. How to filter out\nsuch erroneously activated pixels could be a potential avenue for future\nresearch.\n6. Conclusion\nIn this paper, we analyze the challenges in weakly supervised multi-\nclass medical image segmentation with slice-level labels and propose a\nsemantic affinity-aware weakly-supervised framework to make full use\nof the mutual information in multi-class labels. Specifically, we first\nutilize a feature decomposition module to learn class-specific features\nfor different classes, largely addressing the label symbiosis problem.\nThen, a mutually exclusive loss is proposed to further minimize the\noverlap between the class activation maps (CAMs) corresponding to\ndifferent classes and reduce false positives in multi-class segmentation.\nExperimental results on three datasets demonstrate the effectiveness\nand robustness of the proposed weakly supervised learning framework\nfor multi-class medical image segmentation. We believe the analysis in\nthis paper would inspire future work on weakly-supervised learning for\nmedical imaging applications.\nCRediT authorship contribution statement\nZhuo Kuang: Writing â€“ original draft, Visualization, Validation,\nSoftware, Methodology. Zengqiang Yan: Writing â€“ review & editing,\nSupervision, Funding acquisition. Li Yu: Writing â€“ review & editing,\nSupervision, Funding acquisition.\nDeclaration of competing interest\nNone Declared.\nAcknowledgments\nThis work was supported in part by the National Natural Science\nFoundation of China under Grant 62271220 and Grant 62202179 and\nin part by the Natural Science Foundation of Hubei Province of China\nunder Grant 2022CFB585.\nReferences\n[1] Y. Yang, X. Hou, H. Ren, Efficient active contour model for medical image\nsegmentation and correction based on edge and region information, Expert Syst.\nAppl. 194 (2022) 116436.\n[2] Y. Hua, Z. Yan, Z. Kuang, H. Zhang, X. Deng, L. Yu, Symmetry-aware deep\nlearning for cerebral ventricle segmentation with intra-ventricular hemorrhage,\nIEEE J. Biomed. Health Inf. 26 (10) (2022) 5165â€“5176.\n[3] Z. Zhou, M.M.R. Siddiquee, N. Tajbakhsh, J. Liang, Unet++: Redesigning skip\nconnections to exploit multiscale features in image segmentation, IEEE Trans.\nMed. Imaging 39 (6) (2019) 1856â€“1867.\n[4] J. Wang, B. Xia, Bounding box tightness prior for weakly supervised image\nsegmentation, in: International Conference on Medical Image Computing and\nComputer-Assisted Intervention, Springer, 2021, pp. 526â€“536.\n[5] H. Qu, P. Wu, Q. Huang, J. Yi, G.M. Riedlinger, S. De, D.N. Metaxas, Weakly\nsupervised deep nuclei segmentation using points annotation in histopathology\nimages, in: International Conference on Medical Imaging with Deep Learning,\nPMLR, 2019, pp. 390â€“400.\n[6] H. Kervadec, J. Dolz, M. Tang, E. Granger, Y. Boykov, I.B. Ayed, Constrained-\nCNN losses for weakly supervised segmentation, Med. Image Anal. 54 (2019)\n88â€“99.\n[7] F. Gao, M. Hu, M.-E. Zhong, S. Feng, X. Tian, X. Meng, Z. Huang, M. Lv, T. Song,\nX. Zhang, et al., Segmentation only uses sparse annotations: Unified weakly and\nsemi-supervised learning in medical images, Med. Image Anal. (2022) 102515.\n[8] P. Chikontwe, H.J. Sung, J. Jeong, M. Kim, H. Go, S.J. Nam, S.H. Park,\nWeakly supervised segmentation on neural compressed histopathology with\nself-equivariant regularization, Med. Image Anal. (2022) 102482.\n[9] B. Zhou, A. Khosla, A. Lapedriza, A. Oliva, A. Torralba, Learning deep features for\ndiscriminative localization, in: IEEE Conference on Computer Vision and Pattern\nRecognition, IEEE, 2016, pp. 2921â€“2929.\n[10] Y. Li, Y. Liu, L. Huang, Z. Wang, J. Luo, Deep weakly-supervised breast tumor\nsegmentation in ultrasound images with explicit anatomical constraints, Med.\nImage Anal. 76 (2022) 102315.\n[11] J. Ying, W. Huang, L. Fu, H. Yang, J. Cheng, Weakly supervised segmentation\nof uterus by scribble labeling on endometrial cancer MR images, Comput. Biol.\nMed. 167 (2023) 107582.\n[12] Z. Li, N. Zhang, H. Gong, R. Qiu, W. Zhang, SG-MIAN: Self-guided multiple\ninformation aggregation network for image-level weakly supervised skin lesion\nsegmentation, Comput. Biol. Med. (2024) 107988.\n[13] X. Wang, X. Deng, Q. Fu, Q. Zhou, J. Feng, H. Ma, W. Liu, C. Zheng, A weakly-\nsupervised framework for COVID-19 classification and lesion localization from\nchest CT, IEEE Trans. Med. Imaging 39 (8) (2020) 2615â€“2625.\n[14] Z. Kuang, Z. Yan, L. Yu, X. Deng, Y. Hua, S. Li, Uncertainty-aware deep learning\nwith cross-task supervision for PHE segmentation on CT images, IEEE J. Biomed.\nHealth Inf. (2022).\n[15] J. Xiang, X. Wang, X. Wang, J. Zhang, S. Yang, W. Yang, X. Han, Y. Liu,\nAutomatic diagnosis and grading of prostate cancer with weakly supervised\nlearning on whole slide images, Comput. Biol. Med. 152 (2023) 106340.\n[16] K. Wu, B. Du, M. Luo, H. Wen, Y. Shen, J. Feng, Weakly supervised brain\nlesion segmentation via attentional representation learning, in: International\nConference on Medical Image Computing and Computer-Assisted Intervention,\nSpringer, 2019, pp. 211â€“219.\n[17] I. Laradji, P. Rodriguez, O. Manas, K. Lensink, M. Law, L. Kurzman, W. Parker,\nD. Vazquez, D. Nowrouzezahrai, A weakly supervised consistency-based learning\nmethod for covid-19 segmentation in ct images, in: IEEE/CVF Winter Conference\non Applications of Computer Vision, IEEE, 2021, pp. 2453â€“2462.\n[18] G. Patel, J. Dolz, Weakly supervised segmentation with cross-modality\nequivariant constraints, Med. Image Anal. 77 (2022) 102374.\n[19] X. Liu, Q. Liu, Y. Zhang, M. Wang, J. Tang, TSSK-net: Weakly supervised\nbiomarker localization and segmentation with image-level annotation in retinal\nOCT images, Comput. Biol. Med. 153 (2023) 106467.\n[20] S. Li, Z. Tang, L. Yang, M. Li, Z. Shang, Application of deep reinforcement\nlearning for spike sorting under multi-class imbalance, Comput. Biol. Med. 164\n(2023) 107253.\n[21] C. Yan, X. Chang, M. Luo, Q. Zheng, X. Zhang, Z. Li, F. Nie, Self-weighted\nrobust LDA for multiclass classification with edge classes, ACM Trans. Intell.\nSyst. Technol. 12 (1) (2020) 1â€“19.\n[22] Z. Chen, Z. Tian, J. Zhu, C. Li, S. Du, C-CAM: Causal CAM for weakly supervised\nsemantic segmentation on medical image, in: IEEE/CVF Conference on Computer\nVision and Pattern Recognition, IEEE, 2022, pp. 11676â€“11685.\n[23] L. Chen, W. Wu, C. Fu, X. Han, Y. Zhang, Weakly supervised semantic\nsegmentation with boundary exploration, in: European Conference on Computer\nVision, Springer, 2020, pp. 347â€“362.\n[24] J. Fan, Z. Zhang, T. Tan, C. Song, J. Xiao, Cian: Cross-image affinity net for\nweakly supervised semantic segmentation, in: AAAI Conference on Artificial\nIntelligence, Vol. 34, 2020, pp. 10762â€“10769.\n[25] K. Li, Z. Wu, K.-C. Peng, J. Ernst, Y. Fu, Tell me where to look: Guided\nattention inference network, in: IEEE Conference on Computer Vision and Pattern\nRecognition, IEEE, 2018, pp. 9215â€“9223.\n[26] M. Lin, Q. Chen, S. Yan, Network in network, 2013, arXiv preprint arXiv:\n1312.4400.\n[27] G. Sun, W. Wang, J. Dai, L. Van Gool, Mining cross-image semantics for weakly\nsupervised semantic segmentation, in: European Conference on Computer Vision,\nSpringer, 2020, pp. 347â€“365.\n[28] B.H. Menze, A. Jakab, S. Bauer, J. Kalpathy-Cramer, K. Farahani, J. Kirby, Y.\nBurren, N. Porz, J. Slotboom, R. Wiest, et al., The multimodal brain tumor image\nsegmentation benchmark (BRATS), IEEE Trans. Med. Imaging 34 (10) (2014)\n1993â€“2024.\n[29] S. Bakas, H. Akbari, A. Sotiras, M. Bilello, M. Rozycki, J.S. Kirby, J.B. Freymann,\nK. Farahani, C. Davatzikos, Advancing the cancer genome atlas glioma MRI\ncollections with expert segmentation labels and radiomic features, Sci. Data 4\n(1) (2017) 1â€“13.\n[30] S. Bakas, M. Reyes, A. Jakab, S. Bauer, M. Rempfler, A. Crimi, R.T. Shinohara,\nC. Berger, S.M. Ha, M. Rozycki, et al., Identifying the best machine learning\nalgorithms for brain tumor segmentation, progression assessment, and overall\nsurvival prediction in the BRATS challenge, 2018, arXiv preprint arXiv:1811.\n02629.\n\nComputers in Biology and Medicine 171 (2024) 108228\n14\nZ. Kuang et al.\n[31] O. Maier, M. Wilms, J. von der Gablentz, U.M. KrÃ¤mer, T.F. MÃ¼nte, H. Handels,\nExtra tree forests for sub-acute ischemic stroke lesion segmentation in MR\nsequences, J. Neurosci. Methods 240 (2015) 89â€“100.\n[32] Y. Wang, J. Zhang, M. Kan, S. Shan, X. Chen, Self-supervised scale equivariant\nnetwork for weakly supervised semantic segmentation, 2019, arXiv preprint\narXiv:1909.03714.\n[33] X. Ma, Z. Ji, S. Niu, T. Leng, D.L. Rubin, Q. Chen, MS-CAM: Multi-scale\nclass activation maps for weakly-supervised segmentation of geographic atrophy\nlesions in SD-OCT images, IEEE J. Biomed. Health Inform. 24 (12) (2020)\n3443â€“3455.\n[34] Z. Kuang, X. Deng, L. Yu, H. Wang, T. Li, S. Wang, ğ›¹ -Net: Focusing on the border\nareas of intracerebral hemorrhage on CT images, Comput. Methods Programs\nBiomed. 194 (2020) 105546.\n[35] A.L. Simpson, M. Antonelli, S. Bakas, M. Bilello, K. Farahani, B. Van Ginneken,\nA. Kopp-Schneider, B.A. Landman, G. Litjens, B. Menze, et al., A large annotated\nmedical image dataset for the development and evaluation of segmentation\nalgorithms, 2019, arXiv preprint arXiv:1902.09063.\n[36] M. Tanveer, A. Sharma, P.N. Suganthan, Least squares KNN-based weighted\nmulticlass twin SVM, Neurocomputing 459 (2021) 454â€“464.\n[37] B.-B. Gao, H.-Y. Zhou, Learning to discover multi-class attentional regions for\nmulti-label image recognition, IEEE Trans. Image Process. 30 (2021) 5920â€“5932.",
    "version": "5.3.31"
  },
  {
    "numpages": 15,
    "numrender": 15,
    "info": {
      "PDFFormatVersion": "1.7",
      "Language": "en-US",
      "EncryptFilterName": null,
      "IsLinearized": true,
      "IsAcroFormPresent": false,
      "IsXFAPresent": false,
      "IsCollectionPresent": false,
      "IsSignaturesPresent": false,
      "Creator": "Elsevier",
      "Custom": {
        "CrossMarkDomains[1]": "elsevier.com",
        "CrossmarkMajorVersionDate": "2010-04-23",
        "CreationDate--Text": "13th November 2025",
        "ElsevierWebPDFSpecifications": "7.0.1",
        "robots": "noindex",
        "doi": "10.1016/j.compbiomed.2025.111219",
        "CrossMarkDomains[2]": "sciencedirect.com",
        "CrossmarkDomainExclusive": "true"
      },
      "ModDate": "D:20251113045344Z",
      "Author": "Ahmet SEN",
      "Title": "Weakly supervised learning for scar reconstruction in personalized cardiac models: Integrating 2D MRI to 3D anatomical models",
      "Keywords": "Myocardial scar segmentation,Late gadolinium-enhanced cardiac MRI,Deep learning-based interpolation,Deep learning for medical imaging,Monte Carlo Dropout",
      "CreationDate": "D:20251113005853Z",
      "Producer": "Acrobat Distiller 8.1.0 (Windows)",
      "Subject": "Computers in Biology and Medicine, 198 (2025) 111219. doi:10.1016/j.compbiomed.2025.111219"
    },
    "metadata": {
      "ali:license_ref": "http://creativecommons.org/licenses/by/4.0/",
      "crossmark:crossmarkdomains": "elsevier.comsciencedirect.com",
      "crossmark:crossmarkdomainexclusive": "true",
      "crossmark:doi": "10.1016/j.compbiomed.2025.111219",
      "crossmark:majorversiondate": "2010-04-23",
      "dc:format": "application/pdf",
      "dc:identifier": "10.1016/j.compbiomed.2025.111219",
      "dc:publisher": "Elsevier Ltd",
      "dc:description": "Computers in Biology and Medicine, 198 (2025) 111219. doi:10.1016/j.compbiomed.2025.111219",
      "dc:subject": [
        "Myocardial scar segmentation",
        "Late gadolinium-enhanced cardiac MRI",
        "Deep learning-based interpolation",
        "Deep learning for medical imaging",
        "Monte Carlo Dropout"
      ],
      "dc:title": "Weakly supervised learning for scar reconstruction in personalized cardiac models: Integrating 2D MRI to 3D anatomical models",
      "dc:creator": [
        "Ahmet SEN",
        "Ursula Rohrer",
        "Pranav Bhagirath",
        "Reza Razavi",
        "Mark Oâ€™Neill",
        "John Whitaker",
        "Martin Bishop"
      ],
      "jav:journal_article_version": "VoR",
      "pdf:creationdate--text": "13th November 2025",
      "pdf:producer": "Acrobat Distiller 8.1.0 (Windows)",
      "pdf:keywords": "Myocardial scar segmentation,Late gadolinium-enhanced cardiac MRI,Deep learning-based interpolation,Deep learning for medical imaging,Monte Carlo Dropout",
      "pdfx:creationdate--text": "13th November 2025",
      "pdfx:crossmarkdomains": "sciencedirect.comelsevier.com",
      "pdfx:crossmarkdomainexclusive": "true",
      "pdfx:crossmarkmajorversiondate": "2010-04-23",
      "x9o3codj.mdapy9ygy9ujz8nnnpiolt6nmt6god2pmdv.mtqsy9ajo9eqnmakmmmsmdmtma": "",
      "pdfx:doi": "10.1016/j.compbiomed.2025.111219",
      "pdfx:robots": "noindex",
      "prism:aggregationtype": "journal",
      "prism:copyright": "Â© 2025 The Author(s). Published by Elsevier Ltd.",
      "prism:coverdate": "2025-11-01",
      "prism:coverdisplaydate": "1 November 2025",
      "prism:doi": "10.1016/j.compbiomed.2025.111219",
      "prism:issn": "0010-4825",
      "prism:number": "PB",
      "prism:pagerange": "111219",
      "prism:publicationname": "Computers in Biology and Medicine",
      "prism:startingpage": "111219",
      "prism:url": "https://doi.org/10.1016/j.compbiomed.2025.111219",
      "prism:volume": "198",
      "tdm:policy": "https://www.elsevier.com/tdm/tdmrep-policy.json",
      "tdm:reservation": "1",
      "xmp:createdate": "2025-11-13T00:58:53",
      "xmp:creatortool": "Elsevier",
      "xmp:metadatadate": "2025-11-13T04:53:44",
      "xmp:modifydate": "2025-11-13T04:53:44",
      "xmpmm:documentid": "uuid:1b499bed-4ce8-4c0c-b682-0d58baae1cbe",
      "xmpmm:instanceid": "uuid:b6ed5266-03cb-4855-90c0-636283357f42",
      "xmprights:marked": "True"
    },
    "text": "Contents lists available at ScienceDirect\nComputers in Biology and Medicine\njournal homepage: www.elsevier.com/locate/compbiomed\nWeakly supervised learning for scar reconstruction in personalized cardiac\nmodels: Integrating 2D MRI to 3D anatomical models\nAhmet SEN \nâˆ—\n, Ursula Rohrer , Pranav Bhagirath , Reza Razavi, Mark Oâ€™Neill ,\nJohn Whitaker , Martin Bishop\nKingâ€™s College London, School of Biomedical Engineering & Imaging Sciences, London, United Kingdom\nA R T I C L E I N F O\nKeywords:\nMyocardial scar segmentation\nLate gadolinium-enhanced cardiac MRI\nDeep learning-based interpolation\nDeep learning for medical imaging\nMonte Carlo Dropout\nA B S T R A C T\nAccurate myocardial scar reconstruction is crucial for understanding arrhythmical risk and guiding treatment\nstrategies in cardiac electrophysiology. Beyond providing anatomical insight, these reconstructions also serve\nas a foundational input for constructing patient-specific 3D computational models, which are increasingly used\nto simulate electrical activity and predict therapeutic outcomes. However, traditional interpolation methods\nstruggle with the low inter-slice resolution (typically 8â€“10 mm) of late gadolinium-enhanced cardiac MRI (LGE-\nCMR) data, despite the high in-plane resolution (1â€“2 mm), leading to inaccuracies in the representation of scar\nmorphology. In this study, we propose a deep learning-based interpolation framework that utilizes coordinate-\nbased learning and Monte Carlo Dropout uncertainty estimation to reconstruct myocardial scar regions from\nsparse LGE-CMR slices. While the underlying methods are established, their application to myocardial scar\nreconstruction for use in personalized cardiac modeling represents a novel contribution.\nWe evaluate our method on two different left ventricular (LV) models: a pre-clinical pig LV high-resolution\n(1.2 mm) gold-standard MRI, and a clinical LV with a more complex scar morphology. Our DL framework is\ntrained using 3D anatomical coordinates as input, with a neural network architecture consisting of two hidden\nlayers and dropout layers for uncertainty modeling. To improve consistency between 2D MRI data and 3D LV\nmodels, we introduce a custom loss function that enforces spatial and anatomical constraints.\nExperimental results demonstrate that the DL-based approach significantly outperforms the Log-Odds\nmethod in terms of scar segmentation accuracy, volumetric difference, and Dice similarity coefficient (DSC).\nThe DL model trained with uncertainty estimation achieves the best performance, with MSE reduced to 0.094,\nDSC improved to 0.83, and volumetric error minimized to -2.72%. Additionally, uncertainty-aware training\nenhances the prediction of border zones between healthy and scarred myocardium, where traditional methods\nexhibit high errors.\nThese findings highlight the effectiveness of deep learning-based interpolation for low-resolution LGE-CMR\nscar reconstruction, demonstrating its potential for improving patient-specific computational cardiac modeling.\nBy incorporating uncertainty estimation and anatomical constraints, our approach provides a more accurate\nand clinically meaningful representation of myocardial scar morphology, paving the way for enhanced risk\nstratification and treatment planning in cardiac electrophysiology.\n1. Introduction\nAdvancements in computational modeling have significantly trans-\nformed the landscape of cardiovascular medicine, particularly in di-\nagnosing and treating life-threatening arrhythmia such as ventricular\ntachycardia (VT) following myocardial infarction (MI) [1,2]. Patient-\nspecific computational cardiac models, which integrate both cardiac\nanatomy and electrophysiology (EP), have emerged as powerful tools\nin assessing arrhythmic risk, optimizing cardiac resynchronization ther-\napy (CRT), and identifying ablation targets [3â€“7]. By leveraging late\nâˆ— \nCorresponding author.\nE-mail address: ahmet.1.sen@kcl.ac.uk (A. SEN).\ngadolinium-enhanced cardiac MRI (LGE-CMR), these models provide\ninsights into myocardial scar morphology and critical conducting path-\nways, which are essential in understanding and predicting arrhyth-\nmic circuits [8]. However, despite the potential of LGE-CMR-based\nmodeling, several technical challenges remain, particularly regarding\nthe accurate representation of complex cardiac structures due to the\nlimitations of MRI resolution [8,9].\nOne of the primary challenges stems from the low interslice resolu-\ntion of LGE-CMR, which typically ranges from 6 to 20 mm [8]. While\nhttps://doi.org/10.1016/j.compbiomed.2025.111219\nReceived 12 May 2025; Received in revised form 12 September 2025; Accepted 15 October 2025\nComputers in Biology and Medicine 198 (2025) 111219\nAvailable online 29 October 2025\n0010-4825/Â© 2025 The Author(s). Published by Elsevier Ltd. This is an open access article under the CC BY license (\nhttp://creativecommons.org/licenses/by/4.0/).\n\nA. SEN et al.\nsufficient for large-scale anatomical reconstructions, this resolution\nproves inadequate for capturing the intricate details of myocardial scar-\nring and the narrow conducting isthmuses, often less than 9.6 mm wide,\nthat are critical for sustaining VT [8]. Precise segmentation of these\nstructures is essential not only for accurate diagnosis and treatment\nplanning, but also for ensuring that computational models faithfully\nrepresent patient-specific cardiac anatomy. However, achieving high\nfidelity in cardiac scar segmentation remains a challenge, as anatomical\nartifacts and interpolation errors can significantly compromise both\nclinical interpretation and model accuracy [10]. Traditional interpo-\nlation techniques have been employed to bridge the gaps between\nMRI slices [11â€“13], but these methods can introduce inaccuracies\nby artificially closing crucial conduction pathways or creating unre-\nalistic scar reconstructions [8,14]. Among the available interpolation\ntechniques, the logarithm of odds (Log-Odds) function-based method\nhas demonstrated superior performance in reconstructing cardiac scar\nmorphology. However, its limitations in capturing narrow critical isth-\nmuses highlight the need for more advanced and reliable segmentation\napproaches [15].\nIn recent years, deep learning (DL)-based image segmentation has\ngained traction as a promising solution for automating and improving\nthe accuracy of LGE-CMR segmentation [16â€“18]. Supervised DL models\nhave demonstrated remarkable potential in myocardial segmentation\nand fibrosis detection, offering faster and more consistent results com-\npared to traditional methods [19â€“21]. However, most existing DL-based\napproaches depend on large datasets with fully sampled ground-truth\nimages, which are often unavailable in cardiac imaging due to time\nconstraints in MRI acquisition [22]. This limitation poses a significant\nchallenge in dynamic cardiac MRI applications, where acquiring high-\nresolution data for model training is not always feasible. Therefore,\ndeveloping deep learning techniques that reduce reliance on fully\nsampled training data is crucial to improving DL-based segmentation\nand reconstruction of cardiac structures [23].\nAdditionally, Implicit Neural Representations (INRs), which use\nneural networks to model continuous functions over space by mapping\ncoordinates directly to signal values. Notable examples include Neu-\nral Radiance Fields (NeRF) [24,25], and DeepSDF [26], which have\ndemonstrated impressive performance in 3D shape modeling and ren-\ndering. Inspired by this framework, our approach employs a coordinate-\nbased neural network to learn a mapping from 3D anatomical locations\nto scar probabilities. To our knowledge, there is still a research gap\nin INR-style modeling for myocardial scar reconstruction from sparse\nclinical imaging data.\nThe primary aim of this project is to develop a DL-based interpola-\ntion method that accurately reconstructs myocardial scar morphology\nwithout relying on fully labeled training datasets by integrating 2D\nLGE-CMR with a 3D Anatomical model. A key innovation of this\napproach is the utilization of Monte Carlo Dropout (MC Dropout) to\nintroduce uncertainty estimation during training [27], enabling the\nmodel to learn robust scar representations with coordinate-based super-\nvision. This weakly supervised learning framework aims to overcome\nthe limitations of traditional interpolation methods, which often intro-\nduce artifacts or inaccuracies in scar reconstruction due to the low\ninterslice resolution of MRI. Here, we apply coordinate-based deep\nlearning with uncertainty estimation to reconstruct myocardial scar\nfrom sparse LGE-CMR data, and demonstrate that our method improves\nreconstruction accuracy while providing meaningful uncertainty esti-\nmates that can inform confidence in model predictions. We validated\nour approach using a pre-clinical pig left ventricular (LV) dataset, as\nwell as clinical cases featuring more complex scar morphology.\n2. Method\nOur approach begins with clinical data representation and process-\ning, where we extract scar regions from LGE-CMR and align them with\ncorresponding 3D anatomical models. We then examine traditional\ninterpolation methods, which rely on mathematical approximations\nto reconstruct scar morphology between MRI slices. Then, we intro-\nduce a DL-based method designed for weakly supervised training.\nOur architecture incorporates coordinate-based learning and employs a\ncustom loss function that balances structural accuracy with uncertainty\nquantification, ensuring a more reliable and anatomically precise scar\nreconstruction.\n2.1. Clinical data representation and processing\nIn clinical practice, the acquisition of 3D anatomical models and\nLGE-CMR often occurs at different time points due to factors related\nto imaging modalities, patient management, and workflow logistics.\nAnatomical models are commonly derived from imaging techniques\nsuch as CT, high-resolution MRI, or 3D echocardiography, which are\nfocused on capturing structural details of the LV. In contrast, LGE-\nCMR is a specialized MRI technique that visualizes myocardial scar\nand fibrosis using gadolinium contrast agents, and is typically acquired\nbased on different diagnostic needs [28,29]. However, in the case\nstudied here, the anatomical models were derived from CINE MRI data\nâ€” another sequence acquired during the same imaging session as the\nLGE-CMR â€” allowing for consistent anatomical alignment between\nstructural and scar information.\nFig. 1A presents a 3D anatomical model overlaid with a single\nLGE-CMR slice containing a scar mask. The interslice resolution of the\nMRI is 9.6 mm, which is significantly lower than the resolution of the\nanatomical model.\nFig. 1B focuses on the top-down view of an LGE-CMR slice, illustrat-\ning the underlying grid structure and the corresponding mask values\nused for scar segmentation. Each grid point contains a numerical value\nrepresenting the classification of the LV tissue: âˆ’1 indicates non-cardiac\nregions, 0 represents entirely healthy myocardium, and 1 corresponds\nto fully scarred myocardium. Intermediate values between 0 and 1\nreflect the low-resolution nature of the MRI data, where each voxel\nmay contain a mix of healthy and scarred tissue. For the pre-clinical\npig LV, LGE-CMR slice data were directly available. In contrast, for\nthe human LV, simulated transmural cross-sections were reconstructed\nby interpolating scar intensity values across the concentric LV shells,\nenabling 2D visualization of scar distribution from endocardium to epi-\ncardium. These reconstructions provide MRI-like slice representations\nderived solely from the processed 3D model data.\nFig. 1C illustrates the resulting 3D mesh, which exhibits smoother\ngeometry and significantly higher resolution compared to the original\nMRI-derived representation. A clinical LGE-CMR dataset from a VT\nablation patient with ischemic cardiomyopathy was processed with\nADAS3D LV (ADAS3D Medical, S.L, Barcelona, Spain) [30], which was\nused to segment the left ventricular myocardium and export the data\nas concentric transmural VTK shells. These exported layers were then\nprocessed with a custom Python script and the open-source Meshtool\nsoftware [31] to generate patient-specific 3D volumetric meshes [32â€“\n34].\n2.2. Interslice data generation\nTo simulate reduced resolution in the through-plane direction of\nmedical images, a commonly used method involves applying Gaussian\nblurring along the slice axis followed by sub-sampling. This technique\nmimics the partial volume effects and point spread function that occur\nin real MRI or CT acquisitions with thicker slices. By convolving high-\nresolution images with a Gaussian kernel and then sampling slices at\nwider intervals, the resulting images reflect the intensity smoothing\nand resolution loss seen in lower-resolution scans. This approach has\nbeen widely adopted in the medical imaging community, especially for\ncreating low-resolution data from high-resolution images for tasks such\nas image reconstruction, segmentation, or super-resolution training. It\noffers a practical and computationally efficient way to generate realistic\nComputers in Biology and Medicine 198 (2025) 111219\n2\n\nA. SEN et al.\nFig. 1. Comparison of 3D anatomical left ventricular model and LGE-CMR data. (A) A 3D anatomical LV model overlaid with a single LGE-CMR slice containing a\nscar mask. (B) A top-down view of an LGE-CMR slice, showing a grid structure with mask values: 0 represents healthy myocardium, and 1 indicates fully scarred\ntissue. Intermediate values (0â€“1) reflect mixed tissue due to low MRI resolution. (C) The mesh structure of the 3D anatomical model.\nFig. 2. Impact of LGE-CMR interslice resolution on scar representation. Comparison of LGE-CMR data at different interslice resolutions: 1.2 mm (A) vs. 9.6 mm\n(B). 0 represents non-cardiac regions, 0.5 denotes healthy myocardium, and 1 indicates fully scarred tissue. Intermediate values (0â€“1) reflect mixed tissue due to\nlow MRI resolution.\nlower-resolution images while preserving anatomical consistency [35,\n36].\nFig. 2 illustrates the impact of interslice resolution on LGE-CMR data\nby comparing myocardial scar representation at 1.2 mm and 9.6 mm\ninterslice resolutions. The primary difference between these two res-\nolutions lies in the level of detail captured in the scar morphology.\nWith 1.2 mm resolution, the MRI slices provide a much finer and more\naccurate delineation of scarred and healthy myocardial tissue, allowing\nfor a more precise representation of structural details. In contrast, the\n9.6 mm interslice resolution results in significant loss of detail, leading\nto blurring and interpolation artifacts that obscure critical conduc-\ntion pathways. This loss of resolution can be particularly problematic\nwhen identifying narrow conducting isthmuses, which are essential\nin predicting arrhythmic circuits in VT. In both cases, the LGE-CMR\ndata is represented using a mask-based classification system, where\nâˆ’1 indicates non-cardiac regions, 0 represents healthy myocardium,\nand 1 corresponds to fully scarred myocardium. However, in the low-\nresolution (9.6 mm) case, the presence of intermediate values between\n0 and 1 becomes more prominent due to the voxel-averaging effect,\nwhere each voxel may contain a mixture of scarred and healthy tissue.\n2.3. Deep learning\nTo overcome the limitations of traditional interpolation methods\nand improve myocardial scar reconstruction, we develop a DL-based\nComputers in Biology and Medicine 198 (2025) 111219\n3\n\nA. SEN et al.\nTable 1\nDetails of deep learning architecture.\nInput layer Coordinates\nNumber of hidden layers 3\nHidden layer activation relu\nDropout size 0.2-0.2-0.1\nEpochs number 10 000\nOptimizer Adam\nOutput layer Singular value (0â€“1)\nNumber of neuron 128-64-64\nOutput layer activation sigmoid\nMarco dropout test number 10\nreconstruction approach. This method takes advantage of coordinate-\nbased learning, where spatial coordinates extracted from a 3D anatom-\nical LV model are used as input to predict scar tissue probability at each\nlocation.\nThe process begins with extracting node coordinates from the 3D\nanatomical LV model. Each node represents a spatial location in the LV,\nand these coordinates serve as input features for the neural network\n(NN). The extracted coordinate points allow us to interpolate scar\nregions at any arbitrary position in the LV, overcoming the resolution\nconstraints of LGE-CMR slices.\nWe design a fully connected NN architecture consisting of two\nhidden layers, each with 128 neurons, followed by two dropout layers\nto introduce regularization and estimate uncertainty. The activation\nfunction for the hidden layers is ReLU (Rectified Linear Unit), while the\nfinal output layer consists of a single neuron with a sigmoid activation\nfunction. The sigmoid function restricts predictions to the range [0,1],\nensuring consistency with the scar mask values introduced earlier,\nwhere 0 represents healthy myocardium and 1 represents fully scarred\ntissue. Table 1 shows details of DL architecture. All architecture and\ntrain developed with TensorFlow [37].\nA NN with dropout is defined as:\nğ‘“ (ğ‘¥, ğœƒ) = ğ‘Š \n(ğ‘™) \nâ‹… â„\n(ğ‘™) \n+ ğ‘\n(ğ‘™)\n, (1)\nwhere ğ‘¥ is input vector (dimension: ğ‘‘), ğ‘Š \n(ğ‘™)\n, ğ‘\n(ğ‘™) \nis weights and biases\nfor layer ğ‘™, â„\n(ğ‘™) \nis Activation of layer ğ‘™, ğœ(â‹…) is activation function (ReLU\nfor hidden layers sigmoid for output layer)\nThe first hidden layer transforms the input ğ‘¥:\nâ„\n(1) \n= ğœ \n(\nğ‘Š \n(1)\nğ‘¥ + ğ‘\n(1)\n) \n. (2)\nFor subsequent hidden layers with Dropout:\nâ„\n(ğ‘™) \n= ğ·\n(ğ‘™) \nâ‹… ğœ \n(\nğ‘Š \n(ğ‘™)\nâ„\n(ğ‘™âˆ’1) \n+ ğ‘\n(ğ‘™)\n) \n, (3)\nwhere ğ·\n(ğ‘™) \nis a dropout mask sampled from a Bernoulli distribution:\nğ·\n(ğ‘™)\nğ‘– \nâˆ¼ Bernoulli(ğ‘), (4)\nwhere p is dropout probability. The final logit layer before applying\nsigmoid activation:\n Ì‚ğ‘¦ = ğ‘Š \n(ğ¿)\nâ„\n(ğ¿âˆ’1) \n+ ğ‘\n(ğ¿)\n. (5)\nApplying sigmoid for binary classification:\n Ì‚ğ‘¦ = ğœ \n(\nğ‘Š \n(ğ¿)\nâ„\n(ğ¿âˆ’1) \n+ ğ‘\n(ğ¿)\n) \n= ğ‘“ (ğ‘¥, ğœƒ). (6)\nThe NN is modeled as a function ğ‘“ (ğ‘¥, ğœƒ), where: ğ‘¥ is the input, ğœƒ rep-\nresents the learnable parameters and Ì‚ğ‘¦ represent prediction, Dropout is\napplied at each forward pass.\nBy applying dropout at each forward pass, the network is able to\nmodel uncertainty in the interpolation process, ensuring robustness to\nmissing or noisy data. This allows for a more reliable estimation of\nmyocardial scar distribution, particularly in regions where LGE-CMR\ndata is sparse or ambiguous.\nFig. 3 illustrates the overall framework of our DL-based interpola-\ntion approach for myocardial scar reconstruction. The process begins\nwith a 3D anatomical LV model, shown in Fig. 3A, where the mesh\nstructure defines the spatial framework of the LV. From this model,\nnode coordinates are extracted, as depicted in Fig. 3B. These coordi-\nnates serve as input features for the NN, allowing the model to learn\nspatial relationships and predict scar distribution without relying on\nfully labeled training datasets. Fig. 3C presents the NN architecture\nincorporating dropout for uncertainty estimation. The output layer\nconsists of a single neuron, producing a scar probability prediction P\nconstrained between 0 and 1. This prediction represents the likelihood\nof scar presence at each coordinate, aligning with the LGE-CMR mask\nvalues. Finally, the predicted scar probabilities are compared with\nthe LGE-CMR mask data to compute a custom loss function, ensuring\naccurate interpolation while preserving anatomical consistency.\n2.3.1. Loss function\nTo optimize training, we employ a custom loss function. This loss\nfunction ensures that the NN learns an anatomically meaningful inter-\npolation while preserving the uncertainty inherent in the low-resolution\nLGE-CMR data. We designed a custom loss function based on two key\ncomponents: Monte Carlo (MC) dropout-based uncertainty estimation\nand the constraint between predictions and LGE-CMR mask data.\nThe first component accounts for uncertainty in the predictions by\nleveraging MC dropout. In regions of sparse LGE-CMR data, particularly\nnear border zones, the ground truth is ambiguous due to partial volume\neffects and low slice resolution. Predictive uncertainty is therefore a\nmeaningful signal, not just noise. We incorporate MC Dropout to esti-\nmate epistemic uncertainty and explicitly minimize it during training\nto regularize predictions and discourage overconfident extrapolations\nin under-constrained anatomical regions.\nInstead of treating each prediction deterministically, we run the NN\nmultiple times with dropout activated and compute the variance of\nthe predicted scar probabilities. The uncertainty is quantified using the\nfollowing equations:\nWe run the model ğ‘‡ times with different dropout masks:\n Ì‚ğ‘¦ \n(ğ‘¡) \n= ğ‘“ (ğ‘¥, ğœƒ\n(ğ‘¡)\n). (7)\nThe expected prediction (MC mean) is given by:\nE[Ì‚ğ‘¦ ] = \n1\nğ‘‡\nğ‘‡\nâˆ‘\nğ‘¡=1\nÌ‚ğ‘¦ \n(ğ‘¡)\n. (8)\nThe uncertainty in the model predictions is computed as the vari-\nance of the MC samples:\nVar[Ì‚ğ‘¦ ] = \n1\nğ‘‡\nğ‘‡\nâˆ‘\nğ‘¡=1\n(\nÌ‚ğ‘¦ \n(ğ‘¡) \nâˆ’ E[Ì‚ğ‘¦ ]\n)\n2 \n. (9)\nThis representation underscores the inherent challenge in interpo-\nlating between slices, as the presence of mixed tissue regions compli-\ncates the accurate delineation of scar boundaries.\nThe second component accounts for the NN accurately reconstructs\nmyocardial scar distribution while maintaining consistency between\n2D LGE-CMR data and the 3D anatomical LV model, we define a loss\nfunction that enforces both local and global consistency. To bridge the\ngap between 2D LGE-CMR slices and the 3D anatomical LV mesh, we\nspatially extend each 2D pixel into a volumetric element. Each LGE-\nCMR slice has a known physical thickness (e.g., 9.6 mm), which defines\na 3D rectangular prism for every pixel in the slice. We compute the\nspatial bounds of each prism based on the in-plane resolution and inter-\nslice thickness, assuming orthogonal acquisition. Next, for every node\n(referring to a point on the 3D anatomical LV mesh) in the 3D LV\nmesh, we check whether its (x, y, z) coordinates fall within any of\nthe extended prisms. If so, the scar probability predicted at that node\nis constrained to match the pixel value of the corresponding mask, as\nenforced in our custom loss function. This approach provides a weak\nform of spatial supervision, enabling the network to learn a continuous\n3D mapping from sparse 2D annotations.\nAll mesh nodes located inside this prism are assumed to contribute\nto the corresponding pixel intensity. This encourages the model to\nComputers in Biology and Medicine 198 (2025) 111219\n4\n\nA. SEN et al.\nFig. 3. DL-based interpolation framework. (A) 3D anatomical LV model mesh structure. (B) Extracted node coordinates are used as input for the neural network.\n(C) NN architecture predicts scar probability P, which is compared with LGE-CMR mask data for loss computation.\nalign 3D predictions with the available 2D image information, enabling\nweak supervision and enforcing anatomical consistency between 2D\nMRI slices and the 3D model. The single-point loss ğ¿\nğ‘ ğ‘ \ncompares the\npixel mask value M to the mean predicted scar probability over these\nnodes:\nîˆ¸\nğ‘ ğ‘ \n= ğ‘€ âˆ’ \n1\nğ‘›\nğ‘›\nâˆ‘\nğ‘˜=1\nğ‘ƒ\nğ‘˜\n, (10)\nwhere ğ‘ƒ\nğ‘˜ \nis the predicted probability of node k inside the 3D prism\ncorresponding to the mask pixel. This equation ensures that the mean\nprediction of all nodes within the 3D volume corresponding to an\nLGE-CMR pixel aligns with the actual mask value.\nThe all-point loss îˆ¸\nğ‘ğ‘ \nextends the single-point loss across the entire\ndataset, ensuring that the model maintains global consistency over all\npixels and their corresponding 3D anatomical nodes:\nîˆ¸\nğ‘ğ‘ \n= \n1\nğ‘\nğ‘\nâˆ‘\nğ‘–=1\n(îˆ¸\nğ‘–\nğ‘ ğ‘\n), (11)\nwhere N is total number of pixels, îˆ¸\nğ‘–\nğ‘ ğ‘ \nis the single-point loss for each\nmask pixel i, computed using the first equation.\nThis formulation makes it clear that the all-point loss is simply the\nmean of all single-point losses across the dataset, ensuring that the\nmodelâ€™s predictions remain consistent across all LGE-CMR slices.\nFig. 4 illustrates the integration of 2D LGE-CMR data with the 3D\nanatomical LV model to ensure accurate myocardial scar interpolation.\nIn Fig. 4, a single 2D LGE-CMR slice is shown alongside the 3D LV\nmodel, highlighting the challenge of mapping slice-based imaging onto\na volumetric structure. Since LGE-CMR slices have a finite thickness,\nthey lack direct volumetric information, making scar localization in\n3D space difficult. To address this, Fig. 4 demonstrates how each 2D\nLGE-CMR slice is extended along its interslice resolution, forming a\n3D rectangular prism that distributes the mask data across the missing\ndepth. This allows the model to assign scar probability not just to\na single plane but to all 3D LV model nodes within the extended\nvolume. In Fig. 4, a close-up of an individual extended pixel shows\nhow nodes inside its 3D rectangular prism are identified, ensuring that\ntheir predicted values match the LGE-CMR mask. This method bridges\nthe gap between 2D imaging and 3D modeling, enabling more spatially\nconsistent and anatomically accurate scar predictions.\nBy combining uncertainty loss and all-point loss, our approach en-\nsures that the deep learning model accurately interpolates myocardial\nscar regions while preserving global anatomical consistency, addressing\nchallenges posed by low-resolution and sparse LGE-CMR data. We\ndefine the final loss function as:\nîˆ¸ = îˆ¸\nğ‘ğ‘ \n+ ğœ† â‹… Var[Ì‚ğ‘¦ ], (12)\nwhere ğœ† is a tunable hyperparameter that balances classification accu-\nracy and uncertainty minimization.\n2.4. Evaluation\nTo evaluate the performance of our DL-based interpolation method,\nwe compare it against the gold-standard segmentation using three key\nmetrics: Mean Square Error (ğ‘€ğ‘†ğ¸), Dice Similarity Coefficient (ğ·ğ¶),\nand Volumetric Difference (ğ›¥ğ‘‰ ). Each metric provides insight into\ndifferent aspects of interpolation accuracy, from raw prediction errors\nto spatial overlap and volume consistency.\nğ‘€ğ‘†ğ¸ measures the direct difference between the predicted scar\nprobabilities and the gold-standard values. This metric evaluates how\nclosely the NNâ€™s outputs approximate the actual probability distribution\nof myocardial scar, ensuring that the interpolation does not introduce\nsignificant deviations. ğ‘€ğ‘†ğ¸ is computed as:\nğ‘€ğ‘†ğ¸ = \n1\nğ‘\nğ‘\nâˆ‘\nğ‘–=1\n(ğ‘ƒ\nğ‘– \nâˆ’ ğ‘€\nğ‘–\n)\n2 \n(13)\nwhere ğ‘ƒ\nğ‘– \nis the predicted probability at node ğ‘–, ğ‘€\nğ‘– \nis the corresponding\ngold-standard mask value, and ğ‘ is the total number of evaluated\npoints.\nğ·ğ¶ quantifies the spatial overlap between the predicted scar seg-\nmentation and the gold-standard mask after applying a threshold to\nconvert probability values into binary masks. ğ·ğ¶ is defined as:\nğ·ğ¶ = \n2|ğ‘ƒ âˆ© ğ‘€|\n|ğ‘ƒ | + |ğ‘€| \n(14)\nwhere ğ‘ƒ and ğ‘€ are the binary scar regions obtained from the predicted\nand gold-standard masks, respectively. A higher ğ·ğ¶ value indicates a\nbetter match between the predicted and reference segmentation.\nğ›¥ğ‘‰ assesses how well the predicted scar volume matches the gold-\nstandard volume. Since scar burden plays a critical role in clinical\ndecision-making, maintaining accurate volume estimation is crucial.\nVolumetric difference is calculated as:\nğ›¥ğ‘‰ = \nğ‘‰\nğ‘€ \nâˆ’ ğ‘‰\nğ‘ƒ\nğ‘‰\nğ‘€\nÃ— 100 (15)\nwhere ğ‘‰\nğ‘ƒ \nis the predicted scar volume and ğ‘‰\nğ‘€ \nis the gold-standard scar\nvolume. A lower ğ›¥ğ‘‰ indicates better agreement between the predicted\nand actual scar volumes.\nIn this evaluation, ğ‘€ğ‘†ğ¸ is computed directly from the continuous\nprobability predictions, whereas ğ·ğ¶ and ğ›¥ğ‘‰ are evaluated after ap-\nplying a threshold to convert probabilities into binary segmentation.\nThis multi-metric assessment ensures that our DL-based approach is not\nonly numerically accurate but also spatially consistent with the gold-\nstandard method, providing reliable myocardial scar reconstructions for\nclinical applications.\nComputers in Biology and Medicine 198 (2025) 111219\n5\n\nA. SEN et al.\nFig. 4. Mapping 2D LGE-CMR data to the 3D LV model. First, a 3D LV model and a single 2D MRI slice. Then, the extension of the 2D MRI slice along its\ninterslice resolution to form a 3D volume. Then, A single extended pixel with corresponding 3D model nodes is used for interpolation.\n3. Results\nTo evaluate the effectiveness of our proposed DL-based interpolation\nmethod, we compare its performance against the Log-Odds method (see\nAppendix) on two different LV models: one from a pre-clinical pig LV\nwith a simpler scar geometry and three different human LV with a\nmore complex scar structure. These comparisons allow us to assess the\naccuracy of scar reconstructions under varying anatomical and imaging\nconditions.\n3.1. Pre-clinical pig LV\nThe first model is based on a pre-clinical pig LV, where the scar\ngeometry is relatively simple and well-defined [8,38]. For this case,\nwe have a gold-standard dataset with high-resolution 1.2 mm isotropic\nMRI, providing uniform resolution across all spatial dimensions (x, y,\nand z). The 3D LV model is reconstructed voxel-by-voxel from this\nimage data, where each voxel represents a 1.728 mm\n3 \nvolume element\nderived directly from the MRI intensities. LGE-CMR scans provide a pre-\ncise reference for scar segmentation by highlighting areas of myocardial\nfibrosis. In this dataset, gold-standard mask values are binary (0 or\n1), meaning each voxel is classified as either healthy myocardium (0)\nor fully scarred tissue (1). However, in the low-resolution (inter-slice\nspacing of 9.6 mm and an in-plane resolution of 1.2 mm) LGE-CMR\nscans, which are used for scar reconstruction, the mask values are\ncontinuous between 0 and 1, representing partial-volume, also known\nas border zone (BZ), effects due to lower resolution. This scenario\nallows us to test how well each interpolation method can recover\nhigh-resolution scar details from lower-resolution input data.\nFig. 5 provides a comparative analysis of the Log-Odds method\nand the proposed DL-based interpolation method for myocardial scar\nreconstruction in the pre-clinical pig LV model. This evaluation assesses\nhow different interslice resolutions and interpolation techniques affect\nthe accuracy of scar reconstruction, using high-resolution gold-standard\nimages as the reference. In Fig. 5A, we present the gold-standard mesh\ncreated from high-resolution LGE-CMR scans, where the red regions\nrepresent myocardial scar, and the blue regions correspond to healthy\nmyocardium. This serves as the ground truth for assessing interpolation\naccuracy. In Fig. 5B, we apply the Log-Odds method using data with an\nin-plane resolution of 1.2 mm and an interslice resolution of 1.2 mm\n(denoted as Log-Odds 1.2 mm). The distance between slices in this\ncase is 9.6 mm. In Fig. 5C, we evaluate the Log-Odds method using\nthe same in-plane resolution (1.2 mm) but with a coarser interslice\nresolution of 9.6 mm (denoted as Log-Odds 9.6 mm), with the slice\nspacing remaining at 9.6 mm. By comparing Fig. 5B and C, we can\nevaluate how interslice resolution influences the Log-Odds methodâ€™s\naccuracy, even when the slice spacing remains constant. In Fig. 5D,\nwe present the results of the DL-based interpolation method, trained\nusing 9.6 mm interslice resolution and 1.2 mm in-plane resolution data\nwithout incorporating the certainty equation, with the slice spacing\nremaining at 9.6 mm. (Eq. (9)) (DL without certainty). Comparing Fig.\n5C and D allows us to assess how the DL model performs relative to\nthe Log-Odds method when working with the same input resolution.\nFinally, in Fig. 5E, we show the DL-based method trained with the cer-\ntainty equation (DL with certainty). Comparing Fig. 5D and E helps us\nunderstand the impact of incorporating uncertainty estimation during\ntraining, evaluating whether it improves the modelâ€™s robustness and\naccuracy.\nTable 2 presents the quantitative comparison of the Log-Odds\nmethod and the proposed DL-based interpolation methods against the\ngold standard (Fig. 5A). The evaluation is based on ğ‘€ğ‘†ğ¸, ğ·ğ¶, ğ›¥ğ‘‰ , and\ncomputation time (in minutes).\nThe Log-Odds 1.2 mm achieves a moderate ğ·ğ¶ of 0.754 but exhibits\nan ğ‘€ğ‘†ğ¸ of 0.019 and a ğ›¥ğ‘‰ of âˆ’31.4%, indicating overestimation\nof scar volume. When applied at Log-Odds 9.6 mm, the performance\ndeteriorates significantly, with a ğ·ğ¶ dropping to 0.521 and a ğ›¥ğ‘‰ of\n58.6%.\nThe DL without certainty outperforms the Log-Odds method, achiev-\ning a low ğ‘€ğ‘†ğ¸ of 0.006, a significantly higher ğ·ğ¶ of 0.953, and a ğ›¥ğ‘‰\nof âˆ’6.16%, demonstrating superior reconstruction accuracy. However,\nthe DL with certainty further improves performance, achieving the\nhighest ğ·ğ¶ of 0.958 and the lowest ğ›¥ğ‘‰ of âˆ’2.03%, indicating a nearly\nperfect volume match with the gold standard.\nDespite its superior accuracy, the DL with certainty model requires\nmore computation time (133.3 min) compared to 33.4 min for the DL\nwithout certainty and 5 min for both Log-Odds methods.\nFig. 6 presents a slice-based comparison between the gold-standard\n(from Fig. 5A), Log-Odds 9.6 mm (from Fig. 5C), and DL with certainty\n(from Fig. 5E) methods at two different locations in the LV. The first\nrow shows slices taken 15 mm above the apex, while the second row\ndisplays slices 30 mm above the apex. Each row consists of three im-\nages corresponding to the gold-standard scar segmentation, Log-Odds\n9.6 mm reconstruction, and DL with certainty-based reconstruction.\nThis comparison allows for a direct visual assessment of how each\nComputers in Biology and Medicine 198 (2025) 111219\n6\n\nA. SEN et al.\nFig. 5. (a) Gold-standard segmentation: 1.2 mm in-plane and interslice resolution, with 1.2 mm slice spacing. (b) Log-Odds segmentation: 1.2 mm in-plane and\ninterslice resolution, with 9.6 mm slice spacing. (c) Log-Odds segmentation: 1.2 mm in-plane resolution and 9.6 mm interslice resolution, with 9.6 mm slice\nspacing. (d) Deep learning (DL) without uncertainty modeling: 1.2 mm in-plane resolution and 9.6 mm interslice resolution, with 9.6 mm slice spacing. (e) DL\nwith uncertainty modeling: 1.2 mm in-plane resolution and 9.6 mm interslice resolution, with 9.6 mm slice spacing.\nTable 2\nQuantitative comparison of Log-Odds and DL-based interpolation methods.\nMetrics include Mean Square Error (ğ‘€ğ‘†ğ¸), Dice Similarity Coefficient (ğ·ğ¶),\nVolumetric Difference (ğ›¥ğ‘‰ ), and computation time.\nMethod MSE ğ·ğ¶ ğ›¥ğ‘‰ Time (min)\nLog-Odds 2 mm 0.019 0.754 âˆ’31.4 5\nLog-Odds 9.6 mm 0.071 0.521 58.6 5\nDL without certainty 0.006 0.953 âˆ’6.16 33.4\nDL with certainty 0.006 0.958 âˆ’2.03 133.3\nmethod performs in interpolating myocardial scar regions at different\nLV levels.\nFig. 7 illustrates the differences between training the DL model\nwith and without certainty estimation, highlighting the certainty values\nfor both methods. The certainty values range from 0.6 (blue, lower\ncertainty) to 1.0 (red, higher certainty). In both models, regions at\nthe interface between scar and healthy myocardium exhibit the highest\nuncertainty, indicating areas where the model struggles to confidently\ndistinguish between tissue types. The DL trained with certainty esti-\nmation produces a more structured certainty distribution (minimum\ncertainty is 0.8), reducing uncertainty in well-defined scar regions\nwhile preserving flexibility in transitional areas. This comparison em-\nphasizes the role of uncertainty modeling in improving the robustness\nand interpretability of DL-based myocardial scar reconstruction.\nFig. 8 illustrates how metrics and loss values evolve over training\nepochs for both DL modelsâ€”without certainty (left) and with certainty\n(right). The graphs track ğ·ğ¶, MSE, ğ›¥ğ‘‰ , and Loss over time. After\n2000 epochs, we observe minimal improvements in ğ·ğ¶, ğ‘€ğ‘†ğ¸, and\nloss, indicating that the model has reached a stable performance level.\nHowever, ğ›¥ğ‘‰ changes after 6000 epochs.\n3.2. Three human LV\nThe second model is based on human LVs with a more complex scar\nstructure, reflecting the irregular patterns of myocardial fibrosis. In this\ncase, rather than starting from a high-resolution gold-standard MRI,\nwe use a previously reconstructed scar model generated with ADAS\nsoftware [39], which serves as our reference. From this model, we first\nsynthesized high-resolution images with 1.2 mm in-plane and interslice\nresolution with the same slice spacing. Subsequently, to simulate clin-\nical image conditions with lower through-plane resolution, we applied\nGaussian blurring along the z-axis to generate images with 1.2 mm in-\nplane and 9.6 mm interslice resolution with the same spacing. Unlike\nthe pre-clinical pig LV model, here the scar mask values are continuous\nbetween 0 and 1, indicating varying levels of fibrosis rather than a strict\nbinary classification. This case presents a more challenging scenario, as\nthe interpolation method must reconstruct complex scar distributions\nwhile preserving anatomical accuracy.\nFig. 9 presents the scar mask interpolation results for the human\nLVs model shown in Fig. 9A for LV1, LV2 and LV3. This serves as a ref-\nerence for evaluating different interpolation methods in reconstructing\nmyocardial scar regions in a complex human LV dataset.\nFig. 9B illustrates the Log-Odds method for LV1, LV2, and LV3 with\n1.2 mm in-plane resolution and 9.6 mm interslice resolution with same\nslice spacing, providing a baseline for traditional interpolation. In Fig.\n9C, the DL-based prediction without certainty estimation is shown,\nwhile Fig. 9D demonstrates the DL-based prediction with certainty\nComputers in Biology and Medicine 198 (2025) 111219\n7\n\nA. SEN et al.\nFig. 6. Slice view comparison between (a) Gold-standard segmentation: 1.2 mm in-plane and interslice resolution, with 1.2 mm slice spacing. (b) Log-Odds\nsegmentation: 1.2 mm in-plane resolution and 9.6 mm interslice resolution, with 9.6 mm slice spacing. (c) DL with uncertainty modeling: 1.2 mm in-plane\nresolution and 9.6 mm interslice resolution, with 9.6 mm slice spacing. First-row distance 15 mm from apex, second-row distance 30 mm from apex.\nFig. 7. Certainty values of DL segmentation (a) train without certainty loss, (b) train with certainty loss. The certainty values range from 0.6 (blue, lower\ncertainty) to 1.0 (red, higher certainty). The certainty values range from 0.6 (blue, lower certainty) to 1.0 (red, higher certainty).\nFig. 8. Training progression of DL models with and without certainty.\nComputers in Biology and Medicine 198 (2025) 111219\n8\n\nA. SEN et al.\nFig. 9. Comparison of scar mask between (a) gold-standard models, (b) Log-Odds segmentations, (c) DL segmentations without certainty loss, (d) DL segmentations\nwith certainty loss, Red is scar, Gray is border zone and blue is healthy myocardium. First row human LV1, second row human LV2 and last row human LV3.\nestimation. The comparison helps assess how deep learning interpo-\nlation differs from the Log-Odds method and whether incorporating\nuncertainty estimation enhances scar reconstruction accuracy.\nThe scar mask visualizations use a color scheme where blue rep-\nresents healthy myocardium, gray corresponds to the border zone\nthat includes a mix of healthy and scarred tissue, and red indicates\nscarred myocardium. By analyzing these results, it becomes evident\nhow different methods handle scar boundary regions and how incor-\nporating certainty estimation improves the transition between healthy\nand scarred tissue.\nFig. 10 compares the BZ prediction results across different interpola-\ntion methods against the gold-standard model. Fig. 10A represents the\nLog-Odds method, Fig. 10B shows the DL prediction without certainty\nestimation, and Fig. 10C presents DL prediction with certainty estima-\ntion. This comparison allows us to assess how each method reconstructs\nthe transitional region between healthy and scarred myocardium.\nIn these visualizations, green represents the gold-standard model,\nwhile red indicates the predicted BZ from each method. A higher degree\nof overlap between green and red suggests better agreement with\nthe gold-standard model, while discrepancies highlight areas where\nthe method either overestimates or underestimates the scar transition\nzone. The DL model trained with certainty (Fig. 10C) shows improved\nalignment with the gold-standard model, suggesting that incorporat-\ning uncertainty estimation helps refine scar boundaries and enhances\nprediction accuracy in the BZ.\nTable 3 presents a quantitative comparison of the Log-Odds method\nand two DL-based interpolation approaches â€” one without certainty\nestimation (DL-noCert) and one incorporating certainty estimation\n(DL+Cert) â€” for predicting myocardial scar and border zone regions.\nThe methods were evaluated on three human LV geometries (LV1,\nLV2, and LV3), representing distinct patient-specific heart models.\nPerformance metrics include ğ‘€ğ‘†ğ¸, Dice Similarity Coefficient for\nscar (ğ·ğ¶\nğ‘ ğ‘ğ‘ğ‘Ÿ\n), Volumetric Difference for scar (ğ›¥ğ‘‰\nğ‘ ğ‘ğ‘ğ‘Ÿ\n), Dice Similarity\nCoefficient for the border zone (ğ·ğ¶\nğ‘ğ‘§\n), and Volumetric Difference for\nthe border zone (ğ›¥ğ‘‰\nğ‘ğ‘§\n).\nThe Log-Odds method shows higher ğ‘€ğ‘†ğ¸ values (0.088, 0.070, and\n0.087 for LV1, LV2, and LV3, respectively) and lower ğ·ğ¶\nğ‘ ğ‘ğ‘ğ‘Ÿ \nvalues\n(0.56 to 0.73) compared to the DL-based approaches, suggesting more\nlimited agreement with the reference scar distribution. It also exhibits\nlarger positive ğ›¥ğ‘‰\nğ‘ ğ‘ğ‘ğ‘Ÿ \nvalues, indicating a tendency to overestimate\nscar volume. The border zone segmentation is generally less accurate,\nwith low ğ·ğ¶\nğ‘ğ‘§ \nvalues (0.04â€“0.08) and substantial negative ğ›¥ğ‘‰\nğ‘ğ‘§ \nvalues,\nreflecting challenges in accurately capturing transitional tissue regions.\nThe DL-noCert approach reduces ğ‘€ğ‘†ğ¸ and improves ğ·ğ¶\nğ‘ ğ‘ğ‘ğ‘Ÿ \nvalues\nacross all cases (up to 0.96 in LV3), with scar volume differences closer\nto zero. However, the border zone segmentation remains limited, as\nseen in moderate ğ·ğ¶\nğ‘ğ‘§ \nvalues (0.17â€“0.24) and persistent negative ğ›¥ğ‘‰\nğ‘ğ‘§\nvalues.\nThe DL+Cert method shows further improvements in most metrics.\nIt achieves lower ğ‘€ğ‘†ğ¸ values (as low as 0.009 in LV3) and high ğ·ğ¶\nğ‘ ğ‘ğ‘ğ‘Ÿ\nvalues (up to 0.96), along with more accurate scar volume estimates.\nIncorporating certainty appears to enhance border zone delineation\nmodestly, reflected in higher ğ·ğ¶\nğ‘ğ‘§ \nvalues (up to 0.27) and smaller\nnegative ğ›¥ğ‘‰\nğ‘ğ‘§ \nvalues compared to the other approaches.\nComputers in Biology and Medicine 198 (2025) 111219\n9\n\nA. SEN et al.\nFig. 10. Comparison of scar mask between (a) Log-Odds segmentation, (b) DL segmentation without certainty loss, (c) DL segmentation with certainty loss. Green\nrepresents the reference BZ area and red represents the predicted BZ area. First row human LV1, second row human LV2 and last row human LV3.\nTable 3\nQuantitative comparison of Log-Odds and DL-based interpolation methods.\nMetrics include Mean Square Error (ğ‘€ğ‘†ğ¸), Dice Similarity Coefficient for scar\n(ğ·ğ¶\nğ‘ ğ‘ğ‘ğ‘Ÿ\n), Volumetric Difference for scar (ğ›¥ğ‘‰\nğ‘ ğ‘ğ‘ğ‘Ÿ\n), Dice Similarity Coefficient for\nborder zone (ğ·ğ¶\nğ‘ğ‘§\n), and Volumetric Difference for border zone (ğ›¥ğ‘‰\nğ‘ ğ‘ğ‘ğ‘Ÿ\n).\nHuman LV1\nMethod ğ‘€ğ‘†ğ¸ ğ·ğ¶\nğ‘ ğ‘ğ‘ğ‘Ÿ \nğ›¥ğ‘‰\nğ‘ ğ‘ğ‘ğ‘Ÿ \nğ·ğ¶\nğ‘ğ‘§ \nğ›¥ğ‘‰\nğ‘ğ‘§\nLog-odds 0.088 0.56 20.5 0.08 âˆ’305\nDL-noCert 0.035 0.79 âˆ’3.20 0.17 âˆ’103\nDL+Cert 0.029 0.81 âˆ’1.82 0.21 âˆ’36.9\nHuman LV2\nMethod ğ‘€ğ‘†ğ¸ ğ·ğ¶\nğ‘ ğ‘ğ‘ğ‘Ÿ \nğ›¥ğ‘‰\nğ‘ ğ‘ğ‘ğ‘Ÿ \nğ·ğ¶\nğ‘ğ‘§ \nğ›¥ğ‘‰\nğ‘ğ‘§\nLog-odds 0.070 0.61 20.3 0.04 âˆ’938\nDL-noCert 0.012 0.95 âˆ’9.64 0.17 âˆ’141\nDL+Cert 0.010 0.95 8.31 0.25 âˆ’75.7\nHuman LV3\nMethod ğ‘€ğ‘†ğ¸ ğ·ğ¶\nğ‘ ğ‘ğ‘ğ‘Ÿ \nğ›¥ğ‘‰\nğ‘ ğ‘ğ‘ğ‘Ÿ \nğ·ğ¶\nğ‘ğ‘§ \nğ›¥ğ‘‰\nğ‘ğ‘§\nLog-odds 0.087 0.73 8.94 0.06 âˆ’763\nDL-noCert 0.009 0.96 âˆ’0.42 0.24 âˆ’64.2\nDL+Cert 0.009 0.96 âˆ’0.36 0.27 âˆ’46.2\nTable 4 summarizes the quantitative performance of the Log-Odds\nmethod, DL-noCert, and DL+Cert in predicting infarct transmurality\nacross three human left ventricles (LV1, LV2, and LV3). The results are\npresented separately for three transmural layers: 0%â€“33% (subendo-\ncardial), 33%â€“66% (mid-myocardial), and 66%â€“100% (subepicardial).\nEvaluation metrics include ğ‘€ğ‘†ğ¸, ğ·ğ¶, and ğ›¥ğ‘‰ .\nIn the subendocardial layer (0%â€“33%), both DL-based methods\nshow lower ğ‘€ğ‘†ğ¸ values and higher ğ·ğ¶ scores compared to Log-Odds,\nsuggesting improved agreement with reference transmurality distribu-\ntions. DL+Cert achieves slightly better volume accuracy, reflected in\nsmaller ğ›¥ğ‘‰ values (e.g., âˆ’2.88 in LV1).\nIn the mid-myocardial layer (33%â€“66%), DL-based methods again\nshow lower ğ‘€ğ‘†ğ¸ and higher ğ·ğ¶ scores across all LVs. The DL+Cert\napproach shows slightly more consistent volume differences (e.g., 6.54\nin LV2) and similar or slightly better overlap compared to DL-noCert.\nIn the subepicardial layer (66%â€“100%), DL-based methods maintain\nlower ğ‘€ğ‘†ğ¸ values and higher ğ·ğ¶ scores relative to Log-Odds. DL+Cert\nachieves slightly improved volume estimates in most cases (e.g., âˆ’0.86\nin LV3), while overlap scores remain similar to those without certainty\nestimation.\nOverall, both DL-based methods demonstrate improvements over\nLog-Odds interpolation across all transmural layers and ventricles. The\ninclusion of DL+Cert appears to provide a marginal additional benefit,\nparticularly in reducing volumetric discrepancies.\nTable 5 presents a quantitative comparison of the Log-Odds method,\nDL-noCert, and DL+Cert in predicting infarct regions across different\nLV segments (Apex, Apical, Mid, and Basal) for three human LV ge-\nometries (LV1, LV2, and LV3). Metrics include ğ‘€ğ‘†ğ¸, ğ·ğ¶, and ğ›¥ğ‘‰ ,\nproviding insight into performance in specific anatomical regions.\nIn the apex region, results vary across LVs, with DL-based methods\nachieving lower ğ‘€ğ‘†ğ¸ and slightly higher ğ·ğ¶ values in most cases.\nHowever, challenges remain in certain cases (e.g., LV2), where ğ·ğ¶\nvalues are low and volumetric differences are large for all methods,\nindicating greater difficulty in capturing infarct morphology at the\nextreme apex.\nComputers in Biology and Medicine 198 (2025) 111219\n10\n\nA. SEN et al.\nTable 4\nQuantitative comparison of the Log-Odds method and DL-based interpolation methods (with and without certainty estimation)\nin predicting infarct transmurality across three different human left ventricles (LV1, LV2, and LV3). Performance metrics include\nMean Square Error (ğ‘€ğ‘†ğ¸) and Dice Similarity Coefficient (ğ·ğ¶), assessing agreement with reference transmurality maps.\n0%â€“33%\nMethod Human LV1 Human LV2 Human LV3\nğ‘€ğ‘†ğ¸ ğ·ğ¶ ğ›¥ğ‘‰ ğ‘€ğ‘†ğ¸ ğ·ğ¶ ğ›¥ğ‘‰ ğ‘€ğ‘†ğ¸ ğ·ğ¶ ğ›¥ğ‘‰\nLog-odds 0.092 0.60 âˆ’22.4 0.058 0.74 âˆ’9.81 0.101 0.83 11.1\nDL-noCert 0.033 0.85 âˆ’6.26 0.008 0.96 5.02 0.010 0.96 3.37\nDL+Cert 0.026 0.87 âˆ’2.88 0.008 0.96 4.02 0.010 0.96 2.17\n33%â€“66%\nMethod Human LV1 Human LV2 Human LV3\nğ‘€ğ‘†ğ¸ ğ·ğ¶ ğ›¥ğ‘‰ ğ‘€ğ‘†ğ¸ ğ·ğ¶ ğ›¥ğ‘‰ ğ‘€ğ‘†ğ¸ ğ·ğ¶ ğ›¥ğ‘‰\nLog-odds 0.078 0.58 âˆ’28.62 0.060 0.74 âˆ’40.9 0.077 0.87 âˆ’7.67\nDL-noCert 0.033 0.81 âˆ’10.8 0.011 0.94 9.89 0.009 0.97 âˆ’2.57\nDL+Cert 0.027 0.82 âˆ’7.11 0.010 0.94 6.54 0.009 0.97 âˆ’2.51\n66%â€“100%\nMethod Human LV1 Human LV2 Human LV3\nğ‘€ğ‘†ğ¸ ğ·ğ¶ ğ›¥ğ‘‰ ğ‘€ğ‘†ğ¸ ğ·ğ¶ ğ›¥ğ‘‰ ğ‘€ğ‘†ğ¸ ğ·ğ¶ ğ›¥ğ‘‰\nLog-odds 0.096 0.54 âˆ’29.5 0.093 0.64 âˆ’55.2 0.086 0.86 âˆ’7.56\nDL-noCert 0.038 0.78 7.31 0.017 0.93 15.4 0.010 0.96 âˆ’3.11\nDL+Cert 0.034 0.78 6.10 0.014 0.93 13.7 0.009 0.96 âˆ’0.86\nIn the apical segment, DL-based methods generally show improved\nğ‘€ğ‘†ğ¸ and higher ğ·ğ¶ values compared to Log-Odds, particularly in\nLV3. Yet, in LV2, segmentation accuracy remains limited for all meth-\nods, reflecting anatomical variability and data sparsity in this region.\nFor mid-LV segments, DL-based approaches consistently achieve\nlower ğ‘€ğ‘†ğ¸, higher ğ·ğ¶ values (up to 0.96), and small volumetric\ndifferences close to zero, suggesting good agreement with reference\ninfarct distributions across all LVs.\nIn basal segments, DL-based methods similarly demonstrate lower\nğ‘€ğ‘†ğ¸ and higher ğ·ğ¶ scores than Log-Odds, with more accurate vol-\nume estimations (e.g., ğ›¥ğ‘‰ near zero in LV2 and LV3 for DL+Cert).\nThe inclusion of certainty estimation appears to provide slight addi-\ntional improvements in some cases, particularly in reducing volumetric\ndiscrepancies.\n4. Discussion\nAccurate myocardial scar reconstruction is critical for understand-\ning ventricular arrhythmia risk and guiding clinical interventions such\nas VT ablation. However, low-resolution LGE-CMR data introduces\nchallenges in precisely capturing scar morphology, particularly in the\npresence of narrow conducting isthmuses [40,41]. In this study, we\nproposed a DL-based interpolation framework to address these limi-\ntations, integrating 3D anatomical coordinates with a deep learning\nmodel trained using MC dropout uncertainty estimation [42]. Our\nresults demonstrate that DL-based interpolation significantly improves\nmyocardial scar segmentation accuracy, with further enhancements\nachieved when incorporating certainty estimation into training.\n4.1. Comparison with Log-Odds method\nTraditional interpolation methods such as the Log-Odds approach\nhave been widely used for reconstructing scar morphology from low-\nresolution LGE-CMR slices. However, our findings confirm that Log-\nOdds struggle with the low interslice resolution, particularly when the\nslice spacing is large (9.6 mm), leading to errors in scar delineation.\nThe comparison between high-resolution (1.2 mm) and low-resolution\n(9.6 mm) Log-Odds reconstructions in the pig LV model highlights how\nreduced image resolution significantly affects scar volume estimation.\nIn terms of ğ‘€ğ‘†ğ¸, ğ·ğ¶, and ğ›¥ğ‘‰ , the Log-Odds method at 9.6 mm\nresolution showed the poorest performance. This suggests that conven-\ntional statistical interpolation methods are insufficient for accurately\nreconstructing myocardial scars, particularly in low-resolution imaging\nscenarios. In contrast, our DL-based approach achieved significantly\nbetter agreement with the reference data and minimized volumetric\nerror for demonstrating a superior ability to recover fine details of the\nscar region.\nRegional scar comparison indicates that the DL-based methods gen-\nerally achieve lower ğ‘€ğ‘†ğ¸ values, higher Dice coefficients, and more\naccurate volume estimates than the Log-Odds approach across most\nLV regions and transmural layers. While the DL+Cert variant shows\nslight additional improvements in certain segments and depth ranges,\nperformance varies by anatomical location, with greater challenges\nobserved in the apex and subepicardial regions. These findings suggest\nthat both DL-based approaches offer potential advantages over tradi-\ntional interpolation, but also highlight the importance of considering\nregional variability when evaluating scar reconstruction accuracy.\n4.2. Accurate representation of border-zone\nOne of the key contributions of this study is the integration of\nMC dropout-based uncertainty estimation into the DL training process.\nOur findings show that incorporating certainty estimation into the loss\nfunction further refines scar reconstruction, particularly in border zone\nregions where the transition between healthy and scarred myocardium\nis gradual. The DL model trained with certainty achieved the highest\naccuracy among all methods. This confirms that modeling uncertainty\nleads to more stable and reliable scar predictions, reducing overfitting\nto noisy or ambiguous regions.\nFurthermore, the comparison of border zone segmentation, which\nmay be important for most of the simulation and prediction [2,7,43,\n44], shows that the Log-Odds method struggled significantly in this re-\ngion indicating that the method failed to capture the transition between\nscar and healthy myocardium. The DL without certainty performed\nbetter but still had limitations in modeling the border zone. By contrast,\nthe DL model trained with certainty achieved a significantly better\nğ·ğ¶\nğ‘ğ‘§ \nand a much lower volumetric error, demonstrating its ability to\nimprove scar boundary definition. This suggests that uncertainty-aware\ntraining plays a crucial role in refining scar transition zones, which is\nessential for accurately predicting arrhythmical pathways.\n4.3. Computational considerations of proposed DL approach\nThe training dynamics of our DL models also provide important\ninsights into the relationship between training epochs and model per-\nformance. As shown in Fig. 8, ğ‘€ğ‘†ğ¸, ğ·ğ¶, and loss metrics show\nComputers in Biology and Medicine 198 (2025) 111219\n11\n\nA. SEN et al.\nTable 5\nQuantitative comparison of the Log-Odds method and DL-based interpolation methods (with and without certainty estimation)\nin predicting infarct regions across different left ventricular (LV) segments (Apex, Apical, Mid, and Basal) for three human LV\ngeometries (LV1, LV2, and LV3). Performance metrics include Mean Square Error (ğ‘€ğ‘†ğ¸), Dice Similarity Coefficient (ğ·ğ¶), and\nVolumetric Difference (ğ›¥ğ‘‰ ), assessing accuracy in capturing the spatial distribution of infarction within each segment.\nApex\nMethod Human LV1 Human LV2 Human LV3\nğ‘€ğ‘†ğ¸ ğ·ğ¶ ğ›¥ğ‘‰ ğ‘€ğ‘†ğ¸ ğ·ğ¶ ğ›¥ğ‘‰ ğ‘€ğ‘†ğ¸ ğ·ğ¶ ğ›¥ğ‘‰\nLog-odds 0.060 0.42 31.7 0.038 0.91 11.7 0.167 0.15 85.7\nDL-noCert 0.043 0.40 61.9 0.262 0.00 100 0.011 0.91 âˆ’3.66\nDL+Cert 0.031 0.53 58.4 0.262 0.00 100 0.010 0.92 âˆ’0.48\nApical\nMethod Human LV1 Human LV2 Human LV3\nğ‘€ğ‘†ğ¸ ğ·ğ¶ ğ›¥ğ‘‰ ğ‘€ğ‘†ğ¸ ğ·ğ¶ ğ›¥ğ‘‰ ğ‘€ğ‘†ğ¸ ğ·ğ¶ ğ›¥ğ‘‰\nLog-odds 0.022 0.48 âˆ’27.5 0.002 0.88 20.2 0.040 0.63 âˆ’7.93\nDL-noCert 0.005 0.72 âˆ’4.19 0.006 0.00 100 0.006 0.89 âˆ’1.81\nDL+Cert 0.005 0.72 2.80 0.006 0.00 100 0.006 0.89 âˆ’1.63\nMid\nMethod Human LV1 Human LV2 Human LV3\nğ‘€ğ‘†ğ¸ ğ·ğ¶ ğ›¥ğ‘‰ ğ‘€ğ‘†ğ¸ ğ·ğ¶ ğ›¥ğ‘‰ ğ‘€ğ‘†ğ¸ ğ·ğ¶ ğ›¥ğ‘‰\nLog-odds 0.052 0.59 âˆ’10.1 0.017 0.62 âˆ’90.3 0.077 0.88 âˆ’2.00\nDL-noCert 0.005 0.93 âˆ’0.44 0.001 0.93 1.33 0.007 0.96 âˆ’0.08\nDL+Cert 0.005 0.93 âˆ’0.33 0.001 0.93 0.88 0.007 0.96 âˆ’0.04\nBasal\nMethod Human LV1 Human LV2 Human LV3\nğ‘€ğ‘†ğ¸ ğ·ğ¶ ğ›¥ğ‘‰ ğ‘€ğ‘†ğ¸ ğ·ğ¶ ğ›¥ğ‘‰ ğ‘€ğ‘†ğ¸ ğ·ğ¶ ğ›¥ğ‘‰\nLog-odds 0.102 0.67 âˆ’20.1 0.095 0.72 âˆ’32.86 0.102 0.85 âˆ’16.0\nDL-noCert 0.046 0.80 âˆ’12.9 0.013 0.95 1.48 0.011 0.97 âˆ’0.35\nDL+Cert 0.038 0.82 âˆ’8.13 0.011 0.95 âˆ’0.38 0.011 0.97 âˆ’1.22\nminimal improvement after 2000 epochs, suggesting that the model\nreaches a plateau in performance early in training. However, the ğ›¥ğ‘‰\ncontinues to change until 6000 epochs, highlighting that longer training\nprimarily benefits volumetric accuracy. This behavior is particularly\nimportant in scar volume estimation, where even small inaccuracies can\naffect clinical decision-making.\nWhile the DL with certainty achieves the best overall performance,\nit requires a significantly longer training time compared to the DL with-\nout certainty. As seen in Table 2 reflecting the increased computational\ncomplexity introduced by uncertainty estimation. Despite the added\ncost, the substantial improvement in border zone segmentation and\nvolumetric accuracy justifies the computational trade-off, particularly\nin applications requiring high-precision scar modeling.\n4.4. Performance on clinical data\nTo assess the generalization of our DL-based interpolation method,\nwe extended our evaluation to a human LV model with a more complex\nscar morphology, using ADAS software-generated interpolation as the\ngold-standard model. Unlike the pig LV model, where the gold-standard\nmask is binary (scar vs. healthy myocardium), the human LV dataset\ncontains continuous mask values between 0 and 1, reflecting varying\ndegrees of fibrosis. This presents a greater challenge for interpolation,\nas the model must learn a probabilistic representation of scar tissue\nrather than a strict binary classification.\nThe results in Fig. 9 show that the Log-Odds method fails to ac-\ncurately reconstruct complex scar geometries, particularly in regions\nwith partial scarring. The DL model without certainty performs better\nbut tends to overestimate scar probability in the border zone, leading\nto increased false positives. The DL model with certainty provides the\nbest balance between accuracy and anatomical consistency, producing\na scar distribution that closely matches the gold-standard model. The\nborder zone comparison in Fig. 10 further confirms that certainty-aware\ntraining improves the definition of scar transition regions, leading to\nmore realistic segmentation results.\nThe ability to accurately reconstruct myocardial scar morphology\nfrom low-resolution LGE-CMR scans has significant clinical implica-\ntions, particularly for personalized computational cardiac modeling,\nVT risk assessment, and ablation planning. Traditional interpolation\nmethods such as Log-Odds fail to capture the complexity of scar regions,\nleading to errors in conduction pathway identification. Our DL-based\napproach addresses these limitations, providing a more accurate and\nrobust method for scar interpolation.\nAnalysis a comprehensive quantitative assessment of the proposed\nmethods in capturing both infarct transmurality and spatial distribution\nacross different LV segments. In terms of transmurality (Table 4),\nthe DL-based interpolation methods, particularly the model DL+Cert,\ndemonstrate lower MSE and higher DC values across all three left\nventricles, reflecting improved accuracy and stronger spatial agreement\nwith reference transmurality maps. For spatial distribution across LV\nsegments (Table 5), results show that both DL-based methods achieve\nlower MSE and higher DC values compared to the Log-Odds method\nacross apex, apical, mid, and basal regions. The inclusion of cer-\ntainty estimation further refines volumetric accuracy and segmentation\nconsistency, especially in more challenging regions such as the apex\nand basal segments. These findings indicate that incorporating cer-\ntainty into DL interpolation not only enhances global transmurality\npredictions but also leads to more reliable and anatomically consistent\nsegment-wise infarct characterization, supporting its potential clinical\nutility in detailed patient-specific scar assessment.\n4.5. Limitations & future work\nHowever, despite the improvements achieved, some limitations re-\nmain. First, the DL model still exhibits minor volumetric discrepancies\nin certain regions, particularly in highly irregular scar structures. Fu-\nture work could explore the integration of graph-based NN or spatial\ntransformers to better preserve scar connectivity and anatomical align-\nment. Additionally, the computational cost of uncertainty-aware train-\ning remains a challenge. Exploring more efficient uncertainty quantifi-\ncation techniques, such as Bayesian neural networks or active learning\nComputers in Biology and Medicine 198 (2025) 111219\n12\n\nA. SEN et al.\nstrategies, could help reduce training time while maintaining high\naccuracy.\nOne limitation of our current framework is that the model predicts\nscar probability for all input coordinates within the LV mesh, regardless\nof their relationship to cardiac tissue boundaries. While the anatomi-\ncal mesh restricts predictions to the myocardium, the sigmoid output\ninherently produces values in the [0, 1] range for any spatial input.\nBackground or non-cardiac regions are excluded during training via\npreprocessing (e.g., masked values of âˆ’1), but the model itself does not\nexplicitly learn to distinguish between valid myocardial and irrelevant\nnon-cardiac regions. Incorporating anatomical priors or segmentation-\naware constraints could further improve robustness and specificity,\nespecially in pathological cases with altered geometry.\nAnother important aspect to consider is the clinical validation of\nour method in patient-specific datasets. While our study focuses on\npreclinical and retrospective datasets, future research should involve\nprospective validation on real patient cases, comparing interpolated\nscar reconstructions with invasive electro-anatomical mapping data to\nassess the clinical relevance of predicted conduction pathways.\nA notable limitation is observed in Patient 2 (LV2), specifically at\nthe apex region, where the DL-based methods yield a DC of 0 and\nğ›¥ğ‘‰ of 100. In this case, the scar is predominantly located in the\nbasal region, with minimal or no scar present in the mid and apical\nsegments. This spatial imbalance misleads the DL models, resulting in\npoor apex predictions. Interestingly, under this specific condition, the\nLog-Odds method shows comparatively better performance in the apex,\nhighlighting a scenario where conventional approaches may provide\nmore robust results when scar distribution is highly localized.\nOne of the limitation of our study is that the reference standard\nfor human infarcts was derived from ADAS-LV software reconstructions\nrather than from direct 3D LGE acquisitions. We acknowledge that a\nfully acquired 3D LGE dataset would have provided a more robust\nground truth. However, at the time of data collection, direct 3D LGE\nwas not routinely available across our cohort. ADAS-LV was therefore\nchosen as the reference standard because it is a clinically validated\nand widely used tool for scar characterization in cardiology research\nand practice. This approach ensured consistency across patients and\nprovided a reliable basis for evaluating our proposed method, although\nfuture studies should explore validation against directly acquired 3D\nLGE when available.\nThese findings suggest that while certainty-informed DL interpola-\ntion generally improves global and regional infarct characterization,\ncareful attention should be given to cases with highly localized or\natypical scar distributions to avoid potential misclassification.\n5. Conclusion\nThis study demonstrates that DL-based interpolation significantly\noutperforms traditional Log-Odds interpolation methods in reconstruct-\ning myocardial scar morphology from low-resolution LGE-CMR data.\nBy integrating coordinate-based learning, uncertainty estimation, and\ncustom loss functions, our approach achieves higher segmentation ac-\ncuracy, improved volumetric consistency, and better scar boundary\ndefinition. The incorporation of certainty estimation plays a crucial\nrole in refining border zone predictions, leading to more realistic scar\nrepresentations. These findings highlight the potential of DL-driven scar\ninterpolation for advancing personalized cardiac modeling, ventricular\narrhythmia risk assessment, and treatment planning. Future research\nshould focus on improving computational efficiency, integrating multi-\nmodal imaging data, and validating the method in real-world clinical\nsettings.\nCRediT authorship contribution statement\nAhmet SEN: Writing â€“ original draft, Visualization, Validation,\nSoftware, Methodology, Formal analysis, Conceptualization. Ursula\nRohrer: Writing â€“ review & editing, Resources, Data curation. Pranav\nBhagirath: Writing â€“ review & editing, Resources, Data curation. Reza\nRazavi: Writing â€“ review & editing, Resources, Data curation. Mark\nOâ€™Neill: Writing â€“ review & editing, Resources, Data curation. John\nWhitaker: Writing â€“ review & editing, Resources, Data curation. Mar-\ntin Bishop: Writing â€“ review & editing, Supervision, Software, Re-\nsources, Project administration, Funding acquisition.\nData and code availability\nThe patient imaging data used in this study were obtained under\ninstitutional research ethics approval and are not publicly available due\nto patient confidentiality restrictions. Derived, de-identified datasets\nmay be available from the corresponding author upon reasonable re-\nquest and subject to institutional approval.\nThe code developed for this study is available from the correspond-\ning author upon reasonable request.\nFunding\nThis work was supported by the British Heart Foundation, United\nKingdom through a Project Grant to MJB (PG/22/11159).\nDeclaration of competing interest\nThe authors declare that they have no known competing finan-\ncial interests or personal relationships that could have appeared to\ninfluence the work reported in this paper.\nAppendix. Log-Odds method for 3D infarct reconstruction\nThe Log-Odds method facilitates three-dimensional (3D) reconstruc-\ntion of myocardial infarct geometry from two-dimensional (2D) late\ngadolinium-enhanced cardiac magnetic resonance (LGE-CMR) images.\nThe process begins with binary segmentation ğ‘†\nğ‘–\n(ğ‘¥, ğ‘¦) âˆˆ {0, 1} of in-\nfarcted tissue at each slice ğ‘–, which are smoothed to generate proba-\nbility maps ğ‘ƒ\nğ‘–\n(ğ‘¥, ğ‘¦) âˆˆ [0, 1] using a Gaussian kernel ğº\nğœ \n:\nğ‘ƒ\nğ‘–\n(ğ‘¥, ğ‘¦) = (ğ‘†\nğ‘– \nâˆ— ğº\nğœ \n)(ğ‘¥, ğ‘¦)\nThe probability maps are then transformed into the log-odds (logit)\nspace:\nğ¿\nğ‘–\n(ğ‘¥, ğ‘¦) = log\n( \nğ‘ƒ\nğ‘–\n(ğ‘¥, ğ‘¦)\n1 âˆ’ ğ‘ƒ\nğ‘–\n(ğ‘¥, ğ‘¦)\n)\nInterpolation is performed in the log-odds domain along the\nthrough-plane direction (z-axis) using cubic splines:\nğ¿(ğ‘¥, ğ‘¦, ğ‘§) = spline(ğ¿\nğ‘–\n(ğ‘¥, ğ‘¦), ğ‘§)\nThe interpolated log-odds volume is mapped back to probability\nspace using the inverse logit function:\nğ‘ƒ (ğ‘¥, ğ‘¦, ğ‘§) = \n1\n1 + ğ‘’\nâˆ’ğ¿(ğ‘¥,ğ‘¦,ğ‘§)\nFinally, a binary reconstruction is obtained by thresholding the\nprobability volume at a value ğœ (typically ğœ = 0.5):\nÌ‚\nğ‘†(ğ‘¥, ğ‘¦, ğ‘§) =\n{\n1, if ğ‘ƒ (ğ‘¥, ğ‘¦, ğ‘§) â‰¥ ğœ\n0, otherwise\nThis method preserves anatomical continuity and topological fea-\ntures better than conventional voxel-wise interpolation techniques,\nmaking it particularly suitable for patient-specific modeling in cardiac\nelectrophysiology [15].\nComputers in Biology and Medicine 198 (2025) 111219\n13\n\nA. SEN et al.\nReferences\n[1] S.A. Niederer, J. Lumens, N.A. Trayanova, Computational models in cardiology,\nNat. Rev. Cardiol. 16 (2) (2019) 100â€“111.\n[2] H.J. Arevalo, F. Vadakkumpadan, E. Guallar, A. Jebb, P. Malamas, K.C. Wu, N.A.\nTrayanova, Arrhythmia risk stratification of patients after myocardial infarction\nusing personalized heart models, Nat. Commun. 7 (1) (2016) 11437.\n[3] C.M. Costa, A. Neic, E. Kerfoot, B. Porter, B. Sieniewicz, J. Gould, B. Sidhu,\nZ. Chen, G. Plank, C.A. Rinaldi, et al., Pacing in proximity to scar during\ncardiac resynchronization therapy increases local dispersion of repolarization and\nsusceptibility to ventricular arrhythmogenesis, Hear. Rhythm. 16 (10) (2019)\n1475â€“1483.\n[4] C.M. Costa, A. Neic, K. Gillette, B. Porter, J. Gould, B. Sidhu, Z. Chen, M.\nElliott, V. Mehta, G. Plank, et al., Left ventricular endocardial pacing is less\narrhythmogenic than conventional epicardial pacing when pacing in proximity\nto scar, Hear. Rhythm. 17 (8) (2020) 1262â€“1270.\n[5] A.W. Lee, U.C. Nguyen, O. Razeghi, J. Gould, B.S. Sidhu, B. Sieniewicz, J. Behar,\nM. Mafi-Rad, G. Plank, F.W. Prinzen, et al., A rule-based method for predicting\nthe electrical activation of the heart with cardiac resynchronization therapy from\nnon-invasive clinical data, Med. Image Anal. 57 (2019) 197â€“213.\n[6] M. Strocchi, A.W. Lee, A. Neic, J. Bouyssier, K. Gillette, G. Plank, M.K. Elliott,\nJ. Gould, J.M. Behar, B. Sidhu, et al., His-bundle and left bundle pacing\nwith optimized atrioventricular delay achieve superior electrical synchrony over\nendocardial and epicardial pacing in left bundle branch block patients, Hear.\nRhythm. 17 (11) (2020) 1922â€“1929.\n[7] A. Prakosa, H.J. Arevalo, D. Deng, P.M. Boyle, P.P. Nikolov, H. Ashikaga, J.J.\nBlauer, E. Ghafoori, C.J. Park, R.C. Blake III, et al., Personalized virtual-heart\ntechnology for guiding the ablation of infarct-related ventricular tachycardia,\nNat. Biomed. Eng. 2 (10) (2018) 732â€“740.\n[8] C.M. Costa, P. Gemmell, M.K. Elliott, J. Whitaker, F.O. Campos, M. Strocchi,\nA. Neic, K. Gillette, E. Vigmond, G. Plank, et al., Determining anatomical and\nelectrophysiological detail requirements for computational ventricular models of\nporcine myocardial infarction, Comput. Biol. Med. 141 (2022) 105061.\n[9] M. Motwani, N. Maredia, T.A. Fairbairn, S. Kozerke, A. Radjenovic, J.P. Green-\nwood, S. Plein, High-resolution versus standard-resolution cardiovascular MR\nmyocardial perfusion imaging for the detection of coronary artery disease, Circ.:\nCardiovasc. Imaging 5 (3) (2012) 306â€“313.\n[10] M. Sahota, F.O. Campos, S.E. Williams, M. Cluitmans, G. Plank, S.A. Niederer,\nM.J. Bishop, Effect of scar interpolation methods on simulated ventricular\ntachycardia in infarcted heart models, 2024.\n[11] S.P. Raya, J.K. Udupa, Shape-based interpolation of multidimensional objects,\nIEEE Trans. Med. Imaging 9 (1) (1990) 32â€“42.\n[12] E. Ukwatta, M. Rajchl, J. White, F. Pashakhanloo, D.A. Herzka, E. McVeigh,\nA.C. Lardo, N. Trayanova, F. Vadakkumpadan, Image-based reconstruction of\n3D myocardial infarct geometry for patient specific applications, in: Proceedings\nof Spieâ€“the International Society for Optical Engineering, Vol. 9413, 2015, p.\n94132W.\n[13] Q. Tao, S.R. Piers, H.J. Lamb, R.J. van der Geest, Automated left ventricle\nsegmentation in late gadolinium-enhanced mri for objective myocardial scar\nassessment, J. Magn. Reson. Imaging 42 (2) (2015) 390â€“399.\n[14] D. Deng, H. Arevalo, F. Pashakhanloo, A. Prakosa, H. Ashikaga, E. McVeigh,\nH. Halperin, N. Trayanova, Accuracy of prediction of infarct-related arrhythmic\ncircuits from image-based models reconstructed from low and high resolution\nMRI, Front. Physiol. 6 (2015) 282.\n[15] E. Ukwatta, H. Arevalo, M. Rajchl, J. White, F. Pashakhanloo, A. Prakosa, D.A.\nHerzka, E. McVeigh, A.C. Lardo, N.A. Trayanova, et al., Image-based recon-\nstruction of three-dimensional myocardial infarct geometry for patient-specific\nmodeling of cardiac electrophysiology, Med. Phys. 42 (8) (2015) 4579â€“4590.\n[16] C. Chen, C. Qin, H. Qiu, G. Tarroni, J. Duan, W. Bai, D. Rueckert, Deep learning\nfor cardiac image segmentation: a review, Front. Cardiovasc. Med. 7 (2020) 25.\n[17] D.M. Papetti, K. Van Abeelen, R. Davies, R. MenÃ¨, F. Heilbron, F.P. Perelli, J.\nArtico, A. Seraphim, J.C. Moon, G. Parati, et al., An accurate and time-efficient\ndeep learning-based system for automated segmentation and reporting of cardiac\nmagnetic resonance-detected ischemic scar, Comput. Methods Programs Biomed.\n229 (2023) 107321.\n[18] H. Fadil, J.J. Totman, D.J. Hausenloy, H.-H. Ho, P. Joseph, A.F.-H. Low, A.M.\nRichards, M.Y. Chan, S. Marchesseau, A deep learning pipeline for automatic\nanalysis of multi-scan cardiovascular magnetic resonance, J. Cardiovasc. Magn.\nReson. 23 (1) (2021) 47.\n[19] J. Xing, S. Wang, K.C. Bilchick, A.R. Patel, M. Zhang, Joint deep learning\nfor improved myocardial scar detection from cardiac MRI, in: 2023 IEEE 20th\nInternational Symposium on Biomedical Imaging, ISBI, IEEE, 2023, pp. 1â€“5.\n[20] V.P. Jani, M. Ostovaneh, E. Chamera, Y. Kato, J.A. Lima, B. Ambale-Venkatesh,\nDeep learning for automatic volumetric segmentation of left ventricular my-\nocardium and ischaemic scar from multi-slice late gadolinium enhancement\ncardiovascular magnetic resonance, Eur. Hear. Journal-Cardiovascular Imaging\n25 (6) (2024) 829â€“838.\n[21] H. Qi, P. Qian, L. Tang, B. Chen, D. An, L.-M. Wu, Predicting late gadolinium en-\nhancement of acute myocardial infarction in contrast-free cardiac cine MRI using\ndeep generative learning, Circ.: Cardiovasc. Imaging 17 (9) (2024) e016786.\n[22] D.M. Popescu, H.G. Abramson, R. Yu, C. Lai, J.K. Shade, K.C. Wu, M. Maggioni,\nN.A. Trayanova, Anatomically informed deep learning on contrast-enhanced\ncardiac magnetic resonance imaging for scar segmentation and clinical feature\nextraction, Cardiovasc. Digit. Heal. J. 3 (1) (2022) 2â€“13.\n[23] J.A. Oscanoa, M.J. Middione, C. Alkan, M. Yurt, M. Loecher, S.S. Vasanawala,\nD.B. Ennis, Deep learning-based reconstruction for cardiac MRI: a review,\nBioengineering 10 (3) (2023) 334.\n[24] B. Mildenhall, P.P. Srinivasan, M. Tancik, J.T. Barron, R. Ramamoorthi, R. Ng,\nRepresenting scenes as neural radiance fields for view synthesis, in: Computer\nVision and Pattern Recognition, 2020.\n[25] V. Sitzmann, J. Martel, A. Bergman, D. Lindell, G. Wetzstein, Implicit neural\nrepresentations with periodic activation functions, Adv. Neural Inf. Process. Syst.\n33 (2020) 7462â€“7473.\n[26] J.J. Park, P. Florence, J. Straub, R. Newcombe, S. Lovegrove, Deepsdf: Learning\ncontinuous signed distance functions for shape representation, in: Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2019,\npp. 165â€“174.\n[27] A. Kendall, Y. Gal, What uncertainties do we need in bayesian deep learning for\ncomputer vision? Adv. Neural Inf. Process. Syst. 30 (2017).\n[28] S. WeingÃ¤rtner, Ã–.B. Demirel, F. Gama, I. Pierce, T.A. Treibel, J. Schulz-Menger,\nM. AkÃ§akaya, Cardiac phase-resolved late gadolinium enhancement imaging,\nFront. Cardiovasc. Med. 9 (2022) 917180.\n[29] K. Bratis, M. Henningsson, C. Grigoratos, M. Dellâ€™Omodarme, K. Chasapides, R.\nBotnar, E. Nagel, Image-navigated 3-dimensional late gadolinium enhancement\ncardiovascular magnetic resonance imaging: feasibility and initial clinical results,\nJ. Cardiovasc. Magn. Reson. 19 (1) (2016) 97.\n[30] D. Andreu, J.T. Ortiz-Perez, J. Fernandez-Armenta, E. Guiu, J. Acosta, S.\nPrat-Gonzalez, T.M. De Caralt, R.J. Perea, C. Garrido, L. Mont, et al., 3D delayed-\nenhanced magnetic resonance sequences improve conducting channel delineation\nprior to ventricular tachycardia ablation, EP Eur. 17 (6) (2015) 938â€“945.\n[31] A. Neic, M.A. Gsell, E. Karabelas, A.J. Prassl, G. Plank, Automating image-based\nmesh generation and manipulation tasks in cardiac modeling workflows using\nmeshtool, SoftwareX 11 (2020) 100454.\n[32] P. Bhagirath, F.O. Campos, C.M. Costa, A.A. Wilde, A.J. Prassl, A. Neic, G. Plank,\nC.A. Rinaldi, M.J. GÃ¶tte, M.J. Bishop, Predicting arrhythmia recurrence following\ncatheter ablation for ventricular tachycardia using late gadolinium enhancement\nmagnetic resonance imaging: implications of varying scar ranges, Hear. Rhythm.\n19 (10) (2022) 1604â€“1610.\n[33] P. Bhagirath, F.O. Campos, P.G. Postema, M.J. Kemme, A.A. Wilde, A.J. Prassl, A.\nNeic, C.A. Rinaldi, M.J. GÃ¶tte, G. Plank, et al., Arrhythmogenic vulnerability of\nre-entrant pathways in post-infarct ventricular tachycardia assessed by advanced\ncomputational modelling, Europace 25 (9) (2023) euad198.\n[34] P. Bhagirath, F.O. Campos, H.A. Zaidi, Z. Chen, M. Elliott, J. Gould, M.J. Kemme,\nA.A. Wilde, M.J. GÃ¶tte, P.G. Postema, et al., Predicting postinfarct ventricular\ntachycardia by integrating cardiac MRI and advanced computational reentrant\npathway analysis, Hear. Rhythm. 21 (10) (2024) 1962â€“1969.\n[35] B. Huang, H. Xiao, W. Liu, Y. Zhang, H. Wu, W. Wang, Y. Yang, Y. Yang,\nG.W. Miller, T. Li, et al., MRI super-resolution via realistic downsampling with\nadversarial learning, Phys. Med. Biol. 66 (20) (2021) 205004.\n[36] Y. Li, B. Sixou, F. Peyrin, A review of the deep learning methods for medical\nimages super resolution problems, Irbm 42 (2) (2021) 120â€“133.\n[37] M. Abadi, A. Agarwal, P. Barham, E. Brevdo, Z. Chen, C. Citro, G.S. Corrado,\nA. Davis, J. Dean, M. Devin, S. Ghemawat, I. Goodfellow, A. Harp, G. Irving,\nM. Isard, Y. Jia, R. Jozefowicz, L. Kaiser, M. Kudlur, J. Levenberg, D. ManÃ©,\nR. Monga, S. Moore, D. Murray, C. Olah, M. Schuster, J. Shlens, B. Steiner,\nI. Sutskever, K. Talwar, P. Tucker, V. Vanhoucke, V. Vasudevan, F. ViÃ©gas, O.\nVinyals, P. Warden, M. Wattenberg, M. Wicke, Y. Yu, X. Zheng, TensorFlow:\nLarge-scale machine learning on heterogeneous systems, 2015, URL: http://\ntensorflow.org/ Software available from tensorflow.org.\n[38] J. Whitaker, R. Neji, S. Kim, A. Connolly, T. Aubriot, J.J. Calvo, R. Karim, C.H.\nRoney, B. Murfin, C. Richardson, et al., Late gadolinium enhancement cardio-\nvascular magnetic resonance assessment of substrate for ventricular tachycardia\nwith hemodynamic compromise, Front. Cardiovasc. Med. 8 (2021) 744779.\n[39] H. Summers, The ADAS user manual, version 2.6, 2004, http://www.adas.ac.uk/.\n[40] A. Hennig, M. Salel, F. Sacher, C. Camaioni, S. Sridi, A. Denis, M. Mon-\ntaudon, F. Laurent, P. Jais, H. Cochet, High-resolution three-dimensional late\ngadolinium-enhanced cardiac magnetic resonance imaging to identify the under-\nlying substrate of ventricular arrhythmia, EP Eur. 20 (FI2) (2017) f179â€“f191,\nhttp://dx.doi.org/10.1093/europace/eux278.\n[41] I. Roca-Luque, L. Mont-Girbau, Cardiac magnetic resonance for ventricular\ntachycardia ablation and risk stratification, Front. Cardiovasc. Med. 8 (2022)\n797864.\n[42] E. Begoli, T. Bhattacharya, D. Kusnezov, The need for uncertainty quantification\nin machine-assisted medical decision making, Nat. Mach. Intell. 1 (1) (2019)\n20â€“23.\nComputers in Biology and Medicine 198 (2025) 111219\n14\n\nA. SEN et al.\n[43] H. Ashikaga, T. Sasano, J. Dong, M.M. Zviman, R. Evers, B. Hopenfeld, V. Castro,\nR.H. Helm, T. Dickfeld, S. Nazarian, et al., Magnetic resonanceâ€“based anatomical\nanalysis of scar-related ventricular tachycardia: implications for catheter ablation,\nCirc. Res. 101 (9) (2007) 939â€“947.\n[44] S. Zahid, H. Cochet, P.M. Boyle, E.L. Schwarz, K.N. Whyte, E.J. Vigmond,\nR. Dubois, M. Hocini, M. HaÃ¯ssaguerre, P. JaÃ¯s, et al., Patient-derived models\nlink re-entrant driver localization in atrial fibrillation to fibrosis spatial pattern,\nCardiovasc. Res. 110 (3) (2016) 443â€“454.\nComputers in Biology and Medicine 198 (2025) 111219\n15",
    "version": "5.3.31"
  },
  {
    "numpages": 10,
    "numrender": 10,
    "info": {
      "PDFFormatVersion": "1.7",
      "Language": "en-US",
      "EncryptFilterName": null,
      "IsLinearized": true,
      "IsAcroFormPresent": false,
      "IsXFAPresent": false,
      "IsCollectionPresent": false,
      "IsSignaturesPresent": false,
      "Creator": "Elsevier",
      "Custom": {
        "CrossMarkDomains[1]": "elsevier.com",
        "CrossmarkMajorVersionDate": "2010-04-23",
        "CreationDate--Text": "7th December 2024",
        "ElsevierWebPDFSpecifications": "7.0.1",
        "robots": "noindex",
        "doi": "10.1016/j.patcog.2024.111204",
        "CrossMarkDomains[2]": "sciencedirect.com",
        "CrossmarkDomainExclusive": "true"
      },
      "ModDate": "D:20241207020210Z",
      "Author": "Jia Fu",
      "Title": "UM-CAM: Uncertainty-weighted multi-resolution class activation maps for weakly-supervised segmentation",
      "Keywords": "Segmentation,Brain tumor,Class activation map,Exponential geodesic distance,Noisy label",
      "CreationDate": "D:20241206233019Z",
      "Producer": "Acrobat Distiller 8.1.0 (Windows)",
      "Subject": "Pattern Recognition, 160 (2025) 111204. doi:10.1016/j.patcog.2024.111204"
    },
    "metadata": {
      "ali:license_ref": "http://creativecommons.org/licenses/by-nc/4.0/",
      "crossmark:crossmarkdomains": "elsevier.comsciencedirect.com",
      "crossmark:crossmarkdomainexclusive": "true",
      "crossmark:doi": "10.1016/j.patcog.2024.111204",
      "crossmark:majorversiondate": "2010-04-23",
      "dc:format": "application/pdf",
      "dc:identifier": "10.1016/j.patcog.2024.111204",
      "dc:publisher": "Elsevier Ltd",
      "dc:description": "Pattern Recognition, 160 (2025) 111204. doi:10.1016/j.patcog.2024.111204",
      "dc:subject": [
        "Segmentation",
        "Brain tumor",
        "Class activation map",
        "Exponential geodesic distance",
        "Noisy label"
      ],
      "dc:title": "UM-CAM: Uncertainty-weighted multi-resolution class activation maps for weakly-supervised segmentation",
      "dc:creator": [
        "Jia Fu",
        "Guotai Wang",
        "Tao Lu",
        "Qiang Yue",
        "Tom Vercauteren",
        "SÃ©bastien Ourselin",
        "Shaoting Zhang"
      ],
      "jav:journal_article_version": "VoR",
      "pdf:creationdate--text": "7th December 2024",
      "pdf:producer": "Acrobat Distiller 8.1.0 (Windows)",
      "pdf:keywords": "Segmentation,Brain tumor,Class activation map,Exponential geodesic distance,Noisy label",
      "pdfx:creationdate--text": "7th December 2024",
      "pdfx:crossmarkdomains": "sciencedirect.comelsevier.com",
      "pdfx:crossmarkdomainexclusive": "true",
      "pdfx:crossmarkmajorversiondate": "2010-04-23",
      "lxlf8ywmqmm2kodigzd2rylnnodn-lwesypygn.ynn9n_ymj.n.eno9eqn9iknm-rmm2tma": "",
      "pdfx:doi": "10.1016/j.patcog.2024.111204",
      "pdfx:robots": "noindex",
      "prism:aggregationtype": "journal",
      "prism:copyright": "Â© 2024 The Authors. Published by Elsevier Ltd.",
      "prism:coverdate": "2025-04-01",
      "prism:coverdisplaydate": "1 April 2025",
      "prism:doi": "10.1016/j.patcog.2024.111204",
      "prism:issn": "0031-3203",
      "prism:pagerange": "111204",
      "prism:publicationname": "Pattern Recognition",
      "prism:startingpage": "111204",
      "prism:url": "https://doi.org/10.1016/j.patcog.2024.111204",
      "prism:volume": "160",
      "tdm:policy": "https://www.elsevier.com/tdm/tdmrep-policy.json",
      "tdm:reservation": "1",
      "xmp:createdate": "2024-12-06T23:30:19",
      "xmp:creatortool": "Elsevier",
      "xmp:metadatadate": "2024-12-07T02:02:10",
      "xmp:modifydate": "2024-12-07T02:02:10",
      "xmpmm:documentid": "uuid:1b499bed-4ce8-4c0c-b682-0d58baae1cbe",
      "xmpmm:instanceid": "uuid:b6ed5266-03cb-4855-90c0-636283357f42",
      "xmprights:marked": "True"
    },
    "text": "Contents lists available at ScienceDirect\nPattern Recognition\njournal homepage: www.elsevier.com/locate/pr\nUM-CAM: Uncertainty-weighted multi-resolution class activation maps for\nweakly-supervised segmentation\nJia Fu \na\n, Guotai Wang \na,b,\nâˆ—\n, Tao Lu \nc\n, Qiang Yue \nd\n, Tom Vercauteren \ne\n, SÃ©bastien Ourselin \ne\n,\nShaoting Zhang \na,b\na \nSchool of Mechanical and Electrical Engineering, University of Electronic Science and Technology of China, Chengdu, China\nb \nShanghai Artificial Intelligence Laboratory, Shanghai, China\nc \nDepartment of Radiology, Sichuan Provincial Peopleâ€™s Hospital, University of Electronic Science and Technology of China, Chengdu, China\nd \nDepartment of Radiology, West China Hospital, Sichuan University, Chengdu, China\ne \nSchool of Biomedical Engineering & Imaging Sciences, Kingâ€™s College London, London, UK\nA R T I C L E I N F O\nKeywords:\nSegmentation\nBrain tumor\nClass activation map\nExponential geodesic distance\nNoisy label\nA B S T R A C T\nWeakly-supervised medical image segmentation methods utilizing image-level labels have gained attention for\nreducing the annotation cost. They typically use Class Activation Maps (CAM) from a classification network\nbut struggle with incomplete activation regions due to low-resolution localization without detailed boundaries.\nDifferently from most of them that only focus on improving the quality of CAMs, we propose a more unified\nweakly-supervised segmentation framework with image-level supervision. Firstly, an Uncertainty-weighted\nMulti-resolution Class Activation Map (UM-CAM) is proposed to generate high-quality pixel-level pseudo-labels.\nSubsequently, a Geodesic distance-based Seed Expansion (GSE) strategy is introduced to rectify ambiguous\nboundaries in the UM-CAM by leveraging contextual information. To train a final segmentation model from\nnoisy pseudo-labels, we introduce a Random-View Consensus (RVC) training strategy to suppress unreliable\npixel/voxels and encourage consistency between random-view predictions. Extensive experiments on 2D fetal\nbrain segmentation and 3D brain tumor segmentation tasks showed that our method significantly outperforms\nexisting weakly-supervised methods. Code is available at: https://github.com/HiLab-git/UM-CAM.\n1. Introduction\nAccurate segmentation of medical images is important for disease\ndiagnosis, treatment planning and progress monitoring [1â€“3]. Recent\nmethods based on deep learning models such as Convolutional Neu-\nral Network (CNN) and vision Transformers have achieved promising\nperformance on this task when trained with a large set of images\nwith pixel-level annotations [4â€“6]. However, it is labor-intensive, time-\nconsuming and expensive to collect a large-scale pixel-wise annotated\ndataset, especially for medical images that are volumetric data and\nrequire expertise to interpret [7,8]. To address data scarcity, var-\nious learning techniques such as transfer learning, semi-supervised\nlearning, and weakly-supervised learning have been explored [9]. Cur-\nrently, weakly-supervised segmentation methods with image-level su-\npervision [10] are gaining increasing attention due to their minimal\nannotation cost. However, learning from image-level supervision is ex-\ntremely challenging, as image-level labels only provide the existence of\nan object class but cannot indicate the location and shape information\nof the target that is essential for segmentation tasks [11].\nâˆ— \nCorresponding author at: School of Mechanical and Electrical Engineering, University of Electronic Science and Technology of China, Chengdu, China.\nE-mail addresses: guotai.wang@uestc.edu.cn (G. Wang), zhangshaoting@uestc.edu.cn (S. Zhang).\nFor segmentation tasks, a common method of learning from image-\nlevel labels is to produce a coarse localization of the object based\non Class Activation Maps (CAMs) [12,13] of a classification network.\nHowever, the localization maps usually only activate discriminative\nregions and miss a lot of detailed parts. Despite that, some strategies\nsuch as Discriminative Region Suppression (DRS) [14] and Reliable\nRegion Mining (RRM) [15] have been proposed to mine more complete\nactivation regions. The CAMs in most existing works are obtained from\nvery deep layers in a classification model, leading to low-resolution ac-\ntivation maps with over-smooth object boundaries, which is insufficient\nfor accurate segmentation.\nTo make CAMs better match object boundaries, several methods\nhave been proposed to refine the initial CAMs, such as seed expan-\nsion [16], channel-spatial attention [17], and explicit boundary pre-\ndiction [18]. Especially, AffinityNet [11] uses an additional network to\nlearn inter-pixel semantic similarity that is sent to random walk, and\nChen et al. [18] introduced a boundary prediction network to excavate\nmore object boundaries that are used for refinement by CRF. However,\nhttps://doi.org/10.1016/j.patcog.2024.111204\nReceived 27 April 2024; Received in revised form 17 September 2024; Accepted 15 November 2024\nPattern Recognition 160 (2025) 111204\nAvailable online 26 November 2024\n0031-3203/Â© 2024 The Authors. Published by Elsevier Ltd. This is an open access article under the CC BY-NC license (\nhttp://creativecommons.org/licenses/by-\nnc/4.0/).\n\nJ. Fu et al.\nthe introduction of an additional network increases the complexity of\nthe training process, and more efficient refinement methods to mine\nthe boundary cues are expected.\nAnother issue in weakly-supervised segmentation is using CAMs\nas pseudo-labels for training a segmentation model. As the pseudo-\nlabels are inaccurate and noisy, the model may be corrupted by the\nnoisy labels. However, most existing weakly-supervised segmentation\nmethods directly treat pseudo-labels as full supervision for training,\nignoring the impact of noise. Some methods have been proposed for\nnoise-robust learning, such as using uncertainty estimation to suppress\nunconfident pseudo-labels [\n19]. However, as the uncertain regions\nare often located at the object boundaries, using uncertainty-based\nweighting may be insufficient to supervise the model in these regions.\nIn addition, both noisy and hard labels could lead to high training loss,\ntaking pixels with large loss values as noisy labels will limit the modelâ€™s\nability to learn from hard cases. Therefore, more effective noisy label\nlearning methods are required for weakly-supervised segmentation.\nIn this work, we propose a novel two-stage weakly-supervised\nmethod for accurate medical image segmentation using image-level\nlabels. Differently from most existing works that only focus on im-\nproving the quality of CAMs or dealing with noisy pseudo-labels, we\ndeal with both problems in a unified framework with novel meth-\nods for high-resolution CAM generation, CAM refinement with more\naccurate boundaries and noise-robust learning from pseudo-labels for\nsegmentation. The contribution of our work is three-fold:\n(1) We propose Uncertainty-weighted Multi-resolution CAM (UM-\nCAM) to fuse low- and high-resolution CAMs, which adaptively lever-\nages multi-scale features to generate high-quality raw pseudo-labels.\n(2) We introduce a Geodesic distance-based Seed Expansion (GSE)\nmethod to refine the boundaries of UM-CAMs based on contextual\ninformation, generating more accurate pseudo-labels for segmentation.\n(3) We propose Random-View Consensus (RVC) to learn from noisy\npseudo-labels, which leverages consensus-weighted cross-entropy loss\nand consistency regularization under random spatial transforms to\nmitigate over-fitting to label noise.\nThis paper is a substantial extension of our preliminary work pub-\nlished in MICCAI 2023 [\n20], where we proposed UM-CAM and GSE\nfor high-quality pseudo-label generation and validated it with an in-\nhouse fetal brain dataset. In this extension, we further introduce RVC\nfor learning from noisy pseudo-labels, and additionally validate our\nmethod on the public Brain Tumor Segmentation (BraTS) dataset [\n21â€“\n23]. Compared with state-of-the-art weakly-supervised methods, our\nmethod significantly boosted the average DSC by over 4% and 12%\non the fetal brain and brain tumor segmentation tasks respectively,\nand largely reduced the gap between weakly-supervised and fully\nsupervised segmentation.\n2. Related work\n2.1. Learning from image-level labels for segmentation\nCAM-based methods have attracted much attention for weakly-\nsupervised segmentation [12,13], due to that CAMs obtained from a\nclassification network could provide a rough localization of the target.\nThese methods can be roughly divided into one-stage and two-stage\nmethods. One-stage methods only train a single segmentation model\nfrom the image-level labels. CAM [12] and its variants such as Grad-\nCAM [13], Score-CAM [24] and Ablation-CAM [25] are designed to\nobtain target regions from a classification model. However, as CAMs\nare obtained from low-resolution feature maps in the classification\nmodel, directly using them as segmentation labels will lead to low\nsegmentation accuracy due to over-smooth results without local details.\nRRM [\n15] uses CAMs from a classification branch refined by CRF to\nsupervise a segmentation branch. However, the segmentation branch\nwill be misguided before the classification branch is well-trained, which\nincreases the difficulty of convergence for the segmentation branch.\nIn contrast to one-stage methods, two-stage methods first train a\nclassification model to obtain CAMs, and then use them as pseudo-\nlabels to train a segmentation model [26]. To improve the quality\nof pseudo-labels, CRF is commonly used for refinement [18,19], in\naddition to other strategies such as seed expansion [16] and boundary\nexploration [18]. DRS [14] has also been proposed to mine non-\ndiscriminative regions for more accurate pseudo-labels. However, the\nexisting two-stage methods mainly focus on improving the quality of\npseudo-labels and ignore the noise in pseudo-labels in the segmenta-\ntion model training stage, which could limit the final segmentation\nperformance.\n2.2. Learning from noisy labels\nTo deal with noisy segmentation labels, noise-robust loss functions\nare proposed to prevent over-fitting to noise, such as Generalized\nCross-Entropy (GCE) loss [27], Noise-Robust Dice (NR-Dice) loss [28],\nand noise-weighted loss that assigns lower weights to pixels with\nlarge uncertainty [\n19]. Additionally, some methods focus on noise-\nrobust network architectures and training procedures. For instance,\nCo-teaching [29] involves two networks that select instances with low\nloss values for each other in a mini-batch. Zhang et al. [\n30] proposed a\nTri-network (Tri-Net) learning framework, where consensus predictions\nfrom each two networks are used to supervise the third one. However,\nthese methods require a set of clean labels that are not available in\nweakly-supervised segmentation.\n3. Method\nFig. 1 shows an overview of our proposed method for weakly-\nsupervised segmentation. First, to obtain high-quality raw pseudo-\nlabels, Uncertainty-weighted Multi-resolution CAM (UM-CAM) is pro-\nduced by fusing low- and high-resolution CAMs from different layers\nof the classification network. Second, to rectify the boundaries of the\nraw pseudo-labels, Geodesic distance-based Seed Expansion (GSE) is\nproposed to obtain high-quality refined pseudo-labels. Finally, a seg-\nmentation network is trained from the refined pseudo-labels based on\na Random-View Consensus (RVC) strategy, which uses consensus out-\nputs from two random views and pseudo-labels to suppress unreliable\npseudo-labels and imposes a consistency regularization on predictions\nfrom the two views.\n3.1. Raw pseudo-labels based on UM-CAM\n3.1.1. Initial response via grad-CAM\nA typical classification network consists of convolutional layers as a\nfeature extractor, followed by global average pooling and a fully con-\nnected layer as the output classifier [31]. Given a set of training images\nwith image-level labels, we first train a classification network, and then\nuse Grad-CAM [\n13] to compute the weight ğ›¼\nğ‘ \nfor the ğ‘th channel of a\nfeature map ğ‘“ at a certain layer via gradient backpropagation from the\noutput node for the foreground class. The foreground activation map\nğ´ can be obtained by a weighted combination of feature channels and\nfollowed by a ReLU activation [\n13], which is formulated as:\nğ›¼\nğ‘ \n= \n1\nğ‘\nğ‘\nâˆ‘\nğ‘–=1\nğœ• ğ‘¦\nğœ• ğ‘“\nğ‘ \n(ğ‘–) \n, (1)\nğ´(ğ‘–) = ğ‘…ğ‘’ğ¿ğ‘ˆ (\nâˆ‘\nğ‘\nğ›¼\nğ‘ \nğ‘“\nğ‘ \n(ğ‘–)), (2)\nwhere ğ‘¦ is the classification prediction score for the foreground. ğ‘– is\nthe pixel index, and ğ‘ is the pixel number in the feature map. ğ‘“\nğ‘ \n(ğ‘–)\ndenotes the ğ‘th channel of feature map ğ‘“ at pixel ğ‘–, and ğ´(ğ‘–) denotes\nthe activation value at pixel ğ‘–. Existing methods [\n14,15] obtain the\nCAMs using the last convolutional layer for its high-level semantic\nrepresentation. However, they typically only provide discriminative\nPattern Recognition 160 (2025) 111204\n2\n\nJ. Fu et al.\nFig. 1. An overview of the proposed method for weakly-supervised segmentation.\nregions with over-smooth boundaries due to the low-resolution feature\nmap, for example, the resolution of the last conventional layer of\nVGG [31] is 1/32 of the input. To deal with this problem, we propose\nUncertainty-weighted Multi-resolution CAM (UM-CAM) to fuse multi-\ndepth CAMs adaptively, which leads to more detailed activation maps\nat a higher resolution.\n3.1.2. Uncertainty-weighted multi-resolution CAM (UM-CAM)\nAs shown in Fig. 1(a), the activation maps generated from shallow\nlayers of the classification network contain high-resolution seman-\ntic features but suffer from inaccurate localization, while those from\ndeeper layers have better localization accuracy but cannot provide\nlocal details [32]. To take advantage of activation maps from multi-\ndepth layers, we propose to integrate the multi-resolution CAMs to take\nadvantage of their complementary information to improve the quality\nof CAMs.\nLet us denote a set of activation maps from the last ğ‘€ convolu-\ntional blocks as îˆ­ = {ğ´\nğ‘š\n}\nğ‘€\nğ‘š=1\n. As the activation maps from shallow\nand deep layers have different resolution, they are resampled to the\nsame resolution as the input and normalized to [0, 1] via minâ€“max\nnormalization as\nÌ‚\n îˆ­ = {\nÌ‚\nğ´\nğ‘š\n}\nğ‘€\nğ‘š=1\n. Directly averaging these activation\nmaps may miss fine details due to their different spatial frequencies.\nTo tackle this problem, we propose UM-CAM to integrate the confident\nregion of multi-resolution CAMs adaptively. The UM-CAM is defined as\nthe entropy-weighted fusion of CAMs:\nğ‘¢\nğ‘š\n(ğ‘–) = âˆ’\n(\nÌ‚\nğ´\nğ‘š\n(ğ‘–)ğ‘™ğ‘œğ‘”Ì‚ğ´\nğ‘š\n(ğ‘–) + (1 âˆ’\nÌ‚\n ğ´\nğ‘š\n(ğ‘–))ğ‘™ğ‘œğ‘”(1 âˆ’\nÌ‚\n ğ´\nğ‘š\n(ğ‘–))\n)\n, (3)\nğ‘ƒ\nğ‘ˆ ğ‘€ \n(ğ‘–) =\nâˆ‘\nğ‘€\nğ‘š=1 \nğ‘¤\nğ‘š\n(ğ‘–)\nÌ‚\nğ´\nğ‘š\n(ğ‘–)\nâˆ‘\nğ‘€\nğ‘š=1 \nğ‘¤\nğ‘š\n(ğ‘–) \n, (4)\nwhere ğ‘¢\nğ‘š\n(ğ‘–) and ğ‘¤\nğ‘š\n(ğ‘–) = ğ‘’\nâˆ’ğ‘¢\nğ‘š\n(ğ‘–) \nare the uncertainty map and weight map\nfor the CAM\nÌ‚\n ğ´\nğ‘š \nat pixel ğ‘–, respectively. ğ‘ƒ\nğ‘ˆ ğ‘€ \nis the raw pseudo-label\nbased on UM-CAM for the target.\n3.2. Refined pseudo-label based on GSE\nThough UM-CAM is better than CAM from a single deep layer\nof the classification network, it is still insufficient to provide accu-\nrate object boundaries that are important for segmentation. Motivated\nby geodesic distance transform in interactive segmentation [33], we\npropose a Geodesic distance-based Seed Expansion (GSE) to refine\nthe raw pseudo-label, which generates seed-derived supervision that\ncontains more detailed contextual information. As shown in Fig. 1(b),\nthe refinement process has two steps: seed-derived supervision map\ngeneration based on Exponential Geodesic Distance (EGD) transform\nand refined pseudo-label based on spatially adaptive fusion.\nPattern Recognition 160 (2025) 111204\n3\n\nJ. Fu et al.\n3.2.1. Seed-derived supervision map generation based on EGD transform\nSeed generation: First, we generate foreground and background\nseeds from ğ‘ƒ\nğ‘ˆ ğ‘€ \n. As ğ‘ƒ\nğ‘ˆ ğ‘€ \nis a soft label, we use a threshold ğ‘¡ to convert\nit into a binary label map, and ğ‘¡ is adaptively determined by minimizing\nthe intra-class variance [34].\nğ‘‰ (ğ‘¡) = \n1\n|ğ›º\nğ‘“ \n|\nâˆ‘\nğ‘–âˆˆğ›º\nğ‘“\n(\nğ‘‹(ğ‘–) âˆ’ ğ‘¢\nğ‘“ \n(ğ‘¡)\n)\n2 \n+ \n1\n|ğ›º\nğ‘\n|\nâˆ‘\nğ‘–âˆˆğ›º\nğ‘\n(\nğ‘‹(ğ‘–) âˆ’ ğ‘¢\nğ‘\n(ğ‘¡)\n)\n2\n, \n(5)\nwhere ğ‘‹(ğ‘–) is the intensity of pixel/voxel ğ‘– in an image ğ‘‹, and ğ›º\nğ‘“ \n=\n{ğ‘–|ğ‘ƒ\nğ‘ˆ ğ‘€ \n(ğ‘–) â‰¥ ğ‘¡} and ğ›º\nğ‘ \n= {ğ‘–|ğ‘ƒ\nğ‘ˆ ğ‘€ \n(ğ‘–) < ğ‘¡} are the foreground and back-\nground regions, respectively. ğ‘¢\nğ‘“ \nand ğ‘¢\nğ‘ \nrepresent the average intensity\nin ğ›º\nğ‘“ \nand ğ›º\nğ‘\n, respectively. Based on the optimal threshold value ğ‘¡\nthat minimizes ğ‘‰ (ğ‘¡), we take a bounding box of the corresponding ğ›º\nğ‘“ \n,\nand the centroid of ğ›º\nğ‘“ \nand corner points of the bounding box are\nused as the foreground seed ğ‘†\nğ‘“ \nand background seeds ğ‘†\nğ‘\n, respectively.\nNote that there are 4 and 8 background seeds for 2D and 3D images,\nrespectively.\nSeed affinity map generation: As geodesic distance can help dis-\ntinguish adjacent pixels/voxels with different appearances and im-\nprove label consistency in homogeneous regions, we utilize EGD trans-\nform [33] to encode the background and foreground seeds, thus gener-\nating seed affinity maps ğ‘ƒ\nğ‘ğ‘  \nand ğ‘ƒ\nğ‘“ ğ‘ \n. The values of ğ‘ƒ\nğ‘ğ‘  \nand ğ‘ƒ\nğ‘“ ğ‘  \nrepresent\nthe similarity between each pixel/voxel and background/foreground\nseeds, which are computed as:\nğ‘ƒ\nğ‘ğ‘ \n(ğ‘–) = ğ‘’\nâˆ’ğ›¼ ğ·\nğ‘ğ‘ \n(ğ‘–)\n, ğ‘ƒ\nğ‘“ ğ‘ \n(ğ‘–) = ğ‘’\nâˆ’ğ›¼ ğ·\nğ‘“ ğ‘ \n(ğ‘–)\n, (6)\nğ·\nğ‘ğ‘ \n(ğ‘–) = min\nğ‘—âˆˆğ‘†\nğ‘\nğ·(ğ‘–, ğ‘— , ğ‘‹), ğ·\nğ‘“ ğ‘ \n(ğ‘–) = min\nğ‘—âˆˆğ‘†\nğ‘“\nğ·(ğ‘–, ğ‘— , ğ‘‹), (7)\nğ·(ğ‘–, ğ‘— , ğ‘‹) = min\nğ‘Ÿâˆˆîˆ¾\nğ‘–,ğ‘— \nâˆ«\n1\n0\nâ€–\nâ€–\nâ–½ğ‘‹(ğ‘Ÿ(ğ‘›)) â‹… ğ‘¢(ğ‘›)\nâ€–\nâ€– \nğ‘‘ ğ‘›, (8)\nwhere ğ·(ğ‘–, ğ‘— , ğ‘‹) is the geodesic distance between pixels/voxels ğ‘– and ğ‘—\nin an image ğ‘‹. îˆ¾\nğ‘–,ğ‘— \nis the set of all possible paths between ğ‘– and ğ‘—. ğ‘Ÿ\nis one feasible path parameterized by ğ‘› âˆˆ [0, 1]. ğ‘¢ \n(\nğ‘›\n) =\n ğ‘Ÿ\nâ€² \n(\nğ‘›\n) âˆ•\n \nâ€–\nâ€–\nğ‘Ÿ\nâ€² \n(\nğ‘›\n)\nâ€–\nâ€–\nis a unit vector that is tangent to the direction of the path. ğ·\nğ‘ğ‘ \n(ğ‘–)\nand ğ·\nğ‘“ ğ‘ \n(ğ‘–) represent the minimal geodesic distance between ğ‘– and\nbackground/foreground seeds.\nSeed-derived supervision map generation: After obtaining the\nseed affinity maps ğ‘ƒ\nğ‘ğ‘  \nand ğ‘ƒ\nğ‘“ ğ‘ \n, we then integrate them into a single\nseed-derived supervision map ğ‘ƒ\nğº \nwhere each element indicating the\nprobability of a pixel/voxel being the foreground according to the\nseed affinity cues. We first initialize the foreground probability of a\npixel/voxel in ğ‘ƒ\nğº \nto 0.5 without prior information from seeds. Then,\nfor regions with higher foreground seed affinity than background seed\naffinity and ğ‘ƒ\nğ‘“ ğ‘  \n> 0.5, we set ğ‘ƒ\nğº \nas ğ‘ƒ\nğ‘“ ğ‘ \n. Similarly, for regions with\nhigher background seed affinity than foreground seed affinity and ğ‘ƒ\nğ‘ğ‘  \n>\n0.5, we set ğ‘ƒ\nğº \nas 1 âˆ’ ğ‘ƒ\nğ‘ğ‘ \n. Thus, the seed-derived supervision map ğ‘ƒ\nğº \nis\ndefined as:\nğ‘ƒ\nğº \n(ğ‘–) =\nâ§\nâª\nâ¨\nâª\nâ©\nğ‘ƒ\nğ‘“ ğ‘ \n(ğ‘–), if ğ‘ƒ\nğ‘“ ğ‘ \n(ğ‘–) > 0.5 & ğ‘ƒ\nğ‘“ ğ‘ \n(ğ‘–) > ğ‘ƒ\nğ‘ğ‘ \n(ğ‘–)\n1 âˆ’ ğ‘ƒ\nğ‘ğ‘ \n(ğ‘–), if ğ‘ƒ\nğ‘ğ‘ \n(ğ‘–) > 0.5 & ğ‘ƒ\nğ‘ğ‘ \n(ğ‘–) > ğ‘ƒ\nğ‘“ ğ‘ \n(ğ‘–)\n0.5, Otherwise\n(9)\n3.2.2. Refined pseudo-label based on spatially adaptive fusion\nNote that ğ‘ƒ\nğº \nand ğ‘ƒ\nğ‘ˆ ğ‘€ \ncan provide complementary supervision\nsignals for training a segmentation model, due to that ğ‘ƒ\nğº \nis relatively\nreliable for pixels/voxels near the seeds, and may not be accurate for\npixels/voxels far from the seeds where ğ‘ƒ\nğ‘ˆ ğ‘€ \ncould be more informative.\nTherefore, spatially adaptive fusion is proposed to refine ğ‘ƒ\nğ‘ˆ ğ‘€ \nusing\nseed-derived supervision map ğ‘ƒ\nğº \nand a spatial weight map ğ‘Š\nğº \n. ğ‘ƒ\nğº \nhas\na higher weight when it is more confident (close to seeds), and ğ‘ƒ\nğ‘ˆ ğ‘€\nhas a higher weight when ğ‘ƒ\nğº \nis less confident (far from seeds). The\nweight map ğ‘Š\nğº \nand refined pseudo-label ğ‘ƒ \nğ‘Ÿ \nare defined as:\nğ‘Š\nğº \n= 2 â‹… |ğ‘ƒ\nğº \nâˆ’ 0.5| (10)\nğ‘ƒ \nğ‘Ÿ \n= (1 âˆ’ ğ‘Š\nğº \n) â‹… ğ‘ƒ\nğ‘ˆ ğ‘€ \n+ ğ‘Š\nğº \nâ‹… ğ‘ƒ\nğº \n, (11)\nNote that ğ‘ƒ \nğ‘Ÿ \nis still a soft label, and its supervision signal is weaker\nthan one-hot hard labels in standard fully supervised learning [\n35]. On\nthe other hand, as the pseudo-label is noisy, taking an ğ‘ğ‘Ÿğ‘” ğ‘šğ‘ğ‘¥ of ğ‘ƒ \nğ‘Ÿ \nto\nobtain one-hot label will magnify the effect of noise. As a trade-off, we\napply a sharpening operation to ğ‘ƒ \nğ‘Ÿ \nto improve the supervision signal\nwhile avoiding highlighting the noise:\nğ‘ƒ \nğ‘  \n= \n(ğ‘ƒ \nğ‘Ÿ\n)\n1âˆ•ğ‘‡\n(ğ‘ƒ \nğ‘Ÿ\n)\n1âˆ•ğ‘‡ \n+ (1 âˆ’ ğ‘ƒ \nğ‘Ÿ\n)\n1âˆ•ğ‘‡ \n, (12)\nwhere ğ‘ƒ \nğ‘  \nis the final refined pseudo-label after sharpening. ğ‘‡ âˆˆ (0, 1] is\na hyper-parameter to adjust the sharpness, and a smaller ğ‘‡ value results\nin sharper pseudo-labels.\n3.3. Noise-robust learning from refined pseudo-labels\nBased on the refined pseudo-label ğ‘ƒ \nğ‘ \n, the training dataset for seg-\nmentation can be represented as îˆ° = \n{\nğ‘‹\nğ‘˜\n, ğ‘ƒ \nğ‘ \nğ‘˜\n}\nğ¾\nğ‘˜=1 \nthat contains images\nand corresponding pseudo-labels, where ğ¾ is the number of training\nimages. Training the segmentation model is a noisy label learning\nproblem due to inevitable noise in the refined pseudo-labels. Motivated\nby consistency regularization in semi-supervised learning [\n36,37], we\npropose a Random-View Consensus (RVC) strategy to mitigate overfit-\nting to label noise and encourage consistent predictions under random\nspatial transforms.\nConsensus-weighted Cross Entropy (CCE) loss: We define a set\nof spatial transformations including random scaling, rotation, flipping,\ncropping, etc. For an image ğ‘‹, we obtain two random transformations\nî‰€\nğ‘ \nand î‰€\nğ‘ \nfrom the transformation set, and apply them to ğ‘‹ in parallel,\nleading to two augmented images ğ‘‹\nğ‘ \nand ğ‘‹\nğ‘ \nthat are sent to the\nsegmentation model to obtain two segmentation probability maps. The\nsegmentation probability maps are inversely transformed to the original\nimage space based on î‰€ \nâˆ’1\nğ‘ \nand î‰€ \nâˆ’1\nğ‘ \nrespectively, and the results are\ndenoted as ğ‘ƒ \nğ‘ \nand ğ‘ƒ \nğ‘\n, respectively. Their average is denoted as\nÌ„\n ğ‘ƒ . For\nregions where\nÌ„\n ğ‘ƒ is not consistent with ğ‘ƒ \nğ‘ \n, we treat the pixels/voxels\nas unreliable ones and reduce the supervision from the corresponding\npseudo-labels based on a CCE loss:\nğ¿\nğ‘ ğ‘ ğ‘’ \n= \n1\nğ‘\nâˆ‘\nğ‘–\nğ‘¤(ğ‘–) â‹…\n(\nğ¿\nğ‘ ğ‘’\n(\nÌ„\nğ‘ƒ (ğ‘–), ğ‘ƒ \nğ‘ \n(ğ‘–)\n)\n)\n, (13)\nğ‘¤(ğ‘–) = 1 âˆ’ \n(\nÌ„\nğ‘ƒ (ğ‘–) âˆ’ ğ‘ƒ \nğ‘ \n(ğ‘–)\n)\n2\n, (14)\nwhere ğ¿\nğ‘ ğ‘’ \nis the Cross Entropy (CE) loss for each pixel/voxel, and ğ‘ is\nthe pixel/voxel number.\nConsistency regularization: In addition to CCE loss, we introduce\na consistency regularization for consistent predictions under different\nspatial transforms:\nğ¿\nğ‘ ğ‘œğ‘› \n= \n1\n2\n(\nğ¾ ğ¿(ğ‘ƒ \nğ‘ \nâˆ¥ ğ‘ƒ \nğ‘\n) + ğ¾ ğ¿(ğ‘ƒ \nğ‘ \nâˆ¥ ğ‘ƒ \nğ‘\n)\n)\n, (15)\nwhere ğ¾ ğ¿() is the Kullbackâ€“Leibler divergence function.\nOverall Loss: Finally, we combine ğ¿\nğ‘ ğ‘ ğ‘’ \nand ğ¿\nğ‘ ğ‘œğ‘› \nto obtain the total\nsegmentation loss for learning from noisy pseudo-labels:\nğ¿\nğ‘ ğ‘’ğ‘” \n= ğ¿\nğ‘ ğ‘ ğ‘’ \n+ ğœ†ğ¿\nğ‘ ğ‘œğ‘›\n. (16)\nwhere ğœ† is a weight to balance the two loss terms.\n4. Experiments and results\n4.1. Dataset\nWe validated the proposed method with a fetal brain segmenta-\ntion task and a brain tumor segmentation task from MRI volumes,\nrespectively. Note that for both applications, slice-level labels were\nprovided for training images, i.e., a binary label indicating the presence\nor absence of the target was given for each slice. For validation and\ntesting images, dense annotations (voxel-level labels) were provided by\nexperts as ground truth for evaluating the segmentation performance.\nPattern Recognition 160 (2025) 111204\n4\n\nJ. Fu et al.\n4.1.1. Fetal brain (FB) dataset\nFor fetal brain segmentation, 115 T2-weighed Magnetic Resonance\nImages (MRIs) were collected from pregnant women in the second\ntrimester at Sichuan Provincial Peopleâ€™s Hospital with half-Fourier\nacquisition single-shot turbo spin-echo (HASTE). The inclusion criteria\nwere singleton pregnancy and fetal development coinciding with gesta-\ntional age. Patients were excluded for severe artifacts on MRI images.\nWritten informed consent was obtained from each of the patients, and\nthe use of data was approved by the Research Ethics Committee of the\nhospital.\nEach MRI volume consists of 50â€“56 slices with about 520 Ã— 640 pix-\nels. The in-plane resolution ranged from 0.55 mm Ã— 0.55 mm to 0.72 mm Ã—\n0.72 mm, while the slice thickness was between 5.00â€“7.15 mm. Each\nslice was resampled to a uniform pixel size of 1 mm Ã— 1 mm. Since\nthe in-plane resolution was significantly larger than the through-plane\nresolution, a 2D network was used for slice-by-slice segmentation. At\ninference time, we stacked the slice-level predictions into a volume and\nevaluated the performance in 3D space. We evaluated the proposed\nmethod on the FB dataset via five-fold cross-validation, and the average\nresults are reported in the following experiments.\n4.1.2. Brain tumor segmentation (BraTS) dataset\nFor brain tumor segmentation, we used the Fluid Attenuated Inver-\nsion Recovery (FLAIR) MRI scans from BraTS 2020 Dataset [\n21â€“23]. All\nimages were resampled to a uniform resolution of 1.0 mm\n3 \nwith a size\nof 240 Ã— 240 Ã— 155. We aimed to segment the whole tumor including\nedema and tumor core from FLAIR images. The dataset was randomly\nsplit into 258, 37 and 74 volumes for training, validation and testing,\nrespectively.\n4.2. Implementation details\nOur framework was implemented using PyTorch and PyMIC\n1 \n[38]\non a Ubuntu desktop with an NVIDIA GeForce RTX 3090 GPU. The\nCAM was generated following the code for AI explainability,\n2 \nand the\ngeodesic distance was computed using open source code.\n3 \n[39] We\nused data augmentation strategies during training, including gamma\ncorrection, random rotation, random flipping, and random cropping.\nFor the raw pseudo-label generation stage, we used the modified\nVGG-16 [14] as the classification network, which was trained with 200\nepochs using CE loss and Stochastic Gradient Descent (SGD) optimizer,\nwith batch size of 32, momentum of 0.99, and weight decay of 5 Ã— 10\nâˆ’4\n.\nThe learning rate was initialized to 1 Ã— 10\nâˆ’3\n.\nFor the pseudo-label refinement stage, the GSE-based refinement\nwas conducted on 2D slices and 3D volumes for fetal brain and brain\ntumor, respectively. Based on the best results on the validation set, the\nhyper-parameter ğ‘€ was set as 3 for both fetal brain and brain tumor\nsegmentation tasks, and ğ›¼ was set as 0.2 and 0.4 for the two tasks,\nrespectively. ğ‘‡ was set as 0.5 empirically to obtain more confident\npseudo-labels.\nTo train the final segmentation model, we used 2D and 3D UNet [40]\nfor fetal brain and brain tumor, respectively. The learning rate was set\nas 0.01, and the SGD optimizer was used for training with 200 epochs,\nbatch size of 12, momentum of 0.9, and weight decay of 5 Ã— 10\nâˆ’4\n. ğœ†\nwas set as 3.0 and 0.1 for fetal brain and brain tumor segmentation,\nrespectively.\nFor evaluation, we used Dice Similarity Coefficient (DSC) and 95%\nHausdorff Distance (HD\n95\n) to quantify the accuracy of pseudo masks\nand the segmentation results in 3D space.\n1 \nhttps://github.com/HiLab-git/PyMIC\n2 \nhttps://github.com/jacobgil/pytorch-grad-cam\n3 \nhttps://github.com/masadcv/FastGeodis\nTable 1\nAblation study of UM-CAM and GSE for generating pseudo-labels.\nMethod FB dataset BraTS dataset\nDSC (%) HD\n95 \n(voxels) DSC (%) HD\n95 \n(voxels)\nGrad-CAM 79.45 Â± 4.54 9.60 Â± 5.10 66.50 Â± 14.18 15.62 Â± 8.87\nAverage-CAM 83.78 Â± 3.92 6.78 Â± 7.51 70.75 Â± 13.52 12.52 Â± 9.43\nUM-CAM 84.13 Â± 3.79 6.64 Â± 7.32 71.56 Â± 13.45 13.14 Â± 10.50\nUM-CAM + CRF 84.82 Â± 4.38 8.24 Â± 10.67 76.93 Â± 16.55 12.04 Â± 11.30\nUM-CAM + GSE 87.44 Â± 2.78 4.60 Â± 6.65 78.33 Â± 11.61 11.39 Â± 12.41\nTable 2\nQuantitative evaluation of refined pseudo-labels with different ğ›¼ values.\nğ›¼ FB dataset BraTS dataset\nDSC (%) HD\n95 \n(voxels) DSC (%) HD\n95 \n(voxels)\n0.1 87.06 Â± 2.71 4.68 Â± 6.35 70.84 Â± 21.82 15.41 Â± 15.80\n0.2 87.44 Â± 2.78 4.60 Â± 6.65 74.36 Â± 18.71 14.17 Â± 15.77\n0.3 87.42 Â± 3.41 4.80 Â± 7.24 77.68 Â± 14.50 12.39 Â± 15.60\n0.4 86.58 Â± 3.65 5.24 Â± 7.32 78.33 Â± 11.61 11.39 Â± 12.41\n0.5 85.76 Â± 3.75 5.88 Â± 7.32 77.59 Â± 10.74 11.22 Â± 11.12\n0.7 84.93 Â± 3.80 6.45 Â± 7.33 75.66 Â± 11.42 11.08 Â± 9.64\n1.0 84.63 Â± 3.82 6.59 Â± 7.33 73.89 Â± 11.75 11.41 Â± 9.58\n4.3. Pseudo-labels from UM-CAM with GSE refinement\n4.3.1. Ablation study for pseudo-label generation\nTo evaluate the effectiveness of UM-CAM and GSE-based refine-\nment, we compared several pseudo-label generation methods: (1) Grad-\nCAM (baseline): only using CAMs from the last layer of the clas-\nsification network by using Grad-CAM [\n13]; (2) Average-CAM: fus-\ning multi-resolution CAMs via averaging; (3) UM-CAM: fusing multi-\nresolution CAMs via uncertainty weighting; (4) UM-CAM+CRF: binary\npseudo-labels generated by refining the raw pseudo-labels using CRF\npost-processing. (5) UM-CAM+GSE: refining the raw pseudo-labels via\nseed-derived supervision based on GSE. For quantitative evaluation of\nthese CAMs, they are converted to binary masks using the optimal\nthreshold found by grid search on the validation set, and \nTable 1\nlists the quantitative results on the two datasets. Grad-CAM [13] ob-\ntained an average DSC of 79.45% and 66.50% on the FB and BraTS\ndatasets, respectively. Compared with Grad-CAM [\n13], our method\n(UM-CAM+GSE) improves the average DSC by 7.99 and 11.83 per-\ncentage points on the two datasets, respectively. UM-CAM obtained an\naverage DSC of 84.13% and 71.56% on the two datasets, respectively,\nand UM-CAM+GSE further improved it to 87.44% and 78.33% for the\nfetal brain and brain tumor, respectively. Additionally, it can be ob-\nserved that Average-CAM has a lower performance than UM-CAM, and\nCRF refinement (UM-CAM+CRF) only performs slight improvement in\nDSC and poor HD\n95 \non the FB dataset. This highlights the effectiveness\nof our proposed method for pseudo-label generation and refinement on\nthe boundaries.\nFig. 2 shows a visual comparison between these methods. It can\nbe observed that the Grad-CAM performs poorly on the boundaries of\nthe target. Compared to the Grad-CAM and Average-CAM, there are\nfewer false-positive activation regions in UM-CAM. Additionally, the\npseudo-labels refined by the CRF achieved higher quality with fewer\nmis-activation regions and more detailed boundaries, and the proposed\nGSE further improves the boundaries of the targets, which are closer to\nthe ground truth.\n4.3.2. Effects of layer number m for UM-CAM\nUM-CAM is a weighted combination of multi-resolution CAMs from\nthe last ğ‘€ convolutional blocks as Eq. (\n4). We conducted experi-\nments to investigate the effect of block number ğ‘€ on generating raw\npseudo-labels. Fig. 3 shows the quantitative results of UM-CAM on the\nvalidation set with different ğ‘€ values, respectively. It can be seen\nthat when ğ‘€ increases from 1 to 3, the accuracy of raw pseudo-\nlabels improves. However, a too large ğ‘€ would lead to decreased\nPattern Recognition 160 (2025) 111204\n5\n\nJ. Fu et al.\nFig. 2. Visual comparison between different methods for obtaining pseudo segmentation labels. The color bar encodes foreground probability. Note that the output of CRF is a\nbinary map.\nFig. 3. Quantitative comparison of different ğ‘€ values for UM-CAM generation. Where\nğ‘€ means the number of convolutional blocks towards the top of the classification\nnetwork for fusion.\nperformance. According to Fig. 3, we set ğ‘€ as 3 for fetal brain and\nbrain tumor segmentation in the following experiments.\n4.3.3. Effects of hyper-parameter ğ›¼ on refined pseudo-labels\nWe then investigate the effect of hyper-parameter ğ›¼ on the refined\npseudo-label ğ‘ƒ \nğ‘ \n. Table 2 lists the results under different ğ›¼ values. It\ncan be seen that the quality of pseudo-labels first increases and then\ndecreases with the increase of ğ›¼, which is because that a small ğ›¼ will\nresult in a broader region of influence for seeds, and a too large ğ›¼ will\nrestrict the region of influence. Specifically, when ğ›¼ is set as 0.2 and\n0.4, the highest average DSC is obtained for the FB and BraTS datasets,\nrespectively. Hence, we set ğ›¼ as 0.2 and 0.4 for the two datasets in the\nexperiments, respectively.\n4.4. Segmentation results of learning from refined pseudo-labels\n4.4.1. Ablation study on loss terms\nTo validate the effectiveness of the proposed RVC method for train-\ning the segmentation model from ğ‘ƒ \nğ‘ \n, we conducted an ablation study\non the loss terms in Eq. (16). Table 3 shows quantitative results on the\nvalidation set. It can be seen that using the standard CE loss for training\nobtained an average DSC of 89.93% and 83.54% for FB and BraTS\ndatasets, respectively. Using ğ¿\nğ‘ ğ‘ ğ‘’ \nimproved it to 90.34% and 84.83%\non the two datasets, respectively. Combining ğ¿\nğ‘ ğ‘ ğ‘’ \nwith ğ¿\nğ‘ ğ‘œğ‘› \nfurther\nimproved it to 90.87% and 85.44% for FB and BraTS respectively, and\nit outperformed the other variants.\nTable 3\nAblation study of loss terms for training the segmentation model from pseudo-labels.\nLoss FB dataset BraTS dataset\nDSC (%) HD\n95 \n(voxels) DSC (%) HD\n95 \n(voxels)\nğ¿\nğ‘ ğ‘’ \n89.93 Â± 2.93 3.11 Â± 3.75 83.54 Â± 7.98 7.98 Â± 4.83\nğ¿\nğ‘ ğ‘ ğ‘’ \n90.34 Â± 3.89 4.30 Â± 9.74 84.83 Â± 8.42 7.84 Â± 7.01\nğ¿\nğ‘ ğ‘’ \n+ ğ¿\nğ‘ ğ‘œğ‘› \n90.58 Â± 3.49 4.06 Â± 5.43 84.49 Â± 8.13 8.30 Â± 7.24\nğ¿\nğ‘ ğ‘ ğ‘’ \n+ ğ¿\nğ‘ ğ‘œğ‘› \n90.87 Â± 3.94 2.15 Â± 0.64 85.44 Â± 7.65 7.45 Â± 6.75\nTable 4\nQuantitative comparison with existing noisy label learning methods.\nMethod FB dataset BraTS dataset\nDSC (%) HD\n95 \n(voxels) DSC (%) HD\n95 \n(voxels)\nBaseline 88.02 Â± 5.19 6.32 Â± 10.87 83.06 Â± 12.53 8.48 Â± 10.81\nGCE [27] 89.32 Â± 5.23 3.20 Â± 1.92 83.23 Â± 11.88 8.58 Â± 10.60\nNR-Dice [28] 89.10 Â± 5.60 4.50 Â± 6.24 82.74 Â± 11.43 8.26 Â± 9.98\nCo-teaching [29] 88.86 Â± 5.46 4.71 Â± 8.87 84.37 Â± 11.03 8.03 Â± 10.39\nTri-Net [30] 88.71 Â± 6.04 4.13 Â± 6.52 84.31 Â± 11.47 8.61 Â± 11.11\nURN [19] 89.71 Â± 3.95 4.05 Â± 7.16 84.49 Â± 9.96 7.91 Â± 9.82\nOurs 90.83 Â± 3.40 2.27 Â± 0.81 85.54 Â± 9.12 7.21 Â± 8.24\n4.4.2. Comparison with existing noisy label learning methods\nOur proposed RVC strategy was compared with five state-of-the-art\nnoisy label learning methods: (1) GCE [27] and (2) NR-Dice [28] that\nare noise-robust loss functions; (3) Co-teaching [29] that leverages two\nnetworks to supervise each other by selecting samples with low loss\nvalues; (4) Tri-Net [30] with three networks where each two networks\nselect reliable samples to supervise the third one; (5) URN [19] that\nscales the prediction maps to estimate uncertainty for weighting the\nsegmentation loss. These methods were also compared with Baseline,\nwhich means simply using the CE loss for learning from the pseudo-\nlabels. Note that all these methods used the same network structure\nand the same set of pseudo-labels based on ğ‘ƒ \nğ‘  \nfor a fair comparison.\nTable 4 presents the quantitative evaluation results of these methods\non the two datasets. The baseline method of using standard CE loss\nfor training obtained an average DSC of 88.02% and 83.06% for FB\nand BraTS datasets, respectively. Compared to the baseline, GCE [27]\nand NR-Dice [28] improved by one percentage point on the FB dataset,\nbut only obtained a slight improvement on the BraTS dataset. Co-\nteaching [29] and Tri-Net [30] improved the performance by about one\npercentage point on both FB and BraTS datasets. Among these existing\nnoisy label learning methods, URN [19] achieved the best results on FB\nand BraTS datasets. Compared with URN [19], our proposed method\nPattern Recognition 160 (2025) 111204\n6\n\nJ. Fu et al.\nFig. 4. Visual comparison of our proposed RVC and state-of-the-art noisy label learning methods. The second column shows the pseudo-labels generated from GSE refinement.\nThe red mask and the green contour indicate the pseudo-label/prediction and the boundary of ground truth, respectively.\nimproved the average DSC from 89.71% to 90.83% and 84.49% to\n85.54% on the FB and BraTS datasets, respectively.\nFig. 4 shows a visual comparison of these noisy label learning\nmethods on FB and BraTS datasets. The proposed RVC method achieved\nslightly better performance for the case with clear boundaries, as seen\nin the first row. For more complex cases with ambiguous boundaries\nand more noisy pseudo-labels, as shown in the second and last two\nrows, existing methods exhibit obvious mis-segmentations in the fetal\nbrain and brain tumor regions. In contrast, our method shows improved\nrobustness and accuracy in distinguishing the target from the back-\nground, demonstrating its effectiveness in handling noisy label learning\nfor both 2D and 3D segmentation.\n4.5. Evaluation of RVC on simulated noisy datasets\nTo further assess the robustness of the proposed RVC against noise,\nwe conducted experiments on the FB dataset with artificially intro-\nduced noises, including a random operation of dilation, erosion, shift,\nand edge distortion by ğ‘Ÿ pixels, where hyper-parameter ğ‘Ÿ controls the\nnoise intensity.\nFirst, to explore the impact of dataset dimensionality, we set ğ‘Ÿ = 30\nand used 10%, 20%, 50%, and 100% of the original dataset for training,\nrespectively. The results in Fig. 5(a) show that at different scales of\nthe training set, our method consistently outperformed the baseline and\nURN, and the superiority is more obvious when the training set is small.\nThen, to investigate the effect of the noise intensity, we used the\nentire training set and set ğ‘Ÿ to 10, 30, 50, and 100, respectively. The\nresults in Fig. 5(b) show that our method is more robust than the\nbaseline and URN with large noise intensity.\nFinally, to analyze the impact of non-uniform noise distributions,\nwe divided the training data into three groups based on Gestational\nAge (GA), and Fig. 5(c) shows the results for uniform and non-uniform\nnoise distributions. In the uniform setting (Uni), a fixed value of ğ‘Ÿ =\n30 was used. In non-uniform settings, Non-Uni1 denotes ğ‘Ÿ = 10, 30,\nand 50 for GA < 30, 30 â‰¤ GA â‰¤ 33, and GA > 33, respectively,\nand Non-Uni2 represents ğ‘Ÿ = 50, 30, and 10 for the three groups,\nrespectively. Compared to the uniform distribution, the performance\nof all methods under non-uniform distributions improves, due to the\nnon-uniform settings comprising more high-quality pseudo-labels. Our\nmethod achieves superior segmentation results under both uniform and\nnon-uniform noise distributions than the baseline and URN.\n4.6. Comparison with existing weakly-supervised segmentation methods\nWe compared our method with six state-of-the-art weakly-\nsupervised segmentation methods, including two one-stage methods\n(Grad-CAM [13], Score-CAM [24] and Ablation-CAM [25]) and three\ntwo-stage methods, where the former directly uses CAMs obtained by a\nclassification model as the segmentation results, and the latter trains a\nsegmentation model from pseudo-labels based on improved or refined\nCAMs. The two-stage methods for comparison are: (1) Activation\nModulation and Recalibration (AMR) [17] that incorporates a spotlight\nbranch and a compensation branch to dig out more complete object\nregions for better pseudo-labels; (2) Discriminative Region Suppression\n(DRS) [14] that spreads the attention to adjacent non-discriminative\nregions to generate more detailed localization maps; (3) URN [19] that\nuses uncertainty of the prediction maps to weight the segmentation\nloss when learning from pseudo-labels generated by the classification\nnetwork; (4) Ours (previous) [20] that combines UM-CAM and seed-\nderived supervision to supervise the segmentation network. Note that\nAMR [17], DRS [14], and Ours (previous) [20] do not deal with noise\nin the training process of the segmentation model. These methods\nwere also compared with FullSup which means learning from dense\nannotations based on CE loss.\nTable 5 presents the quantitative results of these methods. It can\nbe seen that the performance of the one-stage methods, i.e., Score-\nCAM [24] and Ablation-CAM [25], was close to that of the Grad-\nCAM [13]. The two-stage methods performed better than the one-\nstage methods. URN [19] and DRS [14] achieved the highest perfor-\nmance among the existing methods with average DSC being 86.56%\nand 73.37% for fetal brain and brain tumor, respectively. Compared\nwith existing weakly-supervised segmentation methods, our previous\nwork [20] significantly improved the average DSC by 2.91 and 7.57\npercentage points on the FB and BraTS datasets, respectively. The\nproposed method in this paper further improved the average DSC to\n90.83% and 85.54% on the FB and BraTS datasets, respectively. Our\nmethod achieved an average HD\n95 \nof 2.27 and 7.21 voxels on the FB\nand BraTS datasets, respectively, which was also significantly better\nthan those of the other weakly-supervised segmentation methods and\nour previous work. Compared with FullSup which achieved an average\nDSC of 95.57% and 89.17% on two datasets, our method narrows the\ngap between fully supervised learning and weakly supervised learning.\nFig. 6 shows a visual comparison between these methods on the\nFB and BraTS datasets. It can be observed that Score-CAM [24] and\nPattern Recognition 160 (2025) 111204\n7\n\nJ. Fu et al.\nFig. 5. Quantitative comparisons of the baseline, URN, and our proposed method on simulated noisy datasets.\nTable 5\nComparison with existing weakly-supervised segmentation methods.\nMethod FB dataset BraTS dataset\nDSC (%) HD\n95 \n(voxels) DSC (%) HD\n95 \n(voxels)\nFullSup 95.57 Â± 7.34 1.09 Â± 0.40 89.17 Â± 6.87 5.39 Â± 7.59\nGrad-CAM [13] 78.93 Â± 6.55 10.27 Â± 7.05 66.26 Â± 12.84 16.34 Â± 11.24\nScore-CAM [24] 78.93 Â± 6.55 10.27 Â± 7.05 65.95 Â± 12.10 15.17 Â± 10.07\nAblation-CAM [25] 78.95 Â± 6.55 10.27 Â± 7.05 66.26 Â± 12.84 16.34 Â± 11.24\nAMR [17] 80.13 Â± 7.00 9.18 Â± 5.36 68.25 Â± 18.73 19.08 Â± 21.14\nDRS [14] 80.71 Â± 6.68 12.03 Â± 9.13 73.37 Â± 10.08 12.53 Â± 12.82\nURN [19] 86.56 Â± 5.72 4.64 Â± 5.93 70.57 Â± 12.54 12.30 Â± 8.87\nOurs (previous) [20] 89.47 Â± 4.47 4.58 Â± 6.79 80.94 Â± 17.90* 10.78 Â± 16.29\nOurs 90.83 Â± 3.40\na \n2.27 Â± 0.81\na \n85.54 Â± 9.12\na \n7.21 Â± 8.24\na\na \nDenotes ğ‘-value < 0.05 when comparing with the second place method.\nFig. 6. Visual comparison of our method and other weakly-supervised segmentation methods. The red mask and the green contour indicate the prediction and the boundary of\nground truth, respectively.\nAblation-CAM [25] only obtained rough segmentation results.\nAMR [17], DRS [14], and URN [19] performed better than them with\nfewer over-segmentation and miss-segmentation. However, their results\nare inaccurate in the boundary regions. Our method obtained more\naccurate segmentation results that are closer to those of fully supervised\nlearning.\n5. Discussion and conclusion\nOur proposed method has several advantages over existing weakly-\nsupervised segmentation methods learning from image-level labels.\nFirst, unlike some existing methods [14,19] using CAMs from a single\ndeep layer, our UM-CAM adaptively fuses multi-resolution CAMs from\nmulti-depth convolutional layers via entropy weighting, which leads\nto better localization of the target with a higher resolution. Second,\ncompared to existing methods exploring boundaries [11,18], our GSE\nmethod leverages the seeds from UM-CAM for refinement, which is ef-\nficient and provides more boundary cues without extra annotation and\ntraining. In addition, the RVC strategy makes the segmentation model\nless affected by noisy pseudo-labels for high performance. Our method\nis feasible to be applied to both 2D and 3D images. Table 5 shows\nour method significantly outperformed existing weakly-supervised seg-\nmentation methods on fetal brain and brain tumor segmentation. Fur-\nthermore, the core principles of our method indicate its potential for\ngeneralization to other medical imaging modalities and applications.\nThis work still has some limitations. First, our method is pro-\nposed for binary segmentation tasks, and it is of interest to investigate\nPattern Recognition 160 (2025) 111204\n8\n\nJ. Fu et al.\nits application to multi-class segmentation in the future. Second, the\nclassification and segmentation networks are trained separately and\nhave different structures. A potential solution for improving the train-\ning efficiency could be using the same backbone and initializing the\nsegmentation network with the trained weights in the classification net-\nwork. Third, our UM-CAM was validated on a CNN using VGG-16. With\nthe rapid development of vision Transformers, applying our method to\nTransformers will be investigated in the future. Moreover, although the\nfinal segmentation model performs the same model size and inference\ntime as the URN method, our method involves a more complex training\nprocess. Our method requires 5 h for training the classification network\nand refining pseudo-labels, and 2 h for segmentation network training\nrespectively, compared with 5.5 h by URN.\nIn conclusion, we propose a novel two-stage method for medical\nimage segmentation using image-level labels. A UM-CAM is introduced\nto improve the quality of raw pseudo-labels by adaptive fusion of\nmulti-resolution activation maps. Geodesic distance-based seed expan-\nsion is proposed to refine the raw pseudo-labels for higher accuracy\nof the boundary. We also design a random view consensus strat-\negy to mitigate the effects of noise in pseudo-labels for training the\nsegmentation model. Our method was evaluated on 2D fetal brain\nsegmentation and 3D brain tumor segmentation tasks, and experi-\nmental results demonstrated the effectiveness of our proposed method\nin pseudo-label generation and noisy label learning, which suggested\nthe potential of our approach in achieving accurate medical image\nsegmentation with low annotation costs.\nCRediT authorship contribution statement\nJia Fu: Writing â€“ original draft, Visualization, Validation, Methodol-\nogy, Investigation, Formal analysis, Conceptualization. Guotai Wang:\nWriting â€“ review & editing, Supervision, Software, Resources, Project\nadministration, Funding acquisition, Conceptualization. Tao Lu: Re-\nsources, Data curation. Qiang Yue: Investigation, Data curation. Tom\nVercauteren: Writing â€“ review & editing, Supervision, Funding acqui-\nsition. SÃ©bastien Ourselin: Writing â€“ review & editing, Supervision,\nFunding acquisition. Shaoting Zhang: Writing â€“ review & editing,\nSupervision, Software, Resources.\nDeclaration of competing interest\nThe authors declare that they have no known competing finan-\ncial interests or personal relationships that could have appeared to\ninfluence the work reported in this paper.\nAcknowledgments\nThis work was supported in part by the National Natural Science\nFoundation of China under Grant 62271115 and Grant 82271961,\nin part by the Fundamental Research Funds for the Central Univer-\nsities, China under Grant ZYGX2022YGRH019, and in part by the\nWellcome/Engineering and Physical Sciences Research Council (EP-\nSRC) under Grant WT203148/Z/16/Z and Grant NS/A000049/1. Tom\nVercauteren is also supported by a Medtronic/Royal Academy of Engi-\nneering Research Chair (RCSRF1819/7/34).\nData availability\nData will be made available on request.\nReferences\n[1] G. Litjens, T. Kooi, B.E. Bejnordi, A.A.A. Setio, F. Ciompi, M. Ghafoorian, J.A.\nVan Der Laak, B. Van Ginneken, C.I. SÃ¡nchez, A survey on deep learning in\nmedical image analysis, Med. Image Anal. 42 (2017) 60â€“88.\n[2] K.M. Sunnetci, E. Kaba, F.B. Celiker, A. Alkan, Deep network-based com-\nprehensive parotid gland tumor detection, Academic Radiol. 31 (1) (2024)\n157â€“167.\n[3] K.M. Sunnetci, E. Kaba, F. Beyazal Ã‡eliker, A. Alkan, Comparative parotid\ngland segmentation by using ResNet-18 and MobileNetV2 based DeepLab v3+\narchitectures from magnetic resonance images, Concurr. Comput.: Pract. Exper.\n35 (1) (2023) e7405.\n[4] L. Sun, W. Shao, Q. Zhu, M. Wang, G. Li, D. Zhang, Multi-scale multi-\nhierarchy attention convolutional neural network for fetal brain extraction,\nPattern Recognit. 133 (2023) 109029.\n[5] R. Azad, A. Kazerouni, M. Heidari, E.K. Aghdam, A. Molaei, Y. Jia, A. Jose, R.\nRoy, D. Merhof, Advances in medical image analysis with vision transformers: a\ncomprehensive review, Med. Image Anal. (2023) 103000.\n[6] C.-P. Manoila, A. Ciurea, F. Albu, SmartMRI framework for segmentation of\nMR images using multiple deep learning methods, in: 2022 E-Health and\nBioengineering Conference, EHB, IEEE, 2022, pp. 01â€“04.\n[7] F. Lu, Z. Zhang, T. Liu, C. Tang, H. Bai, G. Zhai, J. Chen, X. Wu, A weakly\nsupervised inpainting-based learning method for lung CT image segmentation,\nPattern Recognit. 144 (2023) 109861.\n[8] H. Oliveira, P.H. Gama, I. Bloch, R.M. Cesar Jr., Meta-learners for few-\nshot weakly-supervised medical image segmentation, Pattern Recognit. (2024)\n110471.\n[9] L. Alzubaidi, J. Bai, A. Al-Sabaawi, J. SantamarÃ­a, A.S. Albahri, B.S.N. Al-\ndabbagh, M.A. Fadhel, M. Manoufali, J. Zhang, A.H. Al-Timemy, et al., A\nsurvey on deep learning tools dealing with data scarcity: definitions, challenges,\nsolutions, tips, and applications, J. Big Data 10 (1) (2023) 46.\n[10] W. Shen, Z. Peng, X. Wang, H. Wang, J. Cen, D. Jiang, L. Xie, X. Yang, Q. Tian,\nA survey on label-efficient deep image segmentation: Bridging the gap between\nweak supervision and dense prediction, IEEE Trans. Pattern Anal. Mach. Intell.\n(2023) 1â€“20.\n[11] J. Ahn, S. Kwak, Learning pixel-level semantic affinity with image-level su-\npervision for weakly supervised semantic segmentation, in: CVPR, 2018, pp.\n4981â€“4990.\n[12] B. Zhou, A. Khosla, A. Lapedriza, A. Oliva, A. Torralba, Learning deep features\nfor discriminative localization, in: CVPR, 2016, pp. 2921â€“2929.\n[13] R.R. Selvaraju, M. Cogswell, A. Das, R. Vedantam, D. Parikh, D. Batra, Grad-\nCAM: Visual explanations from deep networks via gradient-based localization,\nin: ICCV, 2017, pp. 618â€“626.\n[14] B. Kim, S. Han, J. Kim, Discriminative region suppression for weakly-supervised\nsemantic segmentation, in: AAAI, vol. 35, (no. 2) 2021, pp. 1754â€“1761.\n[15] B. Zhang, J. Xiao, Y. Wei, K. Huang, S. Luo, Y. Zhao, End-to-end weakly\nsupervised semantic segmentation with reliable region mining, Pattern Recognit.\n128 (2022) 108663.\n[16] A. Kolesnikov, C.H. Lampert, Seed, expand and constrain: Three principles for\nweakly-supervised image segmentation, in: ECCV, 2016, pp. 695â€“711.\n[17] J. Qin, J. Wu, X. Xiao, L. Li, X. Wang, Activation modulation and recalibration\nscheme for weakly supervised semantic segmentation, in: AAAI, vol. 36, (no. 2)\n2022, pp. 2117â€“2125.\n[18] L. Chen, W. Wu, C. Fu, X. Han, Y. Zhang, Weakly supervised semantic\nsegmentation with boundary exploration, in: ECCV, 2020, pp. 347â€“362.\n[19] Y. Li, Y. Duan, Z. Kuang, Y. Chen, W. Zhang, X. Li, Uncertainty estimation via\nresponse scaling for pseudo-mask noise mitigation in weakly-supervised semantic\nsegmentation, in: AAAI, vol. 36, (no. 2) 2022, pp. 1447â€“1455.\n[20] J. Fu, T. Lu, S. Zhang, G. Wang, UM-CAM: Uncertainty-weighted multi-resolution\nclass activation maps for weakly-supervised fetal brain segmentation, in: MICCAI,\n2023, pp. 315â€“324.\n[21] B.H. Menze, A. Jakab, S. Bauer, J. Kalpathy-Cramer, K. Farahani, J. Kirby, Y.\nBurren, N. Porz, J. Slotboom, R. Wiest, et al., The multimodal brain tumor image\nsegmentation benchmark (BRATS), IEEE Trans. Med. Imaging 34 (10) (2014)\n1993â€“2024.\n[22] S. Bakas, H. Akbari, A. Sotiras, M. Bilello, M. Rozycki, J.S. Kirby, J.B. Freymann,\nK. Farahani, C. Davatzikos, Advancing the cancer genome atlas glioma MRI\ncollections with expert segmentation labels and radiomic features, Sci. Data 4\n(1) (2017) 1â€“13.\n[23] S. Bakas, M. Reyes, A. Jakab, S. Bauer, M. Rempfler, A. Crimi, R.T. Shinohara,\nC. Berger, S.M. Ha, M. Rozycki, et al., Identifying the best machine learning\nalgorithms for brain tumor segmentation, progression assessment, and overall\nsurvival prediction in the BRATS challenge, 2018, arXiv preprint \narXiv:1811.\n02629\n.\n[24] H. Wang, Z. Wang, M. Du, F. Yang, Z. Zhang, S. Ding, P. Mardziel, X. Hu, Score-\nCAM: Score-weighted visual explanations for convolutional neural networks, in:\nCVPR Workshops, 2020, pp. 24â€“25.\nPattern Recognition 160 (2025) 111204\n9\n\nJ. Fu et al.\n[25] H.G. Ramaswamy, et al., Ablation-CAM: Visual explanations for deep con-\nvolutional network via gradient-free localization, in: WACV, 2020, pp.\n983â€“991.\n[26] L. Zhong, G. Wang, X. Liao, S. Zhang, HAMIL: High-resolution activation maps\nand interleaved learning for weakly supervised segmentation of histopathological\nimages, IEEE Trans. Med. Imaging 42 (10) (2023) 2912â€“2923.\n[27] Z. Zhang, M. Sabuncu, Generalized cross entropy loss for training deep neural\nnetworks with noisy labels, in: NeurIPS, vol. 31, 2018.\n[28] G. Wang, X. Liu, C. Li, Z. Xu, J. Ruan, H. Zhu, T. Meng, K. Li, N. Huang, S. Zhang,\nA noise-robust framework for automatic segmentation of COVID-19 pneumonia\nlesions from CT images, IEEE Trans. Med. Imaging 39 (8) (2020) 2653â€“2663.\n[29] B. Han, Q. Yao, X. Yu, G. Niu, M. Xu, W. Hu, I. Tsang, M. Sugiyama, Co-teaching:\nRobust training of deep neural networks with extremely noisy labels, in: NeurIPS,\nvol. 31, 2018.\n[30] T. Zhang, L. Yu, N. Hu, S. Lv, S. Gu, Robust medical image segmentation from\nnon-expert annotations with Tri-network, in: MICCAI, 2020, pp. 249â€“258.\n[31] K. Simonyan, A. Zisserman, Very deep convolutional networks for large-scale\nimage recognition, 2014, arXiv preprint arXiv:1409.1556.\n[32] L.C. GarcÃ­a-Peraza-Herrera, M. Everson, L. Lovat, H.-P. Wang, W.L. Wang,\nR. Haidry, D. Stoyanov, S. Ourselin, T. Vercauteren, Intrapapillary capillary\nloop classification in magnification endoscopy: open dataset and baseline\nmethodology, Int. J. Comput. Assist. Radiol. Surg. 15 (2020) 651â€“659.\n[33] X. Luo, G. Wang, T. Song, J. Zhang, M. Aertsen, J. Deprest, S. Ourselin, T.\nVercauteren, S. Zhang, MIDeepSeg: Minimally interactive segmentation of unseen\nobjects from medical images using deep learning, Med. Image Anal. 72 (2021)\n102102.\n[34] N. Otsu, A threshold selection method from gray-level histograms, IEEE Trans.\nSyst. Man Cybern. 9 (1) (1979) 62â€“66.\n[35] K. Yi, J. Wu, Probabilistic end-to-end noise correction for learning with noisy\nlabels, in: CVPR, 2019, pp. 7017â€“7025.\n[36] H. Shen, Z. Tang, Y. Li, X. Duan, Z. Chen, HAIC-NET: Semi-supervised OCTA\nvessel segmentation with self-supervised pretext task and dual consistency\ntraining, Pattern Recognit. 151 (2024) 110429.\n[37] M. Wei, C. Budd, L.C. Garcia-Peraza-Herrera, R. Dorent, M. Shi, T. Ver-\ncauteren, SegMatch: A semi-supervised learning method for surgical instrument\nsegmentation, 2023, arXiv preprint arXiv:2308.05232.\n[38] G. Wang, X. Luo, R. Gu, S. Yang, Y. Qu, S. Zhai, Q. Zhao, K. Li, S. Zhang, PyMIC:\nA deep learning toolkit for annotation-efficient medical image segmentation,\nComput. Methods Programs Biomed. 231 (2023) 107398.\n[39] M. Asad, R. Dorent, T. Vercauteren, FastGeodis: Fast generalised geodesic\ndistance transform, 2022, arXiv preprint arXiv:2208.00001.\n[40] O. Ronneberger, P. Fischer, T. Brox, U-Net: Convolutional networks for\nbiomedical image segmentation, in: MICCAI, 2015, pp. 234â€“241.\nJia Fu is a Ph.D. student at the School of Mechanical and Electrical Engineering,\nUniversity of Electronic Science and Technology of China. Her current works focus on\nweakly supervised learning, semi-supervised learning and medical image segmentation.\nGuotai Wang is a Professor at University of Electronic Science and Technology of\nChina. He obtained his Bachelor and Master degree of Biomedical Engineering in\nShanghai Jiao Tong University in 2011 and 2014 respectively. He then obtained his\nPh.D. degree of Medical and Biomedical Imaging in University College London at 2018.\nHis research interests include medical image computing, computer vision and deep\nlearning.\nTao Lu is an Associate Chief Physician at Sichuan Provincial Peopleâ€™s Hospital,\nUniversity of Electronic Science and Technology of China. She received her Master\nand Ph.D. degree in Sichuan University in 2007 and 2021, respectively. Her research\nworks focus on diagnosis and quantitative analysis of fetal and abdominal diseases\nbased on medical imaging.\nQiang Yue is a Chief Physician at West China Hospital of Sichuan University. His\nresearch interests include magnetic resonance imaging, brain abnormality diagnosis,\nquantitative analysis and evaluation of mental illnesses.\nTom Vercauteren is a Professor of Interventional Image Computing at Kings College\nLondon. He is a graduate from Columbia University and Ecole Polytechnique and\nobtained his Ph.D. from Inria Sophia Antipolis. His main research focus is on the\ndevelopment of innovative interventional imaging systems and their translation to the\nclinic. One key driving force of his work is the exploitation of image computing and\nthe knowledge of the physics of acquisition to move beyond the initial limitations of\nthe medical imaging devices that are developed or used in the course of his research.\nSÃ©bastien Ourselin is Head of the School of Biomedical Engineering & Imaging\nSciences and Professor of Healthcare Engineering at Kings College London. His core\nskills are in medical image analysis, software engineering, and translational medicine.\nHe is best known for his work on image registration and segmentation, its exploitation\nfor robust image-based biomarkers in neurological conditions, as well as for his\ndevelopment of image-guided surgery systems.\nShaoting Zhang is a Professor at University of Electronic Science and Technology\nof China. He received Ph.D. in Computer Science from Rutgers in 2011, M.S. from\nShanghai Jiao Tong University in 2007, and B.E. from Zhejiang University in 2005.\nHis research is on the interface of medical imaging informatics, computer vision and\nmachine learning.\nPattern Recognition 160 (2025) 111204\n10",
    "version": "5.3.31"
  },
  {
    "numpages": 13,
    "numrender": 13,
    "info": {
      "PDFFormatVersion": "1.7",
      "Language": "en-US",
      "EncryptFilterName": null,
      "IsLinearized": true,
      "IsAcroFormPresent": false,
      "IsXFAPresent": false,
      "IsCollectionPresent": false,
      "IsSignaturesPresent": false,
      "Creator": "Elsevier",
      "Custom": {
        "CrossMarkDomains[1]": "elsevier.com",
        "CrossmarkMajorVersionDate": "2010-04-23",
        "CreationDate--Text": "28th February 2025",
        "ElsevierWebPDFSpecifications": "7.0.1",
        "robots": "noindex",
        "doi": "10.1016/j.patcog.2025.111368",
        "CrossMarkDomains[2]": "sciencedirect.com",
        "CrossmarkDomainExclusive": "true"
      },
      "ModDate": "D:20250228115223Z",
      "Author": "Shenhai Zheng",
      "Title": "Weak-prior dual cognitive attention lightweight network for abdominal multi-organ segmentation",
      "Keywords": "Multi-organ segmentation,Lightweight model,Cognitive attention,Prior information",
      "CreationDate": "D:20250228090454Z",
      "Producer": "Acrobat Distiller 8.1.0 (Windows)",
      "Subject": "Pattern Recognition, 162 (2025) 111368. doi:10.1016/j.patcog.2025.111368"
    },
    "metadata": {
      "crossmark:crossmarkdomains": "elsevier.comsciencedirect.com",
      "crossmark:crossmarkdomainexclusive": "true",
      "crossmark:doi": "10.1016/j.patcog.2025.111368",
      "crossmark:majorversiondate": "2010-04-23",
      "dc:format": "application/pdf",
      "dc:identifier": "10.1016/j.patcog.2025.111368",
      "dc:publisher": "Elsevier Ltd",
      "dc:description": "Pattern Recognition, 162 (2025) 111368. doi:10.1016/j.patcog.2025.111368",
      "dc:subject": [
        "Multi-organ segmentation",
        "Lightweight model",
        "Cognitive attention",
        "Prior information"
      ],
      "dc:title": "Weak-prior dual cognitive attention lightweight network for abdominal multi-organ segmentation",
      "dc:creator": [
        "Shenhai Zheng",
        "Jianfei Li",
        "Haiguo Zhao",
        "Weisheng Li",
        "Yufei Chen",
        "Lei Yu"
      ],
      "jav:journal_article_version": "VoR",
      "pdf:creationdate--text": "28th February 2025",
      "pdf:producer": "Acrobat Distiller 8.1.0 (Windows)",
      "pdf:keywords": "Multi-organ segmentation,Lightweight model,Cognitive attention,Prior information",
      "pdfx:creationdate--text": "28th February 2025",
      "pdfx:crossmarkdomains": "sciencedirect.comelsevier.com",
      "pdfx:crossmarkdomainexclusive": "true",
      "pdfx:crossmarkmajorversiondate": "2010-04-23",
      "tgji2nmr_n.f_zwmgmwqoz8nnmdmllt6jndagywr9n9_-ndv_nmz_o9eqn9iknm-rndqtma": "",
      "pdfx:doi": "10.1016/j.patcog.2025.111368",
      "pdfx:robots": "noindex",
      "prism:aggregationtype": "journal",
      "prism:copyright": "Â© 2025 Elsevier Ltd. All rights are reserved, including those for text and data mining, AI training, and similar technologies.",
      "prism:coverdate": "2025-06-01",
      "prism:coverdisplaydate": "1 June 2025",
      "prism:doi": "10.1016/j.patcog.2025.111368",
      "prism:issn": "0031-3203",
      "prism:pagerange": "111368",
      "prism:publicationname": "Pattern Recognition",
      "prism:startingpage": "111368",
      "prism:url": "https://doi.org/10.1016/j.patcog.2025.111368",
      "prism:volume": "162",
      "tdm:policy": "https://www.elsevier.com/tdm/tdmrep-policy.json",
      "tdm:reservation": "1",
      "xmp:createdate": "2025-02-28T09:04:54",
      "xmp:creatortool": "Elsevier",
      "xmp:metadatadate": "2025-02-28T11:52:23",
      "xmp:modifydate": "2025-02-28T11:52:23",
      "xmpmm:documentid": "uuid:1b499bed-4ce8-4c0c-b682-0d58baae1cbe",
      "xmpmm:instanceid": "uuid:b6ed5266-03cb-4855-90c0-636283357f42",
      "xmprights:marked": "True"
    },
    "text": "Pattern Recognition 162 (2025) 111368\nAvailable online 20 January 2025\n0031-3203/Â© 2025 Elsevier Ltd. All rights are reserved, including those for text and data mining, AI training, and similar technologies.\nContents lists available at ScienceDirect\nPattern Recognition\njournal homepage: www.elsevier.com/locate/pr\nWeak-prior dual cognitive attention lightweight network for abdominal\nmulti-organ segmentation\nShenhai Zheng \na,b \n, Jianfei Li \nb\n, Haiguo Zhao \nb\n, Weisheng Li \nb \n, Yufei Chen \nc\n, Lei Yu \nd ,\nâˆ—\na \nChongqing Big Data Collaborative Innovation Center, Chongqing, China\nb \nCollege of Computer Science and Technology, Chongqing University of Posts and Telecommunications, 400065, Chongqing, China\nc \nCollege of Electronics and Information Engineering, Tongji University, 200092, Shanghai, China\nd \nEmergency Department, Second Affiliated Hospital of Chongqing Medical University, 400010, Chongqing, China\nA R T I C L E I N F O\nKeywords:\nMulti-organ segmentation\nLightweight model\nCognitive attention\nPrior information\nA B S T R A C T\nAutomated segmentation of organ regions (ASOR) in abdominal images enjoys high time and labor efficiency,\nyet it is still very challenging due to the patient-specific variations and complex characteristics of abdominal\norgans. Existing based ASOR methods suffer from the complex network structure and extensive anatomical\ninformation processing. Motivated by this discovery, this paper proposes a novel and effective weak-prior\ndual attention lightweight segmentation network with the following three-fold ideas: (a) the asymmetric U-\nNet structure is adopted for effectively reducing the network size via the utilization of different residual\nconvolution blocks in the encoder and decoder, (b) the lightweight dual cognitive attention module enhances\npixel-level feature representation without excessive memory consumption or matrix operations, and (c) the\nweak-prior aware module overcomes the difficulties of incorporating strong anatomical prior information\n(position or shape) into the model by using weak prior (relevance and volume ratio) to guide the attention\ncollaboratively. Evaluations of our method are conducted on open AbdomenCT-1K, AMOS2022 and Synapse\ndatasets. Extensively empirical results demonstrate significant efficiency gains as well as highly competitive\nsegmentation accuracy (the codes are public at https://github.com/feijianli/WDCAL-Net).\n1. Introduction\nThe abdomen is an important and complex region of the human\nbody. Segmentation of multiple organs is necessary for the diagno-\nsis and treatment of abdominal diseases [1]. Manual segmentation\nis subjective and labor-intensive, prompting extensive research into\nautomatic segmentation methods in recent years [2]. Nonetheless,\nchallenges persist in automatic segmentation due to limitations in CT\nimaging technology, patient-specific variations, and the complex nature\nof abdominal organs [3].\nTraditional methods for multi-organ segmentation include statistical\nshape models and multi-atlas segmentation [4]. However, both of these\nmethods require extensive domain information, manual feature design,\nand slow iterative computation. With the advancements in deep learn-\ning, methods represented by U-Net [5] have gradually achieved better\nperformance. However, these methods still face two significant chal-\nlenges. Firstly, the abdominal CT background is highly intricate, and\nthe boundaries between adjacent organs are often indistinct, making it\ndifficult to accurately segment these blurred boundaries. Secondly, the\ntarget organs exhibit considerable variability in size and shape result-\ning in subpar segmentation performance for these smaller organs. To\nâˆ— \nCorresponding author.\nE-mail address: yulei@cqmu.edu.cn (L. Yu).\naddress these challenges, an increasing number of improved networks\nbased on U-Net have been proposed, focusing on the design of network\nbackbones and blocks. Improved backbones, such as 3D-UNet [6] and\nCascade of 2D and 3D [7], have shown improvements in segmentation\naccuracy to some extent. Another commonly used approach to enhance\nsegmentation performance is by introducing improved blocks into U-\nNet, including Inception [8], attention mechanisms [9], and multi-scale\ninformation fusion [10]. Among these, the attention mechanism is\ngenerally regarded as a â€˜â€˜simulationâ€™â€™ of cognitive attention in the\nhuman brain, which can assist models in automatically focusing on and\nemphasizing regions of interest within images, thereby enhancing the\naccuracy of segmentation results. However, these algorithms have not\nfully captured the characteristics of human cognitive attention with a\nsubstantial number of model parameters and strong prior information\nrepositories [11].\nTo further enhance segmentation performance, researchers utilized\nanatomical prior information, such as organ shapes [12] and po-\nsitions [13], to facilitate accurate image segmentation. While deep\nlearning-based segmentation methods achieved impressive results at\nhttps://doi.org/10.1016/j.patcog.2025.111368\nReceived 2 September 2024; Received in revised form 29 October 2024; Accepted 12 January 2025\n\nPattern Recognition 162 (2025) 111368\n2\nS. Zheng et al.\nFig. 1. Ball chart reporting the average DSC and NSD score vs computational complexity (FLOPs and Params). In the figure, the same color represents the same comparison\nmethod, with circles representing results from the AbdomenCT-1K dataset and squares from the AMOS2022 dataset. The size of each ball and square represents the modelâ€™s Params.\nthe pixel level, they often struggled to learn strong prior information\nabout the relationships between organs and tissues. However, a key\nquestion remains: what are the implications of using weak prior information\nto constrain deep learning models? Limited research has explored this\ntopic thus far.\nTo address the limitations of the aforementioned algorithms, we\nproposed a novel improvement model called weak-prior dual cognitive\nattention lightweight segmentation network. The introduction of weak\nprior information offers explicit regularization to attention mecha-\nnisms, enhancing the information selectivity of cognitive attention.\nThrough the integration of weak-prior awareness and dual attention\nmechanisms, the proposed model demonstrated high performance in\nabdominal multi-organ segmentation tasks. It can accurately segment\ndifferent organs in abdominal images while maintaining low computa-\ntional complexity and memory usage as shown in \nFig. 1 which illus-\ntrates the average DSC and NSD scores alongside computational com-\nplexity (FLOPs and Params) for various methods on the AbdomenCT-1K\nand AMOS2022 datasets. All in all, the main contributions of this work\nlie as follows:\nâ€¢ A novel improvement model called weak-prior dual cognitive at-\ntention lightweight network is proposed to enhance the ability of\nfeature representation for abdominal multi-organ segmentation.\nâ€¢ A lightweight dual attention (LDA) module, comprising two par-\nallel lightweight position and channel attention modules, is de-\nsigned to improve pixel-level predictive feature representations.\nâ€¢ A weak-prior aware module is proposed to efficiently and effec-\ntively incorporate anatomical prior information into the model\nbased on organ relevance and proportion.\nâ€¢ Comprehensive study validate that the proposed method achieves\na trade-off between performance and model complexity.\nThe remaining paper is organized as follows: Section 2 provides\nreview of related attention-based and prior-guided medical image seg-\nmentation works. Section 3 explains the three important blocks and\nthe architecture of the proposed network. Section 4 presents compre-\nhensive experiments and results. Section 5 presents ablation experi-\nments and results. Finally, Sections 6 and 7 show the discussion and\nconclusion.\n2. Related works\n2.1. Attention in neural networks\nIn recent years, the integration of deep learning with visual atten-\ntion mechanisms has been widely studied. Attention mechanisms can\nbe categorized into local spatial attention, channel attention, hybrid at-\ntention, and non-local attention. Spatial attention emphasizes locating\nregions of interest in the image. For instance, the AG model pro-\nposed by Oktay et al. [\n14] demonstrated the effectiveness of attention\nmechanisms in selectively attending to relevant image features while\nsuppressing irrelevant regions. By incorporating variance uncertainty\nas an indicator for estimating error-prone regions, Lin et al. [15]\ndemonstrated the effectiveness of the variance-aware attention U-Net in\nimproving the segmentation of small organs. On the other hand, chan-\nnel attention focuses on identifying relevant features. Hu et al. [\n16]\nintroduced SE-Net, a channel attention-based method that applies at-\ntention weighting to channels using three-step process. Wang et al. [\n17]\nproposed an Efficient Channel Attention (ECA) module, which em-\nploys a local cross-channel interaction strategy using 1D convolutions.\nHybrid attention mechanisms combine spatial and channel attention.\nWoo et al. [\n18] proposed the Convolutional Block Attention Module\n(CBAM), which effectively learns to focus on informative channels and\nspatial locations. Non-local attention mechanisms have the capability\nto explore correlations between different targets and features. Wang\net al. [\n19] introduced the non-local U-Net, which is equipped with\nflexible global aggregation blocks. Recently, Fiaz et al. [20] have pro-\nposed a guided-attention and gated-aggregation network for capturing\ntissues of irregular shapes and sizes. Although attention mechanisms\nhave demonstrated promising performance, these networks tend to be\ncomplex, large in model size, and computationally intensive, making\nthem unsuitable for clinical applications that require lightweight and\nefficient models.\n2.2. Prior information in human organs\nPrior information plays a crucial role in automated medical image\nsegmentation by helping overcome misleading low-level information,\nsuch as grayscale intensities. One of the pioneering studies in this field\nis the work by Oktay et al. [21], who proposed a novel and gener-\nalizable approach called Anatomically Constrained Neural Networks\n(ACNNs) that integrates shape and label structure priors into the model\nfor medical image analysis tasks. Similarly, Boutillon et al. [\n22] in-\ncorporated anatomical priors into a conditional adversarial framework\nfor scapula segmentation. In addition, deep learning-based segmenta-\ntion methods often encounter challenges such as boundary leakage,\nover-segmentation, and under-segmentation. The inclusion of prior in-\nformation can partially constrain the model, enhance shape regularity,\nand overcome these issues. Advanced segmentation techniques have\nbenefited from incorporating prior information about target structures\ninto the segmentation process. For example, Hamarneh et al. [\n23]\nproposed a topology-aware segmentation network that combines fully\nconvolutional networks with topological information to improve the\naccuracy and robustness of gland segmentation. Furthermore, there are\ntopological relationships between abdominal organs, such as adjacency,\n\nPattern Recognition 162 (2025) 111368\n3\nS. Zheng et al.\nFig. 2. The schematic diagram of the proposed weak-prior aware lightweight dual attention network.\ncontainment, and mutual exclusion. Hu et al. [24] introduced new topo-\nlogical interaction modules that encode topological interactions into\ndeep neural networks, enriching the feature representation of neural\nnetworks. These studies demonstrate the effectiveness and feasibility\nof integrating prior information from abdominal organs with deep\nlearning approaches.\n3. Methods\nFig. 2 illustrates the proposed novel architecture and improved\nblocks. It consists of three main components: an asymmetric U-Net\nbackbone network, a lightweight dual cognitive attention module, and\na weak-prior aware module. These components are designed to achieve\ncomprehensive attention guidance in both spatial and channel dimen-\nsions of the feature maps, as well as prior information guidance on the\nrelevence and size ratio among multiple organs.\n3.1. Asymmetric U-Net\nTo improve the accuracy of multi-organ segmentation without sig-\nnificantly increasing the computational cost, we propose a powerful ar-\nchitecture of the low-cost asymmetric U-Net as the backbone. The U-Net\nbackbone is an end-to-end trainable network consisting of an encoder\nand a decoder, with shortcut connections at each resolution level. To\nobtain more in-depth feature information, residual learning structures\nare introduced in both the encoder and decoder stages. Compared to\nthe standard U-Net, the proposed asymmetric U-Net achieves reduced\ncomputational cost and memory consumption without significantly\naffecting segmentation accuracy.\nAs shown in Fig. 2, the encoder module consists of two residual\nconvolution blocks, and the decoder module consists of one residual\nconvolution block. Each residual convolution block is composed of a 3D\nconvolution layer with instance normalization (IN) [25] and rectified\nlinear unit (ReLU) activation functions. The residual connection occurs\nbefore the ReLU activation. In the encoder module, 3 Ã— 3 Ã— 3 convo-\nlution layers with a stride of 1 Ã— 1 Ã— 1 are used to extract the input\nfeature maps which subsequently serve as the input for the next layer\nto facilitate residual learning. In the decoder module, we separate a\nstandard 3D convolution with a kernel size of 3 Ã— 3 Ã— 3 into a 3 Ã— 3 Ã— 1\nslice-wise convolution and a 1 Ã— 1 Ã— 3 cross-wise convolution, known\nas anisotropic residual module, to further reduce the computational\nburden. Additionally, in the decoding stage, we replace the original\nconcat operation with an improved Reduce Channel + Upsample +\nAdd operation, which helps reduce operation parameters and improve\ncomputational efficiency. Specifically, we use a 1 Ã— 1 Ã— 1 convolution\nto reduce the number of features by half, perform three-dimensional\nbilinear upsampling to double the spatial dimension, and then add it\nto the corresponding spatial horizontal layer of the encoder output for\nmulti-scale feature fusion and optimized segmentation.\n3.2. Lightweight dual attention module\nIn convolutional neural networks, pixel relationships are primarily\nestablished within local neighborhoods through convolutional layers,\nand long-range dependencies are often modeled by deep stacking of\nconvolutional layers. However, directly repeating convolutional layers\nleads to low computational efficiency and difficulties in optimiza-\ntion [26]. To address the issue of limited receptive fields caused by\nconvolutional operations, we explore global contextual information\nby utilizing attention mechanisms to establish correlations between\nfeatures. Additionally, many attention mechanisms exhibit strong per-\nformance but also introduce high network complexity, large model size,\nand heavy computational requirements, making them unsuitable for\nlightweight and efficient models in clinical practice. By contrast, our\ndual attention module is lightweight, which can effectively modeling\nglobal context and enabling it to be applied to multiple layers of the\nbackbone network.\nAs shown in Fig. 3, two parallel attention modules (position and\nchannel attention) were designed to capture the global context of the\ngenerated local features in the network, resulting in better feature\nrepresentation for pixel-level prediction. Thanks to the lightweight\ndesign of our modules, they can easily be applied after each encoding\nand decoding module in the backbone network. The features from the\nencoding and decoding modules are inputted into the two parallel\nattention modules. The lightweight position attention module is used to\ngenerate features capturing spatial distant context information, while\nthe channel attention module captures distant context information\nin the channel dimension. Finally, we aggregate the outputs of the\ntwo attention modules to obtain a better feature representation for\npixel-level prediction.\n3.2.1. Lightweight position attention module\nMost classic approach for modeling long-range dependencies is\ncurrently through self-attention mechanisms, such as the non-local net-\nwork. However, the non-local network calculates pairwise relationships\nbetween individual pixels, resulting in significant memory consumption\nand matrix operations. CCNet [27] addresses these issues by acceler-\nating the non-local network through the stacking of two interleaved\nblocks. Furthermore, findings from GC-Net [28] suggest that the non-\nlocal network actually learns query-agnostic attention maps for each\nquery position, which is a waste of computational resources in mod-\neling pixel-level pairwise relationships. Accurately, the global context\nis actually independent of the query position. Therefore, there is no\nneed to compute query-specific global context for each query position,\nallowing us to simplify the non-local block.\nTo model rich contextual relationships on the local features gen-\nerated by convolutional modules, we proposed a lightweight position\n\nPattern Recognition 162 (2025) 111368\n4\nS. Zheng et al.\nFig. 3. Illustration of the lightweight dual attention module: lightweight position and channel attention.\nattention module. The position attention module encodes more exten-\nsive contextual information into the local features, enhancing their\nfeature representation power. We achieve the lightweight characteristic\nof the module by computing a global (query-agnostic) attention map\nand sharing it among all query positions. This approach significantly\nreduces computational costs and memory consumption compared to the\nnon-local block. As shown the position attention module in Fig. 3, given\na local feature ğ‘‹ âˆˆ R\nğ¶Ã—ğ»Ã—ğ‘Š Ã—ğ·\n, it passed through a convolutional layer\n(ğ‘Š\nğ‘˜\n) to generate a new feature map ğ‘‹ âˆˆ R\n1Ã—ğ»Ã—ğ‘Š Ã—ğ· \nfirstly. Then it\nreshaped to ğ‘‹ âˆˆ R\nğ‘Ã—1Ã—1Ã—1\n, where ğ‘ = ğ» Ã— ğ‘Š Ã— ğ· is the number of\npixels. Next, a one-dimensional convolution (ğ‘Š\n1ğ‘‘ \n) is applied to capture\nlocal spatial interactions and a softmax layer is used to compute the\nposition attention map ğ‘† âˆˆ R\nğ‘Ã—1\n.\nğ‘†\nğ‘— \n= \nğ‘’ğ‘¥ğ‘ \n(\nğ‘Š\n1ğ‘‘\n(\nğ‘Š\nğ‘˜\nğ‘‹\nğ‘—\n))\nâˆ‘\nğ‘\nğ‘š=1 \nğ‘’ğ‘¥ğ‘ \n(\nğ‘Š\n1ğ‘‘\n(\nğ‘Š\nğ‘˜\nğ‘‹\nğ‘š\n)) \n(1)\nwhere ğ‘†\nğ‘— \nrepresents the weight of the ğ‘—th pixel in the position attention\nmap.\nAt the same time, the feature ğ‘‹ reshaped as ğ‘„ âˆˆ R\nğ¶Ã—ğ‘ \n. Then, matrix\nmultiplication between ğ‘„ and ğ‘† is performed, followed by a bottleneck\nlayer (ğ‘Š\nğ‘£\n). Finally, the result is element-wise added with the feature ğ‘‹,\nresulting in the final output ğ‘ âˆˆ R\nğ¶Ã—ğ»Ã—ğ‘Š Ã—ğ·\n, as shown below:\nğ‘\nğ‘– \n= ğ‘‹\nğ‘– \n+ ğ‘Š\nğ‘£\nğ‘\nâˆ‘\nğ‘—=1\n(\nğ‘†\nğ‘— \nğ‘„\nğ‘—\n) \n(2)\nThe resulting feature ğ‘ at each position is a weighted sum of the\nfeatures at all positions and the original features. Therefore, it has a\nglobal contextual view and selectively aggregates context based on the\nspatial attention map. Similar semantic features achieve mutual bene-\nfits, improving intra-class compactness and semantic consistency. Since\nthe non-local block actually learns global context independent of the\nquery, our position attention module simulates the same global context\nas the non-local block, but with significantly reduced computational\ncost. Additionally, our position attention module employs bottleneck\ntransformations to reduce redundancy in the global context features,\nthe number of parameters and FLOPs is further reduced, too.\n3.2.2. Lightweight channel attention module\nExploiting the interdependencies between channel mappings allows\nfor the emphasis of interdependent feature maps and the improvement\nof the representation of specific semantics. Therefore, a lightweight\nchannel attention module is constructed to explicitly model the inter-\ndependencies between channels. The structure of the channel attention\nmodule is shown in Fig. 3. Unlike the position attention module, an\nadaptive average pooling operation (ğ‘Š \nâ€²\nğ‘˜ \n) on the original feature ğ‘‹ âˆˆ\nğ¶Ã—ğ»Ã—ğ‘Š Ã—ğ· \nis performed firtly to generate a new feature map R\nğ¶Ã—1Ã—1Ã—1\n.\nThen, a one-dimensional convolution is applied to capture local cross-\nchannel interaction information. Finally, a softmax layer is applied to\nobtain the channel attention map ğ‘†\nâ€² \nâˆˆ R\nğ¶Ã—1\n.\nğ‘†\nğ‘— \n= \nğ‘’ğ‘¥ğ‘ \n(\nğ‘Š\n1ğ‘‘\n(\nğ‘Š \nâ€²\nğ‘˜ \nğ‘‹\nğ‘—\n))\nâˆ‘\nğ¶\nğ‘š=1 \nğ‘’ğ‘¥ğ‘ \n(\nğ‘Š\n1ğ‘‘\n(\nğ‘Š \nâ€²\nğ‘˜ \nğ‘‹\nğ‘š\n)) \n(3)\nwhere ğ‘†\nâ€²\nğ‘— \nrepresents the weight of the ğ‘—th pixel in the channel attention\nmap.\nSimultaneously, the feature ğ‘‹ is reshaped and transposed into ğ‘„\nâ€² \nâˆˆ\nR\nğ‘Ã—ğ¶ \nand ğ‘ = ğ» Ã—ğ‘Š Ã—ğ· represents the number of pixels. Then, matrix\nmultiplication between ğ‘„\nâ€² \nand ğ‘†\nâ€² \nis performed, followed by reshaping\nthe result into R\n1Ã—ğ»Ã—ğ‘Š Ã—ğ·\n. Finally, the result is element-wise summed\nwith the feature ğ‘‹, resulting in the final output ğ‘\nâ€² \nâˆˆ R\nğ¶Ã—ğ»Ã—ğ‘Š Ã—ğ·\n. In\nthis way, the final feature for each channel is the weighted sum of\nthe features from all channels and the original feature, which models\nthe long-range semantic dependencies between feature maps. It helps\nimprove feature discriminability. The process is shown as follows:\nğ‘\nâ€²\nğ‘– \n= ğ‘‹\nğ‘– \n+\nğ¶\nâˆ‘\nğ‘—=1\n(\nğ‘†\nâ€²\nğ½ \nğ‘„\nâ€²\nğ½\n) \n(4)\nTo fully leverage the long-range contextual information, the features\nfrom both attention modules are combined by performing element-wise\nsummation:\nğ‘€\nğ‘– \n= ğ‘\nğ‘– \n+ ğ‘\nâ€²\nğ‘– \n(5)\nwhere ğ‘€ represents the feature map obtained from the attention\nmodules after processing the feature map ğ‘‹. It does not use concate-\nnation operations as they require additional GPU memory. Notice that\nour attention modules are lightweight and can be directly applied to\nmultiple layers of existing backbone networks. They do not add too\nmany parameters but effectively enhance feature representation.\nCompared to DA-Net [29], the key operations that make this module\nlightweight include: firstly, adaptive average pooling is applied to the\noriginal feature map, significantly reducing the computational burden\nand generating a new feature map. Then, a 1D convolution is used to\ncapture local cross-channel interaction information, thereby reducing\nthe number of parameters and computational complexity. Next, a soft-\nmax layer is used to obtain the channel attention map, normalizing\nthe weights across channels. Finally, through matrix multiplication and\nelement-wise summation, the final feature for each channel is obtained\n\nPattern Recognition 162 (2025) 111368\n5\nS. Zheng et al.\nby a weighted sum of the features from all channels and the original\nfeature, which helps model long-range semantic dependencies between\nfeature maps. Consequently, the module achieves low computational\ncosts compared to the attention module in DA-Net.\nMoreover, LDA differs from the CBAM in [18]. The LDA consists\nof two lightweight parallel Position Attention and Channel Attention\nbranches. The Position Attention Module computes a global (query-\nagnostic) attention map and shares it across all query positions, sig-\nnificantly reducing computational cost and memory consumption. The\nChannel Attention Module uses adaptive average pooling to perform\nadaptive average pooling on the original feature map. The CBAM con-\nsists of two sequential Channel Attention and Spatial Attention Module\nmodules. Compared to CBAM, LDA significantly reduces computational\ncomplexity and memory consumption through its lightweight design\nwhile maintaining high segmentation accuracy as shown in Section\nâ€˜â€˜Experiments and Resultsâ€™â€™.\n3.3. Weak-prior aware module\nIn deep learning-based methods, the exploration of prior informa-\ntion and its effective utilization pose certain challenges. Additionally,\nrecent attention-based approaches learn channel attention, spatial at-\ntention, or point-wise attention to selectively aggregate contextual\ninformation. However, due to the lack of explicit regularization, the\nrelationship descriptions of attention mechanisms are not clear. To\naddress these limitations, we propose to introduce prior information\ninto attention mechanisms. This is a mutually beneficial strategy. Prior\ninformation provides insights that guide attention mechanisms to sim-\nulate human cognitive attention more effectively. At the same time,\nattention mechanisms reciprocate by integrating prior information into\nthe model. In this study, we further explore this mutually beneficial\nrelationship.\nEspecially, we investigate the effective representation of relevance\nand size relationship between different organ categories in segmenta-\ntion and propose a weak-prior aware module to model the relationships\nbetween pixels of different categories. It is observe that while the\nshapes and sizes of segmentation targets in other image datasets exhibit\nhigh uncertainty, the positions and volumes of human organs are\nrelatively consistent in 3D scans. So, this organ volume ratio can serve\nas a prior information to guide our segmentation model. We refer to\nthis prior as a weak-prior because it is specific to medical images and\nuniversally present. It does not require extensive learning of complex\nanatomical prior information, making it easier for us to learn and\ngeneralize, and easier for the model to train and optimize.\nHowever, current deep learning methods have limited ability to\nencode this high-level information. The key challenge is how to enable\nthe model to effectively learn the proportional relationships between\ndifferent organ categories during the training process. In the weak-\nprior aware module, we divide the proportional relationships between\ndifferent organ categories into two aspects: organ relevance and organ\nproportion. Organ relevance refers to the importance of each voxel\nwithin different classes, with high relevance within the same class\nand low relevance between different classes. Organ proportion, on the\nother hand, represents the ratio of the number of voxels for each\nclass. Correspondingly, we propose two approaches to reflect these two\nrelationships, namely organ relevance prior and organ proportion prior.\n3.3.1. Organ relevance prior\nGiven the labels of the input data, we can determine the semantic\nclass for each voxel and obtain the relevance of each voxel (high\nrelevance within the same class and low relevance between different\nclasses). It specifically notes that in the feature maps, each channel of\nthe high-level feature maps can be seen as a response specific to a class,\nand different semantic responses are linked. By using the connections\nbetween channel mappings, it can highlight the interdependent feature\nmaps and enhance the representation of specific semantics. Therefore,\nit can use the correlation between label classes to direct the interdepen-\ndencies between channel mappings during network training, as shown\nin the upper left part of Fig. 4.\nGiven the input image ğ‘€ and the labels ğ¾, where ğ‘‹ represents the\nfeature map obtained by feeding the input image ğ‘€ into the network,\nğ‘‹ âˆˆ R\nğ¶Ã—ğ»Ã—ğ‘Š Ã—ğ·\n, where ğ¶ is the number of classes. First, ğ‘‹ is reshaped\nand trilinear interpolation is applied to it. Then, it is passed through a\nsigmoid function to generate a channel correlation map ğ‘ƒ :\nğ‘ƒ\nğ‘–ğ‘— \n= ğœ\n(\nğ‘‹\nğ‘– \nÃ— ğ‘‹\nğ‘‡\nğ‘—\n)\n(6)\nwhere ğ‘ƒ âˆˆ R\nğ¶Ã—ğ¶ \n, representing the sigmoid function, ğ‘ƒ\nğ‘–ğ‘— \nrepresents the\nmutual dependency between the ğ‘–th channel and the ğ‘—th channel in ğ‘ƒ .\nMeanwhile, the feature ğ‘‹ is reshaped as ğ‘„, where ğ‘„ âˆˆ R\nğ¶Ã—ğ‘ \n, with\nğ‘ = ğ» Ã— ğ‘Š Ã— ğ· being the number of pixels. Then matrix multiplication\nbetween ğ‘„ and ğ‘ƒ is performed, followed by element-wise summation\nwith the feature ğ‘‹ to obtain the output ğ‘Œ âˆˆ R\nğ¶Ã—ğ»Ã—ğ‘Š Ã—ğ·\n, as illustrated\nbelow:\nğ‘Œ\nğ‘– \n= ğ‘‹\nğ‘– \n+\nğ¶\nâˆ‘\nğ‘—=1\n(\nğ‘ƒ\nğ‘–ğ‘— \nğ‘„\nğ‘—\n) \n(7)\nThen, the class prior map ğ´ is constructed guided by the labels. As\nshown the right part of \nFig. 4, given the label ğ¾ âˆˆ R\n1Ã—ğ»Ã—ğ‘Š Ã—ğ·\n, each\ncategorical integer label in the label ğ¾ is encoded using the one-of-K\nscheme, resulting in a matrix of size ğ» Ã— ğ‘Š Ã— ğ· Ã— ğ¶. This matrix is\nthen reshaped into a feature map ğº of size ğ¶ Ã— ğ» ğ‘Š ğ·. Next, matrix\nmultiplication is performed between two matrices (ğº and its transpose\nğº\nğ‘‡ \n) to obtain a feature map, which is then passed through a sigmoid\nfunction to obtain the class prior map ğ´:\nğ´\nğ‘–ğ‘— \n= ğœ\n(\nğº\nğ‘– \nÃ— ğº\nğ‘‡\nğ‘—\n)\n(8)\nwhere ğ´ âˆˆ R\nğ¶Ã—ğ¶ \n, ğ´\nğ‘–ğ‘— \nrepresents the correlation between the ğ‘–th and\nğ‘—th classes in ğ´.\nTo explicitly guide the network in modeling the correlation re-\nlationships between different classes, a prior loss is introduced. For\neach class in the image, this loss encourages the network to consider\nthe correlation between classes. Specifically, the class prior map ğ´ is\nused to supervise the learning of the channel correlation map ğ‘ƒ , as\nshown in the upper left part of \nFig. 4. For each element in the class\nprior map ğ´, the values on the diagonal reflect the number of voxels\nfor the corresponding class, indicating the intra-class correlation. The\noff-diagonal elements reflect the inter-class correlations. To learn the\ndiscrepancy between the values of the channel correlation map ğ‘ƒ and\nthe corresponding elements in the class prior map ğ´, the class prior\nloss is defined based on the mean square error (MSE), which defined\nas follows:\nğ¿\nğ‘š \n=\nâˆš\nâˆš\nâˆš\nâˆš\nâˆš \n1\nğ¶\n2\nğ¶\nâˆ‘\nğ‘–=1\nğ¶\nâˆ‘\nğ‘—=1\n(\nğ´\nğ‘–ğ‘— \nâˆ’ ğ‘ƒ\nğ‘–ğ‘—\n)\n2 \n(9)\nIt should be noted that the Organ Relevance Prior aims to guide\nthe network in learning the correlations and uncorrelations between\ndifferent categories only using label information. Specifically, the ma-\ntrix ğ´ represents the correlation between different categories in the\nlabels, where the diagonal elements reflect the self-correlation of the\ncategories, correctly representing all pixels within the category, and\nthe off-diagonal elements reflect the correlation between different cat-\negories, correctly representing low correlation. That is to say, 0.5\nin the off-diagonal elements signifies that different organ types are\nuncorrelated, aiding the model in reducing false segmentation and\npromoting a one-hot encoding result. To guide the network in learning\nthe relations between categories in ğ´, the category correlation loss ğ¿\nğ‘š\nin Eq. (\n9) was introduced. By minimizing ğ¿\nğ‘š\n, the network aligns the\nchannel correlation matrix ğ‘ƒ with the correlations and uncorrelations\nrepresented in matrix ğ´, thus improving segmentation performance.\n\nPattern Recognition 162 (2025) 111368\n6\nS. Zheng et al.\nFig. 4. Illustration of the weak-prior aware module. It includes organ relevance and organ proportion priors and is used to explore the mutually beneficial relationship of weak\nprior information and attention mechanisms.\n3.3.2. Organ proportion prior\nGiven the labels of the input data, in addition to semantic class\ninformation, we can also obtain the volume (the number of voxels)\nratios for each class. Based on the relationship between the channel\nmaps of high-level features and the responses specific to each class, the\nweights of each channel map can be considered as a response specific to\nthe volume of a particular class. Therefore, it can guide the network to\nlearn the volume proportions of the classes by using the volume ratios\nbetween classes, as shown in the bottom left part of Fig. 4.\nUnlike the category relevance prior, here it directly applys adaptive\naverage pooling to ğ‘‹, followed by a softmax function to generate a\nchannel weight map ğ‘ƒ \nâ€²\n, where ğ‘ƒ \nâ€² \nâˆˆ R\n1Ã—ğ¶ \n. Similarly, the one-hot\nencoding scheme is used to encode each categorical integer label in\nthe label ğ¾, and also adaptive average pooling and a softmax function\nare applied to generate a class ratio map ğ´\nâ€²\n, where ğ´\nâ€² \nâˆˆ R\n1Ã—ğ¶ \n. To more\naccurately measure the difference between the two distributions ğ‘ƒ \nâ€² \nand\nğ´\nâ€²\n, the Kullbackâ€“Leibler (KL) divergence and MSE loss are introduced\nas our class ratio prior loss function:\nğ¿\nğ‘˜ \n=\nâˆš\nâˆš\nâˆš\nâˆš \n1\nğ¶\nğ¶\nâˆ‘\nğ‘–=1\n(\nğ´\nâ€²\nğ‘– \nâˆ’ ğ‘ƒ \nâ€²\nğ‘–\n)\n2 \n+\nğ¶\nâˆ‘\nğ‘–=1\nğ´\nâ€²\nğ‘–\n(\nlog ğ´\nâ€²\nğ‘– \nâˆ’ log ğ‘ƒ \nâ€²\nğ‘–\n) \n(10)\nwhere ğ´\nâ€²\nğ‘– \nrepresents the weight of the ğ‘–th class in the category pro-\nportion map ğ´, and ğ‘ƒ \nâ€²\nğ‘– \nrepresents the weight of the ğ‘–th channel in the\nchannel weight map ğ‘ƒ .\n4. Experiments and results\n4.1. Dataset\nThe AbdomenCT-1K dataset [1] comprises 1112 CT scans collected\nfrom five publicly available datasets: LiTS, KiTS, MSD Intestines, Pan-\ncreas, and NIH Pancreas. The AMOS2022 dataset [30] is a comprehen-\nsive and diverse benchmark for abdominal multi-organ segmentation.\nIt consists of 500 CT scans and 100 MRI scans. Synapse dataset [31]\ncomprises 30 abdominal CT scans for multi-organ segmentation from\nthe MICCAI 2015 Multi-Atlas Abdomen Labeling Challenge.\nTo address the computational challenge of processing 3D data in\nthe AbdomenCT-1K and AMOS2022 datasets, a strategy was adopted\nto reduce training requirements and expedite the training process. This\ninvolved selecting subsets from each dataset to focus our experiments.\nTo ensure a fair evaluation and accurately assess the modelâ€™s effec-\ntiveness, these subsets were divided into 80% for training and 20%\nfor testing. Specifically, 500 cases were randomly sampled from the\nAbdomenCT-1K dataset. 400 cases were used for training, and 100\ncases were reserved for testing. Similarly, from the AMOS2022 dataset,\nselected 200 cases were randomly sampled for training and 40 cases\nwere set aside for testing. Furthermore, within the training dataset of\n400 cases, it was divided into 320 training cases and 80 validation cases\nfurther. Similarly, within the training dataset of 200 cases, it was split\ninto 160 training cases and 40 validation cases. For Synapse datasets,\n19 scans were used for training, 5 scans for validation, and 6 scans for\ntesting.\n4.2. Implementation details\n4.2.1. Network training and testing\nAll experiments were conducted on a desktop computer featuring\nthe NVIDIA RTX A6000 GPU. The deep neural networks were im-\nplemented using the PyTorch framework, specifically utilizing torch\nversion 1.10.1 and torchvision version 0.11.2.\nOur preprocessing pipeline includes several steps to prepare the\ndata for further analysis. Firstly, the images are reoriented to a target\ndirection to ensure consistency. Next, the images are resampled to\nmatch the input size of the model, which is [192, 192, 192]. This\nresampling step helps to standardize the data and ensure compatibility\nwith the models. Additionally, intensity normalization is performed on\nthe images. Moreover, the raw intensity values are truncated to the\nrange of [âˆ’325, 325] to limit extreme values that may occur due to\nvarious factors. Subsequently, each CT case is normalized to have a zero\nmean and unit variance. This normalization step helps to mitigate the\ndata variance introduced by the physical processes involved in medical\nimaging. To improve efficiency, we leverage the computational power\nof the GPU to perform resampling and intensity normalization, reducing\nthe time required for these preprocessing steps. By following this pre-\nprocessing pipeline, the data were properly prepared and standardized\nbefore feeding it into the deep neural networks.\nThe training dataset was augmented using brightness and crop op-\nerations, enhancing the diversity of the data and improving the modelâ€™s\ngeneralization ability. During training, a batch size of 2 was utilized,\nwhereas during testing, a batch size of 1 was used. For optimization,\nthe Adam optimizer was employed with a learning rate of 0.001 and\na weight decay of 0.00005. These hyperparameters were chosen to\neffectively control the learning progress of the network and prevent\noverfitting. In all experiments, both the coarse model and the fine\nmodel were trained for 200 epochs. This duration allowed the models\nto learn from the data and converge to an optimal state. During the\n\nPattern Recognition 162 (2025) 111368\n7\nS. Zheng et al.\ninference stage, the model in FP16 mode was implemented, taking\nadvantage of reduced precision computation [32]. Additionally, to mit-\nigate GPU memory constraints, dynamic empty cache techniques [33]\nwas utilized to efficiently manage memory.\n4.2.2. Loss function\nDuring the training phase, the Dice similarity coefficient is utilized\nas the loss function. In the case of multi-label classification, the soft\nDice coefficient for each class mask is computed, and the Dice loss\nis then calculated for each class and then averaged over all classes,\nproviding a comprehensive loss function that considers the performance\nacross all labels. Our multi-label loss function is defined as follows:\nğ¿\nğ‘‘ \n= 1 âˆ’ \n1\nğ‘š\nğ‘š\nâˆ‘\nğ‘—=1\n2 \nâˆ‘\nğ‘\nğ‘–=1 \nğ¼\nğ‘–ğ‘— \nğ¾\nğ‘–ğ‘—\nâˆ‘\nğ‘\nğ‘–=1 \nğ¼\nğ‘–ğ‘— \n+ \nâˆ‘\nğ‘\nğ‘–=1 \nğ¾\nğ‘–ğ‘—\n(11)\nwhere ğ¿\nğ‘‘ \nis the multi-label soft dice loss, ğ¾ represents the ground-\ntruth class mask, ğ¼ represents the predicted value of the network, ğ‘š\nrepresents the class number, ğ‘ represents the number of voxel points\nin each image module. ğ¼\nğ‘–ğ‘— \nrepresents the value of the ğ‘–th pixel in the\nğ‘—th class of the predicted mask ğ¼, likewise ğ¾\nğ‘–ğ‘— \n.\nTo integrate the proposed category prior module into end-to-end\ntraining, two prior loss functions are introduced, namely ğ¿\nğ‘š \nand ğ¿\nğ‘˜\n.\nThese losses are designed to guide the attention mechanism in aggre-\ngating contextual relationships more efficiently and effectively based\non the prior information which is defined as follows:\nğ¿\ntotal \n= ğ›¼ ğ¿\nğ‘‘ \n+ ğ›½ ğ¿\nğ‘š \n+ ğœ†ğ¿\nğ‘˜ \n(12)\nwhere ğ¿\nğ‘‘ \n, ğ¿\nğ‘š\n, ğ¿\nğ‘˜ \nrepresent the segmentation loss, category correla-\ntion prior loss, and category proportion prior loss, respectively. The\nweights ğ›¼, ğ›½, ğœ† are used to control the importance of each loss term.\nThrough experimental evaluation, it was determined that the best\ntraining performance is achieved when ğ›¼ = 1, ğ›½ = 1, ğœ† = 20\nBy incorporating the category prior losses into the overall loss\nfunction, our model is able to leverage the prior information to guide\nthe attention mechanism, enhancing the effectiveness and efficiency of\ncontextual aggregation. The optimization process aims to minimize the\ntotal loss, which encourages the network to produce more accurate\nand meaningful segmentation while leveraging the category-specific\ninformation captured by the prior module.\n4.3. Segmentation performance\nTo provide a quantitative analysis of the method, two evalua-\ntion metrics, the region-based Dice Similarity Coefficient (DSC) and\nborder-based Normalized Surface Dice (NSD), were utilized to assess\nits performance. These two metrics are both normalized to the range of\n[0, 1], where a high value indicates high performance.\n4.3.1. AbdomenCT-1K dataset\nQuantitative results. To demonstrate the superior performance of our\nalgorithm, we conducted a comparative analysis with several state-\nof-the-art segmentation algorithms using the AbdomenCT-1K dataset.\nWe compared our algorithm with ten different cutting-edge algorithms,\nnamely 3D U-Net [6], Att-Net [14], NL-Net [19], ECA-Net [17], Eff-\nSegNet [\n34], CBAM-Net [18], GC-Net [28], DA-Net [29], CP-Net [35],\nTI-model [24], Swinunetr [36], and nnFormer [37]. All of the baseline\nmodels were sourced from their original implementations and evalu-\nated on the same datasets under consistent experimental conditions.\nThe quantitative segmentation results obtained from this comparison\nare presented in \nTable 1.\nBased on the experimental results presented this table, it is evident\nthat all networks achieved relatively high DSC and NSD scores for the\nliver, kidney, and spleen segmentation tasks. It achieved an average\nDSC of 94.8% and an NSD of 86.6%, demonstrating the superior perfor-\nmance of our algorithm in accurately segment these organs. Especially,\nour method exhibits a more significant advantage compared to previous\nmethods in pancreas segmentation, surpassing the 3D U-Net by nearly\n10% and 12% in terms of DSC and NSD, respectively. Additionally, it\noutperformed the second-best method (EfficientSegNet) about 1 2% in\nterms of DSC and NSD in Pancreas. The only exception is the spleen,\nwhich exhibited a DSC value that is 0.1% lower than that of Swin-\nunetr. This indicates that our proposed strategy of using weak-prior\ninformation to guide attention effectively improves the segmentation\nperformance of small organs.\nFurthermore, we utilize box plots to visualize and compare the seg-\nmentation results of different algorithms. Box plots provide an intuitive\nrepresentation of the central tendency, dispersion, and presence of out-\nliers in the data. Fig. 5 illustrates the use of box plots for this purpose.\nIt presents the segmentation performance using average DSC scores\nand average NSD scores for all organs, denoted as Average-DSC and\nAverage-NSD, respectively. To show the effectiveness of our method\nin small organ segmentation, we additionally provide the average DSC\nand NSD cores specifically for the pancreas, labeled as Pancreas-DSC\nand Pancreas-NSD, respectively.\nFrom Fig. 5, we can see that the proposed method exhibits sig-\nnificant advantages in terms of both median and mean compared to\nthe majority of other methods. This indicates that our method overall\nperforms better, which aligns with the conclusions drawn from Table 1.\nAlthough EffSegNet and CP-Net are close to our method in terms of\nthese metrics, our method has fewer outliers and a more concentrated\ndistribution curve, suggesting that our segmentation method is more\nstable and robust across all different CT cases. Even for the segmen-\ntation of the small and irregular pancreas, our method consistently\noutperforms other state-of-the-art methods.\nQualitative results. Visual comparison of different models on two slices\nis shown in \nFig. 6. As all the models achieve high and close DSC and\nNSD scores in the segmentation results of the liver, kidney and spleen\norgans, these models perform similarly across three organs. However,\nour method exhibits a significant advantage in the segmentation results\nof the small organ pancreas. Other methods often suffer from either\nunder-segmentation or over-segmentation issues in pancreas, as ob-\nserved in cases â€˜â€˜test_00071_0000â€™â€™ and â€˜â€˜test_00234_0000â€™â€™. In contrast,\nour method consistently produces segmentation results that are closest\nto ground truth values.\n4.3.2. AMOS2022 dataset\nQuantitative results. To ensure a fair and comprehensive evaluation of\nour algorithmâ€™s performance and to further demonstrate the generaliza-\ntion ability of our proposed method, we conducted experiments on the\nAMOS2022 dataset further. The quantitative segmentation results are\npresented in Table 2. Our method continues to demonstrate a substan-\ntial advantage over other methods, such as Swinunetr and nnFormer.\nFor instance, our approach achieved an average DSC of 91.6% and NSD\nof 83.2%, while the 3D U-Net only attained an average DSC of 83.0%\nand NSD of 67.7%, and the second-best method (EffSegNet) attained\n90.9% average DSC and 82.1% average NSD. Furthermore, due to the\ncomplexity of this dataset, the segmentation results of all methods in\nTable 2 are relatively lower compared to those in Table 1.\nHowever, our method exhibits the smallest decrease in performance.\nFor example, the result of our method degrade of average DSC is\n3.2% (from 94.8% to 91.6%) and average NSD is 3.4% (from 86.6%\nto 83.2%), while the result of 3D U-Net degrade of average DSC is\n8.6% (from 91.6% to 83.0%) and average NSD is 11.7% (from 79.4%\nto 67.7%), and the second-best method (EffSegNet) are 3.6% (from\n94.5% to 90.9%) in average DSC and 3.8% (from 85.9% to 82.1%)\nin average NSD. This indicates that our approach maintains better\nsegmentation even in complex datasets and further demonstrates the\nstrong generalization ability of our method.\nThe box plots of the segmentation results of all algorithms are\nshown in Fig. 7. Similar to the trends depicted in Fig. 5, our method\nconsistently achieves the highest median and mean values across all\n\nPattern Recognition 162 (2025) 111368\n8\nS. Zheng et al.\nTable 1\nSegmentation results of state-of-the-art methods for multi-organ abdominal segmentation on the AbdomenCT-1K dataset. The best results are shown in bold black.\nNetworks Liver Kidney Spleen Pancreas Average\nDSC (%) NSD (%) DSC (%) NSD (%) DSC (%) NSD (%) DSC (%) NSD (%) DSC (%) NSD (%)\n3D U-Net 96.8 Â± 3.3 82.7 Â± 12.9 95.7 Â± 5.4 86.9 Â± 11.2 95.8 Â± 11.5 91.8 Â± 13.5 76.4 Â± 14.8 59.2 Â± 17.5 91.2 80.2\nAtt-Net 96.5 Â± 3.9 81.5 Â± 13.9 95.4 Â± 5.4 86.5 Â± 11.7 97.0 Â± 5.9 93.5 Â± 8.2 77.6 Â± 14.5 59.6 Â± 17.4 91.6 80.3\nCBAM-Net 97.4 Â± 1.8 84.6 Â± 12.4 96.3 Â± 2.1 88.8 Â± 7.5 97.4 Â± 2.4 94.3 Â± 6.6 82.6 Â± 10.0 65.0 Â± 15.7 93.4 83.2\nECA-Net 97.8 Â± 1.1 87.9 Â± 8.0 96.8 Â± 1.7 91.2 Â± 6.0 97.9 Â± 1.0 95.8 Â± 5.3 85.7 Â± 7.4 69.8 Â± 14.8 94.5 86.2\nNL-Net 97.1 Â± 1.9 83.0 Â± 11.5 95.5 Â± 5.7 86.6 Â± 12.6 97.2 Â± 2.0 93.0 Â± 7.0 79.3 Â± 12.6 61.0 Â± 16.2 92.3 80.9\nEffSegNet 97.9 Â± 1.1 88.2 Â± 8.0 96.5 Â± 2.0 89.5 Â± 7.0 97.9 Â± 1.4 95.6 Â± 6.0 85.8 Â± 7.8 70.5 Â± 14.7 94.5 85.9\nGC-Net 97.3 Â± 1.6 83.5 Â± 12.2 96.1 Â± 2.9 88.6 Â± 9.0 97.3 Â± 2.3 93.5 Â± 7.2 79.2 Â± 12.6 60.7 Â± 16.0 92.5 81.6\nDA-Net 97.0 Â± 1.9 82.3 Â± 8.5 95.8 Â± 2.9 87.0 Â± 8.1 95.3 Â± 12.0 90.9 Â± 13.8 80.6 Â± 9.2 61.0 Â± 13.8 92.2 80.3\nCP-Net 97.7 Â± 0.9 86.0 Â± 7.7 95.9 Â± 3.2 87.4 Â± 8.2 97.8 Â± 1.1 95.0 Â± 5.6 84.9 Â± 7.7 67.9 Â± 14.8 94.1 84.1\nTI-model 96.8 Â± 2.4 81.7 Â± 9.0 95.6 Â± 3.4 86.2 Â± 8.2 96.0 Â± 7.9 91.8 Â± 10.6 79.8 Â± 10.1 60.7 Â± 14.5 92.0 80.1\nSwinunetr 97.5 Â± 2.7 87.4 Â± 9.3 96.5 Â± 2.2 89.8 Â± 7.3 98.0 Â± 1.1 95.9 Â± 5.3 84.3 Â± 10.2 69.1 Â± 15.4 94.1 85.5\nnnFormer 97.2 Â± 2.2 84.5 Â± 8.7 95.4 Â± 3.3 86.8 Â± 7.8 97.6 Â± 1.1 94.7 Â± 5.7 82.7 Â± 10.1 65.6 Â± 15.0 93.2 82.9\nOurs 98.1 Â± 0.9 88.8 Â± 7.3 96.9 Â± 1.0 91.5 Â± 5.2 97.9 Â± 0.7 95.8 Â± 3.0 86.3 Â± 7.0 71.1 Â± 14.0 94.8 86.8\nFig. 5. Boxplots of the Average-DSC, Average-NSD, Pancreas-DSC and Pancreas-NSD for different methods on AbdomenCT-1K dataset. The purple horizontal line, blue circles, and\nred plus signs represent the median, mean, and outliers of the segmentation results, respectively. The curve on the right side represents the normal distribution of the segmentation\nresults. (For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article.)\nfour metrics compared to other methods. Moreover, our approach\ndemonstrates fewer outliers and a more concentrated distribution curve,\nwhich further validates its superior performance and robustness. Our\nproposed algorithm achieves the highest DSC and NSD values compared\nto all other tested methods on AMOS2022 dataset.\n4.3.3. Synapse dataset\nTo further validate the effectiveness and competitiveness of our\nproposed method in multi-organ segmentation tasks, we are conducting\nexperiments on the Synapse multi-organ segmentation dataset. Results\non metric DSC and NSD for Liver, Kidney, Spleen and Pancreas are\npresented in \nTable 3. Specifically, for the Liver, our method achieves a\nDSC of 95.4% and an NSD of 72.6%, outperforming all other methods\nlisted in the table. For the Kidney, although Swinunetr and GC-Net\nexhibit slightly higher performance, our method still ranks among the\ntop, with a DSC of 90.7% and an NSD of 78.2%. Notably, while the pro-\nposed method shows slightly decreased performance on the Kidney and\nSpleen compared to Swinunetr and GC-Net, our method achieves first\nranking with DSC of 84.6% and an NSD of 66.6% among all compared\nmethods. This demonstrates the effectiveness and competitiveness of\nour proposed method in multi-organ segmentation tasks, particularly\nin the pancreas segmentation task.\nIt should be noted that the results of the Synapse dataset are inferior\nto those of AbdomenCT-1K and AMOS2022. This may be due to the\nlimited number of datasets available for Synaps, which leads to inad-\nequate model training and consequently sub-optimal test performance\n(see \nTable 3).\n\nPattern Recognition 162 (2025) 111368\n9\nS. Zheng et al.\nFig. 6. Examples of segmentation results of the all methods on AbdomenCT-1K dataset. The red, yellow, green, blue regions denote the liver, pancreas, kidney and spleen,\nrespectively. For one test image, the segmentation results from top left to bottom right are displayed for Ground Truth, 3D U-Net, Att-Net, NL-Net, ECA-Net, EffSegNet, CBAM-Net,\nGC-Net, DA-Net, CP-Net, TI-model, Swinunetr, nnFormer, as well as our method. (For interpretation of the references to color in this figure legend, the reader is referred to the\nweb version of this article.)\nTable 2\nSegmentation results of state-of-the-art methods for multi-organ abdominal segmentation on the AMOS2022 dataset. The best results are shown in bold black.\nNetworks Liver Kidney Spleen Pancreas Average\nDSC (%) NSD (%) DSC (%) NSD (%) DSC (%) NSD (%) DSC (%) NSD (%) DSC (%) NSD (%)\n3D U-Net 94.4 Â± 3.3 73.6 Â± 9.4 91.7 Â± 5.3 79.7 Â± 10.3 85.2 Â± 16.9 72.8 Â± 22.0 60.7 Â± 17.0 45.0 Â± 15.1 83.0 67.7\nAtt-Net 95.4 Â± 2.7 77.6 Â± 8.1 93.2 Â± 3.1 84.2 Â± 7.7 88.2 Â± 14.7 76.7 Â± 18.0 57.8 Â± 24.0 46.5 Â± 17.8 83.7 71.3\nCBAM-Net 95.3 Â± 2.5 77.3 Â± 8.0 93.2 Â± 2.9 83.4 Â± 7.9 92.2 Â± 8.8 82.9 Â± 13.5 69.9 Â± 15.0 55.9 Â± 15.9 87.6 74.9\nECA-Net 96.8 Â± 1.1 84.8 Â± 5.3 93.3 Â± 1.8 89.1 Â± 5.1 95.0 Â± 3.1 88.5 Â± 8.7 77.9 Â± 11.0 65.8 Â± 14.1 90.7 82.0\nNL-Net 95.6 Â± 1.5 77.4 Â± 6.6 93.6 Â± 2.7 85.0 Â± 8.3 90.5 Â± 11.8 78.9 Â± 15.4 58.9 Â± 17.0 45.8 Â± 15.4 84.6 71.8\nEffSegNet 96.8 Â± 0.8 84.9 Â± 4.7 93.9 Â± 7.3 89.3 Â± 8.3 94.6 Â± 4.6 87.9 Â± 10.0 78.3 Â± 10.6 66.3 Â± 13.5 90.9 82.1\nGC-Net 95.2 Â± 2.2 76.8 Â± 8.0 93.6 Â± 3.1 85.5 Â± 9.1 91.5 Â± 6.0 79.3 Â± 13.7 66.9 Â± 12.4 51.5 Â± 13.5 86.8 73.3\nDA-Net 94.5 Â± 3.7 74.4 Â± 10.0 93.2 Â± 2.8 83.3 Â± 8.8 89.1 Â± 10.8 75.8 Â± 16.5 63.1 Â± 17.3 48.1 Â± 15.8 85.0 70.4\nCP-Net 96.1 Â± 1.8 81.7 Â± 6.9 94.2 Â± 3.9 88.4 Â± 6.2 92.4 Â± 11.6 84.9 Â± 13.9 75.9 Â± 9.5 61.0 Â± 11.9 89.6 79.0\nTI-model 94.4 Â± 3.4 73.3 Â± 9.7 93.3 Â± 2.8 84.1 Â± 8.8 88.6 Â± 10.8 75.7 Â± 17.4 63.4 Â± 16.3 48.7 Â± 15.3 84.9 70.4\nSwinunetr 96.1 Â± 2.2 82.3 Â± 7.7 92.7 Â± 12.1 88.1 Â± 11.5 90.9 Â± 17.2 83.8 Â± 18.4 71.6 Â± 18.4 59.8.1 Â± 18.9 87.8 78.5\nnnFormer 95.5 Â± 1.6 78.2 Â± 6.6 91.3 Â± 12.8 84.2 Â± 12.0 91.4 Â± 15.6 82.6 Â± 16.3 69.7 Â± 19.3 56.2 Â± 17.6 87.0 75.3\nOurs 96.8 Â± 1.0 85.2 Â± 5.1 95.1 Â± 1.2 90.6 Â± 4.5 95.8 Â± 5.7 88.6 Â± 10.6 79.6 Â± 8.1 68.3 Â± 10.4 91.9 83.2\n5. Ablation analysis\nIn order to demonstrate the individual contributions of each novel\nimproved designs to the segmentation performance, we conducted\nablation experiments with different configurations on two datasets. A-\nUNet, LDA and WPA are used to denote the asymmetric U-Net, the\nlightweight dual attention module and the weak-prior aware module,\nrespectively. The result of the ablation analysis is shown in Table 4.\nThe effectiveness of the asymmetric U-Net: To explore the con-\ntribution of the asymmetric U-Net architecture, we conducted four\nexperiments: NO. 1 (3D U-Net) and NO. 2 (A-UNet) on AbdomenCT-\n1K dataset and No. 6 (3D-UNet) and No. 7 (A-UNet) in Table 4 on\nAMOS2022 dataset. The results reveal that the improved designs in the\nasymmetric U-Net have to some degree contributed to the performance,\nsuch as the asymmetric encoderâ€“decoder architecture and anisotropic\nresidual module.\nThe effectiveness of the lightweight dual attention module: We\nevaluated the effectiveness of the lightweight dual attention module\nin the proposed network through experiments No. 3 (A-UNet + LDA)\nand No. 8 (A-UNet + LDA). As indicated in Table 4, the incorporation\nof LDA resulted in a significant performance improvement compared\nto experiments No. 2 and No. 7. This indicates that LDA effectively\ncaptures distant spatial context information and enhances the feature\nrepresentation for pixel-level prediction.\nThe effectiveness of the weak-prior aware module: Similarly, we\nvalidated the guiding significance of the proposed weak-prior and the\n\nPattern Recognition 162 (2025) 111368\n10\nS. Zheng et al.\nFig. 7. Boxplots of the Average-DSC, Average-NSD, Pancreas-DSC and Pancreas-NSD for different methods on AMOS2022 dataset. The purple horizontal line, blue circles, and red\nplus signs represent the median, mean, and outliers of the segmentation results, respectively. The curve on the right side represents the normal distribution of the segmentation\nresults. (For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article.)\nTable 3\nSegmentation results of state-of-the-art methods for multi-organ abdominal segmentation on the Synapse dataset. The best results are shown in bold black.\nNetworks Liver Kidney Spleen Pancreas Average\nDSC (%) NSD (%) DSC (%) NSD (%) DSC (%) NSD (%) DSC (%) NSD (%) DSC (%) NSD (%)\n3D U-Net 85.7 Â± 14.4 53.8 Â± 10.5 89.0 Â± 2.3 68.9 Â± 8.8 71.6 Â± 28.9 51.4 Â± 20.4 44.7 Â± 17.7 26.2 Â± 11.1 72.7 50.1\nAtt-Net 92.4 Â± 2.0 60.6 Â± 4.5 87.4 Â± 2.9 66.7 Â± 7.5 78.7 Â± 19.2 61.2 Â± 22.1 49.9 Â± 16.4 29.1 Â± 11.4 77.1 54.4\nCBAM-Net 76.9 Â± 34.4 50.7 Â± 23.0 89.7 Â± 4.0 73.8 Â± 10.0 77.0 Â± 30.5 65.7 Â± 26.1 53.2 Â± 17.4 33.7 Â± 12.7 74.2 56.0\nECA-Net 85.9 Â± 14.7 52.4 Â± 10.6 89.4 Â± 2.8 70.9 Â± 7.6 70.9 Â± 28.5 47.9 Â± 20.2 48.4 Â± 14.6 26.7 Â± 9.1 73.6 49.5\nNL-Net 77.7 Â± 34.8 53.5 Â± 24.1 88.1 Â± 2.8 69.7 Â± 5.9 73.0 Â± 31.7 55.4 Â± 25.7 46.1 Â± 15.5 26.7 Â± 10.0 71.3 51.3\nEffSegNet 94.6 Â± 1.5 70.2 Â± 6.5 90.4 Â± 4.8 77.9 Â± 12.1 81.2 Â± 26.8 69.1 Â± 24.5 68.8 Â± 24.5 46.0 Â± 13.4 83.8 65.9\nGC-Net 93.9 Â± 1.6 65.5 Â± 6.1 89.9 Â± 3.8 72.1 Â± 11.9 84.4 Â± 9.5 59.7 Â± 19.8 52.5 Â± 22.6 34.2 Â± 14.4 80.2 57.9\nDA-Net 86.3 Â± 14.3 55.5 Â± 12.1 87.2 Â± 3.5 64.9 Â± 8.8 73.6 Â± 26.2 52.3 Â± 19.9 52.2 Â± 15.1 30.6 Â± 10.8 74.8 50.9\nCP-Net 92.0 Â± 1.4 58.0 Â± 5.8 86.8 Â± 4.7 68.2 Â± 7.1 75.8 Â± 28.1 58.1 Â± 22.9 55.7 Â± 15.3 32.4 Â± 11.4 77.2 54.2\nTI-model 94.7 Â± 1.1 70.0 Â± 4.6 90.1 Â± 5.3 75.8 Â± 10.8 77.9 Â± 20.2 65.5 Â± 26.6 68.6 Â± 10.9 45.0 Â± 12.5 82.8 64.0\nSwinunetr 94.2 Â± 1.3 68.0 Â± 4.8 91.9 Â± 1.9 79.3 Â± 6.6 80.8 Â± 15.1 63.4 Â± 14.1 64.4 Â± 14.3 44.1 Â± 14.5 82.8 63.7\nnnFormer 92.3 Â± 1.9 59.8 Â± 4.7 83.4 Â± 7.8 63.1 Â± 13.9 76.7 Â± 31.8 62.3 Â± 25.4 46.4 Â± 21.9 30.3 Â± 14.9 74.7 53.9\nOurs 95.4 Â± 0.8 72.6 Â± 3.5 90.7 Â± 5.9 78.2 Â± 9.4 83.5 Â± 17.2 69.4 Â± 19.4 68.9 Â± 11.8 46.4 Â± 16.9 84.6 66.6\nTable 4\nThe ablation analysis results of different network architecture on two datasets. A-UNet, LDA and WPA are used to denote the asymmetric U-Net, the lightweight dual attention\nmodule and the weak-prior aware module, respectively. The best results are shown in bold black.\nDatasets Networks Liver Kidney Spleen Pancreas Average\nDSC (%) NSD (%) DSC (%) NSD (%) DSC (%) NSD (%) DSC (%) NSD (%) DSC (%) NSD (%)\nAbdomen\nCT-1K\n(NO.1) 3D U-Net 96.8 Â± 3.3 82.7 Â± 12.9 95.7 Â± 5.4 86.9 Â± 11.2 95.8 Â± 11.5 91.8 Â± 13.5 76.4 Â± 14.8 59.2 Â± 17.5 91.2 80.2\n(NO.2) A-UNet 97.5 Â± 1.0 85.1 Â± 7.5 96.0 Â± 1.8 87.3 Â± 7.1 96.6 Â± 9.8 93.6 Â± 11.2 84.8 Â± 7.6 68.3 Â± 14.4 93.7 83.6\n(NO.3) A-UNet+LDA 97.9 Â± 1.0 88.7 Â± 8.0 96.7 Â± 1.9 90.1 Â± 6.9 97.8 Â± 1.6 95.5 Â± 3.0 86.3 Â± 7.2 70.9 Â± 14.1 94.7 86.3\n(NO.4) A-UNet+WPA 97.8 Â± 1.1 87.9 Â± 8.0 96.8 Â± 2.1 91.2 Â± 6.0 97.9 Â± 1.4 95.7 Â± 5.3 85.7 Â± 7.4 69.8 Â± 14.8 94.5 86.2\n(NO.5) A-UNet+LDA+WPA 98.1 Â± 0.9 88.8 Â± 7.3 96.9 Â± 1.0 91.5 Â± 5.2 97.9 Â± 0.7 95.8 Â± 3.0 86.3 Â± 7.0 71.1 Â± 14.0 94.8 86.8\nAMOS\n2022\n(NO.6) 3D U-Net 94.4 Â± 3.3 73.6 Â± 9.4 91.7 Â± 5.3 79.7 Â± 10.3 85.2 Â± 16.9 72.8 Â± 22.0 60.7 Â± 17.0 45.0 Â± 15.1 83.0 67.7\n(NO.7) A-UNet 96.6 Â± 1.2 83.9 Â± 5.3 93.0 Â± 12.3 88.5 Â± 10.8 94.2 Â± 5.6 86.6 Â± 11.4 76.4 Â± 11.2 63.2 Â± 14.2 90.1 80.5\n(NO.8) A-UNet+LDA 96.8 Â± 0.9 85.2 Â± 5.0 95.1 Â± 2.1 90.5 Â± 4.5 94.7 Â± 5.7 88.4 Â± 10.5 78.9 Â± 9.1 67.2 Â± 12.1 91.4 82.9\n(NO.9) A-UNet+WPA 96.6 Â± 1.1 83.3 Â± 5.3 94.9 Â± 1.1 89.6 Â± 4.4 94.6 Â± 4.4 87.5 Â± 9.2 78.1 Â± 9.2 65.9 Â± 12.2 91.0 81.6\n(NO.10) A-UNet+LDA+WPA 96.8 Â± 1.0 85.2 Â± 5.1 95.1 Â± 1.2 90.6 Â± 4.5 95.8 Â± 5.7 88.6 Â± 10.6 79.6 Â± 8.1 68.3 Â± 10.4 91.9 83.2\n\nPattern Recognition 162 (2025) 111368\n11\nS. Zheng et al.\nTable 5\nThe ablation analysis results of different modules on two datasets. The best results are shown in bold black fonts.\nDatasets Networks Liver Kidney Spleen Pancreas Average\nDSC (%) NSD (%) DSC (%) NSD (%) DSC (%) NSD (%) DSC (%) NSD (%) DSC (%) NSD (%)\nAbdomen\nCT-1K\n(NO.11) A-UNet+ECA 97.9 Â± 1.1 88.0 Â± 7.9 96.9 Â± 1.3 91.3 Â± 6.0 97.9 Â± 0.8 95.8 Â± 4.0 85.9 Â± 6.9 69.9 Â± 14.8 94.7 86.3\n(NO.12) A-UNet+GC 97.8 Â± 1.2 88.2 Â± 7.8 96.4 Â± 2.9 89.1 Â± 7.4 97.6 Â± 1.4 95.2 Â± 5.9 85.0 Â± 8.5 69.4 Â± 15.2 94.2 85.5\n(NO.13) A-UNet+DA 97.5 Â± 1.5 87.8 Â± 7.9 96.3 Â± 1.7 89.2 Â± 6.9 97.6 Â± 2.3 95.1 Â± 6.5 84.9 Â± 7.9 69.1 Â± 14.5 94.1 85.3\n(NO.14) A-UNet+CP 97.9 Â± 1.0 88.1 Â± 7.9 96.7 Â± 1.9 89.3 Â± 7.0 97.8 Â± 1.4 95.5 Â± 5.7 85.6 Â± 7.6 70.1 Â± 14.7 94.5 85.7\n(NO.15) A-UNet+TI 98.0 Â± 1.0 88.2 Â± 7.9 96.7 Â± 1.7 90.0 Â± 6.7 97.8 Â± 1.2 95.4 Â± 5.9 85.8 Â± 7.6 70.3 Â± 14.9 94.5 86.0\n(NO.5)A-UNet+AP 98.1 Â± 0.9 88.8 Â± 7.3 96.9 Â± 1.0 91.5 Â± 5.2 97.9 Â± 0.7 95.8 Â± 3.0 86.3 Â± 7.0 71.1 Â± 14.0 94.8 86.8\nAMOS\n2022\n(NO.16) A-UNet+ECA 96.8 Â± 1.3 84.9 Â± 5.8 94.9 Â± 1.8 89.8 Â± 5.0 95.5 Â± 5.9 88.5 Â± 6.7 78.3 Â± 9.2 67.8 Â± 14. 91.4 82.8\n(NO.17) A-UNet+GC 96.8 Â± 1.3 84.5 Â± 4.6 94.9 Â± 1.4 89.8 Â± 5.3 94.4 Â± 5.1 86.7 Â± 11.0 77.2 Â± 10.3 64.8 Â± 12.6 90.8 81.4\n(NO.18) A-UNet+DA 96.5 Â± 1.2 83.7 Â± 5.2 94.4 Â± 3.2 89.2 Â± 6.3 94.5 Â± 4.5 87.3 Â± 9.8 76.9 Â± 11.1 64.0 Â± 13.8 90.6 81.0\n(NO.19) A-UNet+CP 96.7 Â± 0.8 84.5 Â± 5.1 95.0 Â± 1.3 90.0 Â± 5.0 93.4 Â± 8.3 85.8 Â± 13.3 77.3 Â± 9.9 64.1 Â± 13.1 90.6 81.1\n(NO.20) A-UNet+TI 96.6 Â± 1.2 83.1 Â± 5.4 94.8 Â± 1.4 89.4 Â± 5.4 93.9 Â± 6.9 86.3 Â± 12.4 77.7 Â± 9.9 65.0 Â± 12.7 90.7 81.0\n(NO.10) A-UNet+AP 96.8 Â± 1.0 85.2 Â± 5.1 95.1 Â± 1.2 90.6 Â± 4.5 95.8 Â± 5.7 88.6 Â± 10.6 79.6 Â± 8.1 68.3 Â± 10.4 91.9 83.2\neffectiveness of the weak-prior aware module through experiments No.\n4 (A-UNet + WPA) and No. 9 (A-UNet + WPA). As shown in \nTable 4,\nthe incorporation of WPA also resulted in a significant performance\nimprovement in DSC and NSD for different organs compared to A-UNe\non both AbdomenCT-1K and AMOS2022 datasets.\nThe effectiveness of the weak-prior dual cognitive attention: Ad-\nditionally, experiments No. 5 (A-UNet + LDA + WPA) and No. 10\n(A-UNet + LDA + WPA) confirmed the feasibility and effectiveness\nof combining weak-prior with attention mechanisms. These three key\ncomponents make up our final segmentation model. As mentioned\nabove, A-UNet, LDA and WPA have made contribution on segmentation\nperformance individually. Especially, the last 2 columns in Tables 4\ndemonstrate that the proposed method increases accuracy by around\n2 percentage points in the average DSC and NSD. These demonstrate\nthat combination of these modules can further improve the organ\nsegmentation performance due to the novel design of our framework\nand modules. Owe to the novelly designed weak-prior dual cognitive\nattention, the segmentation network can handle complex abdominal\nmulti-organ segmentation difficulties.\nTo further evaluate the performance of our proposed modules, we\nconducted an ablation study comparing them to other spatial infor-\nmation aggregation modules (ECA, GC and DA) and prior information\nmodules (CP and TI) from previous works. ECA refers to the efficient\nchannel attention module in ECANet, which significantly improves per-\nformance while adding few parameters. GC denotes the global context\nmodeling framework in GCNet, effectively capturing long-range de-\npendencies with reduced computation. DA indicates the dual-attention\nnetwork in DANet, enhancing discriminative feature representations for\nsegmentation via self-attention. CP represents the context prior layer\nand affinity loss in CPNet for directly supervising prior learning. TI\nencodes topological interactions into networks using the module from\nTopological Interactions. AP denotes our lightweight dual attention and\nweak-prior aware modules. To compare segmentation performance of\ndifferent modules fairly, we used our proposed Asymmetric U-Net as\nthe baseline and incorporated each module separately for experiments.\nThe ablation results are shown in Table 5.\nFrom Table 5, it is evident that experiment No. 5 (A-UNet + AP)\noutperforms experiments No. 11 (A-UNet + ECA) to No. 15 (A-UNet\n+ TI) in terms of overall segmentation results, particularly for the\nsmall organ pancreas. Experiments No. 16 (A-UNet + ECA) to No. 20\n(A-UNet + TI), in this series of experiments, demonstrate the effec-\ntiveness of the compared attention-based methods and prior-guided\napproaches. However, their results are lower than those of Experiment\nNo. 10 (A-UNet + AP), further highlighting the advantages of our\nproposed lightweight dual attention module and the weak-prior aware\nmodule.\n6. Discussion\n6.1. Why â€˜â€˜weakâ€™â€™ prior\nTo address the challenges of complex network structure and ex-\ntensive anatomical information processing in ASOR, we proposed a\nnovel improvement model called weak-prior dual attention lightweight\nnetwork to simulate the cognitive attention process of human beings\nwhen selectively focuses on certain objects, for abdominal multi-organ\nsegmentation tasks.\nâ€˜â€˜Strongâ€™â€™ priors, such as detailed shapes and exact positions, provide\nprecise anatomical information, enhancing segmentation accuracy for\nspecific organs. They can effectively delineate organ boundaries and\nmanage complex particular structures. However, these strong priors,\nsuch as mutual positional relationships and shape similarities, are typi-\ncally tailored to specific datasets, which restricts their generalization to\nnew or diverse datasets. Furthermore, modeling and integrating them\nrequires substantial computational resources and extensive anatomical\nknowledge. In our study, we define â€˜â€˜weakâ€™â€™ priors as less stringent and\nmore general prior information, such as organ relevance and volume\nratios only using label information, rather than detailed anatomical\nshapes or precise positional relationships. The term â€˜â€˜weakâ€™â€™ is used to\nindicate that these priors are not as specific or detailed as â€˜â€˜strongâ€™â€™\npriors, which may include exact shapes or precise locations of organs.\nThe main advantages of weak priors are threefold.\nFirstly, Weak priors, such as organ relevance and volume ratios, are\napplicable across diverse patients and imaging conditions, capturing\ncommon organ characteristics without overly specific details. Secondly,\nWeak priors can be easily integrated into deep learning models with-\nout extensive anatomical knowledge or complex preprocessing. Lastly,\nWeak priors lessens the computational burden since detailed modeling\nof organ shapes or positions is unnecessary.\nHowever, weak priors may overlook fine-grained anatomical details,\nwhich can be important for certain segmentation tasks, and may not\nfully account for individual anatomical variations. To handle individual\nvariations, the model employs attention mechanisms and weak prior\naware module. For attention mechanisms, the dual cognitive attention\nmodules dynamically adjust the focus based on the input data, allowing\nthe model to adapt to individual anatomical variations. For weak prior\naware module, by integrating weak priors, the model gains a general\nunderstanding of organ relationships, which can be fine-tuned during\ntraining to account for individual differences.\nIn summary, the weak priors used in our method strike a balance\nbetween universality and computational efficiency while providing\nvaluable guidance for segmentation tasks. This characteristic facili-\ntates our learning and generalization and simplifies the training and\noptimization of the model. Compelling evidence on three abdominal\nmulti-organ datasets shows that this lightweight strategy, combining\nlightweight dual attention with weak prior information makes sig-\nnificant contributions to the improvement of abdominal multi-organ\nsegmentation performance.\n\nPattern Recognition 162 (2025) 111368\n12\nS. Zheng et al.\n6.2. Why asymmetric U-Net\nIn our paper, the proposed Asymmetric U-Net aims to address\nthe challenges of multi-organ segmentation in abdominal CT images,\nfocusing on reducing computational complexity while maintaining high\nsegmentation accuracy. First, the encoder and decoder in Asymmetric\nU-Net are designed asymmetrically to optimize computational effi-\nciency. The encoder uses complex residual convolution blocks to extract\nhigh-level features, while the decoder uses simple residual blocks to\nreduce computational burden. This design choice helps balance feature\nextraction and computational efficiency. Second, in the decoder, we re-\nplace standard 3D convolutions with anisotropic residual blocks. These\nblocks decompose 3D convolutions into combinations of 3 Ã— 3 Ã— 1 and\n1 Ã— 1 Ã— 3 convolutions, significantly reducing the number of parameters\nand computational cost. This approach allows the network to effectively\ncapture spatial information while being computationally efficient. Fi-\nnally, in the decoding phase, we use a â€˜â€˜reduce channels + upsample\n+ addâ€™â€™ operation instead of traditional concatenation. This operation\nuses a 1 Ã— 1 Ã— 1 convolution to halve the number of features, performs\n3D bilinear upsampling, and then adds the result to the corresponding\nencoder output. This method helps reduce computational burden and\nimprove the efficiency of multi-scale feature fusion.\nCompared to Res-UNet, our Asymmetric U-Net builds on residual\nconnections and incorporates asymmetric design, anisotropic resid-\nual blocks, and the â€˜â€˜reduce channels + upsample + addâ€™â€™ operation.\nThrough these designs, Asymmetric U-Net optimizes computational\nefficiency and feature fusion strategies while maintaining high seg-\nmentation accuracy. This allows the network to provide efficient and\naccurate segmentation results in complex abdominal CT images, even\nwith limited computational resources.\n6.3. Computational complexity and efficiency analysis\nThe complexity-performance trade-off is a critical aspect in design-\ning efficient neural networks. To achieve it, we have specially designed\nthe model using asymmetric U-Net architecture, anisotropic residual\nblocks in the decoder, lightweight dual attention module, and weak\nprior aware module.\nTo verify the effectiveness of our approach in terms of performance\nand complexity trade-offs, we conducted the relevant comparative\nexperiments on three datasets, and the experimental results are shown\nin Table 1, 2 and 3. Additionally, Fig. 1 illustrates the comparison of\nsegmentation performance and complexity across different methods. In\nthe figure, the same color represents the same comparison method,\nwith circles representing results from the AbdomenCT-1K dataset and\nsquares from the AMOS2022 dataset. The size of each ball and square\nrepresents the modelâ€™s Params. As evident from the experimental re-\nsults, our method achieves best accuracy of 94.8%, 91.9% and 84.6%\nfor DCS, and 86.8%, 83.2% and 66.6% for NSD, on the AbdomenCT-1K,\nAMOS2022 and Synapse datasets respectively. Additionally, it demon-\nstrates superior FLOPs performance with 157.9 GB, achieved with a\ntrade-off of only 3.8M parameters in terms of performance and com-\nplexity compared to previous state-of-the-art methods. These results\nmake our approach more valuable for clinical applications.\n7. Conclusions\nIn conclusion, we propose a novel weak-prior dual attention\nlightweight network to enhance the performance of abdominal multi-\norgan segmentation. It applies a lightweight strategy of combining\nattention with prior information to better simulate human cognitive\nattention. The proposed network includes an asymmetric U-Net back-\nbone network, a lightweight dual attention module, and a weak-prior\naware module. Based on extensive experiments conducted on two\ndatasets, this proposed network demonstrates superior performance in\nthe segmentation of abdominal multi-organs, particularly in accurately\ndelineating the small and irregular pancreas.\nThe proposed method typically involves first segmenting the target\norgan region, followed by a more refined segmentation. Although this\napproach can improve segmentation accuracy, it comes with higher\ncomputational costs and is less efficient when processing large amounts\nof data. In the future, we commit to collaborative interpretable prior\nand semi-supervised segmentation methods [38] to improve the prac-\ntical performance of the model.\nCRediT authorship contribution statement\nShenhai Zheng: Writing â€“ review & editing, Writing â€“ original\ndraft, Validation, Project administration, Formal analysis, Conceptual-\nization. Jianfei Li: Writing â€“ review & editing, Writing â€“ original draft,\nMethodology, Formal analysis. Haiguo Zhao: Writing â€“ original draft,\nVisualization, Validation. Weisheng Li: Supervision, Resources, Project\nadministration, Formal analysis. Yufei Chen: Writing â€“ review & edit-\ning, Supervision, Resources, Methodology. Lei Yu: Writing â€“ review &\nediting, Validation, Resources, Funding acquisition, Conceptualization.\nDeclaration of competing interest\nThe authors declare that they have no known competing finan-\ncial interests or personal relationships that could have appeared to\ninfluence the work reported in this paper.\nAcknowledgments\nThis work was supported in part by the National Natural Sci-\nence Foundation of China (No. 61902046, 61901074, 62331008 and\n62221005) and the Science and Technology Research Program of\nChongqing Municipal Education Commission (No. KJZD-K202200606\nand KJQN202300608) and Chongqing Big Data Collaborative Innova-\ntion Center open fund project (No. CQBDCIC202302) and the Natu-\nral Science Foundation of Chongqing (No. 2023NSCQ-LZX0045 and\nCSTB2022NSCQ-MSX0578).\nData availability\nData will be made available on request.\nReferences\n[1] J. Ma, Y. Zhang, S. Gu, C. Zhu, C. Ge, Y. Zhang, X. An, C. Wang, Q. Wang, X.\nLiu, S. Cao, Q. Zhang, S. Liu, Y. Wang, Y. Li, J. He, X. Yang, AbdomenCT-1K:\nIs abdominal organ segmentation a solved problem? IEEE Trans. Pattern Anal.\nMach. Intell. 44 (10) (2022) 6695â€“6714.\n[2] Q. Zhao, L. Zhong, J. Xiao, J. Zhang, Y. Chen, W. Liao, S. Zhang, G. Wang, Effi-\ncient multi-organ segmentation from 3D abdominal CT images with lightweight\nnetwork and knowledge distillation, IEEE Trans. Med. Imaging 42 (9) (2023)\n2513â€“2523.\n[3] J. Liu, Y. Zhang, K. Wang, M.C. Yavuz, X. Chen, Y. Yuan, H. Li, Y. Yang, A.\nYuille, Y. Tang, Z. Zhou, Universal and extensible language-vision models for\norgan segmentation and tumor detection from abdominal computed tomography,\nMed. Image Anal. 97 (2024) 103226.\n[4] A. Saito, S. Nawano, A. Shimizu, Joint optimization of segmentation and shape\nprior from level-set-based statistical shape model, and its application to the\nautomated segmentation of abdominal organs, Med. Image Anal. 28 (2016)\n46â€“65.\n[5] O. Ronneberger, P. Fischer, T. Brox, U-net: Convolutional networks for biomed-\nical image segmentation, in: Medical Image Computing and Computer Assisted\nIntervention, 2015, pp. 234â€“241.\n[6] Ã–. Ã‡iÃ§ek, A. Abdulkadir, S.S. Lienkamp, T. Brox, O. Ronneberger, 3D U-Net:\nlearning dense volumetric segmentation from sparse annotation, in: Medical\nImage Computing and Computer Assisted Intervention, 2016, pp. 424â€“432.\n[7] H. Liu, D. Wei, D. Lu, X. Tang, L. Wang, Y. Zheng, Simultaneous alignment\nand surface regression using hybrid 2Dâ€“3D networks for 3D coherent layer\nsegmentation of retinal OCT images with full and sparse annotations, Med. Image\nAnal. 91 (2024) 103019.\n\nPattern Recognition 162 (2025) 111368\n13\nS. Zheng et al.\n[8] C. Szegedy, S. Ioffe, V. Vanhoucke, A. Alemi, Inception-v4, inception-resnet and\nthe impact of residual connections on learning, in: Proceedings of the AAAI\nConference on Artificial Intelligence, Vol. 31, No. 1, 2017.\n[9] Q. Yan, S. Liu, S. Xu, C. Dong, Z. Li, J.Q. Shi, Y. Zhang, D. Dai, 3D medical\nimage segmentation using parallel transformers, Pattern Recognit. 138 (2023)\n109432.\n[10] G. Chen, L. Li, J. Zhang, Y. Dai, Rethinking the unpretentious U-net for medical\nultrasound image segmentation, Pattern Recognit. 142 (2023) 109728.\n[11] G.W. Lindsay, Attention in psychology, neuroscience, and machine learning,\nFront. Comput. Neurosci. 14 (2020) 29.\n[12] H. Zheng, L. Lin, H. Hu, Q. Zhang, Q. Chen, Y. Iwamoto, X. Han, Y.-W. Chen,\nR. Tong, J. Wu, Semi-supervised segmentation of liver using adversarial learning\nwith deep atlas prior, in: Medical Image Computing and Computer Assisted\nIntervention, 2019, pp. 148â€“156.\n[13] H. Huang, H. Zheng, L. Lin, M. Cai, H. Hu, Q. Zhang, Q. Chen, Y. Iwamoto, X.\nHan, Y.-W. Chen, R. Tong, Medical image segmentation with deep atlas prior,\nIEEE Trans. Med. Imaging 40 (12) (2021) 3519â€“3530.\n[14] O. Oktay, J. Schlemper, L.L. Folgoc, M. Lee, M. Heinrich, K. Misawa, K. Mori,\nS. McDonagh, N.Y. Hammerla, B. Kainz, et al., Attention U-Net: Learning where\nto look for the pancreas, 2018, arXiv preprint arXiv:1804.03999.\n[15] H. Lin, Z. Li, Z. Yang, Y. Wang, Variance-aware attention U-Net for multi-organ\nsegmentation, Med. Phys. 48 (12) (2021) 7864â€“7876.\n[16] J. Hu, L. Shen, G. Sun, Squeeze-and-excitation networks, in: Proceedings of\nthe IEEE Conference on Computer Vision and Pattern Recognition, 2018, pp.\n7132â€“7141.\n[17] Q. Wang, B. Wu, P. Zhu, P. Li, W. Zuo, Q. Hu, ECA-Net: Efficient chan-\nnel attention for deep convolutional neural networks, in: Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern Recognition, 2020, pp.\n11534â€“11542.\n[18] S. Woo, J. Park, J.-Y. Lee, I.S. Kweon, CBAM: Convolutional block attention\nmodule, in: Proceedings of the European Conference on Computer Vision, ECCV,\n2018, pp. 3â€“19.\n[19] Z. Wang, N. Zou, D. Shen, S. Ji, Non-local U-Nets for biomedical image\nsegmentation, in: Proceedings of the AAAI Conference on Artificial Intelligence,\nVol. 34, No. 04, 2020, pp. 6315â€“6322.\n[20] M. Fiaz, M. Noman, H. Cholakkal, R.M. Anwer, J. Hanna, F.S. Khan, Guided-\nattention and gated-aggregation network for medical image segmentation,\nPattern Recognit. 156 (2024) 110812.\n[21] O. Oktay, E. Ferrante, K. Kamnitsas, M. Heinrich, W. Bai, J. Caballero, S.A. Cook,\nA. De Marvao, T. Dawes, D.P. Oâ€˜Regan, et al., Anatomically constrained neural\nnetworks (ACNNs): application to cardiac image enhancement and segmentation,\nIEEE Trans. Med. Imaging 37 (2) (2017) 384â€“395.\n[22] A. Boutillon, B. Borotikar, V. Burdin, P.-H. Conze, Combining shape priors\nwith conditional adversarial networks for improved scapula segmentation in MR\nimages, in: International Symposium on Biomedical Imaging, ISBI, IEEE, 2020,\npp. 1164â€“1167.\n[23] A. BenTaieb, G. Hamarneh, Topology aware fully convolutional networks for\nhistology gland segmentation, in: International Conference on Medical Image\nComputing and Computer-Assisted Intervention, Springer, 2016, pp. 460â€“468.\n[24] S. Gupta, X. Hu, J. Kaan, M. Jin, M. Mpoy, K. Chung, G. Singh, M. Saltz, T. Kurc,\nJ. Saltz, et al., Learning topological interactions for multi-class medical image\nsegmentation, in: European Conference on Computer Vision, Springer, 2022, pp.\n701â€“718.\n[25] D. Ulyanov, A. Vedaldi, V. Lempitsky, Instance normalization: The missing\ningredient for fast stylization, 2016, arXiv preprint arXiv:1607.08022.\n[26] L. Chi, Z. Yuan, Y. Mu, C. Wang, Non-local neural networks with grouped bilinear\nattentional transforms, in: Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, 2020, pp. 11804â€“11813.\n[27] Z. Huang, X. Wang, L. Huang, C. Huang, Y. Wei, W. Liu, CCNet: Criss-\ncross attention for semantic segmentation, in: Proceedings of the IEEE/CVF\nInternational Conference on Computer Vision, 2019, pp. 603â€“612.\n[28] Y. Cao, J. Xu, S. Lin, F. Wei, H. Hu, GCNet: Non-local networks meet squeeze-\nexcitation networks and beyond, in: Proceedings of the IEEE/CVF International\nConference on Computer Vision Workshops, 2019, pp. 1971â€“1980.\n[29] J. Fu, J. Liu, H. Tian, Y. Li, Y. Bao, Z. Fang, H. Lu, Dual attention network for\nscene segmentation, in: Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, 2019, pp. 3146â€“3154.\n[30] Y. Ji, H. Bai, C. Ge, J. Yang, Y. Zhu, R. Zhang, Z. Li, L. Zhanng, W. Ma, X. Wan, et\nal., Amos: A large-scale abdominal multi-organ benchmark for versatile medical\nimage segmentation, Adv. Neural Inf. Process. Syst. 35 (2022) 36722â€“36732.\n[31] N. Lay, N. Birkbeck, J. Zhang, S.K. Zhou, Rapid multi-organ segmentation using\ncontext integration and discriminative models, in: J.C. Gee, S. Joshi, K.M. Pohl,\nW.M. Wells, L. ZÃ¶llei (Eds.), Information Processing in Medical Imaging, IPMI,\n2013, pp. 450â€“462.\n[32] H. Lu, X. Wei, N. Lin, G. Yan, X. Li, Tetris: Re-architecting convolutional neural\nnetwork computation for machine learning accelerators, in: 2018 IEEE/ACM\nInternational Conference on Computer-Aided Design, ICCAD, IEEE, 2018, pp.\n1â€“8.\n[33] C. Xu, K. Rajamani, A. Ferreira, W. Felter, J. Rubio, Y. Li, dCat: Dynamic cache\nmanagement for efficient, performance-sensitive infrastructure-as-a-service, in:\nProceedings of the Thirteenth EuroSys Conference, 2018, pp. 1â€“13.\n[34] F. Zhang, Y. Wang, H. Yang, Efficient context-aware network for abdominal\nmulti-organ segmentation, 2021, arXiv preprint arXiv:2109.10601.\n[35] C. Yu, J. Wang, C. Gao, G. Yu, C. Shen, N. Sang, Context prior for scene\nsegmentation, in: Proceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition, 2020, pp. 12416â€“12425.\n[36] Y. He, V. Nath, D. Yang, Y. Tang, A. Myronenko, D. Xu, SwinUNETR-V2:\nStronger swin transformers with stagewise convolutions for 3D medical image\nsegmentation, in: Medical Image Computing and Computer Assisted Intervention,\n2023, pp. 416â€“426.\n[37] H.-Y. Zhou, J. Guo, Y. Zhang, X. Han, L. Yu, L. Wang, Y. Yu, nnFormer:\nVolumetric medical image segmentation via a 3D transformer, IEEE Trans. Image\nProcess. 32 (2023) 4036â€“4045.\n[38] Z. Zhao, H. Wang, T. Lei, X. Wang, X. Shen, H. Yao, Balanced feature fusion\ncollaborative training for semi-supervised medical image segmentation, Pattern\nRecognit. 157 (2025) 110856.\nShenhai Zheng received his Ph.D. degree from the College of Computer Science,\nChongqing University. He is currently a teacher at the College of Computer Science\nand Technology, Chongqing University of Posts and Telecommunications. The main\nresearch directions are intelligent computing, pattern recognition, image processing and\nmachine vision.\nJianfei Li received a bachelorâ€™s degree in computer science from Chongqing University\nof Technology, China, in 2020. He is currently pursuing the M.S. degree with the\nCollege of Computer Science and Technology, Chongqing University of Posts and\nTelecommunications. His current research interests include computer vision and deep\nlearning.\nHaiguo Zhao received his M.S. degree in Chongqing University of Posts and Telecom-\nmunications, Chongqing, China. His current research interests include image processing,\ncomputer vision, and pattern recognition.\nWeisheng Li received the B.S. degree from the School of Electronics and Mechanical\nEngineering, Xidian University, Xiâ€™an, China, in July 1997, the M.S. degree from\nthe School of Electronics, Xidian University, in July 2000, and the Ph.D. degree in\nmechanical engineering from the School of Computer Science and Technology, Xidian\nUniversity, in July 2004. He is currently a Professor with the Chongqing University\nof Posts and Telecommunications. His research interests include intelligent information\nprocessing and pattern recognition.\nYufei Chen received the Ph.D. degree from Tongji University, Shanghai, China, in\n2010., She was a Guest Researcher with the Fraunhofer Institute for Computer Graphics\nResearch, Darmstadt, Germany, from 2008 to 2009. She is currently an Associate\nProfessor with the CAD Research Center, Tongji University. Her research topics include\nimage processing and data analysis.\nLei Yu is currently serving as a member of the Toxicology Group of the Emergency\nMedicine Committee of the Chongqing Medical Association and as a member of\nthe Emergency Medicine Committee of the Chongqing Health Promotion and Health\nEducation Association. He has been engaged in clinical work in emergency internal\nmedicine for 15 years.",
    "version": "5.3.31"
  },
  {
    "numpages": 11,
    "numrender": 11,
    "info": {
      "PDFFormatVersion": "1.7",
      "Language": "en-US",
      "EncryptFilterName": null,
      "IsLinearized": true,
      "IsAcroFormPresent": false,
      "IsXFAPresent": false,
      "IsCollectionPresent": false,
      "IsSignaturesPresent": false,
      "Creator": "Elsevier",
      "Custom": {
        "CrossMarkDomains[1]": "elsevier.com",
        "CrossmarkMajorVersionDate": "2010-04-23",
        "CreationDate--Text": "8th July 2025",
        "ElsevierWebPDFSpecifications": "7.0.1",
        "robots": "noindex",
        "doi": "10.1016/j.cmpb.2025.108881",
        "CrossMarkDomains[2]": "sciencedirect.com",
        "CrossmarkDomainExclusive": "true"
      },
      "ModDate": "D:20250708120023Z",
      "Author": "Jonghun Kim",
      "Title": "Weakly-supervised segmentation using sparse single point annotations for lumen and wall of carotid arteries in 3D MRI",
      "Keywords": "Carotid artery segmentation,Weakly supervised learning,Sparse annotation,Point annotation",
      "CreationDate": "D:20250708115715Z",
      "Producer": "Acrobat Distiller 8.1.0 (Windows)",
      "Subject": "Computer Methods and Programs in Biomedicine, 269 (2025) 108881. doi:10.1016/j.cmpb.2025.108881"
    },
    "metadata": {
      "crossmark:crossmarkdomains": "elsevier.comsciencedirect.com",
      "crossmark:crossmarkdomainexclusive": "true",
      "crossmark:doi": "10.1016/j.cmpb.2025.108881",
      "crossmark:majorversiondate": "2010-04-23",
      "dc:format": "application/pdf",
      "dc:identifier": "10.1016/j.cmpb.2025.108881",
      "dc:publisher": "Elsevier B.V.",
      "dc:description": "Computer Methods and Programs in Biomedicine, 269 (2025) 108881. doi:10.1016/j.cmpb.2025.108881",
      "dc:subject": [
        "Carotid artery segmentation",
        "Weakly supervised learning",
        "Sparse annotation",
        "Point annotation"
      ],
      "dc:title": "Weakly-supervised segmentation using sparse single point annotations for lumen and wall of carotid arteries in 3D MRI",
      "dc:creator": [
        "Jonghun Kim",
        "Inye Na",
        "Junmo Kwon",
        "Woo-Keun Seo",
        "Hyunjin Park"
      ],
      "jav:journal_article_version": "VoR",
      "pdf:creationdate--text": "8th July 2025",
      "pdf:producer": "Acrobat Distiller 8.1.0 (Windows)",
      "pdf:keywords": "Carotid artery segmentation,Weakly supervised learning,Sparse annotation,Point annotation",
      "pdfx:creationdate--text": "8th July 2025",
      "pdfx:crossmarkdomains": "sciencedirect.comelsevier.com",
      "pdfx:crossmarkdomainexclusive": "true",
      "pdfx:crossmarkmajorversiondate": "2010-04-23",
      "w8arlz9irywmkz.mgz.n7zcnnzdb_lt_8mwmgymj7m9yjown7y9ylo9eqn9iknm-pnmytma": "",
      "pdfx:doi": "10.1016/j.cmpb.2025.108881",
      "pdfx:robots": "noindex",
      "prism:aggregationtype": "journal",
      "prism:copyright": "Â© 2025 Elsevier B.V. All rights are reserved, including those for text and data mining, AI training, and similar technologies.",
      "prism:coverdate": "2025-09-01",
      "prism:coverdisplaydate": "1 September 2025",
      "prism:doi": "10.1016/j.cmpb.2025.108881",
      "prism:issn": "0169-2607",
      "prism:pagerange": "108881",
      "prism:publicationname": "Computer Methods and Programs in Biomedicine",
      "prism:startingpage": "108881",
      "prism:url": "https://doi.org/10.1016/j.cmpb.2025.108881",
      "prism:volume": "269",
      "tdm:policy": "https://www.elsevier.com/tdm/tdmrep-policy.json",
      "tdm:reservation": "1",
      "xmp:createdate": "2025-07-08T11:57:15",
      "xmp:creatortool": "Elsevier",
      "xmp:metadatadate": "2025-07-08T12:00:23",
      "xmp:modifydate": "2025-07-08T12:00:23",
      "xmpmm:documentid": "uuid:1b499bed-4ce8-4c0c-b682-0d58baae1cbe",
      "xmpmm:instanceid": "uuid:b6ed5266-03cb-4855-90c0-636283357f42",
      "xmprights:marked": "True"
    },
    "text": "Contents lists available at ScienceDirect\nComputer Methods and Programs in Biomedicine\njournal homepage: https://www.sciencedirect.com/journal/computer-methods-and-\nprograms-in-biomedicine\nWeakly-supervised segmentation using sparse single point annotations for\nlumen and wall of carotid arteries in 3D MRI\nJonghun Kim \na,b \n, Inye Na \na,b \n, Junmo Kwon \na,b \n, Woo-Keun Seo \nc\n, Hyunjin Park \na,b ,\nâˆ—\na \nDepartment of Electrical and Computer Engineering, Sungkyunkwan University, Suwon, South Korea\nb \nCenter for Neuroscience Imaging Research, Institute for Basic Science, Suwon, South Korea\nc \nDepartment of Neurology, Samsung Medical Center, Seoul, South Korea\nA R T I C L E I N F O\nKeywords:\nCarotid artery segmentation\nWeakly supervised learning\nSparse annotation\nPoint annotation\nA B S T R A C T\nBackground and objective: Segmentation of the carotid artery is a crucial step in planning therapy for\natherosclerosis. Manual annotation is a time-consuming and labor-intensive process, and there is a need to\nreduce this effort.\nMethods: We propose a weakly supervised segmentation method using only a few annotated axial slices, each\nwith a single-point annotation from 3D magnetic resonance imaging for the lumen and wall of the carotid\nartery. The proposed method contains three loss functions designed to (1) locate the center point of the vessel,\n(2) constrain the range of the vessel radius using prior information implemented with spatial maps, and (3)\nencourage similar segmentation results in adjacent slices. Both the lumen (inner structure) and wall (outer\nstructure) can be segmented by adjusting the range of plausible radii.\nResults: Experimental evaluations on the COSMOS2022 dataset show that our method achieved similar\nperformance results (DSC\nğ‘™ğ‘¢ \n0.821 lumen, DSC\nğ‘¤ğ‘ \n0.841 wall) to those of fully supervised methods with dense\nannotations (DSC\nğ‘™ğ‘¢ \n0.814-0.857, DSC\nğ‘¤ğ‘ \n0.832-0.875). Similar trends were observed on an independent Harvard\ndataset.\nConclusion: Our proposed method demonstrated effective segmentation of crucial arteries, internal carotid\nartery, external carotid artery, and common carotid artery in atherosclerosis. We anticipate that this ef-\nficient approach utilizing single-point annotation will contribute to the effective management of carotid\natherosclerosis. Our code is available at https://github.com/jongdory/CASCA.\n1. Introduction\nAtherosclerosis is a major cause of death and disability world-\nwide [1]. This serious condition is characterized by the buildup of\nplaque in the arteries, which can cause the lumen of the carotid artery\nto become narrow or blocked [2,3]. This, in turn, can impair blood\nflow to the head and neck, increasing the risk of stroke and other\ncomplications [2,3]. Blood vessels have a tubular structure, and the\ncarotid arteries are among the primary vessels that supply blood to the\nneck and head. The human body has two carotid arteries, one on the\nleft and one on the right, each of which is divided into three major\nsegments: the common carotid artery (CCA), internal carotid artery\n(ICA), and external carotid artery (ECA). Atherosclerosis often occurs at\nthe carotid bifurcation, where the CCA splits into the ICA and ECA. In\nmagnetic resonance imaging (MRI), vessels affected by atherosclerosis\ncan be identified by a narrowing of the lumen and the deposition of\nplaque on the outer wall. To diagnose atherosclerosis, images of the\nâˆ— \nCorresponding author at: Department of Electrical and Computer Engineering, Sungkyunkwan University, Suwon, South Korea.\nE-mail address: hyunjinp@skku.edu (H. Park).\ncarotid artery must be segmented to assess the extent of the narrowing\nand determine the appropriate course of treatment [4,5].\nSegmentation methods based on convolutional neural network\n(CNN) models have been widely adopted for this purpose [6,7]. These\nmethods typically require a large dataset of dense annotations at the\npixel level to train a learning model. However, obtaining such dense\nannotations is challenging in practice because creating these annota-\ntions is a time-consuming and labor-intensive process that requires\nspecialized training and expertise. To overcome the challenges posed by\ntime-consuming pixel-wise annotations, self-supervised methods that\ndo not require annotations or weakly supervised methods based on\nweak annotations such as scribbles, bounding boxes, and extreme\npoints have increasingly been developed [8â€“14]. In general, annotation\nmethods used in weakly supervised segmentation seek to balance the\neffort required to label data with accuracy [8,13]. Different weakly\nsupervised segmentation methods demand varying levels of effort for\nhttps://doi.org/10.1016/j.cmpb.2025.108881\nReceived 13 March 2025; Received in revised form 9 May 2025; Accepted 29 May 2025\nComputer Methods and Programs in Biomedicine 269 (2025) 108881\nAvailable online 14 June 2025\n0169-2607/Â© 2025 Elsevier B.V. All rights are reserved, including those for text and data mining, AI training, and similar technologies.\n\nJ. Kim et al.\nannotation. Typically, scribble annotations, which involve roughly\nmarking only parts of an image, are sufficient for learning. Bounding-\nbox annotations involve drawing the smallest rectangle encompassing\nan object, providing an approximation of the objectâ€™s location and size,\nthus offering more information than scribbles. However, the accuracy\nof both scribble and bounding-box annotations in determining the\nprecise shape and boundaries of an object is limited. Extreme-point\nannotations, which mark the edges or extreme points of an object,\nallow for a more accurate inference of its shape. Nonetheless, this\nmethod requires greater precision and thus requires more time and\neffort. Additionally, annotating every slice in 3D medical images is a\nlaborious task requiring considerable time and effort.\nIn this study, we leverage the known tubular structure of vessels,\nalong with information about the diameters of the lumen and outer\nwall, to propose a structure-aware, weakly supervised segmentation\nmethod that can segment both the lumen and outer wall areas using\nonly a single center point. Additionally, the proposed method is able\nto learn from sparsely annotated slices, thus eliminating the need to\nannotate every slice. Our annotation is sparse in both the in-plane\nand out-of-plane directions, where a single point is specified in the\ngiven axial slices and only a few axial slices are annotated in a given\nvolume. Our method is designed to predict a circular structure using the\ncenter point of the lumen and the associated radius of a given slice. By\nadjusting the radius, we can segment both the inner structure (i.e., the\nlumen) and the outer structure (i.e., the vessel wall). To incorporate\nsparse annotations in the out-of-plane direction, we encourage adjacent\naxial slices to have similar probability maps for segmentation so that\nthe results of the annotated slices can be propagated to those without\nannotation.\nContributions:\nâˆ™ We propose a weakly supervised method based on specialized loss\nterms, including the center distance and radius constraint losses,\nthat enables the lumen and outer wall of a carotid artery to be\nsegmented using the single-point annotation of a given slice of\nan MRI scan.\nâˆ™ The proposed method allows for sparse annotation in the out-of-\nplane direction using consistency loss, such that only a few slices\nneed to be annotated.\nâˆ™ Experimental evaluations showed that our approach performed\nas well as fully annotated methods. Furthermore, our method\nachieved accurate segmentation in the clinically crucial regions\nof the ICA, ECA, and CCA.\n2. Related works\n2.1. Medical image segmentation\nU-Net [15] is a CNN architecture specifically designed for medical\nimage segmentation. It is divided into an encoder and a decoder.\nThe encoder extracts features from the input image and progressively\nreduces the spatial dimensions through downsampling. In the decoder,\nthe upsampling procedure gradually expands the spatial dimensions\nand restores the image to its original size. Moreover, at each stage of\nupsampling, the feature maps from the encoder are connected to those\nin the decoder via skip connections. This preserves detailed local infor-\nmation and enhances segmentation accuracy. The structure of U-Net is\ncommonly used in medical image segmentation, and numerous segmen-\ntation models based on various U-Net architectures have emerged [16â€“\n19]. In contrast to research focusing solely on architectural elements,\nnnU-Net [20] emphasizes the importance of non-architectural factors in\nsegmentation. It automatically configures hyperparameters and learn-\ning strategies to achieve state-of-the-art results across various medical\ndomains. It has become one of the most frequently used baseline models\nin recent segmentation studies [20]. However, because nnU-Net is a\nrigidly structured framework, modifying it for weak annotations is\ndifficult. In addition, several studies have been conducted on carotid\nartery segmentation. Alblasa et al. [21] segmented annular vessel walls\nby applying Dijkstraâ€™s shortest path to a 3D U-net-predicted proximity\nmap to find the centerline, polarizing each axial slice with respect to\nthe centerline, and predicting the lumen and outer wall radii using a\nrotation-invariant dilated CNN. Huang et al. [22] segmented the vessel\nlumen with a modified 3D active contour using the centerline, and\nthey then computed the outer wall probability map using a 2.5D CNN\ntrained in the polar coordinate domain to segment the smooth outer\nwall in 3D. MT-Net [23] complements sparse annotation using the\nsegment-anything model (SAM) [24], aligns the CT angiography and\nMR with a domain aligner to segment the vessel lumen, and segments\nthe ring-shaped outer wall with a Gaussian lumen prior.\n2.2. Weakly supervised segmentation\nBounding boxes are the most common type of weak annotation [13,\n14,25], in which a model is provided with weak labels in the form\nof bounding boxes that indicate the approximate location of each\nobject. Bounding-box annotations provide localization information that\ncan constrain the segmentation task. Scribble annotation indicates the\nrough location and shape of each object in the image, but is not as\nprecise as fully supervised pixel-level annotation [8,11,12,26]. These\nannotations consist of a few lines or curves drawn by an annotator\non the regions of an image about which they feel confident. This\nannotation procedure is more efficient than dense annotation while\nstill incorporating spatial and semantic information [8]. Extreme-point\nannotation consists of manually defined points on the boundary of an\nobject [9,10,27]. The single extreme point method uses a random walk\nalgorithm to create initial pseudo-labels from initial manual points [9].\nLaradji et al. [27] achieve weakly supervised segmentation by training\nonly on the parts of an image where annotation points are present.\nDorent et al. [10] use multiple extreme points to create bounding boxes\nand train segmentation models using direct unsupervised regularized\nloss. However, while both methods are somewhat effective when multi-\npoint annotations are available, accurately segmenting one region (let\nalone two different areas such as the lumen and outer wall) from\na single point is not possible. We propose a weakly supervised seg-\nmentation method that leverages the tubular structure of the carotid\nartery and knowledge of the diameters of the lumen and wall to enable\nsegmentation of both the lumen and outer wall areas using only a single\ncenter point for annotation.\n3. Methods\nIn this study, we propose a method that enables the segmentation\nof both the lumen and outer wall of the carotid artery in each slice\nusing a single center point. To the best of our knowledge, no previous\nstudy has segmented two distinct regions using only one point. The\nproposed method is based on the popular 3D U-Net backbone [15]. The\nmain idea is that a carotid vessel is a circular structure in which strong\nprior information about the radius is available in a given slice. Various\nstudies have reported normative carotid vessel measurements, and the\nexpected radius is approximately 3.4 mm [28â€“30], which allows us\nto model a vessel with an annotated center point within the range\nof plausible values for the radius. We also consider the vessel as a\ncontinuous object in 3D, where the segmentation results of adjacent\naxial slices tend to be similar. These two concepts are implemented\nusing the three loss terms of the center distance, radius constraint,\nand slice-wise consistency, as shown in Fig. 1. By adjusting the range\nof radius values on which the model should focus, we construct two\ncircular structures, the smaller of which is the lumen, and the difference\nbetween the larger and smaller structures indicates the vessel wall. The\nbackbone architecture is a 3D model that takes 3D inputs and outputs\na 3D probability map. Each loss is calculated on the 2D slices and then\nbackpropagated.\nComputer Methods and Programs in Biomedicine 269 (2025) 108881\n2\n\nJ. Kim et al.\nFig. 1. Pipeline of our framework. (a) Illustration of center distance loss îˆ¸\nğ‘ğ‘’ğ‘›ğ‘¡_ğ‘‘ğ‘–ğ‘ ğ‘¡ \n, which quantifies the distance between a given sparse single center point ğ¶\nğ‘¡ğ‘ğ‘Ÿğ‘”ğ‘’ğ‘¡ \nand predicted\ncenter point ğ¶\nğ‘ğ‘Ÿğ‘’ğ‘‘ \n. Red and green curves indicate different types of carotid arteries. (b) Illustration of radius constraint loss îˆ¸\nğ‘ğ‘’ğ‘›ğ‘¡_ğ‘ğ‘œğ‘›ğ‘ ğ‘¡ \nwhich focuses on different radii. (c) Illustration\nof consistency loss îˆ¸\nğ‘ğ‘œğ‘›ğ‘ ğ‘–ğ‘  \n. Here, ğ‘ƒ\nğ‘œğ‘Ÿğ‘–ğ‘” \nis the volumetric probability map and ğ‘ƒ\nğ‘ â„ğ‘–ğ‘“ ğ‘¡ \nis the probability map shifted by one pixel in the axial direction.\n3.1. Center distance loss\nIn an axial slice, a blood vessel appears as a circular shape. A\ngiven MRI slice typically contains multiple blood vessels; thus, it is\nnecessary to first identify the carotid arteries by locating their center\npoints. Similar approaches have been proposed for segmenting tubular\nstructures [31], but they have not been adapted for carotid arteries.\nThe model determines the predicted center point from the center of the\nprediction map. However, because the prediction map represents a con-\ntinuous probability, we compute the center point by taking a weighted\naverage of these probabilities. To do this, we compute the 2D centroid\nğ¶\nğ‘ğ‘Ÿğ‘’ğ‘‘ \nof the segmentation probability map ğ‘\nğœƒ \n(ğ‘¢) of position ğ‘¢ using the\nprobability-weighted average of the pixel coordinates [32,33], where\nğ‘\nğœƒ \nrefers to the segmentation network, parameterized by ğœƒ. Note that\nthis method is designed to find the center of any shape, not just circles.\nFig. 2 illustrates the target center point ğ¶\nğ‘¡ğ‘ğ‘Ÿğ‘”ğ‘’ğ‘¡ \nand ğ‘\nğœƒ \n(ğ‘¢) along with the\ncorresponding ğ¶\nğ‘ğ‘Ÿğ‘’ğ‘‘ \n, calculated as follows.\nğ¶\nğ‘ğ‘Ÿğ‘’ğ‘‘ \n=\nâˆ‘\nğ‘¢âˆˆğ›º \nğ‘\nğœƒ \n(ğ‘¢) â‹… ğ‘¢\nâˆ‘\nğ‘¢âˆˆğ›º \nğ‘\nğœƒ \n(ğ‘¢) \n(1)\nWe introduce the center distance loss to align the model-predicted\ncenter points with the actual center points of the carotid arteries. We\nthen use the L2 norm to encourage the computed centroids to be\nsimilar to those of the single-point annotations for slices with available\nannotations.\nîˆ¸\nğ‘ğ‘’ğ‘›ğ‘¡_ğ‘‘ğ‘–ğ‘ ğ‘¡ \n= â€–ğ¶\nğ‘¡ğ‘ğ‘Ÿğ‘”ğ‘’ğ‘¡ \nâˆ’ ğ¶\nğ‘ğ‘Ÿğ‘’ğ‘‘ \nâ€–\n2 \n(2)\nHere, ğ‘¢ âˆˆ ğ›º âŠ‚ R\n2 \ndenotes coordinates in the 2D image space for\nthe given axial slice, and ğ‘\nğœƒ \nâˆ¶ ğ›º â†’ [0, 1] is the predicted pixel-wise\nprobability. This encourages the trained model to match the center\npoints of the predicted map with the actual center points. This allows\nthe model to locate the carotid artery even if the image is rotated.\nHowever, because the center points of the lumen and outer wall are\nidentical, training in this manner alone does not differentiate them.\nFurthermore, more information is required to segment the vascular\nregions more accurately.\n3.2. Radius constraint loss\nBecause blood vessels appear as a circular shape in axial images, we\ncan train the model by finding the center point of a vessel and imposing\nprior information regarding the plausible radius range for the vessel.\nWe utilize prior knowledge about the radii of the lumen and wall to\ndefine an acceptable range [28â€“30]. This allows us to train the model\nto encourage predictions within this specified radius range and avoid\npredictions outside of it. This concept can be considered an extended\nversion of traditional approaches, such as the level-set method [34].\nMotivated by previous work, we design a spatial map îˆ°\nğ‘–ğ‘› \nthat penalizes\npredictions outside of the plausible range for the radius to encourage\nthe model to predict a plausible radius [35]. A representative visualiza-\ntion (Fig. 2) shows that îˆ°\nğ‘–ğ‘› \ndecreases as we move away from the center\npoint, and the rate of decrease is determined by two tunable parameters\nğ‘ and ğ‘Ÿ. Note that îˆ°\nğ‘ \nis the Euclidean distance between position ğ‘¥ and\nthe center point, ReLU is the rectified linear unit activation function, ğ‘\nis a scaling factor, and ğ‘Ÿ is the expected radius of the vessel.\nîˆ°\nğ‘–ğ‘›\n(ğ‘¥) = ğ‘…ğ‘’ğ¿ğ‘ˆ\n(\nğ‘’\nğ‘(1âˆ’ \nîˆ°\nğ‘ \n(ğ‘¥)\nğ‘Ÿ \n) \nâˆ’ 1\n)\n(3)\nMap îˆ°\nğ‘–ğ‘› \nis defined such that values outside the specified region are\nzero and values closer to the center point increase. Furthermore, the\nloss term is used to make predictions within a plausible radius range\nand it can be defined as follows.\nîˆ¸\nğ‘Ÿğ‘ğ‘‘_ğ‘–ğ‘› \n=\nâˆ‘\nğ‘¢âˆˆğ›º \nîˆ°\nğ‘–ğ‘›\n(ğ‘¢)(1 âˆ’ ğ‘\nğœƒ \n(ğ‘¢))\nâˆ‘\nğ‘¢âˆˆğ›º \nğ‘\nğœƒ \n(ğ‘¢) \n(4)\nThis loss function increases if there are no predictions within îˆ°\nğ‘–ğ‘›\n, and\ndecreases as predictions within îˆ°\nğ‘–ğ‘› \nincreases. Since the lumen and wall\nhave different radii, setting distinct radius ğ‘Ÿ values for each allows\nsegmentation training on these two distinct areas.\nThe inclusion of other blood vessels outside the radius should not\naffect the loss. Hence, we structure map îˆ°\nğ‘œğ‘¢ğ‘¡ \nsuch that values outside\nthe region are one and as we move closer to the center point within the\nradius, the values approach zero. Map îˆ°\nğ‘œğ‘¢ğ‘¡ \nshares the same parameters\nğ‘ and ğ‘Ÿ.\nîˆ°\nğ‘œğ‘¢ğ‘¡\n(ğ‘¥) = ğ‘…ğ‘’ğ¿ğ‘ˆ\n(\n1 âˆ’ ğ‘’\nğ‘(1âˆ’ \nîˆ°\nğ‘ \n(ğ‘¥)\nğ‘Ÿ \n)\n)\n(5)\nComputer Methods and Programs in Biomedicine 269 (2025) 108881\n3\n\nJ. Kim et al.\nFig. 2. Examples of MRI input and prediction for a representative axial slice. (Left) Example of the center distance loss overlaid on an enlarged MRI image. The purple map is\nthe wall prediction map and the yellow map is the lumen prediction map. The red point indicates ğ¶\nğ‘¡ğ‘ğ‘Ÿğ‘”ğ‘’ğ‘¡ \nand the blue point indicates ğ¶\nğ‘ğ‘Ÿğ‘’ğ‘‘ \n. (Right) Illustration of examples îˆ°\nğ‘–ğ‘›\nand îˆ°\nğ‘œğ‘¢ğ‘¡ \nin the MRI input slice for the radius constraint loss.\nFig. 3. 3D rendering with different degrees of sparsity in the center points. Even with\nfull annotation, some center points are not visible because of projection issues. Green\nrepresents the ECA, whereas red indicates the ICA and CCA.\nThus, the loss term is used to penalize the predictions outside the\nplausible radius range and hence reduce the number of false positives.\nThe loss function is defined as follows.\nîˆ¸\nğ‘Ÿğ‘ğ‘‘_ğ‘œğ‘¢ğ‘¡ \n=\nâˆ‘\nğ‘¢âˆˆğ›º \nîˆ°\nğ‘œğ‘¢ğ‘¡\n(ğ‘¢)ğ‘\nğœƒ \n(ğ‘¢)\nâˆ‘\nğ‘¢âˆˆğ›º \nğ‘\nğœƒ \n(ğ‘¢) \n(6)\nUsing this method, we can reduce incorrect detection of vessels other\nthan the carotid arteries (i.e., false positives). Finally, the radius con-\nstraint loss is defined by adding the îˆ¸\nğ‘Ÿğ‘ğ‘‘_ğ‘–ğ‘› \nand îˆ¸\nğ‘Ÿğ‘ğ‘‘_ğ‘œğ‘¢ğ‘¡ \nterms. This\ncombines the two loss terms described earlier, creating a single loss\nfunction that promotes predictions within the range of plausible radii\nof the carotid artery and penalizes false positives outside the plausible\nrange.\nîˆ¸\nğ‘Ÿğ‘ğ‘‘_ğ‘ğ‘œğ‘›ğ‘ ğ‘¡ \n= îˆ¸\nğ‘Ÿğ‘ğ‘‘_ğ‘–ğ‘› \n+ îˆ¸\nğ‘Ÿğ‘ğ‘‘_ğ‘œğ‘¢ğ‘¡ \n(7)\nBecause the network relies on these centers rather than a fixed image\norientation, it remains robust to rotations. If an input slice is rotated\n(e.g., by 45\nâ—¦\n), the corresponding centers simply rotate with it, and the\nmodel will still correctly localize and segment each artery.\n3.3. Consistency loss\nAlthough we can utilize the circular nature of the carotid artery\nvessels and prior knowledge of the lumen and outer wall to design two\nloss functions, our model might not always adhere to this structure, and\nsome errors may occur. Therefore, we use the similarity of the carotid\nartery regions in adjacent slices to enhance our learning approach.\nWe adopt a consistency loss term based on previous studies [36]\ndesigned to exploit the high redundancy in adjacent image patches\n(e.g., up to 80%). Our loss term encourages the model to produce\nsimilar (i.e., consistent) predictions for adjacent axial slices. This term\nalso serves as a mechanism for propagating the segmentation results of\nthe annotated slices to those without annotations. This allows effective\nlearning, even in situations where not all slices have annotations,\nachieving results similar to those with annotations on every slice while\nusing only sparse annotations in the axial direction. In particular, at\nthe bifurcation where the ECA and ICA merge into the CCA and the\ncross-section naturally deviates from a perfect circle, the consistency\nloss ensures that the segmentation remains smooth and accurate, even\nunder elliptical geometry. The sparse annotations in the axial direction\nare shown in Fig. 3. In this work, â€˜â€˜consistency lossâ€™â€™ refers to the\nloss that leads to consistent results between adjacent slices. In detail,\nwe compute the binary cross entropy (BCE) loss of the volumetric\nprobability map ğ‘ƒ\nğ‘œğ‘Ÿğ‘–ğ‘” \nand the adjacent shifted volumetric probability\nmap ğ‘ƒ\nğ‘ â„ğ‘–ğ‘“ ğ‘¡ \n[4,5]. Note that ğ‘ƒ\nğ‘œğ‘Ÿğ‘–ğ‘” \nâˆˆ R\nâ„Ã—ğ‘¤Ã—ğ‘‘ \ndenotes the volumetric\nprobability map and ğ‘ƒ\nğ‘ â„ğ‘–ğ‘“ ğ‘¡ \ndenotes ğ‘ƒ\nğ‘œğ‘Ÿğ‘–ğ‘” \nshifted in the third dimension\nby a single pixel in the direction of the head. Because the prediction\nmaps of adjacent slices cannot be identical, we control the sharpness\nof the probability distribution using the temperature parameter ğ‘‡ , a\ntechnique borrowed from knowledge distillation [37]. The probability\nmap may contain spurious clusters that could be suppressed by the tem-\nperature parameter ğ‘‡ , which controls the sharpness of the probability\ndistribution\nğ‘ƒ\nğ‘œğ‘Ÿğ‘–ğ‘” \n= \nexp\n(\nğ‘\nğ‘\nğ‘œğ‘Ÿğ‘–ğ‘” \n(ğ‘¢)âˆ•ğ‘‡ \n)\nâˆ‘\nğ‘âˆˆğ¶ \nexp\n(\nğ‘\nğ‘\nğ‘œğ‘Ÿğ‘–ğ‘” \n(ğ‘¢)âˆ•ğ‘‡ \n) \n(8)\nğ‘ƒ\nğ‘ â„ğ‘–ğ‘“ ğ‘¡ \n=\nexp\n(\nğ‘\nğ‘\nğ‘ â„ğ‘–ğ‘“ ğ‘¡\n(ğ‘¢)âˆ•ğ‘‡ \n)\nâˆ‘\nğ‘âˆˆğ¶ \nexp\n(\nğ‘\nğ‘\nğ‘ â„ğ‘–ğ‘“ ğ‘¡\n(ğ‘¢)âˆ•ğ‘‡ \n) \n(9)\nHere, ğ¶ is the entire label set, and ğ‘\nğ‘\nğ‘œğ‘Ÿğ‘–ğ‘” \nand ğ‘\nğ‘\nğ‘ â„ğ‘–ğ‘“ ğ‘¡ \ndenote the voxel-\nwise output logits from the original and shifted volumetric space,\nrespectively. Probability maps ğ‘ƒ\nğ‘œğ‘Ÿğ‘–ğ‘” \nand ğ‘ƒ\nğ‘ â„ğ‘–ğ‘“ ğ‘¡ \ncan be treated as distilled\nknowledge [37] of the categorical probability distributions. We hence\npropose a consistency loss that aligns the distributions of adjacent slices\nby utilizing information from neighboring slices. The consistency loss\nis defined as the BCE loss between ğ‘ƒ\nğ‘œğ‘Ÿğ‘–ğ‘” \nand ğ‘ƒ\nğ‘ â„ğ‘–ğ‘“ ğ‘¡\n.\nîˆ¸\nğ‘ğ‘œğ‘›ğ‘ ğ‘–ğ‘  \n= îˆ¸\nğµğ¶ğ¸ \n(ğ‘ƒ\nğ‘œğ‘Ÿğ‘–ğ‘” \n, ğ‘ƒ\nğ‘ â„ğ‘–ğ‘“ ğ‘¡\n) (10)\nFormally, we minimize the following total loss function\nîˆ¸\nğ‘¡ğ‘œğ‘¡ğ‘ğ‘™ \n= ğœ†\n1\nîˆ¸\nğ‘ğ‘’ğ‘›ğ‘¡_ğ‘‘ğ‘–ğ‘ ğ‘¡ \n+ ğœ†\n2\nîˆ¸\nğ‘Ÿğ‘ğ‘‘_ğ‘ğ‘œğ‘›ğ‘ ğ‘¡ \n+ ğœ†\n3\nîˆ¸\nğ‘ğ‘œğ‘›ğ‘ ğ‘–ğ‘  \n(11)\nHere, the weights ğœ†\n1\n, ğœ†\n2\n, and ğœ†\n3 \nare empirically set to 1. Our method\nminimize the sum of the three losses, and thus the segmentation results\ncan deviate from a perfect circle.\n3.4. Datasets\nWe used the COSMOS2022 training dataset [38] as our training\ndataset. For model validation, we used two datasets: the COSMOS2022\ntest dataset [39] and the Harvard dataset [40,41]. Detailed information\nabout these datasets is presented in Table 1. The COSMOS2022 dataset\ncontains the MRI scans of 75 patients with atherosclerosis. The image\nhad dimensions of 432 Ã— 432 Ã— 432 with a slice spacing of 0.6 Ã— 0.6 Ã—\n0.6 mm\n3\n. The images are split in half in the second dimension to yield\ntwo sets of images, one for the left carotid arteries and one for the right.\nComputer Methods and Programs in Biomedicine 269 (2025) 108881\n4\n\nJ. Kim et al.\nTable 1\nDetailed information regarding the training and validation datasets used in this study.\nDatasets\nCOSMOS 2022 [38,39] Harvard [40,41]\nScanner 3T Philips MRI Philips Achieva 3.0T\nSequence\nMotion-sensitized\n3D Volumetric Isotropic driven Equilibrium\nTSE Acquisition (VISTA) prepared Rapid\nGradient Echo (MERGE)\nDimension 432 Ã— 432 Ã— 432 720 Ã— (79-100) Ã— (96-172)\n(Voxel size) (0.6 Ã— 0.6 Ã— 0.6mm\n3 \n) (0.35 Ã— 0.35 Ã— 0.35mm\n3 \n)\nParameters TE 20 ms, TR 800 ms â€“\nSubjects 75 (training 50, testing 25) 11\nOf the annotated axial slices, 35% show atherosclerosis, whereas the\nremaining 65% present normal vessels [38]. Manual dense annotations\nof the carotid artery lumen and wall were provided for 20% of the\nslices. The dataset provides annotations for the clinically important\nICA and CCA, but does not provide dense annotations for the ECA.\nThus, the center points of the ECA were drawn by a board-certified\nphysician. The MRI images were acquired using a Philips MRI scanner\nequipped with an 8-channel carotid coil and a 16-channel head coil,\nand the 3D VISTA MRI sequence was used [30]. Following the dataset\nguidelines, 50 samples were used to train the model and 25 were used\nfor testing. We also used a dataset provided by Harvard consisting of\nthe MRI scans of 11 patients as an external test dataset [40]. In the\nHarvard dataset, the left and right carotid artery images are provided\nseparately. The dimensions of the images vary in size, and the slice\nspacing is 0.35 Ã— 0.38 Ã— 0.35 mm\n3\n. The Harvard dataset only provides\ncontour annotations. Therefore, dense annotations of the lumen and\nwalls were drawn by a board-certified physician.\n3.5. Metrics\nThe evaluation was performed using the Dice score coefficient (DSC)\nand Hausdorff distance (HD). They are defined as follows.\nDSC = \n2 âˆ— |X âˆ© Y|\n|X| + |Y| \n(12)\nâ„(X, Y) = max\nğ‘¥âˆˆX \nmin\nğ‘¦âˆˆY \n(ğ‘¥, ğ‘¦) (13)\nHD = max(â„(X, Y), â„(Y, X)) (14)\nBecause of the sparsity of the ground truth, the metrics were calculated\nonly for the slices with annotations. The ECA region was not included\nin the metric evaluation because no ground truth was available.\n3.6. Comparison methods\nWe compared our method with previous weakly supervised segmen-\ntation methods that use point annotations. Specifically, we evaluated\nour approach against the 2D method of Laradji et al. [27], tree energy\nloss [42] and the 3D method of Dorent et al. [10]. We trained the com-\nparison models using annotations from the original proposed methods.\nWe also conducted the comparisons under identical conditions using\nonly a single center point for training in each case. Instead of compar-\ning different segmentation network architectures, we focused on the\ntraining methods. To this end, we fixed the segmentation backbone as\na standard U-Net backbone and evaluated different training techniques\nto improve segmentation performance. In addition, we compared our\nmethod with models trained using supervised learning techniques,\nincluding 2D U-Net [15], 2D nnU-net [20], 2.5D U-Net++ [43], and\n3D U-Net. Given that the dataset contains slices that are sparsely\nannotated, for 3D U-Net, we calculated the loss only for slices for\nwhich annotations were available during the training process. For these\nmethods, we adopted the standard Dice and BCE loss [44], Dice and\nfocal loss [45], or Dice and HD loss [46]. All comparison methods were\ntrained from scratch.\n3.7. Implementation details\nWe used a 3D U-Net architecture as the backbone for all mod-\nels [47], which were trained using the AdamW optimizer with a learn-\ning rate of 10\nâˆ’4\n, ğ›½\n1 \n= 0.9, and ğ›½\n2 \n= 0.99 for 1000 epochs. We trained\nour methods from scratch without using any pretrained weights. The\nbatch size during training was two and only one batch was used for\ntesting. The values of a were empirically set to 6, and ğ‘Ÿ was empiri-\ncally set to 5.5 pixels (equivalent to 3.3 mm) for the lumen and 7.5\npixels (equivalent to 4.5 mm) for the outer wall. These values are in\nline with previously reported vessel radii of the carotid artery [28].\nWhen training our method, we utilized sparse single-point annotations,\nwhich means only 20% of the slices containing the artery were used\n(equivalent to one point annotation every five slices). In our study, we\ntrained and evaluated our method using a 5-fold cross-validation. The\nresults were obtained by ensembling the results from the five folds.\nThe model was implemented using PyTorch 1.12.1 and the experiments\nwere conducted on an NVIDIA A100 with 80 GB of memory.\n4. Results\n4.1. Results of the comparison methods\nWe compared the performances of various weakly supervised seg-\nmentation methods that utilize different types of point annotations.\nAdditionally, we compared these results with those obtained from\nmodels trained using fully supervised methods to determine the dif-\nferences and upper bounds of performance. It is important to note\nthat the MRI data used in this study have a high resolution and very\nthin slice thickness. Therefore, annotating every slice is challenging.\nConsequently, annotations were applied sparsely in the axial direction.\nThis sparse annotation is depicted in the sagittal view of the ground\ntruth in Fig. 4. Among the comparative models, the 3D methods were\ntrained by calculating the loss only on slices with annotations. The\nqualitative results of these comparisons are shown in Figs. 4 and 5.\nAdditionally, a quantitative evaluation of the model was performed\nonly in areas and slices with dense annotations. The ECA region, which\ndoes not have dense annotations, was excluded from the quantitative\nevaluation. Our model has two radius parameters, one each for the\nlumen and outer wall. The radius of the outer wall allowed us to\npredict when the intensity of the outer wall was similar to that of the\nbackground (Fig. 4).\nThe method developed by Laradji et al. [27] calculates the loss\nonly on pixels with point annotations and uses a consistency loss to\nensure that the results are consistent with the rotated images, allowing\nweakly supervised learning. We evaluated a case in which the model\nwas trained using ten points per slice consisting of multiple lumens,\nwalls, and background points, as originally proposed. However, this\nmethod was unable to accurately segment the carotid artery and can\nonly identify its approximate location. Moreover, it generated noisy\nbranches (false positives) that were not part of the carotid arteries. In\naddition, training with only a single center point using this method\nmakes it difficult to accurately detect the targeted area and fails to\ndistinguish between the lumen and outer wall (Fig. 6 top). The ap-\nproach by Dorent et al. [10] employs extreme points that represent\nthe outermost part of an object for training. We evaluated this method\nby training it using six extreme points, as originally proposed. Because\nthere are two classes to be segmented (the lumen and outer wall),\nwe used 12 points per volume for annotation to compare the results.\nFor the segmentation of long and complex tubular structures such as\nblood vessels, this method was not effective. Additionally, it struggles\nto accurately segment areas when trained solely on a single center point\nComputer Methods and Programs in Biomedicine 269 (2025) 108881\n5\n\nJ. Kim et al.\nFig. 4. Carotid artery segmentation of the lumen and wall using different methods on the main dataset. Yellow is the lumen map, and red is the wall map. The first row shows\nthe axial images with CCA and the corresponding outputs. The second row shows axial images at the bifurcation where the ICA and ECA are present. The third row shows the\nsagittal view, with white arrows pointing toward the ICA, ECA, and CCA.\nTable 2\nPerformance comparisons among methods for COSMOS 2022 test dataset. Bold font indicates the best case in a given setting. The proposed method is shown in gray and the\nfully supervised method (upper bound) is showing in blue .\nDataset COSMOS 2022 Testing [39]\nDim Model Method Annotation DSC\nğ‘™ğ‘¢ \nâ†‘ DSC\nğ‘¤ğ‘ \nâ†‘ DSC\nğ‘ğ‘£ğ‘” \nâ†‘ HD\nğ‘™ğ‘¢ \nâ†“ HD\nğ‘¤ğ‘ \nâ†“ HD\nğ‘ğ‘£ğ‘” \nâ†“\n2D U-Net Laradji et al. [27] 1 point per 1 slice 0.023 Â± 0.028 0.076 Â± 0.069 0.049 Â± 0.055 52.15 Â± 12.70 55.80 Â± 13.07 53.97 Â± 12.95\n2D U-Net Laradji et al. [27] 10 points per 1 slice 0.319 Â± 0.067 0.622 Â± 0.110 0.471 Â± 0.097 7.55 Â± 1.67 5.87Â± 1.98 6.71 Â± 1.76\n3D U-Net Dorent et al. [10] 1 point per 1 slice 0.167 Â± 0.051 0.094 Â± 0.027 0.130 Â± 0.043 22.88 Â± 9.11 21.11 Â± 8.03 21.99 Â± 8.70\n3D U-Net Dorent et al. [10] 12 points per volume 0.435 Â± 0.082 0.449 Â± 0.089 0.442 Â± 0.083 16.67 Â± 5.66 18.15 Â± 4.98 16.41 Â± 5.14\n2D DeepLabv3 [48] Tree Energy Loss [42] 1 point per 1 slice 0.247 Â± 0.063 0.344 Â± 0.084 0.295 Â± 0.072 40.01 Â± 6.20 37.54 Â± 6.23 38.78 Â± 6.21\n2D DeepLabv3 [48] Tree Energy Loss [42] 10 points per 1 slice 0.329 Â± 0.067 0.464 Â± 0.128 0.396 Â± 0.085 22.97 Â± 6.23 25.37 Â± 5.49 24.17 Â± 5.70\n3D U-Net ours 1 point per 5 slices 0.821 Â± 0.039 0.841 Â± 0.052 0.832 Â± 0.048 2.37 Â± 1.51 2.80 Â± 1.65 2.58 Â± 1.61\n3D U-Net ours 1 point per 1 slice 0.825 Â± 0.035 0.843 Â± 0.050 0.834 Â± 0.047 2.28 Â± 1.48 2.68 Â± 1.57 2.49 Â± 1.55\n2D U-Net Dice + BCE loss dense 0.814 Â± 0.058 0.832 Â± 0.065 0.823 Â± 0.061 1.95 Â± 1.45 2.35 Â± 1.68 2.16 Â± 1.59\n2D U-Net Dice + Focal loss dense 0.817 Â± 0.033 0.820 Â± 0.061 0.819 Â± 0.046 2.07 Â± 1.53 2.38 Â± 1.68 2.20 Â± 1.54\n2D nnU-Net [20] Dice + BCE loss dense 0.857 Â± 0.026 0.875 Â± 0.024 0.866 Â± 0.026 1.82 Â± 1.28 2.10 Â± 1.27 1.96 Â± 1.28\n2.5D U-Net++ [43] Dice + HD loss [46] dense 0.831 Â± 0.041 0.838 Â± 0.053 0.835 Â± 0.101 1.53 Â± 0.94 1.71 Â± 0.91 1.63 Â± 0.92\n3D U-Net Dice + BCE loss dense 0.851 Â± 0.031 0.869 Â± 0.027 0.860 Â± 0.030 1.55 Â± 0.65 1.74 Â± 0.94 1.65 Â± 0.82\nand cannot differentiate between two distinct areas using center-point\nannotation. The tree energy loss method [42] uses low- and high-level\nstructural information within an image to generate soft pseudo-labels\nfor unlabeled pixels, enabling dynamic online self-training. The method\nwas trained using 10 points consisting of multiple lumen, wall, and\nbackground points. We also evaluated the performance using single-\npoint annotations. Because the unlabeled regions were not included in\nthe training, this method resulted in a large number of false positives\n(noisy branches) and failed to accurately segment the vascular regions.\nIn contrast, our method leverages priors regarding the structure of\nthe carotid artery and the diameter of the lumen and wall to achieve\naccurate segmentation. Fig. 6 (top) shows a slice from a bifurcation\narea, where a single vessel starts to branch into two vessels. Because\nour model was trained to predict both the ICA and ECA, it can gener-\nate a non-circular shape formed by overlapping circles. Furthermore,\nwhen compared with models trained with dense annotations, our ap-\nproach shows promising results, demonstrating its effectiveness even\nwith less comprehensive labeling. Our method not only effectively\nsegments the circular structure of the CCA but also performs well in the\nmore challenging bifurcation areas, which are crucial for diagnosing\natherosclerosis. The segmentation results in these regions are similar\nto the ground truth. In addition, the results for the sagittal view reveal\nthat the 3D U-Net (dense) model only achieves segmentation in limited\nareas. This is because only the CCA and ICA are labeled, and the ground\ntruth does not cover all the slices of the carotid artery. However, despite\nthe use of a 3D U-Net architecture, the proposed method was able to\nsegment all areas of the ICA, ECA, and CCA, indicating a comprehensive\nand effective segmentation capability (see the arrows in Fig. 4).\nOur method also demonstrated promising results in a quantitative\nevaluation, as shown in Table 2. In contrast to other weakly supervised\nmethods using different point annotations, which failed to achieve\naccurate segmentation, our approach not only performed well but\nalso outperformed the 2D U-Net trained with dense annotations in\nsome aspects. Additionally, our model demonstrated a performance\nsimilar to that of fully supervised methods using dense annotations,\nachieving a DSC metric of DSC\nğ‘™ğ‘¢ \n0.821 and DSC\nğ‘¤ğ‘ \n0.847 (compared\nto DSC\nğ‘™ğ‘¢ \n0.814â€“0.857 and DSC\nğ‘¤ğ‘ \n0.832â€“0.875). However, our method\nhas limitations. Since it is trained by estimating a circular structure of\nthe carotid artery, it may show a higher HD for vessels that deviate\nsignificantly from a circular shape. Consequently, in terms of the HD\nmetrics, our performance may be lower than that of models trained\nwith dense annotations.\n4.2. Results of external dataset\nWe further evaluated our method using the Harvard dataset [40,41],\nwhich consists of MERGE sequences that differ from those of the COS-\nMOS2022 dataset [38,39]. The quantitative results of this evaluation\nare presented in Fig. 5. The method developed by Laradji et al. [27]\nwas even less accurate in detecting the vascular area on the external\ndataset and could not differentiate between the lumen and outer wall.\nSimilarly, the method of Dorent et al. [10] failed to detect vascular\nareas in this dataset. The tree energy loss method [42] method was\nComputer Methods and Programs in Biomedicine 269 (2025) 108881\n6\n\nJ. Kim et al.\nFig. 5. Carotid artery segmentation of the lumen and wall for different methods on the Harvard dataset. Yellow indicates the lumen map, and red indicates the wall map. The\nfirst row shows the axial images with CCA and the corresponding outputs. The second row shows axial images in which the ICA and ECA are present. Note that in the second\nrow, the ground truth annotation is provided only on one side; however, both carotid arteries were predicted.\nTable 3\nComparison of the performance of the methods on the Harvard dataset. Bold font indicates the best results in a given setting. The results for the proposed method (Ours) are\nshaded in gray and the results for the fully supervised methods (upper bound) are shaded in blue .\nDataset Harvard [40,41]\nDim Model Method Annotation DSC\nğ‘™ğ‘¢ \nâ†‘ DSC\nğ‘¤ğ‘ \nâ†‘ DSC\nğ‘ğ‘£ğ‘” \nâ†‘ HD\nğ‘™ğ‘¢ \nâ†“ HD\nğ‘¤ğ‘ \nâ†“ HD\nğ‘ğ‘£ğ‘” \nâ†“\n2D U-Net Laradji et al. [27] 1 point per 1 slice 0.041 Â± 0.004 0.192 Â± 0.008 0.117 Â± 0.007 28.47 Â± 6.58 35.66 Â± 12.48 32.06 Â± 10.21\n2D U-Net Laradji et al. [27] 10 points per 1 slice 0.222 Â± 0.056 0.421 Â± 0.123 0.323 Â± 0.088 9.63 Â± 1.68 7.96 Â± 1.75 8.79 Â± 1.69\n3D U-Net Dorent et al. [10] 1 point per 1 slice 0.098 Â± 0.009 0.052 Â± 0.005 0.075 Â± 0.008 15.32 Â± 6.68 17.01 Â± 6.47 16.17 Â± 6.60\n3D U-Net Dorent et al. [10] 12 points per volume 0.207 Â± 0.145 0.244 Â± 0.089 0.225 Â± 0.083 14.59 Â± 6.34 14.19 Â± 5.43 14.40 Â± 5.69\n2D DeepLabv3 [48] Tree Energy Loss [42] 1 point per 1 slice 0.228 Â± 0.056 0.287 Â± 0.070 0.257 Â± 0.061 17.51 Â± 8.41 15.35 Â± 7.86 16.43 Â± 8.13\n2D DeepLabv3 [48] Tree Energy Loss [42] 10 points per 1 slice 0.241 Â± 0.062 0.396 Â± 0.117 0.319 Â± 0.086 17.06 Â± 6.07 15.62 Â± 6.25 16.34 Â± 6.14\n3D U-Net Ours 1 point per 5 slices 0.823 Â± 0.055 0.847 Â± 0.042 0.835 Â± 0.049 2.05 Â± 1.05 2.52 Â± 1.36 2.29 Â± 1.28\n3D U-Net Ours 1 point per 1 slice 0.826 Â± 0.048 0.850 Â± 0.038 0.838 Â± 0.045 1.96 Â± 1.01 2.39 Â± 1.28 2.18 Â± 1.22\n2D U-Net Dice + BCE loss dense 0.826 Â± 0.051 0.860 Â± 0.050 0.843 Â± 0.051 1.91 Â± 0.75 2.43 Â± 0.68 2.17 Â± 0.72\n2D U-Net Dice + Focal loss dense 0.831 Â± 0.054 0.856 Â± 0.078 0.843 Â± 0.066 1.95 Â± 0.87 2.48 Â± 0.53 2.21 Â± 0.65\n2D nnU-Net [20] Dice + BCE loss dense 0.858 Â± 0.031 0.877 Â± 0.034 0.868 Â± 0.033 1.93 Â± 0.69 2.26 Â± 0.76 2.09 Â± 0.73\n2.5D U-Net++ [43] Dice + HD loss [46] dense 0.835 Â± 0.080 0.864 Â± 0.083 0.849 Â± 0.081 1.62 Â± 0.86 2.15 Â± 1.01 1.89 Â± 0.93\n3D U-Net Dice + BCE loss dense 0.862 Â± 0.044 0.886 Â± 0.031 0.875 Â± 0.040 1.65 Â± 0.51 1.95 Â± 0.65 1.81 Â± 0.59\nFig. 6. Segmentation results of the proposed and comparison methods trained using\nonly a single center point annotation. Yellow indicates the lumen map and red indicates\nthe wall map. Top: COSMOS2022 dataset. Bottom: Harvard dataset.\nsimilarly unsuccessful and produced false positives. Similarly, when\nusing the same center point annotation as our method for comparison,\nthe segmentation failed even more severely, as shown in Fig. 6 (bot-\ntom). Our method successfully segmented the carotid artery regions in\nthe Harvard dataset. As shown in the first row of the figure, the axial\nview shows the left vessel near the bifurcation area of the CCA and the\nright vessel just before the bifurcation, dividing into the ICA and ECA\nareas. Because our method relies on prior knowledge of the arteryâ€™s\nradius, it failed to accurately segment a wide left CCA area. However,\nin the right bifurcation area, where other fully supervised methods\nfailed, our method achieved relatively successful segmentation. In the\nsecond row, the segmentation of the CCA is displayed. Our method\ndemonstrated respectable segmentation results, even when compared\nwith other methods that used dense annotations. It is important to\nnote that, as in the previous dataset, the annotations in this dataset are\nsparse; ground truth annotations are provided only for the right vessel,\nalthough the left vessel was correctly identified as part of the CCA.\nWe also conducted a quantitative evaluation, the results of which\nare reported in Table 3. Similar to previous quantitative assessments,\nour model, despite being trained on only a single center point, showed\na performance similar to that of the other methods that utilize dense\nannotations.\n4.3. Center point perturbations\nThe proposed method relies on accurate center point annotations.\nHowever, the proposed approach can tolerate a certain degree of error.\nThe model was trained by applying random perturbations of a fixed\namount to the center points in all directions to all slices. The qualitative\nresults are shown in Fig. 7, and the quantitative results are presented\nin Table 4. The results when perturbations were applied to only 10%,\n20%, or 50% of the slices, rather than to all slices, are detailed in Sup-\nplementary Table A. The segmentation performed well when trained\nwith perturbations of 1.2 mm or less in the COSMOS2022 dataset.\nHowever, we observed a significant decrease in the performance on the\nCOSMOS2022 dataset when all annotations were perturbed by more\nthan 1.8 mm. In practical annotation scenarios, it is unlikely that all\ncenter points would be misaligned. Even if some center points were\nslightly off, the predicted results would be expected to remain largely\nunaffected. As shown in Supplementary Table A, when perturbations\nwere applied to only 20% of the slices, there was little difference\nfrom the no-perturbation case, even with a 1.8 mm perturbation. These\nfindings demonstrate that while our method depends on center point\naccuracy, it can still train a robust vessel segmentation model even with\nslight inaccuracies in the center points.\n4.4. Ablation study\nTo evaluate the effectiveness of each component of the proposed\nmethod, we conducted ablation experiments on the test dataset to\ndemonstrate the incremental effect of each one. The quantitative results\nof this study are reported in Table 5 and the qualitative results are\nshown in Fig. 8. As revealed in Table 5(a), when only îˆ¸\nğ‘ğ‘’ğ‘›ğ‘¡_ğ‘‘ğ‘–ğ‘ ğ‘¡ \nwas\nused for training, the model did not utilize the prior knowledge about\nthe radius of the carotid artery, resulting in inaccurate radius detections\nand an inability to differentiate between the lumen and wall. The model\ncould predict the center points, but the segmentation performance was\nsuboptimal. Table 5(b) reports the results when only îˆ¸\nğ‘Ÿğ‘ğ‘‘_ğ‘ğ‘œğ‘›ğ‘ ğ‘¡ \nwas used.\nThe model accurately predicts the radii for the ICA, CCA, and ECA.\nComputer Methods and Programs in Biomedicine 269 (2025) 108881\n7\n\nJ. Kim et al.\nFig. 7. Segmentation results of the proposed method trained with center points perturbed by random offsets. Yellow indicates the lumen map, and red indicates the wall map.\nTop: COSMOS2022 dataset. Bottom: Harvard dataset.\nTable 4\nResults of models trained with perturbed center points on the COSMOS2022 and Harvard datasets.\nDataset COSMOS 2022 Test [39]\nPerturbations DSC\nğ‘™ğ‘¢ \nâ†‘ DSC\nğ‘¤ğ‘ \nâ†‘ DSC\nğ‘ğ‘£ğ‘” \nâ†‘ HD\nğ‘™ğ‘¢ \nâ†“ HD\nğ‘¤ğ‘ \nâ†“ HD\nğ‘ğ‘£ğ‘” \nâ†“\nNone 0.821Â±0.039 0.841Â±0.052 0.832Â±0.048 2.37Â±1.51 2.80Â±1.65 2.58Â±2.58\n0.6 mm 0.802Â±0.087 0.825Â±0.066 0.814Â±0.070 2.89Â±1.24 3.37Â±1.56 3.13Â±1.55\n1.2 mm 0.772Â±0.081 0.798Â±0.064 0.785Â±0.072 3.32Â±1.44 3.85Â±1.59 3.58Â±1.48\n1.8 mm 0.687Â±0.073 0.758Â±0.064 0.723Â±0.066 4.97Â±1.55 4.57Â±1.49 4.77Â±1.52\n2.4 mm 0.623Â±0.070 0.691Â±0.071 0.657Â±0.070 5.79Â±1.80 5.22Â±1.70 5.50Â±1.74\n3.0 mm 0.587Â±0.071 0.679Â±0.059 0.633Â±0.063 6.36Â±1.42 5.32Â±1.47 5.84Â±1.44\nDataset Harvard [40,41]\nPerturbations DSC\nğ‘™ğ‘¢ \nâ†‘ DSC\nğ‘¤ğ‘ \nâ†‘ DSC\nğ‘ğ‘£ğ‘” \nâ†‘ HD\nğ‘™ğ‘¢ \nâ†“ HD\nğ‘¤ğ‘ \nâ†“ HD\nğ‘ğ‘£ğ‘” \nâ†“\nNone 0.823Â±0.055 0.847Â±0.042 0.835Â± 0.049 2.05Â±1.05 2.52Â±1.36 2.29Â±1.28\n0.35 mm 0.800Â±0.035 0.834Â±0.040 0.817Â±0.036 2.82Â±0.85 3.05Â±0.80 2.93Â±0.81\n0.70 mm 0.744Â±0.041 0.831Â±0.043 0.788Â±0.042 3.16Â±0.83 3.15Â±0.86 3.15Â±0.84\n1.05 mm 0.719Â±0.064 0.802Â±0.044 0.761Â±0.050 3.62Â±0.78 3.58Â±0.92 3.60Â±0.88\n1.40 mm 0.670Â±0.068 0.776Â±0.044 0.723Â±0.053 4.23Â±0.77 3.89Â±0.89 4.06Â±0.85\n1.75 mm 0.645Â±0.067 0.742Â±0.052 0.694Â±0.057 4.57Â±0.71 4.47Â±0.79 4.52Â±0.76\nTable 5\nAblation study of the effects of each loss term on the COSMOS2022 test set. DSC refers to the Dice coefficient for the entire vessel region, which combines the lumen and wall.\nMethod îˆ¸\nğ‘ğ‘’ğ‘›ğ‘¡_ğ‘‘ğ‘–ğ‘ ğ‘¡ \nîˆ¸\nğ‘Ÿğ‘ğ‘‘_ğ‘ğ‘œğ‘›ğ‘ ğ‘¡ \nîˆ¸\nğ‘ğ‘œğ‘›ğ‘ ğ‘–ğ‘  \nDSC\nğ‘™ğ‘¢ \nâ†‘ DSC\nğ‘¤ğ‘ \nâ†‘ DSC\nğ‘ğ‘£ğ‘” \nâ†‘ HD\nğ‘™ğ‘¢ \nâ†“ HD\nğ‘¤ğ‘ \nâ†“ HD\nğ‘ğ‘£ğ‘” \nâ†“\n(a) âœ“ 0.271 Â± 0.062 0.358 Â± 0.057 0.315 Â± 0.060 9.51 Â± 3.96 8.70 Â± 3.78 9.11 Â± 3.87\n(b) âœ“ 0.787 Â± 0.029 0.806 Â± 0.045 0.797 Â± 0.040 2.68 Â± 1.83 2.97 Â± 1.88 2.82 Â± 1.86\n(c) âœ“ âœ“ 0.818 Â± 0.035 0.837 Â± 0.048 0.823 Â± 0.044 2.63 Â± 1.85 3.01 Â± 1.96 2.83 Â± 1.92\nOurs âœ“ âœ“ âœ“ 0.821 Â± 0.039 0.841 Â± 0.052 0.832 Â± 0.048 2.37 Â± 1.51 2.80 Â± 1.65 2.58 Â± 1.61\nHowever, as shown in the first row of Fig. 8, there are occasional false\npositives in other vascular areas. When both îˆ¸\nğ‘ğ‘’ğ‘›ğ‘¡_ğ‘‘ğ‘–ğ‘ ğ‘¡ \nand îˆ¸\nğ‘Ÿğ‘ğ‘‘_ğ‘ğ‘œğ‘›ğ‘ ğ‘¡ \nwere\nutilized, as revealed in Table 5(c), there is a significant improvement in\nthe accuracy compared with the use of each loss independently. The ad-\ndition of îˆ¸\nğ‘ğ‘œğ‘›ğ‘ ğ‘–ğ‘ \n, which leverages adjacent information between slices,\nfurther increases accuracy, particularly in the HD metric, allowing for\nmore flexible and accurate vascular predictions while maintaining the\nassumption of circularity. As shown in Fig. 8, using all proposed loss\ncomponents to train the model results in segmentation outcomes that\nare most similar to the ground truth. As in the previous figure, while the\nbottom row of Fig. 8 shows the correct prediction of CCA, the ground\ntruth annotations are only present on the left side of that slice.\nFurthermore, to determine the most efficient learning configuration\nwith respect to the percentage of annotated slices, we conducted exper-\niments by adjusting the percentage of annotated slices in the 3D volume\nto 5%, 10%, 20%, 50%, and 100%, and the results are shown in Fig.\n9. The left side of Fig. 9 displays the DSC, and the right side of Fig. 9\npresents the HD\nğ‘™ğ‘¢ \nand HD\nğ‘¤ğ‘\n. Our method demonstrated the ability to\nachieve effective learning with as little as 5% of the point annotations,\nbut the optimal performance in terms of annotation efficiency was\nobserved at 20%. Although the best performance was achieved using\n100% of the annotations (i.e., fully supervised), our method achieved\ncomparable performance even with a significant reduction in annota-\ntions. This suggests that annotating roughly one-fifth of the slices is\nsufficient and the setting is effective across individuals and the presence\nof atherosclerosis. We also investigated the impact of adjusting the\nhyperparameters, specifically the lumen radius (ğ‘Ÿ\nğ‘™ğ‘¢\n) and wall radius\n(ğ‘Ÿ\nğ‘¤ğ‘\n), on the segmentation performance. The results are depicted in\nFig. 10 and reveal that setting the lumen radius to 5.5 pixels and\nthe wall radius to 7.5 pixels provided the most suitable segmentation\nperformance.\n5. Discussion\nOur method is a specialized weakly supervised approach for seg-\nmenting carotid artery vessels, demonstrating that effective segmen-\ntation of the lumen and wall in the ICA, ECA, and CCA regions is\npossible even when trained using only a single center point annotation.\nHowever, in our study, the loss function was designed based on the\nassumption that the vessels exhibited a circular shape. This assumption\nmay not hold across all axial views of the carotid artery, potentially\nleading to significant prediction errors where the actual shape deviates\nsubstantially from a circle. Despite these challenges, the proposed\nmethod offers considerable advantages as a weakly supervised tech-\nnique. The use of only a single center point and the ability to maintain\nperformance with sparse annotations substantially reduces the effort\nrequired for annotation. This efficiency enabled accurate segmentation\nacross most carotid artery vessel areas, making our approach highly\npromising. Although we initially set the radius based on existing clin-\nical research [28â€“30], manually determining the appropriate radius\nis not particularly challenging. The radius can be easily estimated by\nComputer Methods and Programs in Biomedicine 269 (2025) 108881\n8\n\nJ. Kim et al.\nFig. 8. Qualitative results of the ablation study. (a), (b), and (c) present the results of the methods outlined in Table 4. The first row displays the results for the ICA and ECA\nregions. Note that ground truth does not include the ECA regions. The second row shows the corresponding CCA regions. When utilizing all three proposed loss functions, the\nresults demonstrate successful segmentation of the ICA, ECA, and CCA regions. The red and blue bounding boxes are magnified to depict the regions of the left and right vessels.\nFig. 9. Variation in the performance of (left) DSC\nğ‘¤ğ‘ \n, (middle) HD\nğ‘™ğ‘¢ \n, and (right) HD\nğ‘¤ğ‘ \nwith respect to the number of point annotations used (x-axis). The line represents the mean\nmetric of a five-fold ensemble model, whereas the shaded band indicates the upper and lower bounds across the five folds for the test dataset.\nFig. 10. Performance of the DSC and HD with respect to radius in the radius constraint loss. (Left) DSC\nğ‘¤ğ‘ \nvalues and (middle) HD\nğ‘™ğ‘¢ \nvalues as the outer wall radius changes, and\n(right) HD\nğ‘¤ğ‘ \nvalues as the lumen radius changes.\nspecifying two points in the CCA region (Supplementary Fig. A). Fur-\nthermore, even if the radius is not precisely determined, the decrease in\nperformance remains limited (Fig. 10). Thus, there is a minor trade-off\nbetween segmentation performance and choice of radius.\nThe consistency loss operates best when the axial slices are per-\npendicular to the direction of the artery. However, this loss remains\napplicable under practical conditions when the slices are not per-\npendicular. Even in these cases, the proposed consistency loss may\ncompensate for this error through regularization. Since adjacent slices\ndo not perfectly match, adjusting the temperature parameter ğ‘‡ en-\ncourages the probability maps of adjacent slices to be similar but\nnot necessarily identical. In addition, we can lower the weight of the\nconsistency loss (ğœ†\n3\n) in the loss function.\nOur method utilizes prior clinical knowledge regarding the range of\ncarotid artery radii and applies it in the modeling. Thus, our method\nis mainly applicable to the normal population, whose vessel radii are\nlikely to fall within the specified range. Our method is likely to suffer\nfrom degradation when applied to the MRIs of patients with certain\ndisease conditions. Further research on the flexibility of the prior in\nterms of the radius is required. However, we can further adjust the\nradius as necessary when considering a patient who could have a\nspecific disease (Supplementary Fig. B). However, there are limitations\nto the use of a fixed radius. The radius of the vessel can vary among\nregions and subjects. Therefore, if the vessel radius can be adaptively\ndetermined for each slice and applied in the modeling, more accurate\nvessel segmentation could be achieved.\nOur method assumes a circular vessel shape, making it challenging\nto accurately model areas with bifurcations or plaques. The perfor-\nmance of atherosclerotic and normal vessels is compared in Supple-\nmentary Table B. We observed that segmentation performance on\natherosclerotic slices (DSC\nğ‘™ğ‘¢ \n0.80) was lower than those of normal\nvessels (DSC\nğ‘™ğ‘¢ \n0.83), as expected. The variability in the segmentation\nComputer Methods and Programs in Biomedicine 269 (2025) 108881\n9\n\nJ. Kim et al.\nof atherosclerotic slices was higher than those of a normal vessel, re-\nflecting the difficulty of segmenting diseased vessels. A similar trend is\nobserved in a representative fully supervised method. The heterogene-\nity of the atherosclerotic vessels made the prediction more difficult,\neven with a supervised method, which yielded less variable results\nthan our weakly supervised method. When the true vessel geometry\ndeviates significantly from circularity, such as in severely diseased or\nbifurcating segments, this imposed constraint may introduce discrep-\nancies, often reflected as an increased HD compared with the ground\ntruth segmentation. Assuming an elliptical rather than a circular shape\nand imposing more precise shape constraints could enable even more\naccurate segmentation. Applying such approaches using just a single\ncenter point is challenging. However, with both center and contour\npoint annotations, we believe that these techniques can be modeled\neffectively. This approach could be further developed in future work.\nWe proposed a method for segmenting the lumen and wall of carotid\nvessels using sparse single center points along the axial direction. The\nproposed method uses a single annotated point per slice and does\nnot require annotation for all slices, thus significantly reducing the\nannotation effort. To validate our proposed method, we conducted an\nablation study that confirmed the positive impact of each component on\nsegmentation performance. Additionally, we demonstrated that using\nonly 20% of the labels instead of all labels still yields adequate segmen-\ntation results, effectively reducing annotation efforts fivefold. We set\nthe radius in our method based on known clinical research and tested it\nwith various radii to verify its effectiveness. Our method demonstrated\na performance similar to that of a model trained using dense annota-\ntions. In addition, the model could effectively segment the ICA, ECA,\nand CCA, which are important for diagnosing arteriosclerosis.\nCRediT authorship contribution statement\nJonghun Kim: Writing â€“ original draft, Methodology, Investigation,\nFormal analysis, Data curation, Conceptualization. Inye Na: Formal\nanalysis. Junmo Kwon: Investigation. Woo-Keun Seo: Investigation.\nHyunjin Park: Writing â€“ original draft, Supervision, Methodology,\nInvestigation.\nEthical statement\nThis study was approved by the Institutional Review Boards of the\ndata collection sites of COSMOS2022 and HARVARD datasets.\nDeclaration of competing interest\nNothing to declare.\nAcknowledgments\nThis study was supported by National Research Foundation of\nKorea (RS-2024-00408040), AI Graduate School Support Program\n(Sungkyunkwan University) (RS-2019-II190421), ICT Creative Con-\nsilience program (IITP-2025-RS-2020-II201821), and the Artificial In-\ntelligence Innovation Hub program (RS-2021-II212068).\nAppendix A. Supplementary data\nSupplementary material related to this article can be found online\nat https://doi.org/10.1016/j.cmpb.2025.108881.\nReferences\n[1] C. Yuan, W.S. Kerwin, MRI of atherosclerosis, J. Magn. Reson. Imaging: An Off.\nJ. Int. Soc. Magn. Reson. Med. 19 (6) (2004) 710â€“719.\n[2] H.R. Underhill, T.S. Hatsukami, Z.A. Fayad, V. Fuster, C. Yuan, MRI of carotid\natherosclerosis: clinical implications and future directions, Nat. Rev. Cardiol. 7\n(3) (2010) 165â€“173.\n[3] R. Corti, V. Fuster, Imaging of atherosclerosis: magnetic resonance imaging, Eur.\nHeart J. 32 (14) (2011) 1709â€“1719.\n[4] I.M. Adame, P.J. De Koning, B.P. Lelieveldt, B.A. Wasserman, J.H. Reiber, R.J.\nVan Der Geest, An integrated automated analysis method for quantifying vessel\nstenosis and plaque burden from carotid MRI images: combined postprocessing\nof MRA and vessel wall MR, Stroke 37 (8) (2006) 2162â€“2164.\n[5] J. Liu, J. Sun, N. Balu, M.S. Ferguson, J. Wang, W.S. Kerwin, D.S. Hippe, A.\nWang, T.S. Hatsukami, C. Yuan, Semiautomatic carotid intraplaque hemorrhage\nvolume measurement using 3D carotid MRI, J. Magn. Reson. Imaging 50 (4)\n(2019) 1055â€“1062.\n[6] J. Wu, J. Xin, X. Yang, J. Sun, D. Xu, N. Zheng, C. Yuan, Deep morphology aided\ndiagnosis network for segmentation of carotid artery vessel wall and diagnosis\nof carotid atherosclerosis on black-blood vessel wall MRI, Med. Phys. 46 (12)\n(2019) 5544â€“5561.\n[7] W. Xu, X. Yang, Y. Li, G. Jiang, S. Jia, Z. Gong, Y. Mao, S. Zhang, Y. Teng, J.\nZhu, et al., Deep learning-based automated detection of arterial vessel wall and\nplaque on magnetic resonance vessel wall images, Front. Neurosci. 16 (2022)\n888814.\n[8] D. Lin, J. Dai, J. Jia, K. He, J. Sun, Scribblesup: Scribble-supervised convolutional\nnetworks for semantic segmentation, in: Proceedings of the IEEE Conference on\nComputer Vision and Pattern Recognition, 2016, pp. 3159â€“3167.\n[9] H.R. Roth, D. Yang, Z. Xu, X. Wang, D. Xu, Going to extremes: weakly supervised\nmedical image segmentation, Mach. Learn. Knowl. Extr. 3 (2) (2021) 507â€“524.\n[10] R. Dorent, S. Joutard, J. Shapey, A. Kujawa, M. Modat, S. Ourselin, T. Ver-\ncauteren, Inter extreme points geodesics for end-to-end weakly supervised image\nsegmentation, in: Medical Image Computing and Computer Assisted Interventionâ€“\nMICCAI 2021: 24th International Conference, Strasbourg, France, September\n27â€“October 1, 2021, Proceedings, Part II 24, Springer, 2021, pp. 615â€“624.\n[11] G. Wang, W. Li, M.A. Zuluaga, R. Pratt, P.A. Patel, M. Aertsen, T. Doel, A.L.\nDavid, J. Deprest, S. Ourselin, et al., Interactive medical image segmentation\nusing deep learning with image-specific fine tuning, IEEE Trans. Med. Imaging\n37 (7) (2018) 1562â€“1573.\n[12] K. Zhang, X. Zhuang, Shapepu: A new pu learning framework regularized by\nglobal consistency for scribble supervised cardiac segmentation, in: International\nConference on Medical Image Computing and Computer-Assisted Intervention,\nSpringer, 2022, pp. 162â€“172.\n[13] G. Papandreou, L.C. Chen, K.P. Murphy, A.L. Yuille, Weakly-and semi-supervised\nlearning of a deep convolutional network for semantic image segmentation, in:\nProceedings of the IEEE International Conference on Computer Vision, 2015, pp.\n1742â€“1750.\n[14] H. Kervadec, J. Dolz, S. Wang, E. Granger, I.B. Ayed, Bounding boxes for weakly\nsupervised segmentation: Global constraints get close to full supervision, in:\nMedical Imaging with Deep Learning, PMLR, 2020, pp. 365â€“381.\n[15] O. Ronneberger, P. Fischer, T. Brox, U-net: Convolutional networks for biomedi-\ncal image segmentation, in: Medical Image Computing and Computer-Assisted\nInterventionâ€“MICCAI 2015: 18th International Conference, Munich, Germany,\nOctober 5â€“9, 2015, Proceedings, Part III 18, Springer, 2015, pp. 234â€“241.\n[16] O. Oktay, J. Schlemper, L.L. Folgoc, M. Lee, M. Heinrich, K. Misawa, K. Mori,\nS. McDonagh, N.Y. Hammerla, B. Kainz, B. Glocker, D. Rueckert, Attention U-\nNet: Learning where to look for the pancreas, in: Medical Imaging with Deep\nLearning, 2018, URL: https://openreview.net/forum?id=Skft7cijM.\n[17] F.I. Diakogiannis, F. Waldner, P. Caccetta, C. Wu, ResUNet-a: A deep learn-\ning framework for semantic segmentation of remotely sensed data, ISPRS J.\nPhotogramm. Remote Sens. 162 (2020) 94â€“114.\n[18] A. Hatamizadeh, Y. Tang, V. Nath, D. Yang, A. Myronenko, B. Landman,\nH.R. Roth, D. Xu, Unetr: Transformers for 3d medical image segmentation, in:\nProceedings of the IEEE/CVF Winter Conference on Applications of Computer\nVision, 2022, pp. 574â€“584.\n[19] A. Hatamizadeh, V. Nath, Y. Tang, D. Yang, H.R. Roth, D. Xu, Swin unetr:\nSwin transformers for semantic segmentation of brain tumors in mri images,\nin: International MICCAI Brainlesion Workshop, Springer, 2021, pp. 272â€“284.\n[20] F. Isensee, P.F. Jaeger, S.A. Kohl, J. Petersen, K.H. Maier-Hein, nnU-Net: a\nself-configuring method for deep learning-based biomedical image segmentation,\nNature Methods 18 (2) (2021) 203â€“211.\n[21] D. Alblas, C. Brune, J.M. Wolterink, Deep-learning-based carotid artery vessel\nwall segmentation in black-blood MRI using anatomical priors, in: Medical\nImaging 2022: Image Processing, vol. 12032, SPIE, 2022, pp. 237â€“244.\n[22] X. Huang, J. Wang, Z. Li, 3D carotid artery segmentation using shape-constrained\nactive contours, Comput. Biol. Med. 153 (2023) 106530.\n[23] X. Li, X. Ouyang, J. Zhang, Z. Ding, Y. Zhang, Z. Xue, F. Shi, D. Shen, Carotid\nvessel wall segmentation through domain aligner, topological learning, and\nsegment anything model for sparse annotation in mr images, IEEE Trans. Med.\nImaging (2024).\nComputer Methods and Programs in Biomedicine 269 (2025) 108881\n10\n\nJ. Kim et al.\n[24] A. Kirillov, E. Mintun, N. Ravi, H. Mao, C. Rolland, L. Gustafson, T. Xiao, S.\nWhitehead, A.C. Berg, W.Y. Lo, et al., Segment anything, in: Proceedings of the\nIEEE/CVF International Conference on Computer Vision, 2023, pp. 4015â€“4026.\n[25] M. Rajchl, M.C. Lee, O. Oktay, K. Kamnitsas, J. Passerat-Palmbach, W. Bai, M.\nDamodaram, M.A. Rutherford, J.V. Hajnal, B. Kainz, et al., Deepcut: Object seg-\nmentation from bounding box annotations using convolutional neural networks,\nIEEE Trans. Med. Imaging 36 (2) (2016) 674â€“683.\n[26] X. Luo, M. Hu, W. Liao, S. Zhai, T. Song, G. Wang, S. Zhang, Scribble-supervised\nmedical image segmentation via dual-branch network and dynamically mixed\npseudo labels supervision, in: International Conference on Medical Image\nComputing and Computer-Assisted Intervention, Springer, 2022, pp. 528â€“538.\n[27] I. Laradji, P. Rodriguez, O. Manas, K. Lensink, M. Law, L. Kurzman, W. Parker,\nD. Vazquez, D. Nowrouzezahrai, A weakly supervised consistency-based learning\nmethod for covid-19 segmentation in ct images, in: Proceedings of the IEEE/CVF\nWinter Conference on Applications of Computer Vision, 2021, pp. 2453â€“2462.\n[28] N. Balu, V.L. Yarnykh, B. Chu, J. Wang, T. Hatsukami, C. Yuan, Carotid plaque\nassessment using fast 3D isotropic resolution black-blood MRI, Magn. Reson.\nMed. 65 (3) (2011) 627â€“637.\n[29] Y. Cai, L. He, C. Yuan, H. Chen, Q. Zhang, R. Li, C. Li, X. Zhao, Atherosclerotic\nplaque features and distribution in bilateral carotid arteries of asymptomatic\nelderly population: a 3D multicontrast MR vessel wall imaging study, Eur. J.\nRadiol. 96 (2017) 6â€“11.\n[30] Y. Qiao, D.A. Steinman, Q. Qin, M. Etesami, M. SchÃ¤r, B.C. Astor, B.A. Wasser-\nman, Intracranial arterial wall imaging using three-dimensional high isotropic\nresolution black blood MRI at 3.0 tesla, J. Magn. Reson. Imaging 34 (1) (2011)\n22â€“30.\n[31] X. Zhang, J. Zhang, L. Ma, P. Xue, Y. Hu, D. Wu, Y. Zhan, J. Feng, D. Shen, Pro-\ngressive deep segmentation of coronary artery via hierarchical topology learning,\nin: International Conference on Medical Image Computing and Computer-Assisted\nIntervention, Springer, 2022, pp. 391â€“400.\n[32] R. Camarasa, H. Kervadec, D. Bos, M. de Bruijne, Differentiable boundary\npoint extraction for weakly supervised star-shaped object segmentation, in:\nInternational Conference on Medical Imaging with Deep Learning, PMLR, 2022,\npp. 188â€“198.\n[33] H. Kervadec, H. Bahig, L. Letourneau-Guillon, J. Dolz, I.B. Ayed, Beyond pixel-\nwise supervision for segmentation: A few global shape descriptors might be\nsurprisingly good! in: Medical Imaging with Deep Learning, PMLR, 2021, pp.\n354â€“368.\n[34] A. Mitiche, I.B. Ayed, Variational and Level Set Methods in Image Segmentation,\nvol. 5, Springer Science & Business Media, 2010.\n[35] A. Sironi, E. TÃ¼retken, V. Lepetit, P. Fua, Multiscale centerline detection, IEEE\nTrans. Pattern Anal. Mach. Intell. 38 (7) (2015) 1327â€“1341.\n[36] D. Pathak, P. Krahenbuhl, J. Donahue, T. Darrell, A.A. Efros, Context encoders:\nFeature learning by inpainting, in: Proceedings of the IEEE Conference on\nComputer Vision and Pattern Recognition, 2016, pp. 2536â€“2544.\n[37] G. Hinton, O. Vinyals, J. Dean, Distilling the knowledge in a neural network,\n2015, arXiv preprint arXiv:1503.02531.\n[38] vws2022, Training dataset for Carotid Vessel Wall Segmentation and Atheroscle-\nrosis Diagnosis Challenge, MICCAI 2022, Zenodo, 2022, http://dx.doi.org/10.\n5281/zenodo.6481870.\n[39] COSMOS 2022, Testing dataset for Carotid Vessel Wall Segmentation and\nAtherosclerosis Diagnosis Challenge, MICCAI 2022, Zenodo, 2022, http://dx.doi.\norg/10.5281/zenodo.6843257.\n[40] H. Guo, Multi-modal Carotid Image from MRI and US, Harvard Dataverse, 2015,\nhttp://dx.doi.org/10.7910/DVN/LYH9UG.\n[41] H. Guo, G. Wang, L. Huang, Y. Hu, C. Yuan, R. Li, X. Zhao, A robust and accurate\ntwo-step auto-labeling conditional iterative closest points (TACICP) algorithm\nfor three-dimensional multi-modal carotid image registration, PLoS One 11 (2)\n(2016) e0148783.\n[42] Z. Liang, T. Wang, X. Zhang, J. Sun, J. Shen, Tree energy loss: Towards sparsely\nannotated semantic segmentation, in: Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, 2022, pp. 16907â€“16916.\n[43] Z. Zhou, M.M. Rahman Siddiquee, N. Tajbakhsh, J. Liang, Unet++: A nested u-\nnet architecture for medical image segmentation, in: Deep Learning in Medical\nImage Analysis and Multimodal Learning for Clinical Decision Support: 4th\nInternational Workshop, DLMIA 2018, and 8th International Workshop, ML-CDS\n2018, Held in Conjunction with MICCAI 2018, Granada, Spain, September 20,\n2018, Proceedings 4, Springer, 2018, pp. 3â€“11.\n[44] S. Jadon, A survey of loss functions for semantic segmentation, in: 2020 IEEE\nConference on Computational Intelligence in Bioinformatics and Computational\nBiology, CIBCB, IEEE, 2020, pp. 1â€“7.\n[45] T.Y. Lin, P. Goyal, R. Girshick, K. He, P. DollÃ¡r, Focal loss for dense object\ndetection, in: Proceedings of the IEEE International Conference on Computer\nVision, 2017, pp. 2980â€“2988.\n[46] H. Zhou, J. Xiao, Z. Fan, D. Ruan, Intracranial vessel wall segmentation\nfor atherosclerotic plaque quantification, in: 2021 IEEE 18th International\nSymposium on Biomedical Imaging, ISBI, IEEE, 2021, pp. 1416â€“1419.\n[47] Ã–. Ã‡iÃ§ek, A. Abdulkadir, S.S. Lienkamp, T. Brox, O. Ronneberger, 3D U-Net:\nlearning dense volumetric segmentation from sparse annotation, in: Medical\nImage Computing and Computer-Assisted Interventionâ€“MICCAI 2016: 19th In-\nternational Conference, Athens, Greece, October 17â€“21, 2016, Proceedings, Part\nII 19, Springer, 2016, pp. 424â€“432.\n[48] L.C. Chen, G. Papandreou, F. Schroff, H. Adam, Rethinking atrous convolution\nfor semantic image segmentation, 2017, arXiv preprint arXiv:1706.05587.\nComputer Methods and Programs in Biomedicine 269 (2025) 108881\n11",
    "version": "5.3.31"
  },
  {
    "numpages": 15,
    "numrender": 15,
    "info": {
      "PDFFormatVersion": "1.7",
      "Language": "English",
      "EncryptFilterName": null,
      "IsLinearized": true,
      "IsAcroFormPresent": false,
      "IsXFAPresent": false,
      "IsCollectionPresent": false,
      "IsSignaturesPresent": false,
      "Creator": "Elsevier",
      "Custom": {
        "CrossMarkDomains[1]": "elsevier.com",
        "CrossmarkMajorVersionDate": "2010-04-23",
        "CreationDate--Text": "10th October 2025",
        "ElsevierWebPDFSpecifications": "7.0.1",
        "robots": "noindex",
        "doi": "10.1016/j.cmpb.2025.109043",
        "CrossMarkDomains[2]": "sciencedirect.com",
        "CrossmarkDomainExclusive": "true"
      },
      "ModDate": "D:20251010040945Z",
      "Author": "Antonio Candito",
      "Title": "A weakly-supervised deep learning model for fast localisation and delineation of the skeleton, internal organs, and spinal canal on whole-body diffusion-weighted MRI (WB-DWI)",
      "Keywords": "Deep learning,Segmentation,Whole-Body Diffusion-Weighted MRI (WB-DWI),Quantitative Imaging Biomarkers,Metastatic bone disease,Multiple Myeloma",
      "CreationDate": "D:20251010005533Z",
      "Producer": "Acrobat Distiller 8.1.0 (Windows)",
      "Subject": "Computer Methods and Programs in Biomedicine, 272 (2025) 109043. doi:10.1016/j.cmpb.2025.109043"
    },
    "metadata": {
      "crossmark:crossmarkdomains": "elsevier.comsciencedirect.com",
      "crossmark:crossmarkdomainexclusive": "true",
      "crossmark:doi": "10.1016/j.cmpb.2025.109043",
      "crossmark:majorversiondate": "2010-04-23",
      "dc:format": "application/pdf",
      "dc:identifier": "10.1016/j.cmpb.2025.109043",
      "dc:publisher": "Elsevier B.V.",
      "dc:description": "Computer Methods and Programs in Biomedicine, 272 (2025) 109043. doi:10.1016/j.cmpb.2025.109043",
      "dc:subject": [
        "Deep learning",
        "Segmentation",
        "Whole-Body Diffusion-Weighted MRI (WB-DWI)",
        "Quantitative Imaging Biomarkers",
        "Metastatic bone disease",
        "Multiple Myeloma"
      ],
      "dc:title": "A weakly-supervised deep learning model for fast localisation and delineation of the skeleton, internal organs, and spinal canal on whole-body diffusion-weighted MRI (WB-DWI)",
      "dc:creator": [
        "Antonio Candito",
        "Alina Dragan",
        "Richard Holbrey",
        "Ana Ribeiro",
        "Ricardo Donners",
        "Christina Messiou",
        "Nina Tunariu",
        "Dow-Mu Koh",
        "Matthew D Blackledge"
      ],
      "jav:journal_article_version": "VoR",
      "pdf:creationdate--text": "10th October 2025",
      "pdf:producer": "Acrobat Distiller 8.1.0 (Windows)",
      "pdf:keywords": "Deep learning,Segmentation,Whole-Body Diffusion-Weighted MRI (WB-DWI),Quantitative Imaging Biomarkers,Metastatic bone disease,Multiple Myeloma",
      "pdfx:creationdate--text": "10th October 2025",
      "pdfx:crossmarkdomains": "sciencedirect.comelsevier.com",
      "pdfx:crossmarkdomainexclusive": "true",
      "pdfx:crossmarkmajorversiondate": "2010-04-23",
      "lmfnsmpioz9j_m9agntuqmsnnnm6rlwenod-godz-zwj8n9eqmwr-o9eqn9iknm2mndytma": "",
      "pdfx:doi": "10.1016/j.cmpb.2025.109043",
      "pdfx:robots": "noindex",
      "prism:aggregationtype": "journal",
      "prism:copyright": "Â© 2025 Published by Elsevier B.V.",
      "prism:coverdate": "2025-12-01",
      "prism:coverdisplaydate": "1 December 2025",
      "prism:doi": "10.1016/j.cmpb.2025.109043",
      "prism:issn": "0169-2607",
      "prism:pagerange": "109043",
      "prism:publicationname": "Computer Methods and Programs in Biomedicine",
      "prism:startingpage": "109043",
      "prism:url": "https://doi.org/10.1016/j.cmpb.2025.109043",
      "prism:volume": "272",
      "tdm:policy": "https://www.elsevier.com/tdm/tdmrep-policy.json",
      "tdm:reservation": "1",
      "xmp:createdate": "2025-10-10T00:55:33",
      "xmp:creatortool": "Elsevier",
      "xmp:metadatadate": "2025-10-10T04:09:45",
      "xmp:modifydate": "2025-10-10T04:09:45",
      "xmpmm:documentid": "uuid:1b499bed-4ce8-4c0c-b682-0d58baae1cbe",
      "xmpmm:instanceid": "uuid:b6ed5266-03cb-4855-90c0-636283357f42",
      "xmprights:marked": "True"
    },
    "text": "A weakly-supervised deep learning model for fast localisation and\ndelineation of the skeleton, internal organs, and spinal canal on whole-body\ndiffusion-weighted MRI (WB-DWI)\nAntonio Candito \na \n, Alina Dragan \na,b \n, Richard Holbrey \na\n, Ana Ribeiro \nb \n, Ricardo Donners \nc\n,\nChristina Messiou \na,b \n, Nina Tunariu \na,b\n, Dow-Mu Koh \na,b\n, Matthew D Blackledge \na,*\na \nThe Institute of Cancer Research, London, UK\nb \nThe Royal Marsden NHS Foundation Trust, London, UK\nc \nUniversity Hospital Basel, Basel, Switzerland\nA R T I C L E I N F O\nKeywords:\nDeep learning\nSegmentation\nWhole-Body Diffusion-Weighted MRI (WB-\nDWI)\nQuantitative Imaging Biomarkers\nMetastatic bone disease\nMultiple Myeloma\nA B S T R A C T\nBackground and Objective: Apparent Diffusion Coefficient (ADC) values and Total Diffusion Volume (TDV) from\nWhole-body diffusion-weighted MRI (WB-DWI) are recognised cancer imaging biomarkers. However, manual\ndisease delineation for ADC and TDV measurements is unfeasible in clinical practice, demanding automation. As\na first step, we propose an algorithm to generate fast and reproducible probability maps of the skeleton, adjacent\ninternal organs (liver, spleen, urinary bladder, and kidneys), and spinal canal.\nMethods: We developed an automated deep-learning pipeline based on a 3D patch-based Residual U-Net archi-\ntecture that localises and delineates these anatomical structures on WB-DWI. The algorithm was trained using\nâ€œsoft-labelsâ€ (non-binary segmentations) derived from a computationally intensive atlas-based approach. For\ntraining and validation, we employed a multi-centre WB-DWI dataset comprising 532 scans from patients with\nAdvanced Prostate Cancer (APC) or Multiple Myeloma (MM), with testing on 45 patients.\nResults: Our weakly-supervised deep learning model achieved an average dice score of 0.67 for whole skeletal\ndelineation, 0.76 when excluding ribcage, 0.83 for internal organs, and 0.86 for spinal canal, with average\nsurface distances below 3 mm. Relative median ADC differences between automated and manual full-body de-\nlineations were below 10 %. The model was 12x faster than the atlas-based registration algorithm (25 s vs. 5\nmin). Two experienced radiologists rated the modelâ€™s outputs as either â€œgoodâ€ or â€œexcellentâ€ on test scans, with\ninter-reader agreement from fair to substantial (Gwetâ€™s AC1=0.27â€“0.72).\nConclusion: The model offers fast, reproducible probability maps for localising and delineating body regions on\nWB-DWI, potentially enabling non-invasive imaging biomarkers quantification to support disease staging and\ntreatment response assessment.\n1. Background\nWhole-body diffusion-weighted MRI (WB-DWI) enables detection,\nstaging, and response assessment of systemic malignant disease in pa-\ntients diagnosed with Advanced Prostate Cancer (APC) or Multiple\nMyeloma (MM) [1â€“3]. Bone lesions exhibit high DWI signal intensity\ndue to restricted water mobility within tumour-infiltrated bone marrow\n[4], which can be quantified using the Apparent Diffusion Coefficient\n(ADC) derived from images acquired at multiple b-values (typically\n0â€“50 and 900â€“1000 s/mm\n2\n) [5,6]. ADC inversely correlates with tissue\ncellularity and serves as a potent biomarker of response [7,8]. Addi-\ntionally, the Total Diffusion Volume (TDV, in millilitres) of delineated\nbone disease on WB-DWI reflects estimated tumour burden [9,10].\nHowever, radiologists need to manually delineate bone lesions\nthroughout the skeleton to measure ADC and TDV. This process is\ncumbersome and time-consuming, often requiring an hour or more,\ndepending on disease burden [11]. As a result, manual delineation is not\nfeasible for routine reporting workflows. There is a growing clinical\ndemand for software solutions that can automate this task, assisting\nradiologists in quantifying bone disease in patients diagnosed with APC\n* Corresponding author.\nE-mail address: Matthew.Blackledge@icr.ac.uk (M.D. Blackledge).\nContents lists available at ScienceDirect\nComputer Methods and Programs in Biomedicine\njournal homepage: www.sciencedirect.com/journal/computer-methods-\nand-programs-in-biomedicine\nhttps://doi.org/10.1016/j.cmpb.2025.109043\nReceived 26 March 2025; Received in revised form 13 August 2025; Accepted 23 August 2025\nComputer Methods and Programs in Biomedicine 272 (2025) 109043\nAvailable online 28 August 2025\n0169-2607/Â© 2025 Published by Elsevier B.V.\n\nor MM [12]. We hypothesise that a logical first step toward automation\nis developing an algorithm to localise and delineate the whole skeleton\nand surrounding anatomy from WB-DWI scans. This would isolate bone\nfrom adjacent anatomical structures and/or non-skeletal regions that\nexhibit hyperintense signals on b900 images. Similar two-step pipelines\nhave been developed for delineating lytic bone lesions on whole-body\nCT [13] and focal bone marrow lesions on MRI of the left pelvis only\n[14].\nAtlas-based registration algorithms and deep learning models have\nbeen proposed for delineating bone marrow from MRI sequences with\nhigher resolution and signal-to-noise ratio (SNR) than WB-DWI. Ceranka\net al [15] developed a multi-atlas delineation algorithm for T1-weighted\n(T1w) MRI in APC patients, achieving a dice score of 0.88 between\nmanual and automated masks. Lavdas et al [16] applied atlas-based\nregistration to T2-weighted (T2w) MRI in healthy volunteers, report-\ning a dice score of 0.71. However, the accuracy of atlas-derived de-\nlineations may be affected by the characteristics of the atlas cohort,\nincluding the extent of disease spread across patients and how experts\ndefined the ground truth, as well as the initial spatial alignment between\nwhole-body target and atlas image [17,18]. The computational time\nrequired for registrations may limits clinical deployment.\nDeep learning models aim to address these limitations. A nnU-Net\ntrained on coronal T1w MRI for bone marrow delineation in myeloma\npatients achieved a dice score of 0.88 [19,20], while Dâ€™Antonoli et al\n[21] released the open-source TotalSegmentator-MRI model for\nmulti-organ delineation from multiple MRI sequences. However,\ndeveloping such models requires high-quality annotations across\nhigh-volume WB-DWI datasets [22], an impracticable task due to time\nand resource constraints commonly faced in radiology departments.\nFurthermore, automated delineation of the bone marrow space from\nWB-DWI scans remains a complex task due to inter-patient heteroge-\nneity, the varied appearance and distribution of disease, and the pres-\nence of imaging artifacts due to low SNR or magnetic susceptibility\neffects [23,24]. For these reasons, we explored a weakly-supervised\ntraining strategy using atlas-derived â€œsoft-labelsâ€ (non-binary segmen-\ntations), which may provide a scalable and probabilistic representation\nof anatomical structures while accounting for uncertainty in regions\nwith ambiguous boundaries.\nIn this article, we present an automated deep learning pipeline for\nderiving annotations of skeletal regions (legs, pelvis, lumbar/thoracic/\ncervical spine, ribcage, and arms/shoulders), along with adjacent in-\nternal organs (liver, spleen, urinary bladder, and kidneys) and the spinal\ncanal contents (spinal cord and CSF) from b-value images and ADC\nmaps. We use a weakly-supervised approach where soft-labels derived\nfrom a previously described atlas-based segmentation method, which\ngenerates a voxel-wise probability of each voxel belonging to a specific\nregion. Although these labels are less precise than manual annotations,\nthey enable the training of a significantly faster weakly-supervised deep\nlearning model that leverages an extensive multi-centre dataset of WB-\nDWI scans from patients diagnosed and treated for APC or MM. As a\nresult, the trained model can quickly generate reproducible probability\nmaps for localising and delineating body regions from WB-DWI.\nOur innovative weakly-supervised deep learning model could\naddress the limitations of existing algorithms by: (i) reducing registra-\ntion errors caused by initial spatial alignment between target and atlas\nimages, (ii) decreasing the computational time required for whole-body\nregistration, (iii) eliminating the need for time-consuming manual an-\nnotations by radiologists during model development, (iv) offering soft-\nboundary maps that capture anatomical structures with unclear\nboundaries due to disease spread, and the low resolution and geometric\ndistortion of DWI.\n2. Methods\n2.1. Patient population\nTo train the weakly-supervised deep learning model, we used three\nanonymised and retrospective WB-DWI datasets. Dataset A consisted of\n217 patients diagnosed with APC, and Dataset B comprised 127 patients\nwith confirmed MM (Table 1). These datasets were obtained from three\ndifferent imaging centres, each varying in experience with WB-DWI\nprotocols for oncological investigations. Finally, Dataset C consisted\nof 15 patients (13 male, 2 female) with confirmed MM with diffuse\npattern disease, with images captured at a single centre. These images\nwere manually contoured and defined our cohort for atlas-based\nsegmentation.\nIn Dataset A, each patient underwent both pre- and post-treatment\nscanning, typically performed three months after treatment initiation\nto assess response or disease progression. These scans were acquired at\none of three imaging centres, with 188, 22, and 7 patients per centre,\nrespectively. For patients with confirmed MM (Dataset B), baseline\nscans were obtained at one of two imaging centres, with 113 and 14\npatients per centre. Additionally, 12.6 % of MM patients had a post-\ntreatment scan, which was included in the dataset alongside the\nTable 1\nScanning protocol and MRI parameters for all WB-DWI datasets investigated in\nour study. Minimum and maximum values are displayed in parenthesis.\nAPC - Dataset (A)\n(217 patients, 434\nWB-DWI scans)\nMM - Dataset (B)\n(127 patients, 143\nWB-DWI scans)\nMM Atlas -\nDataset (C)\n(15 patients, 15\nWB-DWI scans)\nDataset Split Training:\n160 patients, 320\nscans\nValidation:\n42 patients, 84\nscans\nTest:\n15 patients, 30\nscans\nTraining:\n91 patients, 105\nscans\nValidation:\n21 patients, 23\nscans\nTest:\n15 patients, 15\nscans\nLeave-one-out\ncross-validation\n(LOOCV) for\nregistration\nalgorithm,\nTest dataset for\ndeep-learning\nmodels\nMR scanner 1.5T Siemens Aera 1.5T Siemens Aera 1.5T Siemens\nAera\nSequence Diffusion-Weighted\nSS-EPI\nDiffusion-\nWeighted\nSS-EPI\nDiffusion-\nWeighted\nSS-EPI\nAcquisition\nplane\nAxial Axial Axial\nBreathing mode Free breathing Free breathing Free breathing\nb-values\n[s/mm\n2 \n]\nb50/\nb900\n(N =\n21)\nb50/\nb600/\nb900 (N\n= 196)\nb50/\nb900\n(N =\n25)\nb50/\nb600/\nb900\n(N =\n102)\nb50/\nb900\n(N =\n6)\nb50/\nb600/\nb900\n(N = 9)\nNumber of\naverages per\nb-value\n(3,5) [(2,2,4) -\n(3,6,6)]\n(4,4) (2,2,4) (4,4) (3,3,3)\nReconstructed\nresolution\n[mm\n2 \n]\n[1.56 Ã— 1.56 - 3.12\nÃ— 3.12]\n[1.54 Ã— 1.54 -\n1.68 Ã— 1.68]\n[1.54 Ã— 1.54 -\n1.68 Ã— 1.68]\nSlice thickness\n[mm]\n[5 - 6] 5 5\nRepetition time\n[ms]\n[5490 - 12,700] [6150 - 14,500] [6150 - 14,500]\nEcho time [ms] [60 - 79] [66.4 - 69.6] [64 - 69.9]\nInversion time\n(STIR fat\nsuppression)\n[ms]\n180 180 180\nFlip angle [\nâˆ˜ \n] 90 90 90\nReconstructed\nmatrix [mm]\n[98 Ã— 128 - 256 Ã—\n256]\n[208 Ã— 256 - 224\nÃ— 280]\n[208 Ã— 256 - 224\nÃ— 280]\nReceive\nbandwidth\n[Hz/Px]\n[1955 - 2330] [1984 - 2330] [1953 - 2330]\nA. Candito et al. \nComputer Methods and Programs in Biomedicine 272 (2025) 109043\n2\n\nFig. 1. Automated end-to-end pipeline for developing a weakly-supervised deep learning model that automatically localises/delineates seven skeleton regions (legs,\npelvis, ribcage, lumbar/thoracic/cervical spine, and arms/shoulders), four adjacent internal organs (liver, spleen, urinary bladder, and kidneys) and the spinal canal\nfrom WB-DWI scans. We employed an atlas-based registration assisted with two previously trained AI-based models for generating soft-labels (non-binary seg-\nmentations). Next, we used the soft-labels for training a faster 3D patch-based Res U-Net model with 2-channel input, the ADC map and b = 0 s/mm\n2 \nimage, and as\noutput (13 channels) the body region probability maps alongside the image background. As a result, for a new input WB-DWI, the trained Res U-Net model can\ngenerate all body region probability maps without any registration step.\nA. Candito et al. \nComputer Methods and Programs in Biomedicine 272 (2025) 109043\n3\n\nbaseline scans from all patients.\nThe total number of scans from male and female patients was 497\nand 80, respectively. Patients from each dataset were divided into\ntraining, validation, and test cohorts. Two experienced radiologists [NT\nand CM blinded for review] in functional cancer imaging and WB-DWI\nselected 15 patients with APC and 15 patients with MM from Datasets\nA and B, respectively, for the test cohort. The remaining patients (314)\nwere split (80:20) into training and validation cohorts, resulting in 251/\n425 patients/scans for the training set and 63/107 patients/scans for the\nvalidation set.\nPatient selection for the test cohort was stratified to capture a range\nof disease characteristics, including volume (e.g., low vs. high), pattern\n(e.g., focal vs. diffuse), extent of skeletal involvement, and anatomical\nlocation. To evaluate model performance on an independent set, a\nradiologist and a research scientist [RD and AC, blinded for review], each\nwith 6+ years of experience in WB-DWI, manually delineated six\nrandomly selected axial slices per body region on the DWI sequence\nacross all 30 test scans (~2000 slices total). Full-body delineation\ntypically requires 5â€“6 h per scan; therefore, this slice-based approach\nwas used to ensure feasibility. Importantly, these test scans were not\nused for generating soft-labels using the atlas-based registration\nalgorithm.\nAll data were fully anonymised, and the study was performed\nfollowing the Declaration of Helsinki (2013). A local ethical committee\nwaived the requirement of patient consent for the use of these retro-\nspective datasets (Approval Ref: The Royal Marsden Hospital Service\nEvaluations 696/2018 and SE1218/2023).\n2.2. Whole-body MRI protocol\nWB-DWI scans were acquired between 2015 and 2023 using two\n(50/900 s/mm\n2\n) or three (50/600/900 s/mm\n2\n) b-value diffusion\nweighted images on a 1.5T scanner (MAGNETOM Aera/Avanto,\nSiemens Healthcare, Erlangen, Germany). The scans covered 5â€“7 sta-\ntions from the skull base to mid-thigh (for APC) or from the skull vertex\nto the knees (for MM). Each station consisted of 40 slices with a slice\nthickness of 5â€“6 mm. DWI was acquired as axial 2D single shot echo-\nplanar imaging with acceleration factor 2 (GRAPPA) and three/four\nscan trace encoding directions. Additionally, STIR fat suppression with\nan inversion time of 180 ms was employed to enhance image quality and\ncontrast. All MRI parameters for DWI sequence are reported in Table 1\nfor each dataset involved in the study. The whole-body MRI protocol\nalso provides complementary morphological and functional imaging\nthrough the acquisition of sagittal T1/T2 spine images and 3D T1w\nDIXON images [25]. This is a valid and widely accepted protocol,\nincluded in radiological guidelines and recommendations for reporting\nmetastatic bone disease (MET-RADS-P [26]) and patients diagnosed\nwith MM (MY-RADS-P [27]).\n2.3. AI-model for body regions localisation and delineation\nAn overview of the proposed pipeline for our weakly-supervised\ndeep learning model is illustrated in Fig. 1.\n2.3.1. Soft-label annotation using atlas-based registration\nTo generate soft-labels for deep-learning model training, we\nemployed a previously described atlas-based technique [28]. For all\npatients in our atlas cohort (Dataset C), a consultant radiologist [AD\nblinded for review] with 5+ years of experience in functional cancer\nimaging, manually delineated the whole skeleton and internal organs on\nthe DWI sequence across all 15 scans. They assigned labels to seven\ndifferent skeletal regions (legs, pelvis, lumbar/thoracic/cervical spine,\nribcage, and arms/shoulders) and four internal organs (liver, spleen,\nurinary bladder, and kidneys). This cohort only included MM patients\nwith diffuse disease for the following reasons: (i) MM scans have\ngenerally the widest coverage (from skull vertex to knees) within our\ncentre, and (ii) patients diagnosed with diffuse disease pattern exhibits\nelevated skeleton diffusion signal allowing the bone to be more easily\nidentified.\nFor segmentation of each new patient in the training/validation\ncohort of Datasets A and B (â€œtarget dataâ€), we firstly registered the\ncalculated ADC maps from each of the fifteen delineated patients in the\natlas cohort (â€œatlas dataâ€). Two AI-based models were employed for\noptimising the initial alignment between the atlas and target data, as\npreviously described [28,29]: (i) a body region classifier that automat-\nically predicts the position of axial images into one of six potential body\nregions (legs, pelvis, lumbar/thoracic/cervical spine, and head), and (ii)\na model that automatically delineates the spinal cord and surrounding\nCSF using a trained 2D U-Net model. After applying this initial align-\nment, registration was fine-tuned using affine followed by non-linear\ndiffeomorphic demons transformation [30]. We utilised a mean\nsquared error (MSE) optimisation between the target and atlas ADC\ndata. For affine registration, we employed an Amoeba optimiser (sim-\nplex delta = 0.005) with shrink factors (4, 2, 1), smoothing sigmas (4, 2,\n0), and a maximum of 200 iterations. For deformable demons registra-\ntion, Gradient Descent optimisation was applied (learning rate = 2) with\nnon-rigid shrink factors (4, 2, 1), smoothing widths (4, 2, 0), and a\nconvergence window size of 20. The convergence tolerance for both\nmethods was set to 10â»â¶.\nThe derived transforms were then applied to all the pre-countered\nbody region masks in the atlas cohort. Transformed masks from the\nsame region were combined using a weighted summation, where the\nweights are the reciprocal of the MSE values:\nP\nj \n=\nâˆ‘\n15\ni=1 \n1\nMSE\ni \nâ‹… T\ni\n\u0000 \nM\nij\n)\nâˆ‘\n15\ni=1 \n1\nMSE\ni\n(3)\nWhere:\nâ€¢ P\nj \nis a probability for each voxel that the voxel belongs to tissue class\nj.\nâ€¢ T\ni \nis the registration transform derived for atlas patient i.\nâ€¢ M\nij \ndenotes the (untransformed) mask for tissue class j in each atlas\npatient i.\nâ€¢ MSE\ni \nis the mean squared error in ADC values between atlas patient i\nand the target new patient following registration.\nThe labels for the spinal cord and surrounding CSF were directly\nderived from the 2D U-Net model, resulting in 12 probability maps, as\nillustrated in Fig. 1. Probability maps were stacked to generate the soft-\nlabels in tensor form, each including 13 channels per WB-DWI scan,\nwhere the last channel was dedicated to representing the image back-\nground: P\nbackground \n= 1 \u0000 \nâˆ‘\n12\nj=1 \nP\nj\n, which is guaranteed to remain be-\ntween 0 and 1.\n2.3.2. Weakly-supervised model training\nA 3D patch-based Residual U-Net model (Res U-Net) [31] was\ntrained to generate skeleton, internal organ, and spinal canal probability\nmaps, using soft-labels obtained from the soft-label annotation phase.\nThe networks involved a 2-channel input: (i) the ADC map (no thresh-\nolding was applied to remove negative ADC calculations) and (ii) the\nestimated intercept (S0) image at b = 0 s/mm\n2\n. The ADC map and S0\nimage were derived by fitting a monoexponentially decaying model to\nthe diffusion data included in the training and validation cohorts [32,\n33]. The multi-channel, SoftMax output of the model was the probability\nmaps of seven skeleton regions, four internal organs, spinal canal, and in\nthe last channel the image background. All images in the training and\nvalidation cohorts were interpolated to matrix = 256 Ã— 256 and reso-\nlution = 1.6 Ã— 1.6 mm. Input images were normalised using the\nfollowing transformations:\nA. Candito et al. \nComputer Methods and Programs in Biomedicine 272 (2025) 109043\n4\n\nscaled ADC map = ADC map \n/ \n3.5â‹…10\n\u0000 3 \nmm\n2\n/\ns (4)\nscaled S0 image = log(S0 image) / max (log (S0 image)) (5)\nThe Res U-Net symmetrical encoder-decoder architecture consisted\nof 5 convolutional layers, with the number of filters down-sampled or\nup-sampled by a factor of 2 at each layer, starting from 32 filters. Each\nlayer employed skip connections between corresponding encoder and\ndecoder paths. All convolutional layers used a 3 Ã— 3 kernel size with a\nstride of 2, batch normalisation, a dropout rate of 0.2, and the ReLU\nactivation function, except for the final output layer, which used a\nSoftMax activation function. Residual connections were implemented in\neach layer of the encoder and decoder paths to mitigate the vanishing\ngradient problem. We employed stochastic gradient descent optimiza-\ntion with hyperparameters: learning rate of 0.1, momentum of 0.9, and\nweight decay of 4â‹…10\nâ€“5\n, aimed at minimizing a multi-class cross-entropy\nloss function. The training process lasted 300 epochs, using a batch size\nof 4 and patch size of 128 Ã— 128 Ã— 64. Patches were randomly sampled\nfrom the full volume during training to ensure representative coverage\nof anatomical structures. At inference, volumetric delineations were\ngenerated using a sliding window algorithm with 25 % patch overlap to\nreduce edge artifacts and improve prediction consistency at low-\nresolution boundaries. All algorithms were implemented in Python\n(v.3.7) using PyTorch v.1.12.1 and MONAI v.0.9.1 toolboxes, running\nupon a Windows platform (v.10.0.19) accelerated by an NVIDIA\nRTX6000 GPU (Santa Clara, California, US). After training, the weakly-\nsupervised Res U-Net model can automatically generate probability\nmaps for body regions from a new WB-DWI scan using pre-processed\nADC maps and S0 images, eliminating the need for computing atlas-\nbased registration algorithms.\n2.4. Evaluation criteria\n2.4.1. Probability calibration\nProbabilities generated by machine learning models must be cali-\nbrated so that outputs reflect true likelihoods [34]. We calibrated the\noutput of our deep learning model using all patients in Datasets C: each\nchannel of the SoftMax output of our model was partitioned into 20\nequidistant intervals, x, with boundaries [0, 0.05, â€¦, 0.95, 1]; for pixels\nthat fell within each partition we then calculated the proportion occu-\npied with that ground-truth label, y, from manual delineation. The\ngenerated curves then indicate whether the model output under- or\nover-estimates true label frequencies during segmentation and thus\nrepresents a valid probability distribution [35].\nWe used isotonic regression to calibrate the model outputs, which\nensures a monotonic relationship between the predictor and target\nvariables [36]. This technique is formulated as an optimisation problem,\nexpressed as:\nmin \nâˆ‘\nn\ni=1\n(y\ni \n\u0000 f(x\ni\n))\n2\nsubject to f(x\n1\n) â‰¤ f(x\n2\n)â€¦ â‰¤ f(x\nn\n) (6)\nFor our multi-class model, we employed a one-vs-all strategy [37].\nEach channel was calibrated in turn, with the remaining channels being\ngrouped together as the negative class. The success of the\nweakly-supervised model calibration was evaluated by calculating the\nlog-loss between the true labels and calibrated probability maps derived\nfrom the isotonic regression fitting. A decrease in the log-loss indicates\nthe success of the calibration process, ensuring true probabilities and\nmitigating under- and over-predictions by the trained Res U-Net model.\n2.4.2. Quantitative analysis\nWe assessed the accuracy of our calibrated deep-learning Res U-Net\nmodel to expert annotations using two test sets: (i) an independent\ncohort of 30 patients (15 APC and 15 MM) from Datasets A and B, in\nwhich six randomly selected axial slices per body region were manually\nannotated (~2000 slices total); and (ii) the atlas patient cohort (Dataset\nC), which was also used to derive the soft-labels for model training.\nHowever, the ADC maps and S0 images from the atlas dataset were not\nused directly for training. Therefore, we argue that the results from both\ncohorts would form a valid test of our modelâ€™s performance.\nTo compare our approach with that of conventional atlas-based\nsegmentation on test cohort (ii), we employed a leave-one-out-cross-\nvalidation (LOOCV) strategy. For each of the 15 patients, we derived\n(1) an atlas-based segmentation using the method described in Section\n2.3.1 using the remaining 14 patients, and (2) a segmentation as esti-\nmated using our proposed deep-learning model. For test cohort (i), all 15\natlas images were used to generate atlas-derived maps.\nA threshold of 0.45 (empirically chosen to balance sensitivity and\nspecificity) was applied to convert the body region probability maps\nfrom both methods into binary masks. We then computed the average\nsurface distance between manual and automated delineations. Addi-\ntionally, we transferred the automated and manual body region masks\nonto the ADC map to calculate ADC statistics (median) and report\nrelative differences. For overlap-based metrics, we computed the dice\nsimilarity coefficient (DSC) for atlas-based delineations. However, for\nthe Res U-Net model, we employed a continuous (soft) DSC that directly\ncompares the modelâ€™s calibrated probability maps with binary reference\nmasks [38]. This avoids applying an arbitrary threshold on inherently\nprobabilistic outputs and aligns with our aim to retain uncertainty in-\nformation. Significant differences between the results derived from both\nautomated methods were assessed using a Wilcoxon signed-rank test (p\n< 0.05 indicating significance), with the best method selected based on\nthe highest DSC and the lowest computational time.\n2.4.3. Semi-quantitative analysis\nTwo consultant radiologists [AD and RD], blinded to each otherâ€™s\nratings, completed a semi-quantitative analysis to evaluate the accuracy\nof our segmentation model in our 30 test-patient cohort (15 APC and 15\nMM). They assessed segmentation quality by superimposing estimated\nprobability maps for all body regions onto the ADC maps and high b-\nvalue images (b = 900 s/mmÂ²). A qualitative assessment was made using\na 4-point Likert scale: 1 = segmentation failure, 2 = suboptimal per-\nformance, 3 = good accuracy, and 4 = excellent accuracy. Criteria\nconsidered for each WB-DWI scan assessment were: (i) the degree to\nwhich the location of the body region probability maps matched the\nactual patient anatomy, further verified by reviewing T1/T2 spine and\nwhole-body DIXON T1w images; (ii) the success of the derived de-\nlineations in covering the volume of the skeleton and internal organs\nvisible on the WB-DWI scan; (iii) how well the automated delineations\nseparated different anatomical structures (e.g., ensuring predicted kid-\nney masks did not overlap with actual liver boundaries). Inter-reader\nagreement was quantified using both percentage agreement and\nGwetâ€™s AC1 [39], which provides stable estimates in the presence of\nunbalanced rating distributions. Agreement levels were classified ac-\ncording to the categories proposed by Landis and Koch [40]. Finally, this\nexpert assessment would provide insights into whether the automated\ntool is sufficiently developed to serve as preliminary step in isolating the\nwhole bone marrow background, facilitating the development of\nsignal-based AI tools for automated bone lesion detection/delineation\nfrom WB-DWI scans.\n2.4.4. Benchmarking with open-source model\nWe benchmarked our weakly-supervised, calibrated Res U-Net\nmodel against the open-source TotalSegmentator-MRI nnU-Net (version\n2.7.0) [21]. Whole-body b = 50 s/mmÂ² DWI images were used as input,\nas this sequence best matched the modalities used to train the\nopen-source model. We computed dice scores between automated masks\nand manual annotations for the urinary bladder, kidneys, liver, spleen,\nlong bones, pelvis, spine, and spinal canal across all test cases (Section\n2.4.2). The ribcage was excluded, as it is not supported by the Total-\nSegmentator-MRI model. Dice scores for each region were then compared\nacross the three automated methods: atlas-based registration,\nA. Candito et al. \nComputer Methods and Programs in Biomedicine 272 (2025) 109043\n5\n\nTotalSegmentator-MRI, and our Res U-Net model. Statistical differences\nwere assessed using the Wilcoxon signed-rank test (p < 0.05).\n3. Results\n3.1. Probability calibration\nIn Fig. 2, we demonstrated the reliability curves for each body region\nin Dataset C. These curves revealed that the weakly-supervised Res U-\nNet model tended to overestimate or underestimate predictions\ncompared to the true probabilities. In the uncalibrated models for binary\nclassification, data points consistently fell below or above the ideal\ncalibration line. However, following calibration using the isotonic\nregression technique, we observed a notable decrease in log-loss (on\naverage \u0000 15 %) compared to the uncalibrated models, with data points\nmoving closer to the ideal calibration line.\n3.2. Quantitative analysis\nFig. 3 shows the overlap-based metrics, along with average surface\nFig. 2. Reliability curves used to evaluate the calibration of the weakly-supervised Res U-Net model. To perform this analysis, we used all patients included in the\natlas cohort (Dataset C), with a leave-one-out testing approach. Confidence intervals (mean and standard deviation) were calculated for each data point in the plots.\nWe generated 11 reliability curves, assuming models trained for a binary classification task (one-vs-all strategy). Importantly, predictions (all voxels) from the\nSoftMax activation function (uncalibrated model) tended to overestimate or underestimate true probabilities, as evidenced by data points falling below or above the y\n= x line. In contrast, models calibrated using the isotonic regression technique demonstrated a reduced log-loss between true labels and predicted probabilities, with\ndata points aligning closer to the ideal calibration curves. This calibration process ensures that model predictions accurately reflect true probabilities, thereby\nmitigating the risks associated with under or over predictions.\nA. Candito et al. \nComputer Methods and Programs in Biomedicine 272 (2025) 109043\n6\n\ndistances and relative median ADC differences, computed from both\nmanual and automated masks following calibration for each body re-\ngion. The weakly-supervised Res U-Net model outperformed the atlas-\nbased registration algorithm, with average DSC values for all body re-\ngions increasing by more than 8 %, demonstrating statistically signifi-\ncant differences between the methods. The computational time required\nto generate probability maps from the model averaged 25 s, compared to\nthe 5 min needed for the atlas-based registration algorithm on the same\nhardware (2.4 GHz Quad-Core Intel Core i5).\nAcross test cases in Datasets A and B, our deep learning model\nachieved a DSC of 0.73 for the long bones, 0.78 for the pelvis, and 0.82\nfor the spine. However, lower performance was observed for ribcage\ndelineations, with a DSC below 0.5. For internal organs, dice scores\nexceeded 0.77, except for the spleen, which showed a score of 0.71. The\naverage surface distance was below 2 mm, and relative median ADC\ndifferences were below 13 % for both skeletal and internal organ masks.\nConsistent results were observed for test cases in Dataset C. The\nmodel achieved the following results for skeleton delineations: a DSC of\n0.67 with an average surface distance between automated and manual\nexpert-defined delineations of less than 2 mm. Significant improvements\ncompared to the atlas-based registration algorithm were observed in the\npelvis and spine, where we obtained a DSC of 0.73 and 0.84. For long\nbones these overlap-based were slightly lower, averaging just below 0.7.\nHowever, performance decreased for the ribcage, with a DSC of 0.43. In\nterms of internal organs, our model achieved a DSC of 0.86. The spleen\nyielded the lowest score among soft tissues, with an average surface\nFig. 3. Quantitative metrics for evaluating the performance of the calibrated weakly-supervised 3D Res U-Net model in localising and delineating body regions from\nWB-DWI scans, compared to expert-defined delineations. Additionally, the same metrics were derived using the atlas-based registration algorithm (baseline, through\nLOOCV for Dataset C). Overlap-based metrics showed a significant improvement for the deep-learning model over the registration algorithm across all body regions,\nwith at least an 8 % increase in DSC. Moreover, the model generated outputs 12 times faster than the registration algorithm (25 s versus 5 min). Our weakly-\nsupervised model also showed lower relative median ADC differences, with statistically significant improvements observed for whole skeleton delineations. Mask\nfor long bones consisted of legs and arms/shoulders. Mask for skeleton included long bones, pelvis, lumbar/thoracic/cervical spine, and ribcage. Mask for internal\norgans included urinary bladder, kidneys, liver and spleen.\nA. Candito et al. \nComputer Methods and Programs in Biomedicine 272 (2025) 109043\n7\n\n(caption on next page)\nA. Candito et al. \nComputer Methods and Programs in Biomedicine 272 (2025) 109043\n8\n\ndistance exceeding 3 mm. We observed a statistically significant\nreduction in relative median ADC differences for whole skeleton de-\nlineations compared to the atlas-based registration algorithm. The\nrelative median ADC differences for the skeleton and internal organs\nbetween automated and manual expert-defined delineations were below\n10.5 % and 4 %, respectively, except for the bladder, which exceeded 14\n%. Spinal canal delineations showed a DSC of 0.86, consistent with re-\nsults from the 2D U-Net model. Figs. 4 and 5 present manual and derived\ndelineations of the skeleton, internal organs, and spinal canal for three\nrandomly selected test cases (Dataset C), confirming the weakly-\nsupervised deep learning modelâ€™s ability to localise and delineate\nbody regions from WB-DWI scans.\n3.3. Semi-quantitative analysis\nRadiologists rated the calibrated probability maps for all test cases as\neither â€œgoodâ€ or â€œexcellentâ€, with 80 % (24/30) inter-reader agreement\nacross these two categories based on overall scores (substantial agree-\nment; Gwetâ€™s AC1=0.72). Agreement by anatomical region was 63.3 %\n(19/30) for the skeleton and 53.3 % (16/30) for internal organs,\nreflecting fair agreement levels (Gwetâ€™s AC1=0.38 and 0.27, respec-\ntively); see Supplementary Material Tables 1 and 2.\nFigs. 6 and 7 show three randomly selected patients with APC and\nthree with MM. The generated body region probability maps aligned\nwell with the patientsâ€™ anatomy, showing no signs of overfitting. Both\nthe skeleton and internal organs were accurately localised within the\nscans, with probabilities exceeding 0.4, while the image background\ndisplayed values close to 0, as expected. Importantly, our weakly-\nsupervised Res U-Net model generated reproducible skeletal probabil-\nity maps in test patients with heterogenous metastatic and active\nmyeloma bone disease, reflecting the stratified selection criteria used in\nour test cohort.\nGeneral observations provided by the radiologists included: (i)\nexcellent segmentations of the spinal canal; (ii) a tendency to under-\nsegment the inferior pubic rami, but no false positives from including\npelvic organs/vessels; (iii) no false positives from thigh vessels; (iv) no\nfalse positives from axillary lymph nodes. This latter point is particularly\nencouraging, given that lymph nodes were not delineated as part of this\nmodel development.\n3.4. Benchmarking with open-source model\nOur weakly-supervised, calibrated Res U-Net model outperformed\nTotalSegmentator-MRI in skeleton (excluding ribcage) delineation,\nachieving a DSC of 0.78 vs. 0.64 (p < 0.05) for cases in Datasets A and B,\nand 0.76 vs. 0.60 (p < 0.05) in Dataset C, see Tables 2 and 3. The spine\nshowed the largest performance gap, with scores of 0.82 vs. 0.49 (p <\n0.05) in Datasets A and B, and 0.84 vs. 0.57 in Dataset C. Spinal canal\ndelineation also demonstrated a significant difference in Datasets A and\nB (0.79 vs. 0.60, p < 0.05), while no significant difference was observed\nin Dataset C (0.86 vs. 0.81, p â‰¥ 0.05). Although TotalSegmentator-MRI\nachieved slightly higher dice scores for internal organ delineation in\nDataset C (0.86 vs. 0.83), the difference was not statistically significant\n(p â‰¥ 0.05). The spleen showed the most notable performance gap be-\ntween the models (0.78 vs. 0.68, p < 0.05). For internal organs in\nDatasets A and B, both models performed similarly, with dice scores of\n0.85. Finally, our solution generated delineations in 25 s per WB-DWI\nscan, compared to 12 min for the open-source model, with both tested\non a CPU (Apple M3 Max, 16-core).\n4. Discussion\nWe developed and validated a weakly-supervised deep learning\nmodel based on the 3D Res U-Net architecture to automatically localise\nand delineate seven skeleton regions (legs, pelvis, lumbar/thoracic/\ncervical spine, ribcage, and arms/shoulders), four internal organs (liver,\nspleen, urinary bladder, and kidneys), and the spinal canal (spinal cord\nwith surrounding CSF) from WB-DWI scans. We trained a model using an\nheterogenous WB-DWI dataset comprising 532 scans from patients with\nAPC or MM, without requiring resources for laborious manual de-\nlineations of body regions. Instead, we automated the annotation phase\nusing a validated atlas-based registration algorithm. Manual annotations\nof body regions can take up to 5â€“6 h per WB-DWI scan; however, with\nour novel methodology, we generated annotations (soft-labels) for 532\nscans (~127,500 images) within ~48 h (~325 s/scan).\nThe quantitative analysis demonstrated that our weakly-supervised\nRes U-Net model improved accuracy in localising and delineating skel-\neton and internal organs on test datasets compared to the atlas-based\nregistration algorithm (baseline). Across all body regions, the model\nachieved mean DSC of 0.80 compared to 0.69 for the baseline (p < 0.05).\nFor skeleton delineation alone, the DSC was 0.67 vs. 0.63 (p < 0.05), and\nfor internal organs, 0.83 vs. 0.77 (p < 0.05). In contrast to atlas-based\nregistration algorithms, which rely on predefined templates, our deep\nlearning model trained on heterogeneous datasets captures diverse\nfeatures and patterns, enhancing spatial dependencies and structural\ninformation present in WB-DWI scans, thereby leading to improved\ngeneralisation on unseen datasets. However, lower performance was\nobserved for ribcage (DSC <0.5) and spleen delineations (average sur-\nface distance values exceeding 5 mm). This may be due to geometric\nfidelity issues in the ribcage region from DWI sequence and variability in\nspleen position between patients due to respiratory motion during free-\nbreathing acquisition [24].\nSpinal canal delineation accuracy was consistent with a previously\nvalidated 2D U-Net model, achieving a DSC of 0.86. Our previous study\nproven that accurate spinal canal delineations can significantly improve\nthe inter- and intra-patient signal normalisation on WB-DWI [29]. This\napproach can enhance assessment of bone disease from WB-DWI by\nimproving lesion localisation, diameter measurement, and â€œat-a-glanceâ€\nevaluation of post-treatment changes by standardising the display of\nhigh-b-value images.\nFast computation and accurate localisation of body region proba-\nbility maps are key for facilitating clinical deployment of automated\ntools for disease staging and treatment response assessment. The\ncomputational time required to generate probability maps from the\nweakly-supervised model averaged 25 s, compared to the 5 min needed\nfor the atlas-based registration algorithm on the same hardware.\nFurthermore, relative median ADC difference between manual and\nautomated skeleton segmentations was 10 % (95 % CI: 6.59â€“13.7 %),\nwhich is closely comparable with previously reported inter-reader\nrepeatability of ADC measurements (6 %; 95 % CI: 2.70â€“11.5 %),\nbased on manual expertsâ€™ delineations of bone lesions in APC patients\n[41,42].\nThe semi-quantitative analysis provided further evidence of the\nmodelâ€™s generalisability on unseen WB-DWI data from patients with\nAPC and MM. The model showed no signs of overfitting and generated\naccurate delineations of multiple organs even in patients with extensive\nFig. 4. Sagittal and coronal views of Maximum Intensity Projection (MIP) of b900 images for three randomly selected test cases from Dataset C (top row). The\nsecond to fourth rows show body region probability maps from the weakly-supervised, calibrated Res U-Net model (where values close to â€œ1\nâ€³ indicate a high\nprobability that voxels belong to one of the body regions), body region masks (obtained by thresholding the probability maps), and manually expert-defined labels,\nall superimposed on the MIP images. Automated localisation and delineations of all body regions showed promising agreement with the actual patient anatomy, with\ndice scores of 0.64, 0.74, and 0.75 for the three cases when comparing manual and automated masks. However, lower performance is observed in delineating the\nupper long bones. Mask for long bones consisted of legs and arms/shoulders.\nA. Candito et al. \nComputer Methods and Programs in Biomedicine 272 (2025) 109043\n9\n\nFig. 5. Axial b900 images with superimposed manual and automated delineations of the skeleton, adjacent internal organs, and spinal canal for three randomly\nselected test cases from Dataset C. The weakly-supervised, calibrated Res U-Net model demonstrated high accuracy in localising different anatomical structures, as\nshown by the substantial agreement between the generated probability maps (where values close to â€œ1\nâ€³ indicate a high probability that voxels belong to one of the\nbody regions) and actual patient anatomy. Dice scores of 0.64, 0.74, and 0.75 were achieved for the three cases when comparing manual and automated masks.\nHowever, the model presented limitations in capturing complex soft-tissue organ shapes (green arrows), showed misalignment in the pelvic bone delineation for\nPatient 1 (orange arrows), and generated over-segmentation of the ribcage in all cases (red arrows). Mask for long bones consisted of legs and arms/shoulders.\nA. Candito et al. \nComputer Methods and Programs in Biomedicine 272 (2025) 109043\n10\n\nFig. 6. Coronal and sagittal views of Maximum Intensity Projection (MIP) of b900 images and ADC maps for three randomly selected APC patients from the test\ncohort (Dataset A) with confirmed diffuse metastatic bone disease. These lesions (red arrows) appear as darker spots with high signal intensity on b900 images and\nlow ADC values (typically >5â‹…10\n\u0000 3\nmm\n2\n/s) compared to the normal bone marrow background. Superimposed on these MIP images are body region probability maps\ngenerated by the weakly-supervised, calibrated Res U-Net model. The derived probability maps (where values close to â€œ1\nâ€³ indicate a high probability that voxels\nbelong to one of the body regions) accurately matched patient anatomy, ensuring correct positional alignment, volume coverage, and promising boundary delin-\neation between skeletal regions and adjacent internal organs. These findings were supported by expert semi-quantitative analysis, which rated the automated full-\nbody probability maps for these patients as â€œexcellentâ€ (score of 4), except for the skeleton map in â€œPatient 2\nâ€³, which was rated as â€œgoodâ€ (score of 3) due to lower\naccuracy in localising the top of the shoulders and pubic rami (green arrows).\nA. Candito et al. \nComputer Methods and Programs in Biomedicine 272 (2025) 109043\n11\n\nFig. 7. Axial ADC maps and b900 images for three patients diagnosed with MM from the test cohort (Dataset B), presenting different disease patterns: focal bone\nlesions (red arrows), diffuse myeloma involvement (purple arrows), and paramedullary disease (green arrows). Body region probability maps derived from the\nweakly-supervised, calibrated Res U-Net model were superimposed on the b900 images. These maps demonstrated accurate positional alignment, volume coverage,\nand clear separation between skeletal regions and adjacent internal organs compared to actual patient anatomy. Expert evaluation rated them as â€œexcellentâ€ (score of\n4). However, the skeleton map for â€œPatient 3\nâ€³ was rated as â€œgoodâ€ (score of 3) due to reduced accuracy in delineating the anterior iliac (orange arrows).\nA. Candito et al. \nComputer Methods and Programs in Biomedicine 272 (2025) 109043\n12\n\ndisease burden. Two experienced radiologists rated cases as either\nâ€œgoodâ€ or â€œexcellentâ€ (on a 4-point Likert scale: â€œ1: failedâ€, â€œ2: subop-\ntimalâ€, â€œ3: goodâ€, â€œ4: excellentâ€).\nWe also compared the performance our solution with the open-\nsource TotalSegmentator-MRI model. Overall, our calibrated Res U-Net\nachieved higher dice scores (0.8 vs. 0.76, p < 0.05), with the most sig-\nnificant improvement observed in the skeletal delineation (0.76 vs. 0.67,\np < 0.05). These results are encouraging given the key advantages of our\nmodel: (i) it is designed to generate soft-boundary probability maps,\nimproving delineation of anatomical structures with unclear boundaries\ndue to disease spread, as well as the low resolution and geometric\ndistortion of DWI. This should minimise the risk of fully excluding dis-\nease involved areas from the delineated volume; (ii) it achieves fast\ninference times, approx. 25 s per WB-DWI scan compared to 12 min for\nTotalSegmentator-MRI, when both were tested on a CPU. This reduces\ndependence on dedicated GPUs, which are less available in hospitals for\non-premises AI model deployment.\nHowever, there are limitations to our current solution. Firstly,\nalthough we plan to include additional WB-DWI datasets with (semi)\nmanual annotations in future work, the current study reports results\nfrom a test cohort in which only six axial slices per body region were\nmanually annotated. This sampling strategy may not fully capture\nanatomical variability between patients. In the second test cohort, full-\nbody annotations were available; however, the same images were also\nused during atlas-based registration to generate soft-labels, potentially\nintroducing bias.\nSecondly, the semi-quantitative analysis involved only two readers.\nIncluding more readers with varying levels of experience in interpreting\nWB-DWI could provide a more robust reference for assessing the accu-\nracy and reproducibility of the generated body region probability maps.\nThirdly, the model was trained and tested using images acquired\nfrom a single MRI vendor. However, the automated annotation phase\ncould aid in the modelâ€™s re-training or application of transfer learning\ntechniques. This would involve using WB-DWI scans acquired from\ndifferent scanner manufacturers and/or leveraging novel AI-based\nreconstruction methods [43]. Such strategies aim to enhance the\nmodelâ€™s ability to generalise to unseen data without the need for manual\ndelineation of skeleton regions and internal organs.\nFourthly, our model demonstrated lower performance (DSC=0.76,\nexcluding the ribcage) compared to previously published algorithms\ndeveloped using anatomical whole-body MRI for bone marrow seg-\nmentation in patients with malignant bone disease, as well as to reported\ninter-reader agreement levels (DSC=0.88) [19]. Ceranka et al [15]\ndeveloped a multi-atlas segmentation algorithm from T1w MRI,\nreporting a dice score of 0.88 in a LOOCV study with 10 APC patients.\nHowever, their method excluded the ribcage and upper limbs and\nrequired 180 min per scan to generate a mask. Bauer et al [20] trained a\nnnU-Net on 210 coronal T1w MRI scans to delineate the spine, pelvis,\nhumeri, and femora, also achieving a dice score of 0.88 in 21 test pa-\ntients with myeloma-related pathologies. However, their approach\nrelied on T1w coronal MRI, which is not included in the MY-RADS\nguidelines for myeloma assessment and required 18 min per scan for\ninference. Instead, Kedzierska et al [44] developed a nnU-Net for\nWB-DWI using multi-centre datasets. However, their model focused only\non pelvic bone marrow, reporting dice scores of 0.92 and 0.85 for the hip\nand sacral bones, respectively. In our study, the inter-reader agreement\nfor ADC measurements of the pelvic bone (71 Ã— 10â»\n6 \nmmÂ²/s, mean ab-\nsolute difference) aligned with their results (65â€“85 Ã— 10â»\n6 \nmmÂ²/s),\nsupporting reliable ADC quantification despite lower segmentation\noverlap.\nThese lower overlap-based metrics may be explained by: (i) our deep\nlearning model was trained using soft-labels generated from atlas-based\nregistration, rather than high-quality manual full-body annotations, to\nfacilitate training on a high-volume, multi-centre data; (ii) Stand-\nardisation of the WB-DWI protocol is not a trivial task due to the sig-\nnificant number of parameters that can vary across scanners, leading to\nvariability in the quality of the images acquired at different sites,\nincluding variations in geometric distortion and SNR. This makes\nalignment with anatomical MRI sequences difficult, and further chal-\nlenging organ delineations.\nIn conclusion, our novel weakly-supervised deep learning model,\nbased on the 3D Res U-Net architecture, can swiftly generate accurate\nand reproducible probability maps for localising skeletal regions, in-\nternal organs, and the spinal canal from WB-DWI scans. This will facil-\nitate the development of automated tools for delineating suspected bone\ndisease, based on WB-DWI signal intensity. This in turn could enable the\nextraction of quantitative imaging biomarkers for staging and treatment\nresponse assessment. Prospective evaluation of our tool in a multi-centre\nsetting is currently ongoing, as part of a national trial assessing the\neffectiveness of these tools for automatically delineating metastatic bone\ndisease or multifocal myeloma lesions from WB-DWI in patients with\nAPC and MM.\nFunding\nThis project is funded by the NIHR Invention for Innovation award\n(Advanced computer diagnostics for whole body magnetic resonance\nTable 2\nDice scores computed between expert-defined body region annotations and the\noutputs of three automated methods across test cases from Datasets A and B\n(only six slices per body region per scan were annotated): (i) atlas-based regis-\ntration algorithm, (ii) weakly-supervised, calibrated Res U-Net model, and (iii)\nthe open-source TotalSegmentator-MRI model. The highest scores with statisti-\ncally significant differences (Wilcoxon signed-rank test, p < 0.05) are shown in\nbold*.\nDatasets A/B Atlas-based\nregistration\nRes U-Net\nmodel (ours)\nTotalSegmentator-\nMRI\nUrinary bladder 0.72 Â± 0.18 0.83 Â± 0.08 0.85 Â± 0.06\nKidneys 0.72 Â± 0.08 0.77 Â± 0.07 0.83 Â± 0.06*\nLiver 0.83 Â± 0.07 0.91 Â± 0.03* 0.86 Â± 0.05\nSpleen 0.67 Â± 0.08 0.71 Â± 0.09 0.82 Â± 0.05*\nInternal organs 0.79 Â± 0.05 0.85 Â± 0.03 0.85 Â± 0.03\nLong bones 0.65 Â± 0.15 0.73 Â± 0.08 0.74 Â± 0.10\nPelvis 0.73 Â± 0.06 0.78 Â± 0.06* 0.67 Â± 0.10\nSpine 0.76 Â± 0.04 0.82 Â± 0.05* 0.49 Â± 0.08\nSkeleton (excl.\nribcage)\n0.72 Â± 0.05 0.78 Â± 0.04* 0.64 Â± 0.07\nSpinal canal â€“ 0.79 Â± 0.05* 0.60 Â± 0.05\nAll body regions\n(excl. ribcage)\n0.76 Â± 0.04 0.83 Â± 0.03* 0.77 Â± 0.03\nTable 3\nDice scores computed between expert-defined body region annotations and the\noutputs of three automated methods across test cases from Datasets C: (i) atlas-\nbased registration algorithm (through LOOCV), (ii) weakly-supervised, cali-\nbrated Res U-Net model, and (iii) the open-source TotalSegmentator-MRI model.\nThe highest scores with statistically significant differences (Wilcoxon signed-\nrank test, p < 0.05) are shown in bold*.\nDatasets C Atlas-based\nregistration\nRes U-Net\nmodel (ours)\nTotalSegmentator-\nMRI\nUrinary bladder 0.67 Â± 0.11 0.73 Â± 0.14 0.81 Â± 0.06\nKidneys 0.71 Â± 0.06 0.76 Â± 0.06 0.82 Â± 0.02*\nLiver 0.80 Â± 0.04 0.87 Â± 0.04 0.88 Â± 0.05\nSpleen 0.61 Â± 0.14 0.68 Â± 0.15 0.78 Â± 0.06*\nInternal organs 0.77 Â± 0.04 0.83 Â± 0.04 0.86 Â± 0.03\nLong bones 0.63 Â± 0.05 0.69 Â± 0.04 0.70 Â± 0.08\nPelvis 0.67 Â± 0.05 0.73 Â± 0.04 0.73 Â± 0.06\nSpine 0.76 Â± 0.03 0.84 Â± 0.03* 0.57 Â± 0.04\nSkeleton (excl.\nribcage)\n0.67 Â± 0.03 0.76 Â± 0.03* 0.67 Â± 0.03\nSpinal canal â€“ 0.86 Â± 0.05 0.81 Â± 0.05\nAll body regions\n(excl. ribcage)\n0.73 Â± 0.02 0.80 Â± 0.02* 0.76 Â± 0.02\nA. Candito et al. \nComputer Methods and Programs in Biomedicine 272 (2025) 109043\n13\n\nimaging to improve management of patients with metastatic bone can-\ncer II-LA-0216-20007). The views expressed are those of the author(s)\nand not necessarily those of the NIHR or the Department of Health and\nSocial Care.\nInformed consent statement\nPatient consent was waived; study was conducted with retrospective\ndata only and researchers only had access to de-identified data.\nCRediT authorship contribution statement\nAntonio Candito: Writing â€“ review & editing, Writing â€“ original\ndraft, Validation, Software, Methodology, Formal analysis, Data cura-\ntion, Conceptualization. Alina Dragan: Writing â€“ review & editing,\nValidation. Richard Holbrey: Writing â€“ review & editing, Validation,\nSoftware, Data curation. Ana Ribeiro: Writing â€“ review & editing,\nProject administration. Ricardo Donners: Writing â€“ review & editing,\nResources. Christina Messiou: Writing â€“ review & editing, Validation,\nResources. Nina Tunariu: Writing â€“ review & editing, Validation, Re-\nsources. Dow-Mu Koh: Writing â€“ review & editing, Validation, Super-\nvision, Funding acquisition, Conceptualization. Matthew D\nBlackledge: Writing â€“ review & editing, Supervision, Software, Meth-\nodology, Funding acquisition, Formal analysis, Conceptualization.\nDeclaration of competing interest\nA.C., R.H., and M.D.B. have submitted a patent to the UK Intellectual\nProperty Office regarding the work described in this manuscript.\nData Availability Statement\nData can be shared upon request from the authors. However, this is\nsubject to the establishment of an appropriate data-sharing agreement,\ngiven the sensitive nature of patient information involved.\nAcknowledgments\nWe acknowledge support from the National Institute for Health and\nCare Research (NIHR) Biomedical Research Centre at The Royal Mars-\nden NHS Foundation Trust The Institute of Cancer Research, London,\nand by the Royal Marsden Cancer Charity and Cancer Research UK\n(CRUK) National Cancer Imaging Trials Accelerator (NCITA). This work\nuses data provided by patients and collected by the NHS as part of their\ncare and support.\nThe authors would also like to acknowledge Mint MedicalÂ®.\nSupplementary materials\nSupplementary material associated with this article can be found, in\nthe online version, at doi:10.1016/j.cmpb.2025.109043.\nReferences\n[1] A.R. Padhani, et al., Rationale for modernising imaging in advanced prostate\ncancer, Eur. Urol. Focus. 3 (2â€“3) (2017) 223â€“239.\n[2] C. Messiou, et al., Prospective evaluation of whole-body MRI versus FDG PET/CT\nfor lesion detection in participants with myeloma, Radiol. ImAging Cancer 3 (5)\n(2021).\n[3] N. Tunariu, et al., Whatâ€™s new for clinical whole-body MRI (WB-MRI) in the 21st\ncentury, Br. J. Radiol. 93 (2020) 1â€“13.\n[4] R. Perez-lopez, D.N. Rodrigues, I. Figueiredo, J. Mateo, D.J. Collins,\nMultiparametric magnetic resonance imaging of prostate cancer bone disease\ncorrelation with bone biopsy histological and molecular features, Invest. Radiol. 53\n(2) (2018).\n[5] A.R. Padhani, K. Van Ree, D.J. Collins, S. Dâ€™Sa, A. Makris, Assessing the relation\nbetween bone marrow signal intensity and apparent diffusion coefficient in\ndiffusion-weighted MRI, Am. J. Roentgenol 200 (1) (2013) 163â€“170.\n[6] C. Messiou, D.J. Collins, V.A. Morgan, N.M. Desouza, Optimising diffusion\nweighted MRI for imaging metastatic and myeloma bone disease and assessing\nreproducibility, Eur. Radiol. 21 (8) (2011) 1713â€“1718.\n[7] A.R. Padhani, A. Gogbashian, Bony metastases: assessing response to therapy with\nwhole-body diffusion MRI, Cancer ImAging 11 (2011).\n[8] S.L. Giles, et al., Whole-body diffusion-weighted MR imaging for assessment of\ntreatment response in myeloma, Radiology 271 (3) (2014) 785â€“794.\n[9] R. Perez-Lopez, et al., Volume of bone metastasis assessed with whole-body\ndiffusion-weighted imaging is associated with overall survival in metastatic\ncastration resistant prostate cancer, Radiology 280 (1) (2016) 151â€“160.\n[10] R. Perez-Lopez, et al., Diffusion-weighted imaging as a treatment response\nbiomarker for evaluating bone metastases in prostate cancer: a pilot study,\nRadiology 283 (1) (2017) 168â€“177.\n[11] M.D. Blackledge, et al., Assessment of treatment response by total tumor volume\nand global apparent diffusion coefficient using diffusion-weighted MRI in patients\nwith metastatic bone disease: a feasibility study, PLoS. One 9 (4) (2014) 1â€“8.\n[12] A.R. Padhani, D.M. Koh, D.J. Collins, Whole-body diffusion-weighted MR imaging\nin cancer: current status and research directions, Radiology 261 (3) (2011)\n700â€“718.\n[13] S. Faghani, et al., A deep learning algorithm for detecting lytic bone lesions of\nmultiple myeloma on CT, Skelet. Radiol. 52 (1) (2023) 91â€“98.\n[14] M. Wennmann, et al., Automated detection of focal bone marrow lesions from MRI:\na multi-center feasibility study in patients with monoclonal plasma cell disorders,\nAcad. Radiol. (2025).\n[15] J. Ceranka, F. Lecouvet, J. De Mey, and J. Vandemeulebroucke, â€œComputer-aided\ndetection of focal bone metastases from whole-body multi-modal MRI,â€ no. March\n2020, p. 27, 2020.\n[16] I. Lavdas, et al., Fully automatic, multiorgan segmentation in normal whole body\nmagnetic resonance imaging (MRI), using classification forests (CFS),\nconvolutional neural networks (CNNs), and a multi-atlas (MA) approach, Med.\nPhys. 44 (10) (2017) 5210â€“5220.\n[17] M. Cabezas, A. Oliver, X. Llad\nÂ´\no, J. Freixenet, M. Bach Cuadra, A review of atlas-\nbased segmentation for magnetic resonance brain images, Comput. Methods\nPrograms Biomed. 104 (3) (2011) 158â€“177.\n[18] J.E. Iglesias, M.R. Sabuncu, Multi-atlas segmentation of biomedical images: a\nsurvey, Med. Image Anal. 24 (1) (2015) 205â€“219.\n[19] M. Wennmann, et al., Combining deep learning and radiomics for automated,\nobjective, comprehensive bone marrow characterization from whole-body MRI: a\nmulticentric feasibility study, Invest. Radiol. 57 (11) (2022) 752â€“763.\n[20] F. Bauer, et al., Advanced automated model for robust bone marrow segmentation\nin whole-body MRI, Acad. Radiol. (2025).\n[21] T. Akinci Dâ€™Antonoli, et al., TotalSegmentator MRI: robust sequence-independent\nsegmentation of multiple anatomic structures in MRI, Radiology 314 (2) (2025).\n[22] C. Sun, A. Shrivastava, S. Singh, A. Gupta, Revisiting unreasonable effectiveness of\ndata in deep learning era, Online]. Available, https://arxiv.org/abs/1707.02968,\n2017.\n[23] A. Barnes, et al., Guidelines & recommendations: UK quantitative WB-DWI\ntechnical workgroup: consensus meeting recommendations on optimisation,\nquality control, processing and analysis of quantitative whole-body diffusion-\nweighted imaging for cancer, Br. J. Radiol. 91 (1081) (2018) 1â€“12.\n[24] S. Keaveney, et al., Image quality in whole-body MRI using the MY-RADS protocol\nin a prospective multi-centre multiple myeloma study, Insights. ImAging 14 (1)\n(2023) 1â€“14.\n[25] M. Rata, et al., Implementation of whole-body MRI (MY-RADS) within the\nOPTIMUM/MUKnine multi-centre clinical trial for patients with myeloma,\nInsights. ImAging 13 (1) (2022) 1â€“16.\n[26] A.R. Padhani, et al., METastasis reporting and data system for prostate cancer:\npractical guidelines for acquisition, interpretation, and reporting of whole-body\nMagnetic resonance imaging-based evaluations of multiorgan involvement in\nadvanced prostate cancer, Eur. Urol. 71 (1) (2017) 81â€“92.\n[27] C. Messiou, et al., Guidelines for acquisition, interpretation, and reporting of\nwhole-body MRI in Myeloma: myeloma Response Assessment and Diagnosis system\n(MY-RADS), Radiology 291 (7) (2019), 1.\n[28] A. Candito, et al., Deep learning assisted atlas-based delineation of the skeleton\nfrom whole-body diffusion weighted MRI in patients with malignant bone disease,\nBiomed. Signal. Process. Control 92 (2024).\n[29] A. Candito, et al., Deep learning for delineation of the spinal canal in whole-body\ndiffusion-weighted imaging: normalising inter- and intra-patient intensity signal in\nmulti-centre datasets, Bioengineering 11 (2) (2024) 130.\n[30] J.P. Thirion, Image matching as a diffusion process: an analogy with Maxwellâ€™s\ndemons, Med. Image Anal. 2 (3) (1998) 243â€“260.\n[31] E. Kerfoot, J. Clough, I. Oksuz, J. Lee, A.P. King, J.A. Schnabel, Left-ventricle\nquantification using residual U-net, Lect. Notes Comput. Sci. (Incl. Subser. Lect.\nNotes Artif. Intell. Lect. Notes Bioinform.) 11395 (2019) 371â€“380. LNCS.\n[32] M.D. Blackledge, et al., Computed diffusion-weighted MR imaging may improve\ntumor detection, Radiology 261 (2) (2011) 573â€“581.\n[33] M.D. Blackledge, et al., Noise-corrected, exponentially weighted, diffusion-\nweighted MRI (niceDWI) improves image signal uniformity in whole-body imaging\nof metastatic prostate cancer, Front. Oncol. 10 (2020) 1â€“12.\n[34] C.F. Dormann, Calibration of probability predictions from machine-learning and\nstatistical models, Glob. Ecol. Biogeogr. 29 (4) (2020) 760â€“765.\n[35] F.M. Ojeda, et al., Calibrating machine learning approaches for probability\nestimation: a comprehensive comparison, Stat. Med. 42 (29) (2023) 5451â€“5478.\n[36] R. Caruana, Predicting good probabilities with supervised Learning Alexandru\nNiculescu-Mizil, Online]. Available, https://www.cs.cornell.edu/>alexn/papers\n/calibration.icml05.crc.rev3.pdf, 2005.\nA. Candito et al. \nComputer Methods and Programs in Biomedicine 272 (2025) 109043\n14\n\n[37] U. Johansson, H. Bostr\nÂ¨\nom, L. Carlsson, Z. Luo, G. Cherubin, K.A. Nguyen,\nCalibrating multi-class models, Proc. Mach. Learn. Res. 152 (2021) 1â€“20.\n[38] R.S. Reuben, D. Yuval, K. Jinyoung, S. Guillermo, and H. Noam, â€œContinuous dice\ncoefficient: a method for evaluating probabilistic segmentations,â€ 2018 [Online].\nAvailable: https://doi.org/10.1101/306977.\n[39] N. Wongpakaran, T. Wongpakaran, D. Wedding, K.L. Gwet, A comparison of\nCohenâ€™s Kappa and Gwetâ€™s AC1 when calculating inter-rater reliability coefficients:\na study conducted with personality disorder samples, BMC. Med. Res. Methodol. 13\n(1) (2013) 1â€“7.\n[40] J.R. Landis, G.G. Koch, The measurement of Observer agreement for categorical\ndata, Biometrics 33 (1) (1977) 159â€“174.\n[41] R. Donners, et al., Repeatability of quantitative individual lesion and total disease\nmultiparametric whole-body MRI measurements in prostate cancer bone\nmetastases, Br. J. Radiol. 96 (1151) (2023).\n[42] M.D. Blackledge, et al., Inter- and intra-observer repeatability of quantitative\nwhole-body, diffusion-weighted imaging (WBDWI) in metastatic bone disease,\nPLoS. One 11 (4) (2016) 1â€“12.\n[43] K. Zormpas-Petridis, et al., Accelerating whole-body diffusion-weighted mri with\ndeep learning-based denoising image filters, Radiol. Artif. Intell. 3 (5) (2021).\n[44] M. Wennmann, et al., Deep learning for automatic bone marrow apparent diffusion\ncoefficient measurements from whole-body magnetic resonance imaging in\npatients with multiple myeloma: a retrospective multicenter study, Invest. Radiol.\n58 (4) (2023) 273â€“282.\nA. Candito et al. \nComputer Methods and Programs in Biomedicine 272 (2025) 109043\n15",
    "version": "5.3.31"
  },
  {
    "numpages": 10,
    "numrender": 10,
    "info": {
      "PDFFormatVersion": "1.7",
      "Language": "English",
      "EncryptFilterName": null,
      "IsLinearized": true,
      "IsAcroFormPresent": false,
      "IsXFAPresent": false,
      "IsCollectionPresent": false,
      "IsSignaturesPresent": false,
      "Creator": "Elsevier",
      "Custom": {
        "CrossMarkDomains[1]": "elsevier.com",
        "CrossmarkMajorVersionDate": "2010-04-23",
        "CreationDate--Text": "21st January 2025",
        "ElsevierWebPDFSpecifications": "7.0.1",
        "robots": "noindex",
        "doi": "10.1016/j.canlet.2025.217438",
        "CrossMarkDomains[2]": "sciencedirect.com",
        "CrossmarkDomainExclusive": "true"
      },
      "ModDate": "D:20250122020604Z",
      "Author": "Yu Gong",
      "Title": "A foundation model with weak experiential guidance in detecting muscle invasive bladder cancer on MRI",
      "Keywords": "Deep learning,Self-supervised learning,Muscle-invasive bladder cancer,Multiparametric MRI (mpMRI),Vesical imaging-reporting and data system (VI-RADS)",
      "CreationDate": "D:20250121232855Z",
      "Producer": "Acrobat Distiller 8.1.0 (Windows)",
      "Subject": "Cancer Letters, 611 (2025) 217438. doi:10.1016/j.canlet.2025.217438"
    },
    "metadata": {
      "crossmark:crossmarkdomains": "elsevier.comsciencedirect.com",
      "crossmark:crossmarkdomainexclusive": "true",
      "crossmark:doi": "10.1016/j.canlet.2025.217438",
      "crossmark:majorversiondate": "2010-04-23",
      "dc:format": "application/pdf",
      "dc:identifier": "10.1016/j.canlet.2025.217438",
      "dc:publisher": "Elsevier B.V.",
      "dc:description": "Cancer Letters, 611 (2025) 217438. doi:10.1016/j.canlet.2025.217438",
      "dc:subject": [
        "Deep learning",
        "Self-supervised learning",
        "Muscle-invasive bladder cancer",
        "Multiparametric MRI (mpMRI)",
        "Vesical imaging-reporting and data system (VI-RADS)"
      ],
      "dc:title": "A foundation model with weak experiential guidance in detecting muscle invasive bladder cancer on MRI",
      "dc:creator": [
        "Yu Gong",
        "Xiaodong Zhang",
        "Yi-Fan Xia",
        "Yi Cheng",
        "Jie Bao",
        "Nan Zhang",
        "Rui Zhi",
        "Xue-Ying Sun",
        "Chen-Jiang Wu",
        "Fei-Yun Wu",
        "Yu-Dong Zhang"
      ],
      "jav:journal_article_version": "VoR",
      "pdf:creationdate--text": "22nd January 2025",
      "pdf:producer": "Acrobat Distiller 8.1.0 (Windows)",
      "pdf:keywords": "Deep learning,Self-supervised learning,Muscle-invasive bladder cancer,Multiparametric MRI (mpMRI),Vesical imaging-reporting and data system (VI-RADS)",
      "pdfx:creationdate--text": "22nd January 2025",
      "pdfx:crossmarkdomains": "sciencedirect.comelsevier.com",
      "pdfx:crossmarkdomainexclusive": "true",
      "pdfx:crossmarkmajorversiondate": "2010-04-23",
      "zlkjszgemn.z-y.ygodqpmcnnymqolt_9mwegmwv.m.moowqlogmqo9eqnmakmmqjmt2tma": "",
      "pdfx:doi": "10.1016/j.canlet.2025.217438",
      "pdfx:robots": "noindex",
      "prism:aggregationtype": "journal",
      "prism:copyright": "Â© 2025 Elsevier B.V. All rights are reserved, including those for text and data mining, AI training, and similar technologies.",
      "prism:coverdate": "2025-02-28",
      "prism:coverdisplaydate": "28 February 2025",
      "prism:doi": "10.1016/j.canlet.2025.217438",
      "prism:issn": "0304-3835",
      "prism:pagerange": "217438",
      "prism:publicationname": "Cancer Letters",
      "prism:startingpage": "217438",
      "prism:url": "https://doi.org/10.1016/j.canlet.2025.217438",
      "prism:volume": "611",
      "tdm:policy": "https://www.elsevier.com/tdm/tdmrep-policy.json",
      "tdm:reservation": "1",
      "xmp:createdate": "2025-01-21T23:28:55",
      "xmp:creatortool": "Elsevier",
      "xmp:metadatadate": "2025-01-22T02:06:04",
      "xmp:modifydate": "2025-01-22T02:06:04",
      "xmpmm:documentid": "uuid:1b499bed-4ce8-4c0c-b682-0d58baae1cbe",
      "xmpmm:instanceid": "uuid:b6ed5266-03cb-4855-90c0-636283357f42",
      "xmprights:marked": "True"
    },
    "text": "Original Articles\nA foundation model with weak experiential guidance in detecting muscle\ninvasive bladder cancer on MRI\nYu Gong \na,1\n, Xiaodong Zhang \nc,1 \n, Yi-Fan Xia \na\n, Yi Cheng \na\n, Jie Bao \nd\n, Nan Zhang \ne\n, Rui Zhi \na\n,\nXue-Ying Sun \na\n, Chen-Jiang Wu \na\n, Fei-Yun Wu \na,2\n, Yu-Dong Zhang \na,b,*,2\na \nDepartment of Radiology, The First Affiliated Hospital of Nanjing Medical University, Nanjing, Jiangsu Province, 210029, PR China\nb \nThe Affiliated Suqian First Peopleâ€™s Hospital of Nanjing Medical University, Suqian, Jiangsu Province, PR China\nc \nDepartment of Radiology, Peking University First Hospital, Beijing, PR China\nd \nDepartment of Radiology, The First Affiliated Hospital of Soochow University, Suzhou, Jiangsu Province, 215006, PR China\ne \nDepartment of Urology, The Second Affiliated Hospital, School of Medicine, Zhejiang University, Hangzhou, Zhejiang Province, 310009, PR China\nA R T I C L E I N F O\nKeywords:\nDeep learning\nSelf-supervised learning\nMuscle-invasive bladder cancer\nMultiparametric MRI (mpMRI)\nVesical imaging-reporting and data system (VI-\nRADS)\nA B S T R A C T\nPreoperative detection of muscle-invasive bladder cancer (MIBC) remains a great challenge in practice. We\naimed to develop and validate a deep Vesical Imaging Network (ViNet) model for the detection of MIBC using\nhigh-resolution T\n2-\nweighted MR imaging (hrT\n2\nWI) in a multicenter cohort. ViNet was designed using a modified\n3D ResNet, in which, the encoder layers were pretrained using a self-supervised foundation model on over\n40,000 cross-modal imaging datasets for transfer learning, and the classification modules were weakly super-\nvised by an experiential knowledge-domain mask indicated by a nnUNet segmentation model. Optimal ViNet\nmodel was trained in derivation data (cohort 1, n = 312) and validated in multicenter data (cohort 2, n = 79;\ncohort 3, n = 44; cohort 4, n = 56) across a multi-ablation-test for model selection. In internal validation, ViNet\nusing hrT\n2\nWI outperformed all ablation-test models (odds ratio [OR], 7.41 versus 1.85â€“2.70; all P < 0.05). In\nexternal validation, the performance of ViNet using hrT\n2\nWI versus ablation-test models was heterogeneous (OR,\n1.31â€“3.89 versus 0.89â€“9.75; P = 0.03â€“0.15). In addition, clinical benefit of ViNet was evaluated between six\nreaders using the Vesical Imaging-Reporting and Data System (VI-RADS) versus ViNet-adjusted VI-RADS. As a\nresult, ViNet-adjusted VI-RADS upgraded 62.9 % (17/27) of MIBC missed in VI-RADS score 2, while downgraded\n84.1 % (69/84), 62.5 % (35/56) and 67.9 % (19/28) of non-muscle-invasive bladder cancer (NMIBC) over-\nestimated in VI-RADS score 3â€“5. We concluded that ViNet presents a promising alternative for diagnosing MIBC\nusing hrT2WI instead of conventional multiparametric MRI.\n1. Introduction\nBladder cancer (BC) is the 9th most commonly malignancies and\n13th leading cause of cancer-specific mortality worldwide [1,2]. It is a\nprerequisite for treatment to distinguish muscular-invasive bladder\ncancer (MIBC) from non-muscle-invasive bladder cancer (NMIBC). With\nthe development of techniques and medicines for treating BC, patients\nwith NMIBC (stage Ta/T1 or carcinoma in situ) can achieve better\ntherapeutic outcome by transurethral resection of bladder tumor with or\nwithout adjuvant intravesical chemotherapy (CHT) or/and intravesical\nbacillus Calmette-Guerin (BCG) immunotherapy [3]. While MIBCs are\nalways presented with aggressive characteristics, leading to poor prog-\nnosis with a 5-year survival rate of no more than 60 % [4]. Radical\ncystectomy and perioperative chemotherapy are suggested therapeutic\noptions for MIBC [5]. Therefore, accurate staging of BC would have a\nsignificant impact on the treatment decision making, especially for\nMIBCs, to prevent neither underestimate or overtreatment.\nThe recommended diagnostic method for MIBC is the cystoscopy and\n* Corresponding author. Department of Radiology, the First Affiliated Hospital with Nanjing Medical University No. 300, Guangzhou Road, Nanjing, Jiangsu\nProvince, 210029, PR China.\nE-mail addresses: NJMU_GY1201@163.com (Y. Gong), zhxd2009@gmail.com (X. Zhang), xyf9474@njmu.edu.cn (Y.-F. Xia), cheng20230130@163.com\n(Y. Cheng), baojie7346@suda.edu.cn (J. Bao), nanzhang@zju.edu.cn (N. Zhang), njmu_zr@163.com (R. Zhi), njmu_sxy@163.com (X.-Y. Sun), njmu_wcj@163.\ncom (C.-J. Wu), wfy_njmu@163.com (F.-Y. Wu), zhangyd3895@njmu.edu.cn (Y.-D. Zhang).\n1 \nYu Gong and Xiaodong Zhang are co-first authors.\n2 \nFei-Yun Wu and Yu-Dong Zhang are co-corresponding authors.\nContents lists available at ScienceDirect\nCancer Letters\njournal homepage: www.elsevier.com/locate/canlet\nhttps://doi.org/10.1016/j.canlet.2025.217438\nReceived 10 September 2024; Received in revised form 4 December 2024; Accepted 1 January 2025\nCancer Letters 611 (2025) 217438\nAvailable online 2 January 2025\n0304-3835/Â© 2025 Elsevier B.V. All rights are reserved, including those for text and data mining, AI training, and similar technologies.\n\nevaluation of the pathologic tissue obtained by transurethral resection of\nthe bladder tumor (TURBT). While not all BC patients can serve as the\nmain indication of diagnostic TURBT due to potential risk of serious\ncomplication such as bladder perforation [6]. In addition, insufficient\nsampling of the bladder muscle layer leads to the difficulty in accurate\nstaging of the BC tumors with cystoscope biopsy [7], which may lead to\n20 %â€“30 % of MIBC lesions missed [8].\nMultiparametric magnetic resonance imaging (mpMRI) is a great\nalternative to invasive cystoscope for diagnosing and staging BC. Several\nstudies have highlighted promising role of mpMRI in detecting MIBC\n[9]. In 2018, a five-point Vesical Imaging-Reporting and Data System\n(VI-RADS) was introduced, primarily utilizing high-resolution\nT2-weighted imaging (hrT\n2\nWI), diffusion-weighted imaging (DWI),\nand dynamic contrast-enhanced (DCE) imaging to assess the likelihood\nof MIBC [10]. However, the interpretation of VI-RADS was heteroge-\nneous among multiple readers with various levels of experience duo to\nlimited inter-reader consistency [11,12]. As it necessitates the expertise\nof highly experienced radiologists; for example, typical features like\nâ€œstalkâ€ and â€œuninterrupted or interrupted low signal intensityâ€, as\ndefined in VI-RADS, are challenging to quantify on mpMRI [13,14].\nFurthermore, VI-RADS is primarily applicable to untreated patients or\npatients having received â€œonlyâ€ diagnostic TURBT before re-TURBT [13,\n15]. In clinical practice, many patients have had multiple TURBTs\nand/or chemotherapy, complicating the application of VI-RADS [16]. In\naddition, adoption of mpMRI can be time-consuming and increases the\nlikelihood of motion artifacts. Gadolinium (Gd)-based MRI might in-\ncrease the risk of Gd deposition and nephritic systemic fibrosis, partic-\nularly in patients with chronic kidney diseases [17]. There is ongoing\ndebate about the necessity of mpMRI for all BC patients, leading to\nincreased interest in simplified biparametric or single-parametric MRI\napproaches [18,19].\nCurrently, deep learning (DL) is a promising solution for disease\ndiagnosis in medical imaging with the ability to automatically learn\nimage features, segment lesions, and perform lesion classification [20].\nAdvanced algorithms enable DL to meet or even exceed the diagnostic\nperformance of human experts for various diseases [21â€“24]. Application\nof DL for bladder & tumor detection, segmentation, classification, and\nprognostic prediction has been reported in several studies [25,26]. Re-\nsearchers such as Zou et al. and Li et al. proposed multi-task DL models\nfor the detection of MIBC using single-parametric T\n2\nWI or mpMRI,\nshowing potential advantages over VI-RADS, radiomics, and single-task\nDL approaches [25,27,28]. However, the field of DL in medical imaging\nanalysis is currently dominated by supervised DL, which necessitates a\nsubstantial number of annotated instances. Such supervised approach\nresults in increased annotation burden. Newer methodologies aimed for\nDL models can be trained on label-free data, such as self-supervised\nlearning (SSL) [29,30]. SSL aims to alleviate data inefficiency by\nderiving supervisory signals directly from data, rather than relying on\nexpert knowledge for label [31â€“33]. By pretraining on a sufficiently\nlarge number of diverse images, SSL-derived foundation models can\neffectively transfer valuable deep features to new models, thereby\nimproving their performance on previously uncharted datasets [34,35].\nSeveral SSL-derived foundation models have been proposed in medical\nimage anomaly detection, achieving comparable accuracy to conven-\ntional supervised learning in several field-specific images [30,35].\nWe contended that artificial and human intelligences can be effec-\ntively integrated in task-specific practice. Therefore, the aim of this\nstudy was to explore a deep Vesical Imaging Network (ViNet) by\nincorporating SSL-derived foundation models and experiential\nknowledge-domain guidance for MIBC diagnostic pathway using\nhrT\n2\nWI.\n2. Material and methods\n2.1. Patient population and study design\nThis retrospective, multicenter study involved routine care per-\nformed at three centers (center 1, the First Affiliated Hospital of Nanjing\nMedical University; center 2, The Second Affiliated Hospital, School of\nMedicine, Zhejiang University; center 3, the First Affiliated Hospital of\nSoochow University). Ethics committee approval was granted by local\nInstitutional Ethics Review Board (2022-SR-471) and the requirement of\nwritten informed consent for the data used for this research was waived.\nAll procedures involving human participants followed the guidelines set\nforth by the 1975 Helsinki declaration and subsequent amendments.\nWe conducted a retrospective collection of patients who underwent\nbladder mpMRI scans at three centers between Jan 2013 and Sep 2023.\nPatients who met the following criteria were included: 1) urothelial\ncarcinoma of the bladder confirmed by final cystectomy and/or TURBT;\n2) received a standard contrast-enhanced 3.0T mpMRI before surgery;\nand 3) all tumors within patients included were resected and received\npathologic examination separately. For patients with diagnostic TURBT,\na piece of detrusor muscle tissue at the tumor base was also removed for\nhistopathologic examination to evaluate for detrusor muscle invasion.\nPatients with absence of surgical interventions, without urothelial car-\ncinoma of bladder determined at histopathology, and with inadequate\nimage quality or with inadequate pathology for analysis, were excluded.\nFinally, a total of 491 patients (center 1, n = 391; center 2, n = 44; center\n3, n = 56) were retrospectively included into this study. Patients in\ncenter 1 were randomly split into training (n = 312), internal test (n =\n79) for model development and internal test. Patients from center 2 and\ncenter 3 were used for external validation. The overall flowchart of\nstudy and patient enrollment procedures are illustrated in Fig. 1.\nHistopathological outcomes were assessed according to the World\nHealth Organization (WHO) tumor staging for urothelial carcinoma of\nbladder [36]. All pathologic specimens in each center were prepared and\nexamined by two experienced pathologists with 10 years and 15 years of\nexperience in urologic pathology, respectively. MIBC, referring to the\ninvasion of BC into the muscular layer of the bladder wall, was defined\nas the primary clinical endpoint of this study.\n2.2. Image acquisition and analysis\nAll patients underwent mpMRI examination of the bladder on four\n3.0 T MRI scanners (U770, United Imaging Healthcare Corporation,\nShanghai, China, in center 1; MAGNETOM Skyra scanner, Siemens\nHealthineers, Erlangen, Germany, in center 1, center 2, and center 3)\nwith a supine position, after fasting for 4â€“6 h with no food. Patients were\nrequested to urinate 2 h before the examination. The scan protocol\nincluded axial T\n1\nWI; axial, coronal, and sagittal hrT\n2\nWI, axial DWI, and\nDCE imaging. DCE imaging was acquired before and after intravenous\ninjection of 0.1 mmol/kg of gadolinium-based contrast agent (Mag-\nnevist, Bayer Healthcare) at a rate of 2.5 ml/s for a total volume of 25\nml. The apparent diffusion coefficient (ADC) value was measured by\nexpending a mono-exponential fitting model. The median time from\nmpMRI to operation was 9.5 days (range, 3â€“13 days). The imaging pa-\nrameters are listed in supplementary materials (Table E1).\nMpMRI of each patient were retrospectively assessed by six readers\n(Y.F.X., Y.C., and Y.G., with 3-yr experiences in genitourinary imaging;\nJ.B., X.Y.S., and Y.D.Z, with 7â€“15 years of experience in genitourinary\nimaging, respectively) using VI-RADS assessment [10]. All readers were\nblinded to the clinical details of the patients. Each reader made his/her\ninterpretation based on his/her own continuous medical training and\nexperience. The five-point scale reflects the likelihood of MIBC: 1, highly\nunlikely; 2, unlikely; 3, equivocal; 4, likely; 5, very likely. For patients\nwith multiple lesions, tumor with the largest burden (largest tumor size\nor highest VI-RADS score) were selected for imaging analysis. Discrep-\nancies were resolved by a consensus process. In the case of substantial\nY. Gong et al. \nCancer Letters 611 (2025) 217438\n2\n\ndifferences, a group discussion was conducted, with each reader\nproviding his/her interpretation of the images. If a consensus was not\nreached, the final score was determined by the senior reader with the\nmost extensive experience in bladder cancer imaging.\n2.3. Designment of ViNet networks\nThe ViNet networks were implemented with a PyTorch ver. 1.4.0 at\nhttps://pytorch.org/, a fastai ver. 2.1.10 at https://docs.fast.ai/, and an\nopen-source code faimed3d at https://github.com/kbressem/faimed3d\nusing a NVIDIA Tesla V100 GPU. It consists of three key pipelines\nincluding one upstream task, and two downstream tasks, each of which\nincludes three functional modules: an input module, a shared encoder\nmodule, and a separate decoder module. The shared feature extractor\nencoder was based on a modified ResNet3d-18 architecture [37], and\ncustom implementations were used for the 3D image restoration,\nbladder wall and tumor segmentation, and MIBC prediction heads.\nBefore the training, all image data sets are normalized to balance in-\ntensity values and narrow the region of interest using a Z-score to make\nthe scale similar before importing into the model.\nThe construction of ViNet was divided into four primary steps. In the\nfirst step, a 3D nnUNet model was developed to automatically segment\nthe whole bladder wall and tumor for each patient [38] (Fig. 2a). The\nobjective of nnUNet was to segment candidate region likely to be\ninvolved with MIBC. The nnUNet-segmented candidate region covers\nboth tumor and adjacent muscle layer, serving as an experiential\nknowledge-domain mask to guide attention learning in ViNet. To ach-\nieve this, the nnUNet model was first trained on hrT\n2\nWI from 312 pa-\ntients and then tested on 79 hold-out patients in center 1. The two\ncropped volumes of interest (VOIs) of tumor and adjacent bladder wall,\nsegmented by nnUNet model, were then manually rechecked by two\nradiologists. These VOIs were then transformed to resampled & regis-\ntered DWI/ADC and DCE for next-step ViNet development. Readers\naided by automated nnUNet model can achieve faster data labelling\nspeeds.\nThe inputs of ViNet comprise of four-channel image sets, i.e., axial\nhrT\n2\nWI, DWI, ADC and arterial-phase DCE images. As four MR imaging\nmodalities have different spatial resolutions, we first aligned DWI, ADC,\nand DCE to T\n2\nWI on transverse plane with a registration toolbox Elastix\nVer 4.8 using parameter file â€œpar0001bspline16â€. The parameter\nconfiguration for the registration was set according to Klein et al.â€˜s work\n[39]. Specially, the twin causal attention-interactive masks, i.e., entire\ntumor volume and the focal region at the adjacent muscle layer of the\ntumor, were used to guide ViNet model towards attention learning of\nmorphological and intensity features at MIBC candidate regions,\nproviding the capability of attention learning that steers model to\nemulate diagnostic acumen of human expert in diagnosing MIBC on MR\nimages.\nBefore the training of ViNet, we pretrained a 3D SSL foundation\nmodel [40] using a large number of cross-modal data from native da-\ntabases (CT/MRI of brain, heart, lung, and abdomen, n = 36,531) and\ntwo open-source databases (LUNA 2016 [41], n = 888, chest CT; Med-\nical Segmentation Decathlon [42], n = 2,581, CT/MRI of brain, heart,\nlung, and abdomen) for restoration-based transfer learning. Here, 3D\nimage restoration is used to guide SSL foundation model for learning\nimage representation by discriminating and restoring images undergo-\ning different transformations. These transformations are a combination\nof four original individual transformations: non-linear, local-shuffling,\nouter-cutout, and inner-cutout (Fig. 2b). Utilization of 3D image resto-\nration thus can serve as an essential step towards achieving greater ac-\ncuracy and precision in the formation of task-special ViNet model,\nparticularly when compared to traditional 3D scratch models. And the\n3D SSL module is primarily utilized as a foundation model for transfer\nlearning in 3D medical imaging classification tasks [40]. The ViNet\nframeworks comprised of two modified Resnet3d-18 components,\nnamely R3d-18, to process images guided by two interactive attention\nmasks (cropped VOIs between tumor and adjacent bladder wall) sepa-\nrately. The body part of two R3d-18 components comprised the same\nshared encoder module driven from pretrained SSL foundation model as\na function of feature extractor, while the head parts of the two networks\nwere composed of fully connected layers which were preceded by two\nindependent concatenated 3D adaptive pooling layers. Once the SSL\nfoundation model is trained, the shared encoder of which can be\ntransferred and fine-tuned for the downstream MIBC-predictive tasks.\nThis allows ViNet to utilize representative features from pretrained SSL\nfoundation model to improve training efficiency (Fig. 2c). Details of\nViNet architecture and data processing procedures are described in\nsupplementary materials (Text E1-E5; Figure E1-E4).\nFig. 1. The flowchart of patient enrollment and exclusion for this study. MRI = Magnetic Resonance Imaging; TURBT = transurethral resection of the\nbladder tumor.\nY. Gong et al. \nCancer Letters 611 (2025) 217438\n3\n\nTo reduce uncertainty measures in ViNet, we used an ensemble of\nfive model instances with five-fold cross validation to maximize training\ncapability. During inference, the final output was determined by taking\nthe average prediction of five independent models.\n2.4. Ablation study\nIn this section, the effectiveness of each component of our method for\nMIBC diagnosis was tested in multi-imaging vector instances that models\nconstructed using T\n2\nWI, DWI/ADC, DCE separately and mpMRI se-\nquences altogether. In addition, potential superiority of ViNet was\nevaluated in multi-model instances that models trained with a\nRadiomics-based random forest analysis [43], a ResNet using single\ncubic bounding-box mask of tumor (Box-ResNet) [27], a ResNet using\nsingle-cropped mask of tumor (Crop-ResNet) [28], and a Crop-ResNet\nembedded with SSL (B2Net), the architecture of which is similar to\nthe ViNet while the mask of adjacent bladder wall was excluded\n(Fig. 2d). This multi-instance ablation test resulted in a total of 13\nmodels.\n2.5. Clinical performance\nClinical adoption of ViNet required for use by doctors should follow a\ncontinual trust-building approach from preclinical development to real-\nworld clinical deployment. Therefore, the nnUNet and ViNet models\nwere deployed into an AI diagnosis system (DeepCad v 5.0, Normal\nMedical Technology Co., Ltd, Shanghai, China) to assist radiologist for\npersonalized diagnostic tasks (Fig. 2e).\nNext, performance of leading ViNet model was compared six readers\nwho used VI-RADS for MRI interpretation both in internal (center 1, n =\n79) and external (center 2 and center 3, n = 100) validation data. As\ndata from center 2 and center 3 were highly heteronomous to center 1,\nspecially, we evaluated the impact of imaging setting & quality on the\nperformance of ViNet: 1) hrT\n2\nWI of 44 patients in center 2 was scanned\nwith fat-suppression; for this instance, performance of ViNet in data\nbetween with fat-suppression and non-fat-suppression hrT\n2\nWI was\ncompared; 2) image quality in center 3 was heterogeneous due to the\nretrospective collection; for this instance, image quality was evaluated\nin this data set by two readers; then the performance of ViNet was\ncompared between low and intermediate/high image quality groups.\nFig. 2. Overview of ViNet architecture. The construction of ViNet consists of four steps: in step 1 (a), a nnUNet network was used to train a nnUNet for the\nsegmentation of volumetric tumor and bladder wall on axial hrT\n2\nWI. This step is to aid readers for data labelling efficiently; in addition, nnNnet identifies the\ncandidate region where MIBC is probably involved, with aim to guide the ViNet model for next-step experiential knowledge domain attention learning. In step 2 (b),\nbefore training ViNet, we pretrained a 3D self-supervised learning (SSL) foundation model using a large number of cross-modal imaging data (n = 40,000) for 3D\nimage restoration. In step 3 (c), ViNet was constructed using a modified two-branch collaborative 3D ResNet, in which the encoder layers were shared with pre-\ntrained SSL foundation model for transfer learning. In classification modules, nnUNet provided an anatomy awareness focusing on â€œMIBC-casual regionsâ€ to guide\nViNet towards experiential knowledge-domain attention leaning. By this way, the powers of SSL foundation model and experiential knowledge-domain awareness\nwere combined to improve learning efficiency. In step 4 (d), ablation test was performed in multi-instances to verify accuracy and reproducibility of ViNet model.\nLast, in step 5 (e), the nnUNet and ViNet were integrated from preclinical development to clinical deployment into an AI-aided diagnostic system for clinical usage.\nY. Gong et al. \nCancer Letters 611 (2025) 217438\n4\n\n2.6. Statistical analysis\nInter-reader agreement for VI-RADS scoring assessment was tested\nwith a Cronbachâ€™s alpha coefficient of concordance. Performance of\nmodels for the diagnosis of MIBC was evaluated using an area under\nreceiver operating characteristic curve (AUC-ROC) analysis with a\nDeLong test. Regarding ablation test, the importance, accuracy, F1\nscore, precision, and recall of each-instance model was plotted using a\nNaÃ¯ve bayes regression analysis. Diagnostic heterogeneity across the\nAblation Test Models was evaluated using a Mantel-Haenszel (MH) test.\nThe reported statistical significance levels were two-sided, with statis-\ntical significance set at 0.05 for all analyses. All statistical analyses were\nperformed using R statistical software (version 4.2.2), SPSS software\n(version 26), Stata software (version 17), Origin 2021, and MedCalc\nsoftware (version 20.0).\n3. Results\n3.1. Baseline characteristics\nOf all patients included, MIBC was confirmed in surgical specimens\nof 108/312 (34.6 %) patients in training group, 27/79 (34.2 %) in in-\nternal test group and 38/100 (38.0 %) in external validation group,\nrespectively. The details of the baseline characteristics of all patients\nincluded in three centers are summarized in Table 1. Age and gender\nwere not significantly different between the groups of MIBC and NMIBC\n(all P values > 0.05). Surgery differed significantly between the two\ngroups (P < .001).\n3.2. Results of nnUNet model\nThe Dice, precision, and recall of nnUNet model for the segmentation\nof tumor and entire bladder wall in internal and external test data sets\nare summarized in Table 2. It shows that the results of nnUNet model for\nthe segmentation of tumor and bladder wall are consistent (P > .05)\nbetween MIBC and NMIBC in internal data. While, in external data, Dice\nscore of the model for tumor and bladder segmentation was significantly\n(P = .040 and P = .025, respectively) lower in NMIBC data, and the\nrecall of model for bladder segmentation was significantly (P = .006)\nlower in NMIBC patients.\n3.3. Ablation test\nResults of ablation test in 79 internal test data sets are summarized\n(Fig. 3). The correlation heatmap of 13 Ablation-Test models in terms of\nPearsonâ€™s and Spearmanâ€™s correlation coefficient (r) is plotted in Fig. 3a.\nIt shows that the output of ViNet_mpMRI (r = 0.60) and ViNet_T\n2\nWI (r =\n0.56) show relatively higher positive correlation ships with the rate of\nMIBC compared to the other models. Crop-ResNet and Box-ResNet (r =\n0.94), ViNet_mpMRI and ViNet_T\n2\nWI (r = 0.91) achieved excellently\ngood consistency in terms of prediction outputs compared with Radio-\nmics (r = 0.12 \u0000 0.33) and B2Net models (r = 0.37 \u0000 0.86). The plot of\npredictions as the variables in the stepwise recursive NaÃ¯ve bayes\nanalysis shows that ViNet_T\n2\nWI other than ViNet_mpMRI or the other\nmodels ranked first by calculating variable importance (Fig. 3b).\nThe performance of all 13 models in terms of AUC-ROC versus AUC-\nTable 1\nThe baseline characteristics of the patients included.\nVariable Training (n =\n312)\nInternal test\n(n = 79)\nExternal test (n\n= 100)\nP-\nvalue\nb\nAge (yr)\na \n67.0 Â± 11.5\n(26â€“92)\n65.4 Â± 11.9\n(34â€“87)\n69.6 Â± 11.0\n(42â€“92)\n0.046\nGender 0.450\nMale 276 (88.5 %) 66 (83.5 %) 89 (89.0 %)\nFemale 36 (11.5 %) 13 (16.5 %) 11 (11.0 %)\nHistory of TURBT before MRI 0.898\nNon 215 (68.9 %) 55 (69.6 %)\n1 59 (18.9 %) 10 (12.7 %)\n2 15 (4.8 %) 8 (10.1 %)\n3 or more 23 (7.4 %) 6 (7.6 %)\nSurgery 0.057\nDiagnostic\nTURBT\n171 (54.8 %) 43 (54.4 %) 68 (68.0 %)\nRC 141 (45.2 %) 36 (45.6 %) 32 (32.0 %)\nPathological\nMIBC\n0.808\nAbsent 204 (65.4 %) 52 (65.8 %) 62 (62.0 %)\nPresent 108 (34.6 %) 27 (34.2 %) 38 (38.0 %)\nNote. Note. Unless otherwise indicated not indicated, values are numbers of\npatients with percentage in brackets.\nTURBT = transurethral resection of the bladder tumor (TURBT); MIBC = muscle\ninvasive bladder cancer.\na \nValues are mean Â± std, with ranges in brackets.\nb \nOne-Way Analysis of Variance test for Age; Chi-square and Mann-Whitney U\ntest for counting data.\nTable 2\nEvaluation of the nnUNet model in internal and external test data.\nMetrics Group No. In-lesion segmentation In-bladder\nsegmentation\nM [P\n25\n, P\n75\n] P\nvalue\nM [P\n25\n, P\n75\n] P\nvalue\nDice\nInternal\ntest\nMIBC 27 0.756\n[0.550,\n0.880]\n0.573 0.734\n[0.548,\n0.788]\n0.772\nNMIBC 52 0.746\n[0.470,\n0.879]\n0.681\n[0.502,\n0.820]\nExternal\ntest\nMIBC 38 0.742\n[0.430,\n0.894]\n0.040* 0.663\n[0.406,\n0.806]\n0.025*\nNMIBC 62 0.288\n[0.077,\n0.932]\n0.521\n[0.325,\n0.692]\nPrecision\nInternal\ntest\nMIBC 27 0.860\n[0.625,\n0.935]\n0.776 0.733\n[0.408,\n0.834]\n0.844\nNMIBC 52 0.849\n[0.574,\n0.914]\n0.735\n[0.406,\n0.836]\nExternal\ntest\nMIBC 38 0.884\n[0.746,\n0.990]\n0.450 0.950\n[0.840,\n0.976]\n0.064\nNMIBC 62 0.965\n[0.554,\n1.000]\n0.971\n[0.908,\n0.995]\nRecall\nInternal\ntest\nMIBC 27 0.786\n[0.555,\n0.900]\n0.323 0.760\n[0.668,\n0.871]\n0.176\nNMIBC 52 0.734\n[0.336,\n0.899]\n0.825\n[0.694,\n0.883]\nExternal\ntest\nMIBC 38 0.719\n[0.378,\n0.897]\n0.056 0.591\n[0.409,\n0.751]\n0.006*\nNMIBC 62 0.221\n[0.045,\n0.894]\n0.355\n[0.202,\n0.613]\nNote. Note. Unless otherwise indicated not indicated, values are means with\npercentage 25 % (P\n25\n) and percentage 75 % (P\n75\n) in brackets. *, significant at the\nindependent t-test.\nADC = apparent diffusion coefficient; DWI = diffusion-weighted imaging; T2WI\n= T2-weighted imaging; DCE = dynamic-contrast enhancement; mpMRI =\nmultiparametric MRI; DL = deep learning; SSL = self-supervised learning; VI-\nRADS = vesical imaging-reporting and data system; MIBC = muscle-invasive\nbladder cancer; TURBT = transurethral resection of the bladder tumor.\nY. Gong et al. \nCancer Letters 611 (2025) 217438\n5\n\nPR in 79 internal test data is plotted in Fig. 3c. Among all models,\nViNet_T\n2\nWI resulted in highest AUC-ROC & AUC-PR (0.87/0.72), fol-\nlowed by ViNet_mpMRI (0.82/0.71) and B2Net_mpMRI (0.81/0.70).\nRegarding inter-B2Net models, except B2Net_mpMRI vs B2Net_DWI (P\n= .042), the difference of AUC-ROC in inter-B2Net models is insignifi-\ncant at pairwise test (P = .13 \u0000 0.83). Regarding inter-ViNet model,\nViNet_T\n2\nWI produced relatively higher AUC-ROC than ViNet_DWI,\nViNet_ADC, and ViNet_DCE at pairwise test (P = .09, P = .07, and P =\n.17, respectively). Against ViNet_T\n2\nWI, ViNet_mpMRI did not achieve\nsignificant increase in AUC-ROC (P = .13). Compared to Radiomics, Box-\nResNet, Crop-ResNet, ViNet_T2WI and ViNet_mpMRI resulted in signif-\nicantly higher AUC-ROC (P = .008â€“0.015). Considering higher resolu-\ntion, no concerns for Gd-agent exposure, saving time of MRI\nexamination, and not compromising diagnostic performance, ViNe-\nt_T\n2\nWI was selected as leading model for clinical usage. The probability\ndensity of ViNet_T\n2\nWI is plotted in Fig. 3d, showing consistency of\nprediction output between train, internal, and external data sets.\n3.4. Clinical evaluation of ViNet\nThe performance of ViNet_T\n2\nWI versus Box-ResNet, Crop-ResNet,\nRadiomics, and six readers using VI-RADS for the diagnosis of MIBC is\nsummarized Fig. 4. Using the optimal Youden index-based cut-off point\nderived from ROC analysis in train data as the diagnostic threshold for\nMIBC, the performance of four models (ViNet_T\n2\nWI vs Box-ResNet,\nCrop-ResNet, and Radiomics) is compared using the Mantel-Haenszel\nanalysis. It shows obviously (P < .05) diagnostic heterogeneity in\nterms of sensitivity (Fig. 4a) and specificity (Fig. 4b) of four models in\nthree test cohorts featured with various imaging parameter and image\nquality. Overall, radiomics is inferior (P < .05 with pairwise test) to the\nother three models in all three test cohorts. ViNet_T\n2\nWI performed better\nthan Box-ResNet, Crop-ResNet, and Radiomics (diagnostic odds ratio\n[OR], 7.41 vs 1.85â€“2.70; P < .05) in internal test data; while the dif-\nference in OR between ViNet_T\n2\nWI vs Box-ResNet and Crop-ResNet is\nheterogeneous (P = .03â€“0.15) in external test data (Fig. 4c). The effect of\nfat suppression on model performance is insignificant (P > .05) in Box-\nResNet and Crop-ResNet, while the effect is obvious (P < .05) in ViNe-\nt_T\n2\nWI and Radiomics. All four models performed better (P < .05) in\ndata with high image quality than in data with low image quality.\nSimilar results are observed in Meta-regression summary ROC analysis\n(Fig. 4d).\nRegarding VI-RADS assessment for MIBC, the inter-reader agreement\nof four readers in assessing 79 internal data and two readers in assessing\n100 external data is moderate, with the Cronbachâ€™s coefficients from\n0.53 to 0.75 (Fig. 4e). The ROC analysis of ViNet_T\n2\nWI vs VI-RADS score\nassessed by six readers is plotted in (Fig. 4f). In internal test, AUC of\nViNet_T\n2\nWI (0.87 [95 % CI, 0.77â€“0.93]) is slightly higher than that of\ntwo readers (R4: 0.75 [95 % CI, 0.64â€“0.84], P = .08; R2, 0.76 [95 % CI,\nFig. 3. Ablation test in 13 models based on different network architectures and imaging combinations. Correlation matrix with Pearson and Spearman\nanalysis was plotted to show correlation ship of 13 Ablation-Test models with presence of MIBC, in which numbers in box are inter-model coefficients (a). Important\nindex of individual model for the prediction of MIBC is plotted with a NaÃ¯ve Bayes analysis, in which ViNet_T\n2\nWI ranks first among all 13 models (b). Results of\nscatter diagram of AUC-ROC vs AUC-PR of 13 Ablation-Test models, in which, ViNet_T\n2\nWI produces the highest AUC both in ROC and PR curves (c). Distribution of\nprobability density of selected ViNet_T\n2\nWI model to show the intra-group agreement between training, internal and external test data (d).\nY. Gong et al. \nCancer Letters 611 (2025) 217438\n6\n\n0.65â€“0.85], P = .07); while is insignificant versus that of R1 (0.83 [95 %\nCI, 0.73â€“0.91], P = .58) and R3 (0.77 [95 % CI, 0.67â€“0.86], P = .12). In\nexternal test, the difference of AUC between ViNet_T\n2\nWI (0.75 [95 % CI,\n0.66â€“0.83]), R5 (0.78 [95 % CI, 0.69â€“0.88]) and R6 (0.77 [95 % CI,\n0.68â€“0.85]) is insignificant (P = .65 and 0.76).\nIn addition, clinical benefit of VI-RADS adjusted by ViNet_T\n2\nWI was\npresented in Fig. 4g. It showed that, ViNet_T2WI-adjusted VI-RADS\nreidentified 62.9 % (17/27) of MIBC lesions which were missed in VI-\nRADS score-2 categories by six readers, and eliminated 62.5 % (35/\n56) and 67.9 % (19/28) of NMIBC lesions overestimated in VI-RADS\nscore 4â€“5 categories by six readers. Specially, it eliminated 84.1 %\n(69/84) of NMIBC lesions overestimated in equivocal score-3 category\nby six readers. To show interpretability of ViNet_T\n2\nWI, a Gradient-\nweighted Class Activation Mapp (Grad-CAM) was illustrated in\nFig. 5a. To indicate clinical application, a tumor diagnosed with ViNe-\nt_T\n2\nWI was reconstructed in Fig. 5b. To explain â€œmore may be not bet-\nterâ€ using mpMRI for the diagnosis of MIBC, the images of a patient with\nnotable artifact and â€œenhancement overlapping effectâ€ on DCE images\nare illustrated in Fig. 5c.\n4. Discussion\nAccurately detecting MIBC preoperatively is of clinical relevance for\ntreatment plan [44]. In this study, we presented a knowledge-embedded\ntransfer learning approach for the detection of MIBC on using hrT\n2\nWI\nMRI. The resulted demonstrated that ViNet did outperform previously\nestablished DL methods such as B2Net, ResNet, and Radiomics model. In\naddition, we found there was notable inter-reader disagreement in\nVI-RADS assessment among six radiologist readers. While such reader\nexperience-associated migration can be overcome by integrating\ndisease-specific medical knowledges into our ViNet networks. There-\nfore, we believed that this ViNet is safer and more convenient, thus\ncould be a great assistant to human readers for MIBC detection with\nMRI.\nTraditional experience-based assessment of MRI for MIBC remains a\ngreat challenge, with substantial variability in image interpretation\namong readers [45â€“47]. Our results demonstrated that ViNet on single\nhrT\n2\nWI can accurately detect MIBC in an objective and quantitative\nmanner, showing improved performance to six readers using VI-RADS\nFig. 4. Results of clinical performance of ViNet. Performance of the model in terms of sensitivity (a), specificity (b), diagnostic odds ratio (OR) (c) and summary\nROC analysis (d) was plotted using Meta-regression analysis among 16 multi-instance tests. No. 1 to No. 4 represent four candidate models such as ViNet_T\n2\nWI, Crop-\nResNet, Box-ResNet and Radiomics tested at 79 internal patients in center 1. No. 5 to No. 8 represent four models tested at 44 external patients in center 2, in which,\nT\n2\nW images were scanned with fat suppression (FS). No. 9 to No. 12 represent the four models tested at 33 external data with high quality (HQ) in center 3. No. 13 to\nNo. 16 represent four models tested at 23 external data with low quality (LQ) in center 3. Inter-reader agreement of six readers (R1 to R6) was tested with Cronbachâ€™s\nalpha coefficient (e). Performance of ViNet_T\n2\nWI vs VI-RADS of six readers in internal (int.) and external (ext.) data was compared with ROC analysis (f). Benefits of\nViNet_T\n2\nWI to VI-RADS score within six readers was plotted, where x-axis represents the counts of patients assigned by standard VI-RADS, and y-axis indicated counts\nof patients adjusted by ViNet_T\n2\nWI. By ViNet adjustment, the upgrades (red oval region) and downgrades (green oval regions) of the lesions falsely diagnosed by VI-\nRADS were presented (g). (For interpretation of the references to colour in this figure legend, the reader is referred to the Web version of this article.)\nY. Gong et al. \nCancer Letters 611 (2025) 217438\n7\n\nassessment. Unlike ablation-test â€œblack-boxâ€ models [48], ViNet in-\ncorporates two foundational SSL-derived components that process\nimage data guided by twin causal attention masks separately, which\nallowed to steer ViNet towards attention learning of morphological and\nintensity features on causal region where MIBC occurs. As a result, ViNet\nexhibits enhanced robustness and explainability, setting it apart from\ntraditional single-mask networks that overlook adjacent-peritumor re-\ngions. Additionally, nnUNet automatically segments the bladder wall\nand tumor, enabling our model adjust into our AI system swiftly and\naccurately in clinical adoption, thereby facilitating the broader accep-\ntance and deployment of our models.\nAgainst human-based assessments, ViNet offers a more objective and\nprecise diagnosis, unaffected by inter-reader variability. This improves\nthe credibility of imaging diagnosis by eliminating potential errors\ncaused by human subjectivity. Second, from a clinical perspective, we\nfrequently diagnose MIBC based on imaging features such as tumor with\nor without stalk, the length of tumor-wall interface, disruption of the\nmuscle layer, and the presence of a disrupted localized extravesical fatty\nspace [9,10,49,50]. Consequently, experienced radiologists tend to\nfocus on additional tumor-basal layer interactive region, which en-\ncompasses the tumor, the tumor-wall interface, and the adjacent muscle\nlayer of the tumor [10]. The consistency between this region and\nattention-learning region of ViNet offers a meaningful clinical pathway\nfor our AI model to emulate diagnostic acumen of experts in bladder\nimages assessment, enhancing the interpretability of clinical applica-\ntions. Third, the automatic segmentation capabilities of our AI model,\nalong with its ability to identify highly suspicious areas of MIBC, can\nhighlight tumor growth and provide crucial information for surgical\noperations. Fourth, for patients who have undergone multiple TURBT or\nchemotherapy, the application of VI-RADS may become complex;\nhowever, ViNet exhibits strong generalizability in such cases. Finally,\ntumors scored with VI-RADS 3 present a significant diagnostic challenge\n[45], especially without preoperative pathology. Our ViNet can serve as\na reliable diagnostic reference, surpassing expert performance and\nfacilitating a more effective human-AI collaborative approach for MIBC\ndiagnosis, ultimately supporting more accurate treatment decisions.\nNotably, our ViNet performed on hrT\n2\nWI, outperformed any other\nindividual MRI or combined mpMRI. This result may appear counter-\nintuitive to the common adage \"the more, the better.\" We attribute this\nresult to several potential factors: While mpMRI is generally regarded as\nthe state-of-the-art imaging modality for staging MIBC, some studies\nhave questioned the necessity of DCE in real-world MIBC assessments.\nThese studies have found that adding DCE to biparametric MRI does not\nsignificantly improve diagnostic accuracy [19]. This issue is also re-\nflected in our clinical routine, as DCE scan requires long acquisition time\n(4â€“5 min), the signal of which is always motion-sensitive and can be\nnoised by urines. Moreover, peritumor fibrosis and interstitial inflam-\nmation would cause enhancement overlapping between tumor and\nadjacent normal muscle layers, leading to the disturbing or overstaging\nof MIBC. Contrast to DWI and DCE, T\n2\nWI is motion less, advanced with\nextremely high spatial and tissue resolution, which is particularly\nappreciate for deep learning-based feature analysis. Therefore, we\nbelieved that ViNet with spMRI, such as hrT\n2\nWI, offers a safer, more\nconvenient alternative for MIBC detection with MRI, without compro-\nmising staging accuracy.\nAlthough encouraging results were obtained, several limitations\nwarrant mention in this study. First, ViNet was trained on a single-center\ndata with limited sample size. Even ViNet presented with good perfor-\nmance in internal data, its performance was heterogeneous in external\ndata. This may be associated with limited cohort size, image quality,\ndata heterogeneity, and selection bias of external validations. Therefore,\nits performance still needs to be demonstrated in a multicenter, larger\ncohort to establish its validity and reliability. Second, adding T2WI in\nsagittal and coronal planes may provide more benefits in the\nFig. 5. Additional role of ViNet for imaging interpretation of bladder cancer. Gradient-weighted Class Activation Mapp (Grad-CAM) shows the hot attention\nregion (arrow) mainly focus on â€œMIBC-candidate regionâ€ involving both tumor and adjacent bladder wall, demonstrating its interpretability in clinical application\n(a). Three-dimensional reconstruction of ViNet-based segmentation and detection in a patient with MIBC, which can be used to guide the treatment planning (b).\nExplanation of potential adverse effect of â€œoverestimateâ€ on dynamic-contrast enhancement (DCE) imaging (c). This is a patient with NMIBC, the basal muscle layer\nof tumor is uncontacted with tumor on T\n2\nWI (black arrow), while potentially caused by artifacts from motion and â€œenhancement overlapping effectâ€, tumor seems to\nbe muscle invasive on DCE images (white arrows). This may produce overestimate when ViNet_mpMRI is performed.\nY. Gong et al. \nCancer Letters 611 (2025) 217438\n8\n\nconstruction of the ViNet. Future updates to the model should incor-\nporate multi-plane T2WI. Third, the nnUNet performed with lower Dice\nscore in NMIBC lesions in external data. This may be due to smaller\nlesion sizes and poorer image quality of those NMIBC data. Afterall, it\nacted as an assistant to clinicians for releasing clinical workload. Last,\nthe model developed in this study was not able to fully leverage the\nclinical predictors of patients due to the limited availability of clinical\ninformation. However, an AI model plus with clinical predictors could\npotentially enhance the performance in detecting MIBC. This is one of\nour primary focus areas for future research.\n5. Conclusion\nWe designed a knowledge-guided DL model for the detection of\nMIBC. The integrated segmentation and detection models deposited on\nan AI system enables it to be applicated in real-world clinical settings.\nFurthermore, the proposed ViNet can be performed on single hrT\n2\nWI,\nwhich is safer, faster, and more convenient, while allow for improving\ndiagnostic performance of individual readers, making it as an attractive\ntool for bladder cancer assessment.\nCRediT authorship contribution statement\nYu Gong: Writing â€“ review & editing, Writing â€“ original draft,\nVisualization, Validation, Supervision, Software, Resources, Project\nadministration, Methodology, Investigation, Formal analysis, Data\ncuration, Conceptualization. Xiaodong Zhang: Writing â€“ review &\nediting, Writing â€“ original draft, Visualization, Validation, Supervision,\nSoftware. Yi-Fan Xia: Resources, Formal analysis. Yi Cheng: Project\nadministration, Data curation, Conceptualization. Jie Bao: Validation.\nNan Zhang: Validation. Rui Zhi: Resources, Formal analysis. Xue-Ying\nSun: Resources. Chen-Jiang Wu: Investigation, Formal analysis. Fei-\nYun Wu: Writing â€“ review & editing, Supervision, Project administra-\ntion, Data curation. Yu-Dong Zhang: Writing â€“ review & editing,\nWriting â€“ original draft, Visualization, Validation, Supervision, Soft-\nware, Resources, Project administration, Methodology, Investigation,\nFormal analysis, Data curation, Conceptualization.\nFunding\nThis research did not receive any specific grant from funding\nagencies in the public, commercial, or not-for-profit sectors.\nDeclaration of competing interest\nThe authors declare that they have no known competing financial\ninterests or personal relationships that could have appeared to influence\nthe work reported in this paper.\nAppendix A. Supplementary data\nSupplementary data to this article can be found online at https://doi.\norg/10.1016/j.canlet.2025.217438.\nReferences\n[1] S. Antoni, J. Ferlay, I. Soerjomataram, A. Znaor, A. Jemal, F. Bray, Bladder cancer\nincidence and mortality: a global overview and recent trends, Eur. Urol. 71 (2017)\n96â€“108.\n[2] R.J. Sylvester, O. Rodriguez, V. Hernandez, D. Turturica, L. Bauerova, H.M. Bruins,\nJ. Brundl, T.H. van der Kwast, A. Brisuda, J. Rubio-Briones, M. Seles, A.\nE. Hentschel, V.R.M. Kusuma, N. Huebner, J. Cotte, L.S. Mertens, D. Volanis,\nO. Cussenot, J.D. Subiela Henriquez, E. de la Pena, F. Pisano, M. Pesl, A.G. van der\nHeijden, S. Herdegen, A.R. Zlotta, J. Hacek, A. Calatrava, S. Mannweiler,\nJ. Bosschieter, D. Ashabere, A. Haitel, J.F. Cote, S. El Sheikh, L. Lunelli, F. Algaba,\nI. Alemany, F. Soria, W. Runneboom, J. Breyer, J.A. Nieuwenhuijzen, C. Llorente,\nL. Molinaro, C.A. Hulsbergen-van de Kaa, M. Evert, L. Kiemeney, J. Nâ€™Dow,\nK. Plass, O. Capoun, V. Soukup, J.L. Dominguez-Escrig, D. Cohen, J. Palou,\nP. Gontero, M. Burger, R. Zigeuner, A.H. Mostafid, S.F. Shariat, M. Roupret, E.\nM. Comperat, M. Babjuk, B.W.G. van Rhijn, European association of urology (EAU)\nprognostic factor risk groups for non-muscle-invasive bladder cancer (NMIBC)\nincorporating the WHO 2004/2016 and WHO 1973 classification systems for\ngrade: an update from the EAU NMIBC guidelines panel, Eur. Urol. 79 (2021)\n480â€“488\n.\n[3] M. Babjuk, A. Bohle, M. Burger, O. Capoun, D. Cohen, E.M. Comperat,\nV. Hernandez, E. Kaasinen, J. Palou, M. Roupret, B.W. van Rhijn, S.F. Shariat,\nV. Soukup, R.J. Sylvester, R. Zigeuner, EAU guidelines on non-muscle-invasive\nurothelial carcinoma of the bladder: update 2016, Eur. Urol. 71 (2017) 447â€“461.\n[4] J. Dobruch, S. Daneshmand, M. Fisch, Y. Lotan, A.P. Noon, M.J. Resnick, S.\nF. Shariat, A.R. Zlotta, S.A. Boorjian, Gender and bladder cancer: a collaborative\nreview of etiology, biology, and outcomes, Eur. Urol. 69 (2016) 300â€“310\n.\n[5] J.A. Witjes, H.M. Bruins, R. Cathomas, E.M. Comperat, N.C. Cowan, G. Gakis,\nV. Hernandez, E. Linares Espinos, A. Lorch, Y. Neuzillet, M. Rouanne, G.\nN. Thalmann, E. Veskimae, M.J. Ribal, A.G. van der Heijden, European association\nof urology guidelines on muscle-invasive and metastatic bladder cancer: summary\nof the 2020 guidelines, Eur. Urol. 79 (2021) 82â€“104\n.\n[6] Z. Klaassen, A.M. Kamat, W. Kassouf, P. Gontero, H. Villavicencio, J. Bellmunt, B.\nW.G. van Rhijn, A. Hartmann, J.W.F. Catto, G.S. Kulkarni, Treatment strategy for\nnewly diagnosed T1 high-grade bladder urothelial carcinoma: new insights and\nupdated recommendations, Eur. Urol. 74 (2018) 597â€“608\n.\n[7] S.S. Chang, J.M. Hassan, M.S. Cookson, N. Wells, J.A. Smith Jr., Delaying radical\ncystectomy for muscle invasive bladder cancer results in worse pathological stage,\nJ. Urol. 170 (2003) 1085â€“1087.\n[8] J.T. Ark, K.A. Keegan, D.A. Barocas, T.M. Morgan, M.J. Resnick, C. You, M.\nS. Cookson, D.F. Penson, R. Davis, P.E. Clark, J.A. Smith Jr., S.S. Chang, Incidence\nand predictors of understaging in patients with clinical T1 urothelial carcinoma\nundergoing radical cystectomy, BJU Int. 113 (2014) 894â€“899.\n[9] H. Ahn, S.I. Hwang, H.J. Lee, G. Choe, J.J. Oh, S.J. Jeong, S.S. Byun, J.K. Kim,\nQuantitation of bladder cancer for the prediction of muscle layer invasion as a\ncomplement to the vesical imaging-reporting and data system, Eur. Radiol. 31\n(2021) 1656â€“1666.\n[10] V. Panebianco, Y. Narumi, E. Altun, B.H. Bochner, J.A. Efstathiou, S. Hafeez,\nR. Huddart, S. Kennish, S. Lerner, R. Montironi, V.F. Muglia, G. Salomon,\nS. Thomas, H.A. Vargas, J.A. Witjes, M. Takeuchi, J. Barentsz, J.W.F. Catto,\nMultiparametric magnetic resonance imaging for bladder cancer: development of\nVI-RADS (vesical imaging-reporting and data system), Eur. Urol. 74 (2018)\n294â€“306.\n[11] M. Pecoraro, M. Takeuchi, H.A. Vargas, V.F. Muglia, S. Cipollari, C. Catalano,\nV. Panebianco, Overview of VI-rads in bladder cancer, AJR Am. J. Roentgenol. 214\n(2020) 1259â€“1268.\n[12] A.L. Lai, Y.M. Law, VI-RADS in bladder cancer: overview, pearls and pitfalls, Eur. J.\nRadiol. 160 (2023) 110666\n.\n[13] A.G. van der Heijden, J.A. Witjes, Vesical imaging-reporting and data system (VI-\nRADS) for bladder cancer diagnostics: the replacement for surgery? Eur. Urol.\nOncol. 3 (2020) 316â€“317.\n[14] D.J.A. Margolis, J.C. Hu, Vying for standardization of bladder cancer MRI\ninterpretation and reporting: VI-RADS, Radiology 291 (2019) 675â€“676.\n[15] Y. Ueno, M. Takeuchi, T. Tamada, K. Sofue, S. Takahashi, Y. Kamishima, N. Hinata,\nK. Harada, M. Fujisawa, T. Murakami, Diagnostic accuracy and interobserver\nagreement for the vesical imaging-reporting and data system for muscle-invasive\nbladder cancer: a multireader validation study, Eur. Urol. 76 (2019) 54â€“56.\n[16] J.A. Witjes, H.M. Bruins, R. Cathomas, E.M. Comperat, N.C. Cowan, G. Gakis,\nV. Hernandez, E. Linares Espinos, A. Lorch, Y. Neuzillet, M. Rouanne, G.\nN. Thalmann, E. Veskimae, M.J. Ribal, A.G. van der Heijden, European association\nof urology guidelines on muscle-invasive and metastatic bladder cancer: summary\nof the 2020 guidelines, Eur. Urol. 79 (2021) 82â€“104.\n[17] S. Sanyal, P. Marckmann, S. Scherer, J.L. Abraham, Multiorgan gadolinium (Gd)\ndeposition and fibrosis in a patient with nephrogenic systemic fibrosisâ€“an autopsy-\nbased review, Nephrol. Dial. Transplant. 26 (2011) 3616â€“3626.\n[18] Y. Liu, X. Xu, H. Wang, Y. Liu, Y. Wang, Q. Dong, Z. Li, Y. Guo, H. Lu, The\nadditional value of tri-parametric MRI in identifying muscle-invasive status in\nbladder cancer, Acad. Radiol. 30 (2023) 64â€“76.\n[19] A. Delli Pizzi, D. Mastrodicasa, M. Marchioni, G. Primiceri, F. Di Fabio, R. Cianci,\nB. Seccia, B. Sessa, E. Mincuzzi, M. Romanelli, P. Castellan, R. Castellucci,\nA. Colasante, L. Schips, R. Basilico, M. Caulo, Bladder cancer: do we need contrast\ninjection for MRI assessment of muscle invasion? A prospective multi-reader VI-\nRADS approach, Eur. Radiol. 31 (2021) 3874â€“3883.\n[20] J. De Fauw, J.R. Ledsam, B. Romera-Paredes, S. Nikolov, N. Tomasev, S. Blackwell,\nH. Askham, X. Glorot, B. Oâ€™Donoghue, D. Visentin, G. van den Driessche,\nB. Lakshminarayanan, C. Meyer, F. Mackinder, S. Bouton, K. Ayoub, R. Chopra,\nD. King, A. Karthikesalingam, C.O. Hughes, R. Raine, J. Hughes, D.A. Sim, C. Egan,\nA. Tufail, H. Montgomery, D. Hassabis, G. Rees, T. Back, P.T. Khaw, M. Suleyman,\nJ. Cornebise, P.A. Keane, O. Ronneberger, Clinically applicable deep learning for\ndiagnosis and referral in retinal disease, Nat. Med. 24 (2018) 1342â€“1350.\n[21] A. Esteva, B. Kuprel, R.A. Novoa, J. Ko, S.M. Swetter, H.M. Blau, S. Thrun,\nDermatologist-level classification of skin cancer with deep neural networks, Nature\n542 (2017) 115â€“118.\n[22] X. Fan, J. Li, B. Huang, H. Lu, C. Lu, M. Pan, X. Wang, H. Zhang, Y. You, X. Wang,\nQ. Wang, J. Zhang, Noninvasive radiomics model reveals macrophage infiltration\nin glioma, Cancer Lett. 573 (2023) 216380.\n[23] Y. Liu, J. Shi, W. Liu, Y. Tang, X. Shu, R. Wang, Y. Chen, X. Shi, J. Jin, D. Li, A deep\nneural network predictor to predict the sensitivity of neoadjuvant\nchemoradiotherapy in locally advanced rectal cancer, Cancer Lett. 589 (2024)\n216641.\nY. Gong et al. \nCancer Letters 611 (2025) 217438\n9\n\n[24] C. Liu, X. Cheng, K. Han, L. Hong, S. Hao, X. Sun, J. Xu, B. Li, D. Jin, W. Tian,\nY. Jin, Y. Wang, W. Fang, X. Bao, P. Zhao, D. Chen, A novel molecular subtyping\nbased on multi-omics analysis for prognosis predicting in colorectal melanoma: a\n16-year prospective multicentric study, Cancer Lett. 585 (2024) 216663.\n[25] Y. Zou, L. Cai, C. Chen, Q. Shao, X. Fu, J. Yu, L. Wang, Z. Chen, X. Yang, B. Yuan,\nP. Liu, Q. Lu, Multi-task deep learning based on T2-weighted images for predicting\nmuscular-invasive bladder cancer, Comput. Biol. Med. 151 (2022) 106219.\n[26] J. Yu, L. Cai, C. Chen, X. Fu, L. Wang, B. Yuan, X. Yang, Q. Lu, Cascade path\naugmentation unet for bladder cancer segmentation in MRI, Med. Phys. 49 (2022)\n4622â€“4631.\n[27] J. Li, K. Cao, H. Lin, L. Deng, S. Yang, Y. Gao, M. Liang, C. Lin, W. Zhang, C. Xie,\nK. Zhang, J. Luo, Z. Pan, P. Yue, Y. Zou, B. Huang, Predicting muscle invasion in\nbladder cancer by deep learning analysis of MRI: comparison with vesical imaging-\nreporting and data system, Eur. Radiol. 33 (2023) 2699â€“2709.\n[28] J. Li, Z. Qiu, K. Cao, L. Deng, W. Zhang, C. Xie, S. Yang, P. Yue, J. Zhong, J. Lyu,\nX. Huang, K. Zhang, Y. Zou, B. Huang, Predicting muscle invasion in bladder\ncancer based on MRI: a comparison of radiomics, and single-task and multi-task\ndeep learning, Comput. Methods Progr. Biomed. 233 (2023) 107466.\n[29] N. Ghaffari Laleh, H.S. Muti, C.M.L. Loeffler, A. Echle, O.L. Saldanha, F. Mahmood,\nM.Y. Lu, C. Trautwein, R. Langer, B. Dislich, R.D. Buelow, H.I. Grabsch, H. Brenner,\nJ. Chang-Claude, E. Alwers, T.J. Brinker, F. Khader, D. Truhn, N.T. Gaisa, P. Boor,\nM. Hoffmeister, V. Schulz, J.N. Kather, Benchmarking weakly-supervised deep\nlearning pipelines for whole slide classification in computational pathology, Med.\nImage Anal. 79 (2022) 102474.\n[30] M.Y. Lu, D.F.K. Williamson, T.Y. Chen, R.J. Chen, M. Barbieri, F. Mahmood, Data-\nefficient and weakly supervised computational pathology on whole-slide images,\nNat. Biomed. Eng. 5 (2021) 555â€“570.\n[31] M. Moor, O. Banerjee, Z.S.H. Abad, H.M. Krumholz, J. Leskovec, E.J. Topol,\nP. Rajpurkar, Foundation models for generalist medical artificial intelligence,\nNature 616 (2023) 259â€“265.\n[32] A. Sharma, R. Kumar, G. Yadav, P. Garg, Artificial intelligence in intestinal polyp\nand colorectal cancer prediction, Cancer Lett. 565 (2023) 216238.\n[33] K. Orostica, F. Mardones, Y.A. Bernal, S. Molina, M. Ochard, R.A. Verdugo,\nD. Carvajal-Hausdorf, K. Marcelain, S. Contreras, R. Armisen, Advances in machine\nlearning for tumour classification in cancer of unknown primary: a mini-review,\nCancer Lett. (2024) 217348.\n[34] H. Zhang, K. AbdulJabbar, T. Grunewald, A.U. Akarca, Y. Hagos, F. Sobhani, C.S.\nY. Lecat, D. Patel, L. Lee, M. Rodriguez-Justo, K. Yong, J.A. Ledermann, J. Le\nQuesne, E.S. Hwang, T. Marafioti, Y. Yuan, Self-supervised deep learning for highly\nefficient spatial immunophenotyping, EBioMedicine 95 (2023) 104769\n.\n[35] Y. Zhou, M.A. Chia, S.K. Wagner, M.S. Ayhan, D.J. Williamson, R.R. Struyven,\nT. Liu, M. Xu, M.G. Lozano, P. Woodward-Court, Y. Kihara, U.K.B. Eye, C. Vision,\nA. Altmann, A.Y. Lee, E.J. Topol, A.K. Denniston, D.C. Alexander, P.A. Keane,\nA foundation model for generalizable disease detection from retinal images, Nature\n622 (2023) 156â€“163.\n[36] A.T. Lenis, P.M. Lec, K. Chamie, M.D. Mshs, Bladder cancer: a review, JAMA 324\n(2020) 1980â€“1991\n.\n[37] T. Du, H. Wang, L. Torresani, J. Ray, M. Paluri, A closer look at spatiotemporal\nconvolutions for action recognition. 2018 IEEE/CVF Conference on Computer\nVision and Pattern Recognition (CVPR), 2018\n.\n[38] F. Isensee, P.F. Jaeger, S.A.A. Kohl, J. Petersen, K.H. Maier-Hein, nnU-Net: a self-\nconfiguring method for deep learning-based biomedical image segmentation, Nat.\nMethods 18 (2021) 203â€“211.\n[39] S. Klein, K. Staring M Fau - Murphy, M.A. Murphy K Fau - Viergever, J.P.\nW. Viergever Ma Fau - Pluim, J.P. Pluim, elastix: a toolbox for intensity-based\nmedical image registration, IEEE Trans. Med. Imag. 29 (2010) 196â€“205\n.\n[40] Z. Zhou, V. Sodha, M.M.R. Siddiquee, R. Feng, N. Tajbakhsh, M.B. Gotway,\nJ. Liang, Models genesis: generic autodidactic models for 3D medical image\nanalysis, Med. Image Comput. Comput. Assist Interv. 11767 (2019) 384â€“393.\n[41] A.A.A. Setio, A. Traverso, T. de Bel, M.S.N. Berens, C.V.D. Bogaard, P. Cerello,\nH. Chen, Q. Dou, M.E. Fantacci, B. Geurts, R.V. Gugten, P.A. Heng, B. Jansen, M.M.\nJ. de Kaste, V. Kotov, J.Y. Lin, J. Manders, A. Sonora-Mengana, J.C. Garcia-\nNaranjo, E. Papavasileiou, M. Prokop, M. Saletta, C.M. Schaefer-Prokop, E.\nT. Scholten, L. Scholten, M.M. Snoeren, E.L. Torres, J. Vandemeulebroucke,\nN. Walasek, G.C.A. Zuidhof, B.V. Ginneken, C. Jacobs, Validation, comparison, and\ncombination of algorithms for automatic detection of pulmonary nodules in\ncomputed tomography images: the LUNA16 challenge, Med. Image Anal. 42\n(2017) 1â€“13.\n[42] M. Antonelli, A. Reinke, S. Bakas, K. Farahani, A. Kopp-Schneider, B.A. Landman,\nG. Litjens, B. Menze, O. Ronneberger, R.M. Summers, B. van Ginneken, M. Bilello,\nP. Bilic, P.F. Christ, R.K.G. Do, M.J. Gollub, S.H. Heckers, H. Huisman, W.\nR. Jarnagin, M.K. McHugo, S. Napel, J.S.G. Pernicka, K. Rhode, C. Tobon-Gomez,\nE. Vorontsov, J.A. Meakin, S. Ourselin, M. Wiesenfarth, P. Arbelaez, B. Bae,\nS. Chen, L. Daza, J. Feng, B. He, F. Isensee, Y. Ji, F. Jia, I. Kim, K. Maier-Hein,\nD. Merhof, A. Pai, B. Park, M. Perslev, R. Rezaiifar, O. Rippel, I. Sarasua, W. Shen,\nJ. Son, C. Wachinger, L. Wang, Y. Wang, Y. Xia, D. Xu, Z. Xu, Y. Zheng, A.\nL. Simpson, L. Maier-Hein, M.J. Cardoso, The medical segmentation Decathlon,\nNat. Commun. 13 (2022) 4128.\n[43] Y. Hou, J. Bao, Y. Song, M.L. Bao, K.W. Jiang, J. Zhang, G. Yang, C.H. Hu, H.B. Shi,\nX.M. Wang, Y.D. Zhang, Integration of clinicopathologic identification and deep\ntransferrable image feature representation improves predictions of lymph node\nmetastasis in prostate cancer, EBioMedicine 68 (2021) 103395.\n[44] A. Kirti, F.Z. Simnani, S. Jena, S.S. Lenka, C. Kalalpitiya, S.S. Naser, D. Singh,\nA. Choudhury, R.N. Sahu, A. Yadav, A. Sinha, A. Nandi, P.K. Panda, N.K. Kaushik,\nM. Suar, S.K. Verma, Nanoparticle-mediated metronomic chemotherapy in cancer:\na paradigm of precision and persistence, Cancer Lett. 594 (2024) 216990.\n[45] H. Wang, C. Luo, F. Zhang, J. Guan, S. Li, H. Yao, J. Chen, J. Luo, L. Chen, Y. Guo,\nMultiparametric MRI for bladder cancer: validation of VI-RADS for the detection of\ndetrusor muscle invasion, Radiology 291 (2019) 668â€“674.\n[46] M.I. Metwally, N.A. Zeed, E.M. Hamed, A.S.F. Elshetry, R.M. Elfwakhry, A.M. Alaa\nEldin, A. Sakr, S.A. Aly, W. Mosallam, Y.M.A. Ziada, R. Balata, O.A. Harb, M.A.\nA. Basha, The validity, reliability, and reviewer acceptance of VI-RADS in assessing\nmuscle invasion by bladder cancer: a multicenter prospective study, Eur. Radiol. 31\n(2021) 6949â€“6961.\n[47] Y. Ueno, T. Tamada, M. Takeuchi, K. Sofue, S. Takahashi, Y. Kamishima, Y. Urase,\nA. Kido, N. Hinata, K. Harada, M. Fujisawa, Y. Miyaji, T. Murakami, VI-RADS:\nmultiinstitutional multireader diagnostic accuracy and interobserver agreement\nstudy, AJR Am. J. Roentgenol. 216 (2021) 1257â€“1266.\n[48] J. Li, K. Cao, H. Lin, L. Deng, S. Yang, Y. Gao, M. Liang, C. Lin, W. Zhang, C. Xie,\nK. Zhang, J. Luo, Z. Pan, P. Yue, Y. Zou, B. Huang, Predicting muscle invasion in\nbladder cancer by deep learning analysis of MRI: comparison with vesical imaging-\nreporting and data system, Eur. Radiol. 33 (2023) 2699â€“2709.\n[49] S. Yajima, S. Yoshida, T. Takahara, Y. Arita, H. Tanaka, Y. Waseda, M. Yokoyama,\nJ. Ishioka, Y. Matsuoka, K. Saito, K. Kihara, Y. Fujii, Usefulness of the inchworm\nsign on DWI for predicting pT1 bladder cancer progression, Eur. Radiol. 29 (2019)\n3881â€“3888\n.\n[50] M. Takeuchi, S. Sasaki, M. Ito, S. Okada, S. Takahashi, T. Kawai, K. Suzuki,\nH. Oshima, M. Hara, Y. Shibamoto, Urinary bladder cancer: diffusion-weighted MR\nimagingâ€“accuracy for diagnosing T stage and estimating histologic grade,\nRadiology 251 (2009) 112â€“121.\nY. Gong et al. \nCancer Letters 611 (2025) 217438\n10",
    "version": "5.3.31"
  },
  {
    "numpages": 10,
    "numrender": 10,
    "info": {
      "PDFFormatVersion": "1.7",
      "Language": "en-US",
      "EncryptFilterName": null,
      "IsLinearized": true,
      "IsAcroFormPresent": false,
      "IsXFAPresent": false,
      "IsCollectionPresent": false,
      "IsSignaturesPresent": false,
      "Creator": "Elsevier",
      "Custom": {
        "CrossMarkDomains[1]": "elsevier.com",
        "CrossmarkMajorVersionDate": "2010-04-23",
        "CreationDate--Text": "23rd January 2026",
        "ElsevierWebPDFSpecifications": "7.0.1",
        "robots": "noindex",
        "doi": "10.1016/j.neunet.2025.108329",
        "CrossMarkDomains[2]": "sciencedirect.com",
        "CrossmarkDomainExclusive": "true"
      },
      "ModDate": "D:20260123085509Z",
      "Author": "Wenzong Li",
      "Title": "Weakly semi-supervised cardiac MRI segmentation with frequency-domain pseudo label dynamic mixed supervision and partial-dice constraints",
      "Keywords": "Cardiac mri segmentation,Weakly semi-supervised learning,Pseudo-label,Partial dice loss",
      "CreationDate": "D:20260123081111Z",
      "Producer": "Acrobat Distiller 8.1.0 (Windows)",
      "Subject": "Neural Networks, 196 (2026) 108329. doi:10.1016/j.neunet.2025.108329"
    },
    "metadata": {
      "crossmark:crossmarkdomains": "elsevier.comsciencedirect.com",
      "crossmark:crossmarkdomainexclusive": "true",
      "crossmark:doi": "10.1016/j.neunet.2025.108329",
      "crossmark:majorversiondate": "2010-04-23",
      "dc:format": "application/pdf",
      "dc:identifier": "10.1016/j.neunet.2025.108329",
      "dc:publisher": "Elsevier Ltd",
      "dc:description": "Neural Networks, 196 (2026) 108329. doi:10.1016/j.neunet.2025.108329",
      "dc:subject": [
        "Cardiac mri segmentation",
        "Weakly semi-supervised learning",
        "Pseudo-label",
        "Partial dice loss"
      ],
      "dc:title": "Weakly semi-supervised cardiac MRI segmentation with frequency-domain pseudo label dynamic mixed supervision and partial-dice constraints",
      "dc:creator": [
        "Wenzong Li",
        "Hui Liu",
        "Jingcui Qin",
        "Hongdang Zheng",
        "Lin Zhang"
      ],
      "jav:journal_article_version": "VoR",
      "pdf:creationdate--text": "23rd January 2026",
      "pdf:producer": "Acrobat Distiller 8.1.0 (Windows)",
      "pdf:keywords": "Cardiac mri segmentation,Weakly semi-supervised learning,Pseudo-label,Partial dice loss",
      "pdfx:creationdate--text": "23rd January 2026",
      "pdfx:crossmarkdomains": "sciencedirect.comelsevier.com",
      "pdfx:crossmarkdomainexclusive": "true",
      "pdfx:crossmarkmajorversiondate": "2010-04-23",
      "xledun9mrnpf.mdugy9ermsnnytqplt79ndqgogysm9_.nwqnzdqko9eqnmakmmmqodytma": "",
      "pdfx:doi": "10.1016/j.neunet.2025.108329",
      "pdfx:robots": "noindex",
      "prism:aggregationtype": "journal",
      "prism:copyright": "Â© 2025 Elsevier Ltd. All rights are reserved, including those for text and data mining, AI training, and similar technologies.",
      "prism:coverdate": "2026-04-01",
      "prism:coverdisplaydate": "1 April 2026",
      "prism:doi": "10.1016/j.neunet.2025.108329",
      "prism:issn": "0893-6080",
      "prism:pagerange": "108329",
      "prism:publicationname": "Neural Networks",
      "prism:startingpage": "108329",
      "prism:url": "https://doi.org/10.1016/j.neunet.2025.108329",
      "prism:volume": "196",
      "tdm:policy": "https://www.elsevier.com/tdm/tdmrep-policy.json",
      "tdm:reservation": "1",
      "xmp:createdate": "2026-01-23T08:11:11",
      "xmp:creatortool": "Elsevier",
      "xmp:metadatadate": "2026-01-23T08:55:09",
      "xmp:modifydate": "2026-01-23T08:55:09",
      "xmpmm:documentid": "uuid:1b499bed-4ce8-4c0c-b682-0d58baae1cbe",
      "xmpmm:instanceid": "uuid:b6ed5266-03cb-4855-90c0-636283357f42",
      "xmprights:marked": "True"
    },
    "text": "Contents lists available at ScienceDirect\nNeural Networks\njournal homepage: www.elsevier.com/locate/neunet\nFull Length Article\nWeakly semi-supervised cardiac MRI segmentation with frequency-domain\npseudo label dynamic mixed supervision and partial-dice constraints\nWenzong Li \na\n, Hui Liu \na\n, Jingcui Qin \nb\n, Hongdang Zheng \na\n, Lin Zhang \na,âˆ—\na \nSchool of Information and Control Engineering, China University of Mining and Technology, Xuzhou, 221116, Jiangsu, China\nb \nDepartment of Geriatrics, Xuzhou No.1 Peopleâ€™s Hospital, Xuzhou, 221000, Jiangsu, China\na r t i c l e i n f o\nKeywords:\nCardiac mri segmentation\nWeakly semi-supervised learning\nPseudo-label\nPartial dice loss\na b s t r a c t\nMagnetic Resonance Imaging (MRI) of the heart provides precise visualization of cardiac anatomical structures,\nmaking accurate automatic segmentation highly valuable for assisting in the diagnosis of cardiovascular diseases.\nNumerous deep learning-based segmentation methods have been proposed to date. While these approaches have\nyielded promising results, most rely heavily on large-scale datasets with meticulously annotated labelsâ€“a pro-\ncess that is both labor-intensive and time-consuming. To address this limitation, we introduce a novel weakly\nsemi-supervised segmentation framework, termed PDFMSeg. It requires only a small amount of sparsely labeled\ndata along with a large volume of unlabeled data for training, significantly reducing the dependency on finely\nannotated datasets. Specifically, we designed the Frequency-Domain Pseudo-Label Dynamic Mixed Supervision\n(FPLMS) and Partial Dice Loss (ğ¿\nğ‘ğ·ğ‘–ğ‘ğ‘’\n). The FPLMS enhances boundary constraint capability by dynamically\nblending high- and low-frequency components from pseudo-labels derived from diverse sources. Meanwhile,\nğ¿\nğ‘ğ·ğ‘–ğ‘ğ‘’ \nenriches the form of scribble supervision by integrating Dice loss with an ignore mask and the log-cosh\nfunction, further improving segmentation performance. Experimental results demonstrate that PDFMSeg outper-\nforms several state-of-the-art weakly semi-supervised methods. At a 10 % annotation ratio, PDFMSeg achieved the\nbest performance on the public cardiac MRI datasets ACDC and MSCMR, with the following average scores: ğ·ğ‘†ğ¶\n(83.57 % and 74.29 %), ğ½ ğ¶ (71.81 % and 59.09 %), and ğ»ğ·95 (13.29 and 31.66). Moreover, the model requires\nonly 1.82M parameters and 2.32G FLOPs. These results confirm the effectiveness of PDFMSeg and underscore\nits potential for clinical application. Code and models are available at: https://github.com/labiip/PDFMSeg.\n1. Introduction\nCardiovascular diseases (CVDs), identified by the World Health Or-\nganization as the leading cause of death globally, pose a significant\nthreat to public health. Epidemiological data consistently indicate that\nCVDs not only substantially diminish the quality of life for patients\nbut also impose a heavy burden on healthcare systems and the econ-\nomy. Consequently, the development of precise and efficient diagnos-\ntic methods for heart disease has become both a critical clinical need\nand a social imperative. Cardiac magnetic resonance imaging (MRI) has\nemerged as a key diagnostic tool in this context. It offers clear imaging,\nhigh-resolution soft tissue visualization, and the significant advantage\nof being free from ionizing radiation. This modality enables a detailed,\nnon-invasive assessment of the heartâ€™s anatomical structure and func-\ntional status, thereby playing a pivotal role in the diagnosis, treatment,\nand prognostic monitoring of CVDs (Chen et al., 2025b). The accurate\ndiagnosis and effective treatment of CVDs hinge on the precise mea-\nâˆ— \nCorresponding author.\nE-mail addresses: wenzong.li@cumt.edu.cn (W. Li), hui.liu@cumt.edu.cn (H. Liu), qjc1911101@163.com (J. Qin), hongdang.zheng@cumt.edu.cn (H. Zheng),\nlin.zhang@cumt.edu.cn (L. Zhang).\nsurement of quantitative cardiac parameters. This demand has led to an\nincreased need for advanced segmentation of cardiac MRI scans. Tradi-\ntional manual segmentation methods, however, are time-consuming and\nlabor-intensive, rendering them impractical for rapid clinical decision-\nmaking. As a result, the development of efficient automated segmenta-\ntion techniques for cardiac MRI has become an urgent challenge in this\nfield. In recent years, deep learning technologies have demonstrated ex-\nceptional capabilities in feature extraction and relationship modeling\nacross various image processing tasks (Chen et al., 2022, 2025a). These\nadvancements offer promising solutions to address the challenges as-\nsociated with cardiac MRI segmentation. By leveraging the power of\ndeep learning, there is potential to enhance diagnostic precision, stream-\nline clinical workflows, and ultimately improve patient outcomes in the\nmanagement of cardiovascular diseases.\nBased on the annotation requirements of the model training set,\ndeep learning-based image segmentation methods can be categorized\ninto fully supervised (Liu et al., 2025; Wu et al., 2025), unsupervised\nhttps://doi.org/10.1016/j.neunet.2025.108329\nReceived 12 April 2025; Received in revised form 2 September 2025; Accepted 13 November 2025\nNeural Networks 196 (2026) 108329\nAvailable online 23 November 2025\n0893-6080/Â© 2025 Elsevier Ltd. All rights are reserved, including those for text and data mining, AI training, and similar technologies.\n\nW. Li et al.\nFig. 1. Performance of different segmentation paradigms under 10 % scribble\nsupervision on the ACDC (a) and MSCMR (b) validation sets.\n(Zhang et al., 2025b), and weakly semi-supervised approaches (Liu\net al., 2024b). Fully supervised methods require pixel-by-pixel annota-\ntions of the training set, which can be expensive and labor-intensive. Un-\nsupervised methods, while eliminating the need for labeled data, gener-\nally result in unsatisfying segmentation performance. The weakly semi-\nsupervised method offers a compromise, utilizing a small number of\nroughly labeled samples and a larger set of unlabeled samples to achieve\nsatisfactory segmentation. It has gained significant attention in medical\nimage segmentation, where data annotation is costly and requires expert\nknowledge. Consequently, a growing number of researchers are focusing\non developing weakly semi-supervised image segmentation techniques.\nCurrent research in weakly semi-supervised segmentation primarily\nfocuses on developing effective supervision methods, using a small num-\nber of rough labels to supervise the segmentation of a large number of\nunlabeled pixels. These methods generally fall into two categories: pixel-\nlevel supervision and scribble supervision. Pixel-level supervision is typ-\nically implemented using pseudo labels, which can be generated through\ntwo common approaches: 1) Superpixel-based generation. The approach\ninvolves selecting superpixel regions traversed by scribble markers as\npseudo labels. For instance, Li et al. (2024) proposed a method that\ncombines the simple linear iterative clustering algorithm with scribble\nmarkers to generate pseudo labels. However, it tends to produce de-\nfective labels, resulting in incomplete supervision. 2) Model-predicted\npseudo labels. It leverages model predictions as source pseudo labels.\nA multi-branch architecture is often employed, where each branch gen-\nerates its own source pseudo labels, which are subsequently combined\nto produce high-quality pseudo labels. Common mixing strategies in-\nclude entropy-based methods and random coefficient-based methods.\nThe entropy-based approach typically employs a â€™high entropy, low co-\nefficientâ€™ strategy to mix source pseudo labels. For example, Wang et al.\n(2023) utilized weighted entropy to guide the mixing of source pseudo\nlabels from dual branches. Although effective, it may introduce segmen-\ntation noise during the early stages of training, potentially hindering the\nmodelâ€™s learning ability. In contrast, the random coefficient-based ap-\nproach assigns random weights to mixed source pseudo labels (PLMS)\n(Zhang et al., 2025a). Although effective, the PLMS method exhibits\nlimitations in handling high-frequency noise present in source pseudo-\nlabels.\nTo facilitate a clearer explanation, we employ the ğ·ğ‘†ğ¶ and ğ»ğ·95\nmetrics to evaluate regional and boundary segmentation quality, re-\nspectively. According to wavelet transform theory, the accuracy of\nlow-frequency and high-frequency components in a pseudo-label corre-\nsponds to its regional and boundary segmentation performance. As illus-\ntrated in Fig. 1, source pseudo-labels generated by the CNN paradigm\ndemonstrate accurate low-frequency information but exhibit deficien-\nFig. 2. Examples of different pseudo-label fusion methods. PLMS represents a\nrepresentative approach and FPLMS denotes the method proposed in this work.\nA tick (\nâˆš\n) indicates a relatively accurate result, while a cross (Ã—) denotes an\ninaccurate or unreliable one.\ncies in high-frequency components. In contrast, those produced by the\nViT paradigm achieve high accuracy in both low- and high-frequency\naspects. Fig. 2 provides a visual comparison of different pseudo-label\nfusion strategies. It can be observed that the pseudo-labels generated\nby PLMS contain inaccurate high-frequency information, reflecting sub-\noptimal boundary segmentation capability. This issue stems from the\nneed to sustain model parameter updates: the pseudo-label generation\nprocess should avoid prolonged dominance by any single source. How-\never, since PLMS conducts blending solely in pixel space, it struggles to\npreserve high-frequency accuracy while facilitating parameter updates.\nTherefore, it is imperative to develop an effective fusion strategy to over-\ncome this challenge. Scribble supervision, on the other hand, is based\non a specific loss function that directly uses the limited number of cor-\nrectly classified pixels in scribble labels for supervision, enhancing the\nmodelâ€™s segmentation ability. However, the existing scribble supervision\nloss function is generally limited to partial cross-entropy loss, which may\nnot fully capture the potential of scribble labels. Hence, there is a need to\ndesign more effective loss functions tailored to scribble labels, diversify\nthe forms of scribble supervision, and ultimately improve segmentation\nperformance.\nIn summary, existing methods continue to encounter limitations\nin pseudo-label mixing strategies, particularly when addressing source\npseudo-labels derived from different segmentation paradigms. It re-\nmains a challenge to simultaneously tackle both parameter updates and\nboundary supervision effectively. Furthermore, the design of loss func-\ntions for scribble supervision necessitates further refinement. To address\nthese challenges, we propose a weakly semi-supervised segmentation al-\ngorithm that leverages frequency-domain pseudo-label dynamic mixed\nsupervision alongside partial Dice loss constraints. By employing the\nHaar wavelet transform to decompose the source pseudo labels and pro-\ncess their high-frequency and low-frequency components separately, the\nproposed algorithm maintains the constraint capability of each source\npseudo label while optimizing supervisory effectiveness. Additionally,\nthe form of scribble supervision is significantly enhanced through the\nintroduction of a partial Dice loss function. The key contributions are as\nfollows:\n1) Frequency-Domain Pseudo Label Dynamic Mixed Supervision\n(FPLMS): A novel strategy is developed to address the issue of in-\nadequate boundary supervision caused by differences in boundary\nconstraints across various source pseudo-labels. By applying the Haar\nwavelet transform to separate the high-frequency and low-frequency\ncomponents of source pseudo labels, the method reduces excess high-\nfrequency components, ensuring better pixel-level shape constraints\nand enhancing region supervision.\n2) Partial Dice Loss (ğ¿\nğ‘ğ·ğ‘–ğ‘ğ‘’\n): A new partial Dice loss is proposed to\naddress the inadequacies of current scribble supervision loss. This\napproach removes irrelevant background pixel interference by de-\nsigning an ignore mask and uses the log cosh function to reduce loss\noscillation caused by diminishing attention pixels, thus enriching the\nscribble supervision loss.\nNeural Networks 196 (2026) 108329\n2\n\nW. Li et al.\n3) Experiments on two publicly available cardiac MRI datasets, ACDC\nand MSCMR, demonstrate that the proposed PDFMSeg outperforms\nexisting weakly semi-supervised methods, achieving superior seg-\nmentation performance with comparable model parameters and\nfloating-point computational complexity.\nThe remainder of this article is organized as follows. The PDFMSeg\nis introduced in Section 2. Section 3 describes the datasets, implemen-\ntation details, evaluation metrics, experiment results, and analysis. Fur-\nther analysis of the experiment is presented in Section 4. Finally, the\nconclusion is given in Section 5.\n2. Methods\nThe proposed PDFMSeg is built on a four-branch CNN-ViT struc-\nture and incorporates two key components: the FPLMS strategy and\nthe ğ¿\nğ‘ğ·ğ‘–ğ‘ğ‘’ \nmethod. FPLMS ensures pixel-level shape constraints and re-\ngion supervision by independently mixing the high-frequency and low-\nfrequency components of the source pseudo labels. Meanwhile, ğ¿\nğ‘ğ·ğ‘–ğ‘ğ‘’\nremoves interference from unmarked background pixels, making scrib-\nble information directly usable for model supervision. The overall struc-\nture of PDFMSeg is illustrated in Fig. 3.\n2.1. Four-branch CNN-ViT structure\nWithin a weakly semi-supervised setting, a reduced number of an-\nnotated pixels often compromises the modelâ€™s ability to capture com-\nplex patterns. To mitigate this issue, we introduce consistency learn-\ning techniques inspired by semi-supervised learning paradigms. Specifi-\ncally, the PDFMSeg framework incorporates multiple consistency learn-\ning strategiesâ€“encompassing both intra-architecture consistency (e.g.,\nViT and ViT_E) and inter-architecture consistency (e.g., CNN and ViT).\nThese strategies collectively strengthen the modelâ€™s ability to learn ro-\nbust representations, forming the basis of the multi-branch structure\nproposed in this work. Specifically, the ViT and ViT_E branches are\nresponsible for capturing global features, while the CNN and CNN_A\nbranches focus on extracting local features. The combination of these\nbranches enhances the modeling and representation capacity of PDFM-\nSeg. Furthermore, the CNN and ViT branches jointly supply raw input\nFig. 3. Overall block diagram of PDFMSeg. This model is built on a CNN-ViT\nfour-branch structure and primarily consists of the FPLMS module and the ğ¿\nğ‘ğ·ğ‘–ğ‘ğ‘’\nfunction. The CNN_A branch shares parameters with the CNN branch, while the\nViT_E branchâ€™s parameters are updated by the ViT branch using the EMA strat-\negy. The FPLMS module ensures pseudo-label shape constraints and region su-\npervision, while the ğ¿\nğ‘ğ·ğ‘–ğ‘ğ‘’ \nfunction enables the direct use of scribble information\nfor model supervision. Gaussian noise is added to the original image, serving as\ninput for the ViT branch.\nFig. 4. Pseudo-label frequency-domain dynamic hybrid supervision strategy.\nfor FPLMS, ensuring the generation of high-quality supervision signals.\nThe ViT_E branch, which is based on unlabeled data, provides auxil-\niary supervision for both the CNN and ViT branches. Finally, the CNN_A\nbranch boosts the robustness of PDFMSeg by learning from augmented\ndata.\n2.2. Frequency-domain pseudo label dynamic mixed supervision\nTo compensate for the limitation that scribble annotations only offer\nsparse supervision, we consider introducing a representative pseudo-\nlabel supervision methodâ€“PLMS. This approach provides region-level\nsupervision and facilitates parameter updates through dynamic mix-\ning of predictions from different branches (Zhang et al., 2025a).\nHowever, this method tends to overlook the distinct high-frequency\ncharacteristics present in the predictions, making it difficult to en-\nsure effective boundary supervision. To address this, we introduce\nthe Haar wavelet transform, which decomposes the predictions from\nthe CNN and ViT branches (denoted as ğ‘„\nğ‘–ğ‘ \n= {ğ‘„\nğ‘™\nğ‘–ğ‘ \n, ğ‘„\nğ‘¢\nğ‘–ğ‘ \n} and ğ‘„\nğ‘–ğ‘£ \n=\n{ğ‘„\nğ‘™\nğ‘–ğ‘£\n, ğ‘„\nğ‘¢\nğ‘–ğ‘£\n}) into low frequency (ğ¿ğ¿\nğ‘–ğ‘ \n, ğ¿ğ¿\nğ‘–ğ‘£\n) and high frequency (\n{{ğ¿ğ»\nğ‘–ğ‘ \n, ğ»ğ¿\nğ‘–ğ‘ \n, ğ»ğ»\nğ‘–ğ‘ \n}, {ğ¿ğ»\nğ‘–ğ‘£\n, ğ»ğ¿\nğ‘–ğ‘£\n, ğ»ğ»\nğ‘–ğ‘£\n}}) components, respectively.\nThese components are then mixed separately using distinct dynamic pa-\nrameters (r\n1\n, r\n2\n), and the pseudo labels ğ‘ƒ ğ¿\n1\nğ‘– \nare restored through the\nHaar wavelet inverse transform, as shown in Fig. 4. By independently\nhandling the mixing of low-frequency and high-frequency components,\nthis strategy effectively enhances both the shape constraints and region\nsupervision capabilities of the pseudo labels. The supervision of ğ‘„\nğ‘–ğ‘ \nand\nğ‘„\nğ‘–ğ‘£ \nby ğ‘ƒ ğ¿\n1\nğ‘– \nis carried out through the loss ğ¿\nğ‘“ ğ‘ğ‘™ğ‘šğ‘ \n.\nğ¿\nğ‘“ ğ‘ğ‘™ğ‘šğ‘  \n= ğ¿\nğ·ğ‘–ğ‘ğ‘’\n(ğ‘ƒ ğ¿\n1\nğ‘– \n, ğ‘„\nğ‘–ğ‘ \n) + ğ¿\nğ·ğ‘–ğ‘ğ‘’\n(ğ‘ƒ ğ¿\n1\nğ‘– \n, ğ‘„\nğ‘–ğ‘£\n), (1)\nwhere, ğ¿\nğ·ğ‘–ğ‘ğ‘’ \nrepresents the Dice loss.\n2.3. Design of partial dice loss\nFurther analysis reveals that when supervised information is shifted\nfrom pixel-by-pixel labels to scribble labels, traditional Dice loss can\nbe significantly influenced by background pixels, which interferes with\nconstraining the target category. To address this issue, we introduce a\npartial Dice loss function, ğ¿\nğ‘ğ·ğ‘–ğ‘ğ‘’\n, specifically designed for scribble la-\nbels. An ignore mask ğ‘š is employed to retain only the predictions that\ncorrespond to the positions of the scribble labels, enabling the ğ¿\nğ‘ğ·ğ‘–ğ‘ğ‘’\nloss function to concentrate more effectively on the target area, thereby\nminimizing interference from background pixels. Additionally, the log\ncosh function is used to optimize the loss, ensuring that the loss value\nis minimized during gradient descent, thus reducing Dice loss oscil-\nlations caused by a decrease in the predicted pixels involved in the\nloss calculation. The specific calculation process of ğ¿\nğ‘ğ·ğ‘–ğ‘ğ‘’ \nis shown in\nAlgorithm 1.\nNeural Networks 196 (2026) 108329\n3\n\nW. Li et al.\nAlgorithm 1 Algorithm of ğ¿\nğ‘ğ·ğ‘–ğ‘ğ‘’\n.\nInput: ğ‘¥ as prediction, ğ‘¦ as scribble, ğ‘› as the number of classes.\nOutput: ğ¿\nğ‘ğ·ğ‘–ğ‘ğ‘’\n1: ğ‘š = ğ½\nğ‘™ğ‘’ğ‘›ğ‘”ğ‘¡â„(ğ‘¦[âˆ¶,1]),ğ‘™ğ‘’ğ‘›ğ‘”ğ‘¡â„(ğ‘¦[1,âˆ¶])\n2: ğ‘š[ğ‘¦ == ğ‘›] = 0\n3: ğ‘¦ = ğ‘œğ‘›ğ‘’â„ğ‘œğ‘¡(ğ‘¦)\n4: ğ‘ ğ‘šğ‘œğ‘œğ‘¡â„ = ğ‘’\nâˆ’10\n5: ğ¿ğ‘œğ‘ ğ‘  = 0.0\n6: if ğ‘¥ â‰  0 then\n7: for ğ‘– in ğ‘Ÿğ‘ğ‘›ğ‘”ğ‘’(0, ğ‘›) do\n8: ğ‘–ğ‘›ğ‘¡ğ‘’ğ‘Ÿğ‘ ğ‘’ğ‘ğ‘¡ = ğ‘ ğ‘¢ğ‘š\n(\nğ‘¥\n[âˆ¶\n, ğ‘–\n] \nâŠ™ ğ‘¦\n[âˆ¶\n, ğ‘–\n] \nâŠ™ ğ‘š\n)\n9: ğ‘¦_ğ‘ ğ‘¢ğ‘š = ğ‘ ğ‘¢ğ‘š\n(\nğ‘¦\n[âˆ¶\n, ğ‘–\n] \nâŠ™ ğ‘¦\n[âˆ¶\n, ğ‘–\n] \nâŠ™ ğ‘š\n)\n10: ğ‘¥_ğ‘ ğ‘¢ğ‘š = ğ‘ ğ‘¢ğ‘š\n(\nğ‘¥\n[âˆ¶\n, ğ‘–\n] \nâŠ™ ğ‘¥\n[âˆ¶\n, ğ‘–\n] \nâŠ™ ğ‘š\n)\n11: ğ‘‘ğ‘–ğ‘ğ‘’ = \n2Ã—ğ‘–ğ‘›ğ‘¡ğ‘’ğ‘Ÿğ‘ ğ‘’ğ‘ğ‘¡+ğ‘ ğ‘šğ‘œğ‘œğ‘¡â„\nğ‘¥_ğ‘ ğ‘¢ğ‘š+ğ‘¦_ğ‘ ğ‘¢ğ‘š+ğ‘ ğ‘šğ‘œğ‘œğ‘¡â„\n12: ğ‘™ğ‘œğ‘ ğ‘  = 1 âˆ’ ğ‘‘ğ‘–ğ‘ğ‘’\n13: ğ¿ğ‘œğ‘ ğ‘ + = ğ‘™ğ‘œğ‘ ğ‘ \n14: end for\n15: else\n16: ğ¿ğ‘œğ‘ ğ‘  = 0.0\n17: end if\n18: ğ¿\nğ‘ğ·ğ‘–ğ‘ğ‘’ \n= ğ‘™ğ‘œğ‘”ğ‘ğ‘œğ‘ â„(ğ¿ğ‘œğ‘ ğ‘ âˆ•ğ‘›)\n19: return Output\n2.4. Cost function\nThe PDFMSeg cost function, ğ¿\nğ‘¡ğ‘œğ‘¡ğ‘ğ‘™ \n, is composed of six components:\nğ¿\nğ‘ ğ‘ğ‘Ÿğ‘–ğ‘ğ‘ğ‘™ğ‘’\n, ğ¿\nğ‘ğ‘Ÿğ‘œğ‘ ğ‘ \n, ğ¿\nğ‘ğ‘¢ğ‘¥\n, ğ¿\nğ‘“ ğ‘ğ‘™ğ‘šğ‘ \n, ğ¿\nğ‘ğ‘¢ğ‘” \n, and ğ¿\nğ‘”ğ‘ğ‘Ÿğ‘“ \n. Among these, ğ¿\nğ‘ ğ‘ğ‘Ÿğ‘–ğ‘ğ‘ğ‘™ğ‘’ \nsu-\npervises the CNN and ViT branches through scribble label.\nğ¿\nğ‘ ğ‘ğ‘Ÿğ‘–ğ‘ğ‘ğ‘™ğ‘’ \n= ğ¿\nğ‘ğ¶ğ¸ \n(ğ‘¦\nğ‘™\nğ‘– \n, ğ‘„\nğ‘™\nğ‘–ğ‘ \n) + ğ¿\nğ‘ğ·ğ‘–ğ‘ğ‘’\n(ğ‘¦\nğ‘™\nğ‘– \n, ğ‘„\nğ‘™\nğ‘–ğ‘ \n) + ğ¿\nğ‘ğ¶ğ¸ \n(ğ‘¦\nğ‘™\nğ‘– \n, ğ‘„\nğ‘™\nğ‘–ğ‘£\n) + ğ¿\nğ‘ğ·ğ‘–ğ‘ğ‘’\n(ğ‘¦\nğ‘™\nğ‘– \n, ğ‘„\nğ‘™\nğ‘–ğ‘£\n),\n(2)\nwhere, ğ¿\nğ‘ğ¶ğ¸ \nrepresents the partial cross-entropy loss. ğ‘„\nğ‘™\nğ‘–ğ‘ \nand ğ‘„\nğ‘™\nğ‘–ğ‘£ \nde-\nnote the predictions obtained from the CNN and ViT branches for the\nlabeled data ğ‘¥\nğ‘™\nğ‘– \n, respectively, while ğ‘¦\nğ‘™\nğ‘– \nrepresents the scribble label cor-\nresponding to ğ‘¥\nğ‘™\nğ‘– \n. ğ¿\nğ‘ğ‘Ÿğ‘œğ‘ ğ‘  \nenables cross supervision between the CNN and\nViT branches.\nğ¿\nğ‘ğ‘Ÿğ‘œğ‘ ğ‘  \n= ğ¿\nğ·ğ‘–ğ‘ğ‘’\n(ğ‘ğ‘Ÿğ‘”ğ‘šğ‘ğ‘¥(ğ‘„\nğ‘¢\nğ‘–ğ‘£\n), ğ‘„\nğ‘¢\nğ‘–ğ‘ \n) + ğ¿\nğ·ğ‘–ğ‘ğ‘’\n(ğ‘ğ‘Ÿğ‘”ğ‘šğ‘ğ‘¥(ğ‘„\nğ‘¢\nğ‘–ğ‘ \n), ğ‘„\nğ‘¢\nğ‘–ğ‘£\n), (3)\nwhere, ğ‘„\nğ‘¢\nğ‘–ğ‘ \nand ğ‘„\nğ‘¢\nğ‘–ğ‘£ \nrepresent the predictions of unlabeled data ğ‘¥\nğ‘¢\nğ‘– \nob-\ntained through the CNN and ViT branches, respectively. ğ¿\nğ‘ğ‘¢ğ‘¥ \nimple-\nments auxiliary supervision for the CNN and ViT branches through the\nViT_E branch.\nğ¿\nğ‘ğ‘¢ğ‘¥ \n= ğ¿\nğ‘€ğ‘†ğ¸ \n(ğ‘„\nğ‘¢\nğ‘–ğ‘£\nâ€² \n, ğ‘„\nğ‘¢\nğ‘–ğ‘ \n) + ğ¿\nğ‘€ğ‘†ğ¸ \n(ğ‘„\nğ‘¢\nğ‘–ğ‘£\nâ€² \n, ğ‘„\nğ‘¢\nğ‘–ğ‘£\n), (4)\nwhere, ğ¿\nğ‘€ğ‘†ğ¸ \nrepresents the mean squared error (MSE) loss. ğ‘„\nğ‘¢\nğ‘–ğ‘£\nâ€² \ndenotes\nthe prediction obtained from ğ‘¥\nğ‘¢\nğ‘– \nthrough the ViT_E branch. To enhance\nthe expressive power of the model, a data augmentation strategy was\ndesigned based on CutMix (Yun et al., 2019). Mask matrices ğ‘€ and\n Ì„\n ğ‘€\nof size â„Ã—ğ‘¤ are defined, where the area in ğ‘€ with size ğ›½â„ Ã— ğ›½ğ‘¤ is set\nto a pixel value of 0, while the remaining area is set to a pixel value\nof 1.\n Ì„\n ğ‘€ = 1 âˆ’ ğ‘€. Enhanced samples ğ‘¥\nğ‘ 1\nğ‘– \nare then generated using the\nmatrices ğ‘€ and\n Ì„\n ğ‘€.\nğ‘¥\nğ‘ 1\nğ‘– \n= ğ‘¥\nğ‘™\nğ‘– \nâŠ™ ğ‘€ + ğ‘¥\nğ‘™\nğ‘— \nâŠ™Ì„ ğ‘€, (5)\nwhere, ğ‘¥\nğ‘™\nğ‘— \nrepresents labeled data, and âŠ™ denotes the Hadamard product.\nThe supervision of ğ‘„\nğ‘ 1\nğ‘– \nis as follows:\nğ¿\nğ‘ğ‘¢ğ‘” \n= ğ¿\nğ‘“ ğ‘ 1 \n+ ğ¿\nğ‘ğ‘ 1 \n= ğ¿\nğ‘ğ¶ğ¸ \n(ğ‘¦\nğ‘™\nğ‘– \nâŠ™ ğ‘€, ğ‘„\nğ‘ 1\nğ‘– \nâŠ™ ğ‘€) + ğ¿\nğ‘ğ·ğ‘–ğ‘ğ‘’\n(ğ‘¦\nğ‘™\nğ‘– \nâŠ™ ğ‘€, ğ‘„\nğ‘ 1\nğ‘– \nâŠ™ ğ‘€)\n+ ğ¿\nğ‘ğ¶ğ¸ \n(ğ‘¦\nğ‘™\nğ‘— \nâŠ™Ì„ ğ‘€, ğ‘„\nğ‘ 1\nğ‘– \nâŠ™Ì„ ğ‘€) + ğ¿\nğ‘ğ·ğ‘–ğ‘ğ‘’\n(ğ‘¦\nğ‘™\nğ‘— \nâŠ™Ì„ ğ‘€, ğ‘„\nğ‘ 1\nğ‘– \nâŠ™Ì„ ğ‘€), (6)\nwhere, ğ‘¦\nğ‘™\nğ‘— \nrepresents the scribble label corresponding to ğ‘¥\nğ‘™\nğ‘— \n, and ğ‘„\nğ‘ 1\nğ‘– \nde-\nnotes the prediction obtained from the CNN_A branch of ğ‘¥\nğ‘ 1\nğ‘– \n. To further\nenhance the shape constraint capability of the model, a Gated Condi-\ntional Random Field loss, ğ¿\nâ€²\nğ‘”ğ‘ğ‘Ÿğ‘“ \n(Obukhov et al., 2019), is introduced.\nThis allows the model to focus on context learning, concentrating on se-\nmantic boundaries and effectively addressing the issue of poor boundary\nsegmentation. The CNN branch is supervised using ğ¿\nğ‘”ğ‘ğ‘Ÿğ‘“ \n.\nğ¿\nğ‘”ğ‘ğ‘Ÿğ‘“ \n= ğ¿\nâ€²\nğ‘”ğ‘ğ‘Ÿğ‘“ \n(ğ‘¥\nğ‘¢\nğ‘– \n, ğ‘„\nğ‘¢\nğ‘–ğ‘ \n) + ğ¿\nâ€²\nğ‘”ğ‘ğ‘Ÿğ‘“ \n(ğ‘¥\nğ‘™\nğ‘– \n, ğ‘„\nğ‘™\nğ‘–ğ‘ \n). (7)\nğ¿\nğ‘¡ğ‘œğ‘¡ğ‘ğ‘™ \nis represented as\nğ¿\nğ‘¡ğ‘œğ‘¡ğ‘ğ‘™ \n= ğœ†\n1 \nâ‹… ğ¿\nğ‘ ğ‘ğ‘Ÿğ‘–ğ‘ğ‘ğ‘™ğ‘’ \n+ ğœ†\n2 \nâ‹… ğ¿\nğ‘“ ğ‘ğ‘™ğ‘šğ‘  \n+ ğœ†\n3 \nâ‹… ğ¿\nğ‘ğ‘¢ğ‘”\n+ ğœ†\n4 \nâ‹… ğ¿\nğ‘”ğ‘ğ‘Ÿğ‘“ \n+ ğœ†\n5 \nâ‹… ğ¿\nğ‘ğ‘¢ğ‘¥\nğ‘™ğ‘’ğ‘‘ + ğœ†\n6 \nâ‹… ğ¿\nğ‘ğ‘Ÿğ‘œğ‘ ğ‘ \n, (8)\nwhere, ğœ†\n1 \nto ğœ†\n6 \nare used to balance the contributions from the different\ncomponents of the ğ¿\nğ‘¡ğ‘œğ‘¡ğ‘ğ‘™ \n.\n3. Experiments and results\n3.1. Dataset\n3.1.1. ACDC\nThe ACDC (Bernard et al., 2018) dataset contains MRI scans of 100\nsubjects, with 1902 slices capturing both end-diastolic and end-systolic\nphases. It also includes pixel-level and scribble annotations for the left\nventricle (LV), right ventricle (RV), and myocardium (MYO). Follow-\ning previous work, the dataset is divided into training, validation, and\ntesting sets in a 7:1:2 ratio (Luo et al., 2022a).\n3.1.2. MSCMR\nThe MSCMR (Zhang & Zhuang, 2022) dataset comprises 686 slices of\ngadolinium-enhanced MRI scans from 45 patients with cardiomyopathy.\nAmong these, 25 scan sets include only scribble annotations for the LV,\nRV, and MYO, which are used as the training set. The remaining 20\nscan sets provide both pixel-level and scribble annotations for the LV,\nRV, and MYO, and are objectively split into a validation set and a test\nset in a 1:3 ratio.\n3.2. Implementation details\nThe CNN and CNN_A branches of PDFMSeg use the UNet (Ron-\nneberger et al., 2015) architecture, while the ViT and ViT_E branches\nemploy the Swin-UNet (Cao et al., 2022) structure. The experiment is\nimplemented using the PyTorch framework, running on hardware con-\nsisting of an Intel(R) Xeon(R) W-2175 CPU and an NVIDIA GeForce RTX\n2080Ti GPU. The SGD optimizer is used with parameters set to weight\ndecay = 0.0004 and momentum = 0.9. An exponential decay strat-\negy is applied to adjust the learning rate during training. The initial\nlearning rate, batch size, and total number of iterations are set to 0.01,\n8, and 30000, respectively. In our experiments, hyperparameter tun-\ning primarily involves analyzing the impact of their corresponding loss\nterms, defining appropriate numerical ranges, and optimizing these val-\nues through empirical methods. Considering the proportion of the target\narea to be segmented in the original image, ğ›½ is set to 2/3. r\n1 \nâˆˆ (0, 1) and\nr\n2 \nâˆˆ (0, 1), are updated after every iteration. The hyperparameters ğœ†\n1 \nto\nğœ†\n4 \nobtained through grid search are as follows: ACDC - {0.5, 0.06, 0.025,\n0.02}, MSCMR - {0.5, 0.08, 0.02, 0.01}. Given the low quality of pseudo\nlabels in the initial stages of network training, which can hinder model\nperformance, the weights ğœ†\n5 \nand ğœ†\n6 \ncorresponding loss functions are ad-\njusted using a time-dependent Gaussian warming up function (Luo et al.,\n2022a).\nğœ†\n5\n(ğ‘¡) = ğ‘’\nâˆ’5Ã—(1âˆ’ğ‘¡\nğ‘–\n/\nğ‘¡\nğ‘¡ğ‘œğ‘¡ğ‘ğ‘™ \n)\n2\n; ğœ†\n6\n(ğ‘¡) = 7 Ã— ğœ†\n5\n(ğ‘¡), (9)\nwhere ğ‘¡\nğ‘– \nis the current iteration and ğ‘¡\nğ‘¡ğ‘œğ‘¡ğ‘ğ‘™ \ndenotes the total iteration num-\nber. Considering the critical role of ğ¿\nğ‘ğ‘Ÿğ‘œğ‘ ğ‘  \nin supervising unlabeled data,\nğœ†\n6 \nis assigned a larger value. All data undergo random transformations\nsuch as flipping, cropping, etc., and are scaled to a size of 224 Ã— 224. All\nresults represent the statistical outcomes (mean Â± std) from three exper-\nimental trials, with the top two values selected from each trial.\nNeural Networks 196 (2026) 108329\n4\n\nW. Li et al.\nTable 1\nAblation study of different modules.\nBackbone FPLMS ğ¿\nğ‘ğ·ğ‘–ğ‘ğ‘’\nACDC MSCMR\nğ·ğ‘†ğ¶â†‘(%) ğ½ ğ¶â†‘(%) ğ»ğ·95â†“(mm) ğ·ğ‘†ğ¶â†‘(%) ğ½ ğ¶â†‘(%) ğ»ğ·95â†“(mm)\nâˆš \n69.60 Â± 2.51 53.39 Â± 2.95 62.79 Â± 8.65 63.90 Â± 4.13 46.97 Â± 4.46 80.97 Â± 18.88\nâˆš âˆš \n83.27 Â± 1.02 71.43 Â± 2.96 19.23 Â± 1.49 74.25 Â± 3.09 59.05 Â± 3.91 45.96 Â± 10.34\nâˆš âˆš \n83.10 Â± 1.27 71.18 Â± 1.85 25.62 Â± 2.75 73.98 Â± 2.24 58.79 Â± 2.82 66.60 Â± 7.86\nâˆš âˆš âˆš \n83.57 Â± 0.31 71.81 Â± 0.44 13.29 Â± 2.09 74.29 Â± 1.00 59.09 Â± 1.26 31.66 Â± 3.37\na\nOptimal results are indicated in bold.\n3.3. Evaluation metrics\nThe Dice Similarity Coefficient (ğ·ğ‘†ğ¶), Jaccard Index (ğ½ ğ¶), and 95 %\nHausdorff Distance (ğ»ğ·95) were used to evaluate the segmentation per-\nformance of the model. ğ·ğ‘†ğ¶ and ğ½ ğ¶ are primarily used to assess the\nsegmentation quality of the target area by measuring the overlap be-\ntween the segmented target ğ´ and its ground truth ğµ.\nğ·ğ‘†ğ¶(ğ´, ğµ) = \n2\n|\nğ´ âˆ© ğµ\n|\n|\nğ´\n| \n+ \n|\nğµ\n| \nÃ— 100 %, (10)\nğ½ ğ¶(ğ´, ğµ) = \n|\nğ´ âˆ© ğµ\n|\n|\nğ´ âˆª ğµ\n| \nÃ— 100 %. (11)\nğ»ğ·95 is primarily used to evaluate the segmentation accuracy of the\ntarget boundaries, defined as the distance between the region bound-\nary and the ground truth region boundary. It is calculated as the 95th\npercentile of the Hausdorff distance distribution between ğ´ and ğµ.\n3.4. Results\n3.4.1. Ablation study\nThe effectiveness of different modules. We first remove the FPLMS and\nğ¿\nğ‘ğ·ğ‘–ğ‘ğ‘’ \nmodules from PDFMSeg, using the remaining structure as the\nbackbone network to individually assess the effectiveness of FPLMS and\nğ¿\nğ‘ğ·ğ‘–ğ‘ğ‘’\n. The results, presented in Table 1, demonstrate that the inclusion\nof the FPLMS module and ğ¿\nğ‘ğ·ğ‘–ğ‘ğ‘’ \nsignificantly enhances boundary seg-\nmentation performance while maintaining the accuracy of region seg-\nmentation. This improvement is reflected in the increase in ğ·ğ‘†ğ¶ and\nğ½ ğ¶ scores, as well as the notable decrease in ğ»ğ·95 scores. The per-\nformance boost can be attributed to the following factors: the source\npseudo labels generated by the CNN and ViT branches, which, after\nbeing decomposed and independently mixed using the Haar wavelet\ntransform, effectively suppress excessive high-frequency components.\nThis approach helps preserve boundary constraints and significantly en-\nhances boundary recognition. Additionally, ğ¿\nğ‘ğ·ğ‘–ğ‘ğ‘’ \nmitigates the inter-\nference from background pixels through an ignore mask, enabling the\nmodel to focus more effectively on the target area and improving its\nrecognition capabilities. As a result, the introduction of both FPLMS and\nğ¿\nğ‘ğ·ğ‘–ğ‘ğ‘’ \nmodules, forming a joint supervision strategy, greatly enhances\nthe segmentation performance of PDFMSeg by optimizing pseudo labels\nand improving the loss function.\nThe effectiveness of different mixed strategies. To evaluate the perfor-\nmance differences among various source pseudo-label mixing strategies,\nwe conducted comparative experiments on the Entropy (Wang et al.,\n2023), PLMS (Zhang et al., 2025a), and FPLMS strategies. The results,\npresented in Table 2, reveal important insights. The Entropy strategy ef-\nfectively enhances the supervision of pseudo labels at the regional level\nby dynamically assigning pixel-wise weights based on model prediction\nprobabilities, particularly benefiting difficult samples in the MSCMR\ndataset. However, it is prone to overconfidence when predicting outliers,\nwhich can degrade boundary segmentation accuracy. The PLMS strat-\negy directly blends pseudo-labels in the pixel space, which is straight-\nforward to implement. However, it often fails to eliminate unnecessary\nhigh-frequency noise originating from the source pseudo-labels. Such\nnoise adversely affects the modelâ€™s ability to learn edge features and re-\nsults in blurred boundaries. Our PDFMSeg incorporates a wavelet trans-\nform prior to pseudo-label mixing. Unlike PLMS, our method decom-\nposes the source pseudo-labels into high-frequency and low-frequency\ncomponents and performs mixing independently in the frequency do-\nmain. By dynamically selecting and blending high- and low-frequency\ncomponents from different source pseudo-labels in adaptive proportions\nduring training, this approach effectively reduces the introduction of\nhigh-frequency noise and mitigates its negative impact. As quantita-\ntively demonstrated in Table 2, FPLMS outperforms PLMS in terms of\nthe ğ»ğ·95 metric on both the ACDC and MSCMR datasets, confirming\nthe effectiveness of the proposed strategy.\nWe further investigated the impact of different wavelet basesâ€“\nincluding Haar, Daubechies, Coiflets, and Symletsâ€“on experimental per-\nformance. The results, summarized in Table 3, indicate that the Haar\nwavelet basis yields the best performance. This is mainly because the\nHaar wavelet demonstrates a strong ability to capture edge informa-\ntion in images, which plays a critical role in target recognition tasks.\nIn contrast, although more advanced wavelet bases such as Daubechies,\nCoiflets, and Symlets also possess certain edge detection capabilities,\nthey are generally more suitable for identifying specific frequency\nanomalies rather than optimizing edge representation in image seg-\nmentation. Whatâ€™s more, the Haar wavelet offers notable computational\nefficiency. Its implementation involves only addition and shift opera-\ntions, making it hardware-friendly and highly efficient. On the other\nhand, wavelets like Daubechies, Coiflets, and Symlets require floating-\npoint multiplication and addition operations, leading to significantly\nhigher computational costs and memory access overhead. Based on\nthese advantages in both representational capacity and computational\nefficiency, we selected the Haar wavelet for our model design.\nTo further assess the effectiveness of the FPLMS strategy, we visu-\nally compared the high-frequency components in the pseudo labels gen-\nerated by PLMS and FPLMS, as shown in Fig. 5. Compared to PLMS,\nTable 2\nAblation study of mixed strategy.\nStrategy \nACDC MSCMR\nğ·ğ‘†ğ¶â†‘(%) ğ½ ğ¶â†‘(%) ğ»ğ·95â†“(mm) ğ·ğ‘†ğ¶â†‘(%) ğ½ ğ¶â†‘(%) ğ»ğ·95â†“(mm)\nEntropy 82.86 Â± 1.41 70.78 Â± 2.05 16.08 Â± 2.35 74.59 Â± 2.57 59.48 Â± 3.27 59.86 Â± 8.44\nPLMS 82.74 Â± 1.58 70.65 Â± 2.29 18.14 Â± 2.24 74.15 Â± 1.20 58.92 Â± 1.51 50.44 Â± 3.58\nFPLMS 83.57 Â± 0.31 71.81 Â± 0.44 13.29 Â± 2.09 74.29 Â± 1.00 59.09 Â± 1.26 31.66 Â± 3.37\nb\nOptimal results are indicated in bold.\nNeural Networks 196 (2026) 108329\n5\n\nW. Li et al.\nTable 3\nAblation study of different wavelet bases in FPLMS.\nWavelet \nACDC MSCMR\nğ·ğ‘†ğ¶â†‘(%) ğ½ ğ¶â†‘(%) ğ»ğ·95â†“(mm) ğ·ğ‘†ğ¶â†‘(%) ğ½ ğ¶â†‘(%) ğ»ğ·95â†“(mm)\ndb1 82.55 Â± 0.37 70.28 Â± 0.53 16.47 Â± 5.21 74.73 Â± 2.47 59.69 Â± 3.11 55.22 Â± 26.49\ndb2 78.09 Â± 0.91 64.06 Â± 1.23 35.69 Â± 7.15 66.14 Â± 2.39 49.44 Â± 2.67 79.14 Â± 37.55\ndb4 79.41 Â± 0.29 65.83 Â± 0.40 23.81 Â± 4.18 57.83 Â± 6.00 40.84 Â± 5.90 101.2 Â± 35.88\ncoif1 78.40 Â± 0.37 64.47 Â± 0.51 25.87 Â± 9.90 66.56 Â± 4.95 50.01 Â± 5.47 93.72 Â± 5.35\nsym2 78.54 Â± 0.19 64.66 Â± 0.26 30.95 Â± 6.57 66.47 Â± 4.34 49.89 Â± 4.86 83.39 Â± 20.81\nhaar 83.57 Â± 0.31 71.81 Â± 0.44 13.29 Â± 2.09 74.29 Â± 1.00 59.09 Â± 1.26 31.66 Â± 3.37\nc\nOptimal results are indicated in bold.\nFig. 5. The high-frequency distribution maps of pseudo labels obtained from\nPLMS and FPLMS, where LH, HL, and HH represent the horizontal high fre-\nquency, vertical high frequency, and diagonal high frequency of the image, re-\nspectively.\nthe FPLMS strategy effectively suppresses excess high-frequency compo-\nnents in the pseudo labels, and their distribution is closer to the ground\ntruth (GT). This suggests that FPLMS can effectively mitigate the high-\nfrequency noise introduced by segmentation false positives, guiding the\nmodel to focus more on the true contours of the segmentation target.\nIt is this optimization of frequency domain characteristics that enables\nFPLMS to provide more accurate boundary supervision signals, leading\nto improved segmentation performance, particularly in terms of bound-\nary accuracy.\nThe effectiveness of different ğ¿\nğ‘ğ·ğ‘–ğ‘ğ‘’ \ncomponent. This section evaluates the\ncontribution of key components in ğ¿\nğ‘ğ·ğ‘–ğ‘ğ‘’\n. As shown in Table 4, the ig-\nnore mask significantly improves the aggregation ability of limited pix-\nels in scribble labels by filtering out background pixel interference, re-\nsulting in excellent region segmentation performance of PDFMSeg on\nthe ACDC dataset. However, on the MSCMR dataset, this strategy led to\na decrease in segmentation accuracy. A closer analysis suggests that this\nphenomenon may be due to the low quality of the MSCMR dataset itself,\nwhere excessive suppression of background pixels hampers the modelâ€™s\nfeature learning ability. The introduction of the log-cosh function effec-\ntively mitigates the loss fluctuations caused by the ignore mask across\ndifferent datasets by constraining the gradient descent behavior of the\nloss function. This function, known for its smooth optimization charac-\nteristics, adaptively balances training stability under different data dis-\ntributions, thereby enhancing the modelâ€™s generalization ability across\ndatasets. Experimental results show that this mechanism effectively pre-\nvents performance degradation on the MSCMR dataset while maintain-\ning the performance advantage on the ACDC dataset, ultimately leading\nto robust segmentation results.\n3.4.2. Comparision with existing methods\nComparision with weakly supervised methods. To evaluate the perfor-\nmance advantages of PDFMSeg, we compared it with two state-of-the-\nart weakly supervised segmentation methods: MAAG (Valvano et al.,\n2021) and CycleMix (Zhang & Zhuang, 2022). Notably, both MAAG\nand CycleMix were trained using full scribble labels in the experiment,\nwhereas PDFMSeg was trained with only 20 % scribble labels. The re-\nsults presented in Table 5 indicate that MAAG exhibits overall segmenta-\ntion advantages on the MSCMR dataset, primarily due to its multi-scale\ngenerative adversarial network architecture. This architecture generates\nhigh-quality pseudo labels for many challenging samples in the dataset,\nthereby providing more accurate supervision signals. In contrast, PDFM-\nSeg achieved the best segmentation performance on the ACDC dataset\nand a suboptimal ğ·ğ‘†ğ¶ score on the MSCMR dataset. These findings sug-\ngest that even with extremely sparse annotations, PDFMSeg can deliver\nsegmentation results that are comparable to, or even surpass, those of\nother weakly supervised methods. This performance can be attributed to\nthe following critical design features: 1) The FPLMS mechanism, which\nenhances boundary segmentation accuracy by focusing on the contours\nof the target to be segmented; and 2) The ğ¿\nğ‘ğ·ğ‘–ğ‘ğ‘’ \nstrategy, which sig-\nnificantly improves the modelâ€™s ability to handle scribble supervision\nand boosts overall segmentation performance. Collectively, these com-\nponents enable PDFMSeg to maintain robust feature learning capabili-\nties under limited supervision conditions.\nComparision with semi-supervised methods. To comprehensively evalu-\nate the performance of PDFMSeg, we compared it with typical semi-\nsupervised segmentation methods, ICT (Verma et al., 2022) and CCT\nTable 4\nAblation study of ğ¿\nğ‘ğ·ğ‘–ğ‘ğ‘’ \ncomponent.\nDice ignore mask log-cosh \nACDC MSCMR\nğ·ğ‘†ğ¶â†‘(%) ğ½ ğ¶â†‘(%) ğ»ğ·95â†“(mm) ğ·ğ‘†ğ¶â†‘(%) ğ½ ğ¶â†‘(%) ğ»ğ·95â†“(mm)\nâˆš \n76.34 Â± 1.65 61.77 Â± 2.16 32.56 Â± 5.61 74.14 Â± 2.52 58.91 Â± 3.18 49.9 Â± 8.08\nâˆš âˆš \n76.29 Â± 3.57 61.76 Â± 4.64 30.39 Â± 6.36 74.24 Â± 1.94 59.03 Â± 2.43 45.65 Â± 8.34\nâˆš âˆš \n78.03 Â± 0.97 63.98 Â± 1.30 28.76 Â± 2.19 73.36 Â± 1.43 57.97 Â± 1.78 48.25 Â± 4.55\nâˆš âˆš âˆš \n83.57 Â± 0.31 71.81 Â± 0.44 13.29 Â± 2.09 74.29 Â± 1.00 59.09 Â± 1.26 31.66 Â± 3.37\nd\nOptimal results are indicated in bold.\nNeural Networks 196 (2026) 108329\n6\n\nW. Li et al.\nTable 5\nComparision with weakly supervised methods.\nMethods Labeled Unlabeled \nACDC MSCMR\nğ·ğ‘†ğ¶â†‘(%) ğ½ ğ¶â†‘(%) ğ»ğ·95â†“(mm) ğ·ğ‘†ğ¶â†‘(%) ğ½ ğ¶â†‘(%) ğ»ğ·95â†“(mm)\nMAAG 100 % 0 % 83.40 Â± 0.74 71.66 Â± 1.08 9.50 Â± 4.12 84.51 Â± 1.09 73.18 Â± 1.63 7.80 Â± 5.70\nCycleMix 100 % 0 % 84.87 Â± 0.80 73.77 Â± 1.21 17.9 Â± 5.97 80.00 Â± 1.06 66.8 Â± 1.47 19.10 Â± 10.65\nPDFMSeg 20 % 80 % 85.35 Â± 0.57 74.45 Â± 0.87 9.18 Â± 2.42 80.99 Â± 0.81 68.01 Â± 1.14 30.56 Â± 2.89\ne\nOptimal results are indicated in bold.\nTable 6\nComparision with semi-supervised methods.\nMethods Labeled Unlabeled \nACDC MSCMR\nğ·ğ‘†ğ¶â†‘(%) ğ½ ğ¶â†‘(%) ğ»ğ·95â†‘(mm) ğ·ğ‘†ğ¶â†‘(%) ğ½ ğ¶â†‘(%) ğ»ğ·95â†“(mm)\nICT \n5 % 95 % 58.15 Â± 1.04 40.99 Â± 1.03 22.80 Â± 3.78 38.42 Â± 1.80 23.78 Â± 1.38 92.36 Â± 4.22\n10 % 90 % 83.62 Â± 0.53 71.85 Â± 0.78 6.15 Â± 2.25 51.22 Â± 1.16 34.46 Â± 1.05 61.88 Â± 3.31\nCCT \n5 % 90 % 58.68 Â± 0.89 41.52 Â± 0.89 27.90 Â± 2.27 20.95 Â± 2.19 11.71 Â± 1.36 64.10 Â± 5.16\n10 % 90 % 85.36 Â± 0.47 74.46 Â± 1.21 6.25 Â± 1.06 47.48 Â± 1.53 31.33 Â± 1.31 32.73 Â± 2.04\nPDFMSeg \n5 % 95 % 79.83 Â± 1.14 66.48 Â± 1.57 14.39 Â± 5.25 73.35 Â± 5.34 57.95 Â± 6.67 37.73 Â± 6.98\n10 % 90 % 83.57 Â± 0.31 71.81 Â± 0.44 13.29 Â± 2.09 74.29 Â± 1.00 59.09 Â± 1.26 31.66 Â± 3.37\nf \nOptimal results are indicated in bold.\nTable 7\nComparison between PDFMSeg and other weakly semi-supervised methods on ACDC dataset.\nMethods \nRatio=5 % Ratio=10 % Ratio=20 %\nğ·ğ‘†ğ¶â†‘(%) ğ½ ğ¶â†‘(%) ğ»ğ·95â†“(mm) ğ·ğ‘†ğ¶â†‘(%) ğ½ ğ¶â†‘(%) ğ»ğ·95â†“(mm) ğ·ğ‘†ğ¶â†‘(%) ğ½ ğ¶â†‘(%) ğ»ğ·95â†“(mm)\nCPS 26.78 Â± 2.73 15.47 Â± 1.82 24.82 Â± 12.64 48.85 Â± 1.18 32.32 Â± 1.03 14.01 Â± 9.64 75.19 Â± 1.07 60.25 Â± 1.37 11.29 Â± 3.23\nURPC 21.73 Â± 1.26 12.19 Â± 0.79 17.99 Â± 8.52 46.73 Â± 1.45 31.25 Â± 1.23 14.91 Â± 8.19 80.84 Â± 0.61 67.85 Â± 0.85 8.97 Â± 7.79\nCBCT 47.37 Â± 1.45 31.05 Â± 1.24 22.6 Â± 16.16 52.83 Â± 1.04 35.91 Â± 0.95 18.10 Â± 6.92 79.43 Â± 0.84 65.88 Â± 1.15 9.31 Â± 3.27\nEVIL 43.10 Â± 2.42 27.47 Â± 1.96 23.24 Â± 13.01 63.42 Â± 1.47 46.44 Â± 1.51 13.77 Â± 11.53 76.01 Â± 0.89 61.31 Â± 1.16 10.73 Â± 4.00\nCLB 43.86 Â± 2.04 28.10 Â± 1.67 17.43 Â± 9.77 62.17 Â± 1.32 45.11 Â± 1.39 16.16 Â± 7.26 72.78 Â± 1.02 57.21 Â± 1.26 12.29 Â± 1.94\nDD-Net 10.06 Â± 1.66 5.30 Â± 0.92 54.05 Â± 6.52 22.65 Â± 1.93 12.78 Â± 1.23 48.69 Â± 4.77 30.83 Â± 0.78 18.22 Â± 0.54 43.96 Â± 3.58\nPDFMSeg 79.83 Â± 1.14 66.48 Â± 1.57 14.39 Â± 5.25 83.57 Â± 0.31 71.81 Â± 0.44 13.29 Â± 2.09 85.35 Â± 0.57 74.45 Â± 0.87 9.18 Â± 2.42\ng\nOptimal results are indicated in bold.\n(Ouali et al., 2020). It is important to note that both ICT and CCT re-\nquire pixel-by-pixel annotations, while PDFMSeg only requires sparse\nscribble annotations. Analyzing the results in Table 6 reveals the limi-\ntations of the ICT and CCT methods, both of which are based on tradi-\ntional CNN architectures: on the one hand, they lack the ability to model\nglobal contextual information; on the other hand, under extremely low\nannotation ratios (5 %), the models struggle to learn sufficient feature\nrepresentations, leading to a significant decline in segmentation perfor-\nmance. The experimental data confirms this, as both methods perform\npoorly across multiple evaluation metrics. In contrast, PDFMSeg effec-\ntively combines the strengths of CNN and ViT: 1) the CNN branches\nfocus on extracting local detail features, and 2) the ViT branch mod-\nels global spatial dependencies. This multi-branch collaborative design\nenhances the modelâ€™s feature representation capabilities, allowing it to\nachieve overall performance advantages on the MSCMR dataset. This\ndemonstrates PDFMSegâ€™s exceptional segmentation ability, even with\nextremely low annotation costs.\nComparision with weakly semi-supervised methods. This section presents\na thorough evaluation of the performance differences between PDFM-\nSeg and current mainstream weakly semi-supervised segmentation\nmethods on the ACDC and MSCMR datasets. The comparison meth-\nods include CPS (Chen et al., 2021), URPC (Luo et al., 2022b),\nCBCT (Luo et al., 2022a), EVIL (Chen et al., 2024), CLB (Liu et al.,\n2024a), and DD-Net (Wang et al., 2025). The experimental results are\nsummarized in Tables 7 and 8.\nAnalysis indicates that CPS utilizes a shared structure dual-branch\nmodel, leveraging unlabeled data via a cross-pseudo supervision mech-\nanism. However, its lack of regional-level supervision for labeled data\nlimits learning efficiency and ultimately results in suboptimal segmen-\ntation performance. URPC, based on a single-branch CNN architec-\nture, employs intermediate layer features for supervision, which makes\nit challenging to effectively propagate high-level semantic informa-\ntion, thereby negatively affecting final segmentation accuracy. CBCTâ€™s\ninability to apply implicit perturbations to input data may lead to\noverconfidence in supervised information, resulting in inaccurate learn-\ning. EVIL filters low-confidence pseudo labels using DS evidence theory;\nhowever, its stringent confidence threshold might excessively filter valu-\nable supervision signals, impairing segmentation performance. The CLB\nmethod integrates both shape-agnostic and shape-aware networks by\nconcatenating the output of the shape-agnostic network with the origi-\nnal image as input. This reduces the modelâ€™s focus on the target region,\nleading to less precise segmentation. In contrast, PDFMSeg mitigates\nthese limitations by minimizing irrelevant background pixel interfer-\nence through ğ¿\nğ‘ğ·ğ‘–ğ‘ğ‘’\n, thereby enhancing the modelâ€™s capacity to capture\nmeaningful pixels within the segmentation area. It also leverages FPLMS\nto suppress the generation of false positive regions in pseudo labels,\nimproving both shape constraints and regional supervision capabilities.\nExperimental results confirm that PDFMSeg achieves superior segmenta-\ntion performance on both datasets, significantly outperforming existing\nweakly semi-supervised methods and demonstrating its practical value\nin cardiac MRI segmentation.\nFig. 6 further illustrates the visual segmentation results of each com-\nparison method on the ACDC and MSCMR datasets. The segmenta-\ntion performance among the various methods shows clear differences.\nCPS, lacking constraints on the quality of pseudo labels, results in poor\nboundary segmentation. URPC, constrained by the feature representa-\ntion capabilities of its single-branch CNN structure, is prone to under-\nsegmentation. CBCT, lacking data perturbation, often leads to overconfi-\ndence in segmentation. EVIL, due to its overly strict pseudo-label filter-\nNeural Networks 196 (2026) 108329\n7\n\nW. Li et al.\nTable 8\nComparison between PDFMSeg and other weakly semi-supervised methods on MSCMR dataset.\nMethods \nRatio=5 % Ratio=10 % Ratio=20 %\nğ·ğ‘†ğ¶â†‘(%) ğ½ ğ¶â†‘(%) ğ»ğ·95â†“(mm) ğ·ğ‘†ğ¶â†‘(%) ğ½ ğ¶â†‘(%) ğ»ğ·95â†“(mm) ğ·ğ‘†ğ¶â†‘(%) ğ½ ğ¶â†‘(%) ğ»ğ·95â†“(mm)\nCPS 30.30 Â± 5.81 17.86 Â± 4.04 103.6 Â± 23.55 31.38 Â± 2.77 18.61 Â± 1.94 41.33 Â± 24.06 55.23 Â± 1.50 38.15 Â± 1.43 32.20 Â± 17.75\nURPC 38.80 Â± 4.15 24.07 Â± 3.19 106.7 Â± 20.02 50.53 Â± 2.94 33.81 Â± 2.63 56.69 Â± 17.11 74.36 Â± 1.21 59.18 Â± 1.53 53.89 Â± 6.64\nCBCT 64.55 Â± 3.29 47.66 Â± 3.58 95.74 Â± 18.36 68.56 Â± 1.63 52.16 Â± 1.88 48.54 Â± 15.25 69.96 Â± 1.37 53.80 Â± 1.62 37.16 Â± 7.47\nEVIL 36.03 Â± 4.63 21.97 Â± 3.44 52.39 Â± 15.44 53.31 Â± 2.51 36.34 Â± 2.33 46.61 Â± 10.38 62.79 Â± 2.34 45.76 Â± 2.48 38.77 Â± 5.23\nCLB 49.45 Â± 5.04 32.85 Â± 4.45 98.27 Â± 12.64 53.78 Â± 3.22 36.78 Â± 3.01 71.31 Â± 6.61 67.31 Â± 1.96 50.73 Â± 2.22 51.79 Â± 4.47\nDD-Net 8.89 Â± 4.72 4.7 Â± 2.56 120.0 Â± 10.17 17.11 Â± 2.18 9.36 Â± 1.3 100.3 Â± 13.92 27.32 Â± 2.01 15.82 Â± 1.34 90.16 Â± 8.12\nPDFMSeg 73.35 Â± 5.34 57.95 Â± 6.67 37.73 Â± 6.98 74.29 Â± 1.00 59.09 Â± 1.26 31.66 Â± 3.37 80.99 Â± 0.81 68.01 Â± 1.14 30.56 Â± 2.89\nh\nOptimal results are indicated in bold.\nFig. 6. Qualitative comparison between PDFMSeg and other weakly semi-supervised methods on ACDC and MSCMR datasets at three different annotation ratios\n(5 %, 10 %, 20 %).\ning strategy, experiences performance degradation, such as the near-\ncomplete loss of the right ventricle target category on the MSCMR\ndataset under low labeling ratio conditions (ratio = 5 %). The CLB\nmethod concatenates the output of the shape-agnostic network with\nthe original image, which can reduce the modelâ€™s focus on the target\narea, leading to redundant segmentation. In contrast, PDFMSeg reduces\nbackground pixel interference through ğ¿\nğ‘ğ·ğ‘–ğ‘ğ‘’\n, allowing the model to\nfocus more on the relevant segmentation area. The FPLMS strategy em-\nphasizes the contour area of the target to be segmented, improving the\nboundary constraint capabilities of the pseudo labels and achieving su-\nperior segmentation results.\nIn addition, from a practical application perspective, we evaluated\nthe computational efficiency of each compared method. Given that hard-\nware resources are generally sufficient and unconstrained during train-\ning, we focused specifically on the computational cost during the infer-\nence phase. Table 9 provides a detailed comparison of model parameters\nand floating-point operations (FLOPs) across all methods. In our design,\nparameters from different branches are stored separately. The analysis\nTable 9\nComparison with servel models on param-\neters and FLOPs.\nMethods Parameters (M) FLOPs (G)\nCPS 1.81 2.30\nURPC 1.82 2.32\nCBCT 1.81 2.30\nEVIL 1.81 2.30\nCLB 1.81 2.30\nViT 27.15 5.93\nPDFMSeg 1.82 2.32\nindicates that both the proposed PDFMSeg and mainstream approachesâ€“\nsuch as CPS, URPC, and CBCTâ€“employ lightweight CNN branches during\ninference. This design choice helps maintain a simple model architecture\nand eliminates the need for additional processing of intermediate fea-\ntures. Consequently, these methods exhibit reduced model parameters\nNeural Networks 196 (2026) 108329\n8\n\nW. Li et al.\nand FLOPs. The efficient structure of PDFMSeg enhances its suitability\nfor practical deployment, particularly in resource-constrained scenarios,\nwhile still meeting requirements for computational performance.\n4. Disscussion\nThe proposed PDFMSeg framework is designed to reduce annota-\ntion workload by operating under a weakly semi-supervised paradigm,\nwhile still achieving excellent performance. Key contributions include\nan enhanced pseudo-label supervision mechanism, which incorporates\nspecialized processing of high-frequency components to better preserve\ntarget edge information and strengthen boundary constraints. Further-\nmore, the ğ¿\nğ‘ğ·ğ‘–ğ‘ğ‘’ \nis introduced to enrich scribble-based supervision\nand intensify its regulatory effect. The architecture employs a multi-\nbranch decoupled design, wherein only the CNN branch is utilized dur-\ning inference, ensuring efficient and lightweight deployment. Together,\nthese innovations enable PDFMSeg to outperform existing weakly semi-\nsupervised segmentation methods.\nCompared with weakly supervised and semi-supervised learning\nmodels, the core advantage of the proposed method lies in its signifi-\ncantly reduced annotation workload. Specifically, while weakly super-\nvised learning requires sparse annotations for 100 % of the training im-\nages, and semi-supervised learning relies on a subset of images with\nfull pixel-level labels, our weakly semi-supervised approach only uses\nthe same number of sparse annotations as the semi-supervised setting.\nAs a result, PDFMSeg requires far fewer annotated pixels for supervi-\nsion than either of the compared paradigms. By accepting a modest de-\ncrease in segmentation accuracy in exchange for a substantial reduction\nin annotation effort, PDFMSeg approach effectively lowers the cost of\nlabel acquisition. It is worth noting that the performance gap may grad-\nually narrow with future advances in deep learning models and more\nsophisticated algorithm designs. Therefore, the proposed weakly semi-\nsupervised segmentation framework holds considerable practical signif-\nicance.\nIn rare cases where scribble annotations are sparse or unevenly dis-\ntributed across the image, we propose to adaptively adjust the ignore\nmask in ğ¿\nğ‘ğ·ğ‘–ğ‘ğ‘’ \nbased on local annotation density. Our preliminary idea\ninvolves generating supervoxels covering the target segmentation re-\ngions using the original image and a superpixel algorithm. The super-\nvoxels intersected by scribbles of a given class are regarded as the funda-\nmental segmentation unit for that category. We then compute the ratio\nof annotated pixels to the total number of pixels within each correspond-\ning fundamental segmentation unit. If this ratio falls below a predefined\nthreshold, the ignore mask values for all pixels within that fundamental\nsegmentation unit are set to 1. This expands the set of pixels included\nin the loss calculation and enables density-aware adaptive adjustment.\nThis concept represents our initial conceptual direction; a more com-\nprehensive and experimentally validated solution will be developed in\nfuture work.\n5. Conclution\nThis article proposes a weakly semi-supervised segmentation method\nfor cardiac MRI based on pseudo-label frequency domain dynamic\nmixed supervision and partial Dice loss constraints, with the aim of\noptimizing supervision quality. Through the Haar wavelet transform,\nhigh-frequency and low-frequency components of the source pseudo\nlabels are independently and dynamically mixed, effectively reducing\nredundant high-frequency components in the supervision information\nand improving the supervision effect. This process not only optimizes\nthe quality of supervised information but also provides new insights\ninto the fusion of effective information across different segmentation\nparadigms. Additionally, a special Dice loss function was constructed\nto enable scribble labels to be directly used for model supervision, fur-\nther enriching the forms of scribble supervision. The experimental re-\nsults demonstrate that, compared to multiple advanced methods, the\nproposed approach achieves state-of-the-art segmentation performance\nwhile maintaining a reasonable number of model parameters and FLOPs.\nHowever, the sample labeling strategy used in this experiment, ran-\ndom labeling, may not fully represent the overall sample distribution\nof the training set, which limits the potential to further improve seg-\nmentation performance. In recent years, active learning techniques have\nshown significant advantages in selecting annotated samples. Looking\nahead, we plan to integrate active learning by starting with a small ini-\ntial labeled dataset of randomly selected samples and training both the\nmodel and an uncertainty estimator. The trained model will then predict\non all unlabeled data, and the uncertainty estimator will assign an un-\ncertainty score to each unlabeled sample. The most uncertain samples\nwill be selected for expert annotation and added to the labeled pool.\nThis iterative process allows the model to autonomously choose infor-\nmative samples for labeling, improving both the representativeness of\nthe labeled data and the overall segmentation performance.\nCRediT authorship contribution statement\nWenzong Li: Data curation, Formal analysis, Software, Writing\nâ€“ original draft; Hui Liu: Project administration, Conceptualization;\nJingcui Qin: Validation, Writing â€“ review & editing; Hongdang Zheng:\nValidation, Writing â€“ review & editing; Lin Zhang: Conceptualization,\nWriting â€“ review & editing, Project administration.\nData availability\nWe used two public datasets to evaluate our segmentation model.\nThe two datasets are openly available at the following URLs: ACDC and\nMSCMR.\nDeclaration of competing interest\nThe authors declare that they have no known competing financial\ninterests or personal relationships that could have appeared to influence\nthe work reported in this paper.\nAcknowledgments\nThis work was supported in part by Nantong Science and Technology\nInnovation Plan.\nReferences\nBernard, O., Lalande, A., Zotti, C., Cervenansky, F., Yang, X., Heng, P.-A., Cetin, I., Lekadir,\nK., Camara, O., Gonzalez Ballester, M. A., Sanroma, G., Napel, S., Petersen, S., Tzir-\nitas, G., Grinias, E., Khened, M., Kollerathu, V. A., Krishnamurthi, G., RohÃ©, M.-M.,\nâ€¦ Jodoin, P.-M. (2018). Deep learning techniques for automatic MRI cardiac multi-\nstructures segmentation and diagnosis: Is the problem solved? IEEE Transactions on\nMedical Imaging, 37(11), 2514â€“2525.\nCao, H., Wang, Y., Chen, J., Jiang, D., Zhang, X., Tian, Q., & Wang, M. (2022). Swin-\nunet: Unet-like pure transformer for medical image segmentation. In 17th European\nconference on computer vision, ECCV 2022 (pp. 205â€“218). Tel Aviv, Israel.\nChen, H., Li, C., Wang, G., Li, X., Mamunur Rahaman, M., Sun, H., Hu, W., Li, Y., Liu,\nW., Sun, C., Ai, S., & Grzegorzek, M. (2022). Gashis-transformer: A multi-scale visual\ntransformer approach for gastric histopathological image detection. Pattern Recogni-\ntion, 130, 108827.\nChen, R., Nian, F., Cen, Y., Peng, Y., Wang, H., Yu, Z., & Luo, J. (2025a). L-SSHNN:\nA larger search space of semi-supervised hybrid NAS network for echocardiography\nsegmentation. Expert Systems with Applications, 276, 127084.\nChen, X., Yuan, Y., Zeng, G., & Wang, J. (2021). Semi-supervised semantic segmentation\nwith cross pseudo supervision. In 2021 IEEE/CVF conference on computer vision and\npattern recognition (CVPR), Nashville, TN, USA (pp. 2613â€“2622).\nChen, Y., Yang, Z., Shen, C., Wang, Z., Zhang, Z., Qin, Y., Wei, X., Lu, J., Liu, Y., & Zhang,\nY. (2024). Evidence-based uncertainty-aware semi-supervised medical image segmen-\ntation. Computers in Biology and Medicine, 170, 108004.\nChen, Z., Zhao, W., Liu, J., Xie, P., Hou, S., Nian, Y., Yang, X., Ma, R., Ding, H., & Xiao, J.\n(2025b). Active learning for cross-modal cardiac segmentation with sparse annotation.\nPattern Recognition, 162, 111403.\nLi, S., Zhao, J., Liu, S., Dai, X., Zhang, C., & Song, Z. (2024). SP3: Superpixel-propagated\npseudo-label learning for weakly semi-supervised medical image segmentation. arXiv,\nabs/2411.11636.\nNeural Networks 196 (2026) 108329\n9\n\nW. Li et al.\nLiu, J., Desrosiers, C., Yu, D., & Zhou, Y. (2024a). Semi-supervised medical image segmen-\ntation using cross-style consistency with shape-aware and local context constraints.\nIEEE Transactions on Medical Imaging, 43(4), 1449â€“1461.\nLiu, T., Tan, Z., Jiang, H., & Huang, K. (2025). Stagger network: Rethinking information\nloss in medical image segmentation with various-sized targets. Neural Networks, 188,\n107386.\nLiu, X., Zhu, X., Zhang, Y., & Wang, M. (2024b). Point based weakly semi-supervised\nbiomarker detection with cross-scale and label assignment in retinal OCT images. Com-\nputer Methods and Programs in Biomedicine, 251, 108229.\nLuo, X., Hu, M., Song, T., Wang, G., & Zhang, S. (2022a). Semi-supervised medical im-\nage segmentation via cross teaching between CNN and transformer. In Proceedings of\nthe 5th international conference on medical imaging with deep learning (PMLR), Zurich,\nSwitzerland (pp. 820â€“833).\nLuo, X., Wang, G., Liao, W., Chen, J., Song, T., Chen, Y., Zhang, S., Metaxas, D. N., & Zhang,\nS. (2022b). Semi-supervised medical image segmentation via uncertainty rectified\npyramid consistency. Medical Image Analysis, 80, 102517.\nObukhov, A., Georgoulis, S., Dai, D., & Gool, L. V. (2019). Gated CRF loss for weakly\nsupervised semantic image segmentation. arXiv, abs/1906.04651.\nOuali, Y., Hudelot, C., & Tami, M. (2020). Semi-supervised semantic segmentation with\ncross-consistency training. In 2020 IEEE/CVF conference on computer vision and pattern\nrecognition (CVPR), Seattle, WA, USA (pp. 12671â€“12681).\nRonneberger, O., Fischer, P., & Brox, T. (2015). U-Net: Convolutional networks for\nbiomedical image segmentation. In 25Th international conference on medical image com-\nputing and computer assisted intervention (MICCAI), Munich, Germany (pp. 234â€“241).\nValvano, G., Leo, A., & Tsaftaris, S. A. (2021). Learning to segment from scribbles using\nmulti-scale adversarial attention gates. IEEE Transactions on Medical Imaging, 40(8),\n1990â€“2001.\nVerma, V., Kawaguchi, K., Lamb, A., Kannala, J., Solin, A., Bengio, Y., & Lopez-Paz, D.\n(2022). Interpolation consistency training for semi-supervised learning. Neural Net-\nworks, 145, 90â€“106.\nWang, A., Xu, M., Zhang, Y., Islam, M., & Ren, H. (2023). S\n2 \nME: Spatial-spectral mutual\nteaching and ensemble learning for scribble-supervised polyp segmentation. In Medical\nimage computing and computer assisted intervention â€“ MICCAI 2023 (MICCAI), Vancouver,\nBC, Canada (pp. 35â€“45).\nWang, B., Huang, T., Yang, S., Yang, Y., Zhai, J., & Zhang, X. (2025). Dual-decoder data\ndecoupling training for semi-supervised medical image segmentation. Biomedical Signal\nProcessing and control, 100, 106984.\nWu, J., Ma, J., Xi, H., Li, J., & Zhu, J. (2025). Multi-scale graph harmonies: Unleashing\nu-netâ€™s potential for medical image segmentation through contrastive learning. Neural\nNetworks, 182, 106914.\nYun, S., Han, D., Chun, S., Oh, S., Yoo, Y., & Choe, J. (2019). Cutmix: Regularization strat-\negy to train strong classifiers with localizable features. In 2019 IEEE/CVF conference on\ncomputer vision and pattern recognition (CVPR), Seoul, Korea (South) (pp. 6022â€“6031).\nZhang, K., & Zhuang, X. (2022). Cyclemix: A holistic strategy for medical image segmen-\ntation from scribble supervision. In 2022 IEEE/CVF conference on computer vision and\npattern recognition (CVPR), New Orleans, LA (pp. 11646â€“11655).\nZhang, L., Li, W., Bi, K., Li, P., Zhang, L., & Liu, H. (2025a). Fddseg: Unleashing the\npower of scribble annotation for cardiac mri images through feature decomposition\ndistillation. IEEE Journal of Biomedical and Health Informatics, 29(1), 285â€“296.\nZhang, Y., Guo, J., Yue, H., Zheng, S., & Liu, C. (2025b). Illumination-guided progressive\nunsupervised domain adaptation for low-light instance segmentation. Neural Networks,\n183, 106958.\nNeural Networks 196 (2026) 108329\n10",
    "version": "5.3.31"
  },
  {
    "numpages": 9,
    "numrender": 9,
    "info": {
      "PDFFormatVersion": "1.7",
      "Language": "en-US",
      "EncryptFilterName": null,
      "IsLinearized": true,
      "IsAcroFormPresent": false,
      "IsXFAPresent": false,
      "IsCollectionPresent": false,
      "IsSignaturesPresent": false,
      "Creator": "Elsevier",
      "Custom": {
        "CrossMarkDomains[1]": "elsevier.com",
        "CrossmarkMajorVersionDate": "2010-04-23",
        "CreationDate--Text": "29th August 2024",
        "ElsevierWebPDFSpecifications": "7.0",
        "robots": "noindex",
        "doi": "10.1016/j.compmedimag.2024.102416",
        "CrossMarkDomains[2]": "sciencedirect.com",
        "CrossmarkDomainExclusive": "true"
      },
      "ModDate": "D:20240829033749Z",
      "Author": "Yijie Qu",
      "Title": "ScribSD+: Scribble-supervised medical image segmentation based on simultaneous multi-scale knowledge distillation and class-wise contrastive regularization",
      "Keywords": "Fetal MRI,Weakly supervised learning,Knowledge distillation,Contrastive learning",
      "CreationDate": "D:20240829014628Z",
      "Producer": "Acrobat Distiller 8.1.0 (Windows)",
      "Subject": "Computerized Medical Imaging and Graphics, 116 (2024) 102416. doi:10.1016/j.compmedimag.2024.102416"
    },
    "metadata": {
      "crossmark:crossmarkdomains": "elsevier.comsciencedirect.com",
      "crossmark:crossmarkdomainexclusive": "true",
      "crossmark:doi": "10.1016/j.compmedimag.2024.102416",
      "crossmark:majorversiondate": "2010-04-23",
      "dc:format": "application/pdf",
      "dc:identifier": "10.1016/j.compmedimag.2024.102416",
      "dc:publisher": "Elsevier Ltd",
      "dc:description": "Computerized Medical Imaging and Graphics, 116 (2024) 102416. doi:10.1016/j.compmedimag.2024.102416",
      "dc:subject": [
        "Fetal MRI",
        "Weakly supervised learning",
        "Knowledge distillation",
        "Contrastive learning"
      ],
      "dc:title": "ScribSD+: Scribble-supervised medical image segmentation based on simultaneous multi-scale knowledge distillation and class-wise contrastive regularization",
      "dc:creator": ["Yijie Qu", "Tao Lu", "Shaoting Zhang", "Guotai Wang"],
      "jav:journal_article_version": "VoR",
      "pdf:creationdate--text": "29th August 2024",
      "pdf:producer": "Acrobat Distiller 8.1.0 (Windows)",
      "pdf:keywords": "Fetal MRI,Weakly supervised learning,Knowledge distillation,Contrastive learning",
      "pdfx:creationdate--text": "29th August 2024",
      "pdfx:crossmarkdomains": "sciencedirect.comelsevier.com",
      "pdfx:crossmarkdomainexclusive": "true",
      "pdfx:crossmarkmajorversiondate": "2010-04-23",
      "zifnootz8y9ymmmigmwesnsnnm96rlwj8zwugn9n_z9yongynntv7o9eqnmakmmqlodutma": "",
      "pdfx:doi": "10.1016/j.compmedimag.2024.102416",
      "pdfx:robots": "noindex",
      "prism:aggregationtype": "journal",
      "prism:copyright": "Â© 2024 Elsevier Ltd. All rights are reserved, including those for text and data mining, AI training, and similar technologies.",
      "prism:coverdate": "2024-09-01",
      "prism:coverdisplaydate": "1 September 2024",
      "prism:doi": "10.1016/j.compmedimag.2024.102416",
      "prism:issn": "0895-6111",
      "prism:pagerange": "102416",
      "prism:publicationname": "Computerized Medical Imaging and Graphics",
      "prism:startingpage": "102416",
      "prism:url": "https://doi.org/10.1016/j.compmedimag.2024.102416",
      "prism:volume": "116",
      "tdm:policy": "https://www.elsevier.com/tdm/tdmrep-policy.json",
      "tdm:reservation": "1",
      "xmp:createdate": "2024-08-29T01:46:28",
      "xmp:creatortool": "Elsevier",
      "xmp:metadatadate": "2024-08-29T03:37:49",
      "xmp:modifydate": "2024-08-29T03:37:49",
      "xmpmm:documentid": "uuid:1b499bed-4ce8-4c0c-b682-0d58baae1cbe",
      "xmpmm:instanceid": "uuid:b6ed5266-03cb-4855-90c0-636283357f42",
      "xmprights:marked": "True"
    },
    "text": "Contents lists available at ScienceDirect\nComputerized Medical Imaging and Graphics\njournal homepage: www.elsevier.com/locate/compmedimag\nScribSD+: Scribble-supervised medical image segmentation based on\nsimultaneous multi-scale knowledge distillation and class-wise contrastive\nregularization\nYijie Qu \na\n, Tao Lu \nb\n, Shaoting Zhang \na,c\n, Guotai Wang \na,c,\nâˆ—\na \nUniversity of Electronic Science and Technology of China, Chengdu, China\nb \nSichuan Provincial Peopleâ€™s Hospital, Chengdu, China\nc \nShanghai AI lab, Shanghai, China\nA R T I C L E I N F O\nKeywords:\nFetal MRI\nWeakly supervised learning\nKnowledge distillation\nContrastive learning\nA B S T R A C T\nDespite that deep learning has achieved state-of-the-art performance for automatic medical image segmen-\ntation, it often requires a large amount of pixel-level manual annotations for training. Obtaining these\nhigh-quality annotations is time-consuming and requires specialized knowledge, which hinders the widespread\napplication that relies on such annotations to train a model with good segmentation performance. Using\nscribble annotations can substantially reduce the annotation cost, but often leads to poor segmentation\nperformance due to insufficient supervision. In this work, we propose a novel framework named as ScribSD+\nthat is based on multi-scale knowledge distillation and class-wise contrastive regularization for learning from\nscribble annotations. For a student network supervised by scribbles and the teacher based on Exponential\nMoving Average (EMA), we first introduce multi-scale prediction-level Knowledge Distillation (KD) that\nleverages soft predictions of the teacher network to supervise the student at multiple scales, and then propose\nclass-wise contrastive regularization which encourages feature similarity within the same class and dissimilarity\nacross different classes, thereby effectively improving the segmentation performance of the student network.\nExperimental results on the ACDC dataset for heart structure segmentation and a fetal MRI dataset for placenta\nand fetal brain segmentation demonstrate that our method significantly improves the studentâ€™s performance\nand outperforms five state-of-the-art scribble-supervised learning methods. Consequently, the method has a\npotential for reducing the annotation cost in developing deep learning models for clinical diagnosis.\n1. Introduction\nWith the advancement of medical imaging technology and its wider\napplication, automatic medical image segmentation plays an important\nrole for accurate computer-assisted diagnosis of disease and treat-\nment planning. Convolutional Neural Networks (CNNs) (Milletari et al.,\n2016; Ã‡iÃ§ek et al., 2016; Chen et al., 2021a) have achieved great suc-\ncess in automatic image segmentation. Additionally, Transformer-based\nnetworks such as TransUNet (Chen et al., 2021b; Zou et al., 2023) have\nalso demonstrated high performance by leveraging self-attention mech-\nanisms. However, most of these methods require large-scale images\nwith accurate pixel-level dense annotations for model training (Isensee\net al., 2021; Kushnure and Talbar, 2021; Ammar et al., 2021). Un-\nfortunately, medical image annotation is a time-consuming, expensive,\nand expert-dependent process, making obtaining large-scale annotated\ndatasets difficult.\nâˆ— \nCorresponding author at: University of Electronic Science and Technology of China, Chengdu, China.\nE-mail address: guotai.wang@uestc.edu.cn (G. Wang).\nIn order to reduce the annotation cost of model training, researchers\nare actively exploring more annotation-efficient methods. Weakly Su-\npervised Learning (WSL) uses sparse annotations to train the model\n(Valvano et al., 2021; Dolz et al., 2021; Dorent et al., 2021), avoiding\nthe need for dense annotations. Weak annotations such as image-level\nlabels (Wu et al., 2019), bounding boxes (Rajchl et al., 2016; Ou et al.,\n2023), point annotations (Zhai et al., 2023), and scribbles (Lin et al.,\n2016; Valvano et al., 2021) are more accessible to obtain than pixel-\nlevel dense annotations, and can effectively reduce the annotation cost.\nCompared with image-level labels, bounding boxes and point annota-\ntions, scribbles are more flexible to annotate targets with various shapes\nand often lead to better performance (Lin et al., 2016). Therefore, this\nwork focuses on exploring using scribble annotations to efficiently and\nrobustly train high-performance medical image segmentation models.\nAn intuitive challenge is that although scribbles can flexibly anno-\ntate targets of various shapes, the sparse annotations still lack sufficient\nhttps://doi.org/10.1016/j.compmedimag.2024.102416\nReceived 15 January 2024; Received in revised form 16 June 2024; Accepted 4 July 2024\nComputerized Medical Imaging and Graphics 116 (2024) 102416\nAvailable online 9 July 2024\n0895-6111/Â© 2024 Elsevier Ltd. All rights are reserved, including those for text and data mining, AI training, and similar technologies.\n\nY. Qu et al.\nstructural information, as illustrated in Fig. 1. Most existing methods\nrequire pseudo labels to generate supervision signals for unlabeled\npixels, which easily leads to (i) poor performance of the learned model\ndue to that the pseudo labels may be inaccurate in the presence of large\namounts of noise; (ii) high uncertainty in boundary regions since the\nscribbles are often given in the central region of a target, leading to\nambiguous distribution of different classes at the boundary. Directly\nusing pseudo labels without considering the noise may limit the modelâ€™s\nperformance, and failing to deal with the boundary regions could lead\nto inaccurate segmentation.\nTo deal with these problems, we propose an efficient scribble-based\nlearning method called ScribSD+ that is based on multi-scale Knowl-\nedge Distillation (KD) and class-wise contrastive regularization for\nmedical image segmentation based on scribble annotations. The con-\ntributions are three-fold. Firstly, we introduce a KD-based method for\nrobustly learning from scribbles for segmentation. Compared with ex-\nisting works using hard pseudo labels (Luo et al., 2022a), we introduce\nMulti-scale Prediction Distillation (Ms-PD) that uses soft predictions\nobtained by a teacher to supervise the student at multiple scales, as\nsoft predictions are more noise-robust, while hard labels tend to contain\nmore noise (Xu et al., 2020). Secondly, in order to reduce ambiguous\npredictions at the boundary and enhance class-wise feature distinc-\ntiveness, we introduce Multi-scale class-wise Contrastive Regularization\n(Ms-CR) that uses contrastive loss to encourage the feature vectors of\nthe same class from the teacher and student to be close and those\nof different classes to be far apart. Multi-scale design helps to better\ntransfer knowledge at different resolution levels from the teacher to\nstudent. In addition, compared with classic KD methods (Hinton et al.,\n2015; Xu et al., 2020) that require pre-training a teacher model, we use\nself-distillation that efficiently generates the teacher on-the-fly based on\nthe Exponential Moving Average (EMA) of model parameters.\nThis paper is a substantial extension of our preliminary work\nScribSD published in MICCAI 2023 MILLanD Workshop (Qu et al.,\n2023), where we proposed prediction distillation based on soft pre-\ndictions to deal with noisy pseudo labels and multi-scale feature dis-\ntillation that transfers the high-quality knowledge contained in the\nteacher model to the student, and validated its effectiveness on a fetal\nbrain and placenta segmentation dataset. In this extension, we give\na more detailed description and analysis of the multi-scale prediction\ndistillation, and replace our previous multi-scale feature distillation (Qu\net al., 2023) by multi-scale class-wise contrastive regularization, where\nthe latter can learn more distinctive feature representations for different\nclasses for better performance while supervising the student by the\nteacher at the feature level. Additionally, our method is further val-\nidated on the public ACDC dataset for heart structure segmentation.\nCompared with learning only from the scribbles with partial Cross-\nEntropy on these two datasets, our method improved the average\nDice by around 6.0 and 15.5 percentage points, respectively, and\nit significantly outperformed five state-of-the-art weakly supervised\nmethods.\n2. Related works\n2.1. Weakly-supervised learning\nTo learn from scribble annotations for segmentation, existing meth-\nods are mainly based on introducing regularization or pseudo labels for\nunlabeled pixels. For regularization-based methods, Lin et al. (2016)\nbuilt a superpixel-based graphical model trained by propagating infor-\nmation from scribbles to unlabeled pixels. Kim and Ye (2019) proposed\na level-set-based Mumfordâ€“Shah function to regularize the segmenta-\ntion results. Kervadec et al. (2019) constrained the volume of the pre-\ndicted mask by introducing higher-order inequalities in the loss func-\ntion and employed deep tightness prior for learning from bounding box\nannotations. Valvano et al. (2021) proposed a learning method based\non multi-scale adversarial attention gates for raining a segmentation\nFig. 1. Examples of dense and scribble annotations. RV, Myo and LV represent the right\nventricle, myocardium, left ventricle respectively. BG and UA represent the background\nand unannotated pixels respectively.\nmodel with scribble annotations. Scribble2D5 (Chen and Hong, 2022)\nexpands the scribbles based on super-pixels and uses active contours to\nconstrain the prediction results. ShapePU (Zhang and Zhuang, 2022)\ncombines positive-unlabeled learning and global consistency regular-\nization to obtain supervisory information from unlabeled pixels to train\na heart segmentation model. Conditional Random Field (CRF) (Tang\net al., 2018b) loss and normalized cut loss (Tang et al., 2018a) have\nalso been proposed for regularization. Additionally, pseudo-label-based\nmethods utilize weak annotations to generate pseudo labels for network\ntraining. Scribble2Label (Lee and Jeong, 2020) generates pseudo labels\nbased on the EMA of the predictions and filters out low-quality labels\nfor robust learning. Luo et al. (2022a) proposed Dynamically Mixed\nPseudo Label Supervision (DMPLS) where the predictions of two de-\ncoders are mixed randomly to serve as pseudo labels. The Uncertainty-\naware Self-ensembling and Transformation-consistent Mean Teacher\nModel (USTM) (Liu et al., 2022) leverages an uncertainty-aware mean\nteacher to generate pseudo labels for supervising the student based on\na Mean Square Error (MSE) loss. These attempts have been effective\nto a certain extent, however, regularization-based methods often face\nperformance limitations due to their inability to fully capture complex\npatterns in the data, potentially leading to suboptimal model perfor-\nmance. For pseudo-label-based methods, since the generated pseudo\nlabels are inaccurate under the influence of noise, directly using them\nas supervision signals may limit the performance of the model.\n2.2. Contrastive learning\nAs a self-supervised learning method, contrastive learning usually\nadopts a contrastive loss to force the representations of positive pairs\nto be close and those of negative pairs to be far away to learn feature\nrepresentations (Hadsell et al., 2006). Previous contrastive learning\nmethods were mainly used for self-supervised pre-training, with the\naim of training a powerful and representative feature extractor that\ncan distinguish between similar and dissimilar samples, so that it can\nbe fine-tuned for downstream tasks (He et al., 2020; Chen et al.,\n2020). Recently in the fields of computer vision and medical image\nanalysis, contrastive learning has also been applied to annotation-\nefficient learning (Zhao et al., 2024). For example, Kang et al. (2019)\nproposed a contrastive adaptation network to achieve class-aware do-\nmain alignment by minimizing the intra-class distance and maximizing\nthe inter-class distance. Chaitanya et al. (2020) proposed a combi-\nnation of global contrastive loss and local contrastive loss for semi-\nsupervised 3D medical image segmentation. Lei et al. (2021) proposed\ncontrastive learning of relative position regression for one-shot tar-\nget localization of 3D medical images. You et al. (2022) proposed a\nmomentum-contrastive voxel-level representation learning method for\nsemi-supervised medical image segmentation by combining low-level\nComputerized Medical Imaging and Graphics 116 (2024) 102416\n2\n\nY. Qu et al.\nFig. 2. The proposed ScribSD+ framework based on multi-scale knowledge distillation and class-wise contrastive regularization. Multi-scale Prediction Distillation (ğ¿\nğ‘€ğ‘ âˆ’ğ‘ƒ ğ· \n) based\non soft labels and Multi-scale class-wise Contrastive Regularization (ğ¿\nğ‘€ğ‘ âˆ’ğ¶ğ‘… \n) are combined for supervising the student on unannotated pixels.\nand high-level contrastive objectives with consistency loss. Gu et al.\n(2022) proposed a contrastive semi-supervised learning method for\nmedical image segmentation, using a cross-domain contrastive learn-\ning strategy to extract domain-invariant features. Zhou et al. (2023)\nlearned prototypes of each class and encouraged the similarity between\nthe class-wise feature to its prototypes. However, it only encourages\nthe compactness of each class, while the dissimilarity of difference\nclasses is not considered. Differently from this work, we design a\nnovel class-wise contrastive regularization strategy without complex\nprototype learning process for scribble-based learning in segmentation\ntasks. Our method simultaneously encourages intra-class compactness\nand inter-class distinctness between the features of teacher and student\nmodels.\n3. Method\nOur proposed ScribSD+ based on multi-scale knowledge distillation\nand class-wise contrastive regularization for learning from scribble\nannotations is illustrated in Fig. 2. For a student model supervised\nby the scribbles and a regularization loss based on CRF, we generate\na teacher model based on EMA for self-distillation, where multi-scale\nprediction-level distillation and class-wise contrastive regularization\nare combined for the student to better learn segmentation information\nand feature representations from the teacher.\n3.1. Student model with scribble supervision\nIn this work, we consider a UNet-like (Ronneberger et al., 2015)\nstructure for segmentation due to its high performance in a wide\nrange of tasks. For a training set with scribble annotations, a partial\nCross-Entropy (pCE) loss is used to learn from the scribbles:\nğ¿\nğ‘ğ¶ğ¸ \n= âˆ’ \n1\n|\n|\nğ›º\nğ‘† \n|\n|\nâˆ‘\nğ‘–âˆˆğ›º\nğ‘†\nğ¶\nâˆ‘\nğ‘=0\nğ‘†\nğ‘\nğ‘– \nğ‘™ğ‘œğ‘”(ğ‘ƒ \nğ‘\nğ‘– \n) (1)\nwhere ğ¶ denotes the class number for segmentation. ğ‘† is the one-\nhot scribble annotation map, ğ›º\nğ‘† \nis the set of labeled pixels, and ğ‘ƒ is\nthe Softmax-based probability map predicted by the student model. ğ‘†\nğ‘\nğ‘–\nand ğ‘ƒ \nğ‘\nğ‘– \nare probabilities of the ğ‘–th pixel being class ğ‘ in the scribble\nannotation and network prediction, respectively.\nSince just applying ğ¿\nğ‘ğ¶ğ¸ \non the sparse annotations usually leads to\ninadequate supervision, especially around boundaries (Obukhov et al.,\n2019), we further use a CRF loss (ğ¿\nğ¶ğ‘…ğ¹ \n) to regularize the network\noutput, which encourages prediction consistency between neighboring\npixels with similar features:\nğ¿\nğ¶ğ‘…ğ¹ \n=\nğ¶\nâˆ‘\nğ‘=0\nâˆ‘\n(ğ‘–,ğ‘—)âˆˆğ›·\nğº(ğ‘–, ğ‘—)ğ‘ƒ \nğ‘\nğ‘– \n(1 âˆ’ ğ‘ƒ \nğ‘\nğ‘— \n) (2)\nwhere ğ›· denotes the set of pixel pairs in the image. ğº(ğ‘–, ğ‘—) refers to the\nGaussian kernel bandwidth filter:\nğº \n(\nğ‘–, ğ‘—\n) = \n1\nğœ” \nğ‘’ğ‘¥ğ‘\n(\nâˆ’ \nâ€–ğ»\nğ‘– \nâˆ’ ğ»\nğ‘— \nâ€–\n2\n2ğœ\n2\nğ»\nâˆ’ \nâ€–ğ¼\nğ‘– \nâˆ’ ğ¼\nğ‘— \nâ€–\n2\n2ğœ\n2\nğ¼\n)\n(3)\nwhere ğ»\nğ‘– \nand ğ¼\nğ‘– \ndenote the spatial position and the intensity value of\npixel ğ‘–, respectively. ğœ” is the weight for normalization, ğœ\nğ» \nand ğœ\nğ¼ \nare\nhyper-parameters to control the scale of Gaussian kernels.\n3.2. Scribble-supervised self-distillation\nInstead of pre-training an extra teacher model offline for KD (Hinton\net al., 2015; Xu et al., 2020), we obtain the teacher model during the\ntraining process on the fly, where the teacher has the same structure\nas the student model, but with different model weights. As shown in\nFig. 2(a), let ğœƒ\nğ‘† \nand ğœƒ\nğ‘‡ \ndenote the weights of the student and teacher\nmodels, respectively. The teacher is updated as an EMA of the student:\nğœƒ\nğ‘‡\nğ‘¡ \n= ğœ†ğœƒ\nğ‘‡\nğ‘¡âˆ’1 \n+ (1 âˆ’ ğœ†)ğœƒ\nğ‘†\nğ‘¡ \n, where ğœƒ\nğ‘†\nğ‘¡ \nand ğœƒ\nğ‘‡\nğ‘¡âˆ’1 \nare weights of the teacher and\nstudent at iteration ğ‘¡ and ğ‘¡ âˆ’ 1, respectively. ğœ† = ğ‘šğ‘–ğ‘›(1 âˆ’ \n1\nğ‘¡+1 \n, 0.99) is the\nEMA momentum that gradually increases to 0.99, where ğ‘¡ denotes the\ncurrent number of iterations.\n3.2.1. Multi-scale prediction distillation\nWhen sparse scribble annotations are used to generate pseudo la-\nbels, the quality of the pseudo labels is usually poor due to the presence\nComputerized Medical Imaging and Graphics 116 (2024) 102416\n3\n\nY. Qu et al.\nof a large amount of noise, and using low-quality pseudo labels may\nlimit training of the student model. To address this problem, we pro-\npose Multi-scale Prediction Distillation (Ms-PD). We use high-quality\nsoft predictions obtained by the teacher at multiple scales to supervise\nthe student, as soft predictions are more robust to noise, while hard\nlabels tend to contain more noise (Xu et al., 2020). In addition, by\nusing supervised training at multiple scales, the pseudo labels generated\nby the high-performance teacher model can be better utilized, and the\nstudent model can learn multi-level and rich information.\nSpecifically, we obtain soft predictions for the teacher and student\nat ğ· scales in the decoder. For scale ğ‘‘, let ğ¹ \nğ‘‘ \nâˆˆ îˆ¾\nğ¶\nğ‘‘ \nÃ—ğ»\nğ‘‘ \nÃ—ğ‘Š\nğ‘‘ \ndenote\nthe last feature map in the decoder, where ğ¶\nğ‘‘ \nis the channel number\nand ğ»\nğ‘‘ \nÃ— ğ‘Š\nğ‘‘ \nis the spatial dimension. We use 1 Ã— 1 convolution to\nconvert the channel number to ğ¶ and then obtain soft predictions based\non a temperature-calibrated Softmax (ğœ-Softmax) that are more noise-\ntolerant (MÃ¼ller et al., 2019). Let\nÌƒ\n ğ‘ƒ \nğ‘‘ \nand\nÌƒ\n ğ‘„\nğ‘‘ \ndenote the soft predictions\nat the ğ‘‘th scale for the student and teacher, respectively. Let\nÌƒ\n ğ‘„\nğ‘‘\nğ‘,ğ‘›\ndenote the probability value in the ğ‘th channel at the ğ‘›th pixel of\nÌƒ\n ğ‘„\nğ‘‘ \n.\nIt is defined as:\nÌƒ\nğ‘„\nğ‘‘\nğ‘,ğ‘› \n= \nğ‘’ğ‘¥ğ‘(ğ‘§\nğ‘‘\nğ‘,ğ‘›\nâˆ•ğœ)\nâˆ‘\nğ‘ \nğ‘’ğ‘¥ğ‘(ğ‘§\nğ‘‘\nğ‘,ğ‘›\nâˆ•ğœ) \n(4)\nwhere ğ‘§\nğ‘‘\nğ‘,ğ‘› \nis the logit output on the ğ‘th channel at the ğ‘›th pixel\nobtained at scale ğ‘‘. ğœ â‰¥ 1 is a hyper-parameter to control the softness,\nwhere a larger ğœ value leads to a softer output, i.e., the probability for\neach class is less likely to be the extreme values of 0.0 or 1.0. Note\nthat ğœ = 1 corresponds to the standard Softmax operation. Considering\nthat as the training progresses, the performance of the teacher model\ngradually improves and the impact of noise on the quality of pseudo\nlabels gradually becomes smaller, we decrease the value of ğœ from a\nmaximum value ğœ\nğ‘šğ‘ğ‘¥ \nto a minimum value ğœ\nğ‘šğ‘–ğ‘› \naccording to the cosine\nramp down function, which is expressed as:\nğœ = ğ‘šğ‘ğ‘¥\n(\nğœ\nğ‘šğ‘–ğ‘›\n, ğœ\nğ‘šğ‘ğ‘¥ \nâ‹… \n1\n2 \n(ğ‘ğ‘œğ‘ ( \nğœ‹ğ‘¡\nğ‘¡\nğ‘šğ‘ğ‘¥\n) + 1)\n) \n(5)\nwhere ğ‘¡ denotes the current iteration number and ğ‘¡\nğ‘šğ‘ğ‘¥ \nis the maximum\niteration number for ramp-down.\nBased on the teacherâ€™s soft prediction\nÌƒ\n ğ‘„\nğ‘‘ \nand studentâ€™s soft pre-\ndiction\nÌƒ\n ğ‘ƒ \nğ‘‘ \nat scale ğ‘‘, we use KL-Divergence to encourage the class\ndistribution obtained by the student to be close to that of the teacher:\nğ¿\nğ‘‘\nğ‘ƒ ğ· \n= \n1\nğ‘\nğ‘\nâˆ‘\nğ‘›=0\nğ¶\nâˆ‘\nğ‘=0\nÌƒ\nğ‘„\nğ‘‘\nğ‘,ğ‘›\nğ‘™ğ‘œğ‘”(\nÌƒ\nğ‘„\nğ‘‘\nğ‘,ğ‘›\nâˆ•\nÌƒ\n ğ‘ƒ \nğ‘‘\nğ‘,ğ‘›\n) (6)\nwhere ğ‘ denotes the number of pixels in the image. The multi-scale\nprediction-level distillation loss ğ¿\nğ‘€ğ‘ âˆ’ğ‘ƒ ğ· \nis defined as:\nğ¿\nğ‘€ğ‘ âˆ’ğ‘ƒ ğ· \n=\nğ·\nâˆ‘\nğ‘‘=1\nğ›¾\nğ‘‘ \nâ‹… ğ¿\nğ‘‘\nğ‘ƒ ğ· \n(7)\nwhere ğ›¾\nğ‘‘ \nis the weight at scale ğ‘‘, and the sum of ğ›¾\nğ‘‘ \nat different scales\nis 1.0. The value of ğ‘‘ ranges from 1 to ğ·. Note that ğ‘‘ = 1 means we\nonly use the single-scale soft predictions at the end of decoder. A larger\nğ‘‘ value means a lower resolution that is closer to the bottleneck.\n3.2.2. Multi-scale class-wise contrastive regularization\nIn addition to Ms-PD which focuses on prediction-level distillation\nat multiple scales, we also consider feature-level distillation for improv-\ning the studentâ€™s performance. In our preliminary work ScribSD (Qu\net al., 2023), we directly maximized the similarity between the stu-\ndentâ€™s features and the teacherâ€™s features. However, that method does\nnot regularize the featuresâ€™ compactness and distinctiveness. To deal\nwith this problem, we propose Multi-scale class-wise Contrastive Reg-\nularization (Ms-CR) for better feature representations. As shown in\nFig. 2(b), we first use a class-aware feature aggregator to obtain class-\nwise features based on feature maps and the corresponding soft pre-\ndictions, then use a contrastive loss to enhance the similarity of the\nsame class and dissimilarity of different classes between the teacher\nand student, so that the model can learn more distinctive multi-class\nfeature distributions under sparse scribble annotations.\nFollowing Section 3.2.1, we apply an extra 1 Ã— 1 convolution to\nğ¹ \nğ‘‘ \nâˆˆ îˆ¾\nğ¶\nğ‘‘ \nÃ—ğ»\nğ‘‘ \nÃ—ğ‘Š\nğ‘‘ \nto convert its channel number to a fixed value of\nğ¶\nâ€²\n, and the result is denoted as ğ¹ \nâ€²ğ‘‘ \nâˆˆ îˆ¾\nğ¶\nâ€²\nÃ—ğ»\nğ‘‘ \nÃ—ğ‘Š\nğ‘‘ \n. For the student\nnetwork, we obtain the class-wise feature at scale ğ‘‘ from the feature\nmap corresponding to each class by weighted average:\nğ‘¢\nğ‘‘\nğ‘ \n=\nâˆ‘\nğ‘\nğ‘›=0 \nğ¹ \nâ€²ğ‘‘\nğ‘›\nÌƒ\n \nğ‘ƒ \nğ‘‘\nğ‘,ğ‘›\nâˆ‘\nğ‘\nğ‘›=0\nÌƒ\n \nğ‘ƒ \nğ‘‘\nğ‘,ğ‘›\n(8)\nwhere ğ‘¢\nğ‘‘\nğ‘ \nâˆˆ îˆ¾\nğ¶\nâ€² \nis the average feature vector for class ğ‘ at scale ğ‘‘,\nand ğ¹ \nâ€²ğ‘‘\nğ‘› \nâˆˆ îˆ¾\nğ¶\nâ€² \nis the feature vector at pixel ğ‘› in ğ¹ \nâ€²ğ‘‘ \n. Similarly, for\nthe teacher network, we use\nÌƒ\n ğ‘„\nğ‘‘ \nto obtain the class-wise feature and\ndenote it as ğ‘£\nğ‘‘\nğ‘ \n. Let ğœ„ and ğœ… to denote two class indexes. As shown in\nFig. 2(c), for the same class (i.e. ğœ„ = ğœ…), we treat (ğ‘¢\nğ‘‘\nğœ„ \n, ğ‘£\nğ‘‘\nğœ… \n) as a positive\npair. For different classes (i.e. ğœ„ â‰  ğœ…), (ğ‘¢\nğ‘‘\nğœ„ \n, ğ‘£\nğ‘‘\nğœ… \n) is considered as a negative\npair. For contrastive regularization at scale ğ‘‘, we encourage class-wise\nfeatures in a positive pair to be close and those in a negative pair to\nbe far apart to learn more compact and distinctive class-wise feature\nrepresentations.\nğ¿\nğ‘‘\nğ¶ğ‘… \n= \n1\nğ¶\nğ¶\nâˆ‘\nğœ„=0\nâˆ’ğ‘™ğ‘œğ‘” \nğ‘’ğ‘¥ğ‘\n(\nğ‘ ğ‘–ğ‘š(ğ‘¢\nğ‘‘\nğœ„ \n, ğ‘£\nğ‘‘\nğœ„ \n)âˆ•ğœŒ\n)\nâˆ‘\nğ¶\nğœ…=0 \nğ‘’ğ‘¥ğ‘\n(\nğ‘ ğ‘–ğ‘š(ğ‘¢\nğ‘‘\nğœ„ \n, ğ‘£\nğ‘‘\nğœ… \n)âˆ•ğœŒ\n) \n(9)\nwhere ğ‘ ğ‘–ğ‘š(â‹…, â‹…) denotes the cosine similarity between two feature vec-\ntors, ğœŒ is a hyper-parameter and it is set to 0.07 following He et al.\n(2020). The multi-scale class-wise contrastive regularization loss\nğ¿\nğ‘€ğ‘ âˆ’ğ¶ğ‘… \nis defined as:\nğ¿\nğ‘€ğ‘ âˆ’ğ¶ğ‘… \n=\nğ·\nâˆ‘\nğ‘‘=1\nğ›¾\nğ‘‘ \nâ‹… ğ¿\nğ‘‘\nğ¶ğ‘… \n(10)\nFollowing Eq. (7), ğ›¾\nğ‘‘ \nis the weight at scale ğ‘‘ and the sum of ğ›¾\nğ‘‘ \nat\ndifferent scales is 1.0.\n3.3. The overall loss function\nBased on ğ¿\nğ‘€ğ‘ âˆ’ğ‘ƒ ğ· \nand ğ¿\nğ‘€ğ‘ âˆ’ğ¶ğ‘…\n, the overall loss for student training\nis:\nğ¿ = ğ¿\nğ‘ğ¶ğ¸ \n+ ğ›½\nğ‘¡\n(ğ›¼ â‹… ğ¿\nğ¶ğ‘…ğ¹ \n+ ğ¿\nğ‘€ğ‘ âˆ’ğ‘ƒ ğ· \n+ ğ¿\nğ‘€ğ‘ âˆ’ğ¶ğ‘…\n) (11)\nwhere ğ›¼ is the weight for ğ¿\nğ¶ğ‘…ğ¹ \n, following Obukhov et al. (2019), and\nbased on the result of the validation set, ğ›¼ is set to 0.01 in our method.\nğ›½\nğ‘¡ \nis a weight for the ramp-up terms. Since the teacher network uses\nthe EMA weights of successive student networks, the performance of\nthe teacher model is poor at the early stage of training. As the training\nprogresses, the teacherâ€™s performance gradually improves, which is\nleveraged by a larger ğ›½\nğ‘¡\n. Therefore ğ›½\nğ‘¡ \nramps up from 0 to its maximum\nvalue ğ›½ according to sigmoid ramp up function ğ›½\nğ‘¡ \n= ğ›½ âˆ— ğ‘’\nâˆ’5(1âˆ’ğ‘¡âˆ•ğ‘¡\nğ‘šğ‘ğ‘¥\n)\n2 \n,\nwhere ğ‘¡ denotes the current iteration number and ğ‘¡\nğ‘šğ‘ğ‘¥ \nis the maximum\niteration number for ramp-up.\n4. Experiment and results\n4.1. Dataset\nFetal MRI dataset. MRI scans of 88 fetuses in the second trimester\nwere collected by Single Shot Fast Spin Echo (SSFSE). The image size\nis 256 Ã— 256, and each volume has 25â€“100 slices with pixel size 0.60â€“\n1.24 mm and slice thickness 3.00â€“7.15 mm. Each scan has annotations\nfor two structures, i.e., the placenta and brain. The images used in\nthis work were manually annotated by a Radiologist. We used scribbles\nfor the training set and pixel-level annotations for the validation and\ntesting sets. In scribble annotations, the number of labeled foreground\npixels approximately accounts for 6% of the total foreground pixels in\nthe original dense annotations. The images were randomly split into\n40, 10, and 38 scans for training, validation, and testing, respectively.\nComputerized Medical Imaging and Graphics 116 (2024) 102416\n4\n\nY. Qu et al.\nTable 1\nComparison between our method and existing weakly-supervised methods with the same backbone of UNet (Ronneberger et al., 2015) on the Fetal MRI dataset.\nMethod Dice (%)â†‘ ASSD (mm)â†“\nPlacenta Brain Average Placenta Brain Average\npCE 77.73 Â± 9.25 87.06 Â± 5.48 82.40 Â± 6.14 5.72 Â± 4.26 3.63 Â± 2.18 4.67 Â± 2.45\nMLoss (Kim and Ye, 2019) 76.87 Â± 8.78 86.76 Â± 6.09 81.81 Â± 6.17 5.89 Â± 4.37 3.52 Â± 1.91 4.71 Â± 2.52\nTV (Luo et al., 2022b) 76.61 Â± 9.51 87.25 Â± 5.96 81.93 Â± 6.45 5.98 Â± 4.33 3.30 Â± 1.93 4.64 Â± 2.44\nUSTM (Liu et al., 2022) 80.92 Â± 6.62 88.78 Â± 4.84 84.85 Â± 4.55 4.42 Â± 3.22 3.07 Â± 2.04 3.74 Â± 2.00\nEM (Grandvalet and Bengio, 2004) 76.92 Â± 19.47 89.34 Â± 4.39 83.13 Â± 10.34 10.37 Â± 22.33 2.58 Â± 1.29 6.47 Â± 11.16\nDMPLS (Luo et al., 2022a) 78.18 Â± 7.15 86.94 Â± 7.10 82.56 Â± 5.53 7.25 Â± 5.80 2.84 Â± 1.30 5.04 Â± 3.03\nScribSD (Qu et al., 2023) 82.91 Â± 8.87 92.46 Â± 3.94 87.68 Â± 5.49 3.68 Â± 2.96 1.48 Â± 0.88 2.58 Â± 1.70\nScribSD+ 83.81 Â± 9.31* 92.95 Â± 3.29* 88.38 Â± 5.84* 3.37 Â± 3.19 1.36 Â± 0.84* 2.37 Â± 1.74*\n* Denotes significantly higher performance than the existing methods (ğ‘-value < 0.05 based a paired t-test).\nACDC dataset (Bernard et al., 2018). 200 short-axis cine-MRI scans\nfrom 100 patients were acquired in breath hold with a retrospective\nor prospective gating and with a Steady-state free precession (SSFP)\nsequence in short axis orientation. Each patient has two scans that are\nfor end-diastolic (ED) and end-systolic (ES) phases, respectively. And\neach scan has annotations for three structures, i.e., the right ventricle\n(RV), myocardium (Myo), and left ventricle (LV). Each volume has 6â€“\n18 slices with pixel size 0.70â€“1.20 mm and slice thickness 5.00â€“10 mm.\nThe minimum slice size is 154 Ã— 154 and the maximum is 428 Ã— 512.\nThe scribbles used in this work were manually annotated by Valvano\net al. (2021), and the number of labeled foreground pixels accounts for\naround 12% of all foreground pixels in the original dense annotations.\nThe images were split at patient level into 70%, 10% and 20% for\ntraining, validation and testing. Scribbles were used for the training\nset and pixel-level annotations were used for the validation and testing\nsets.\n4.2. Implementation details\nThe experiments were conducted on a Ubuntu desktop with PyMIC\n(Wang et al., 2023) and an NVIDIA GeForce RTX 3090Ti GPU. For\nthese two datasets, due to the large slice thickness, we employed 2D\nCNNs for slice-level segmentation and stacked the results in a volume\nfor 3D evaluation. 2D UNet (Ronneberger et al., 2015) was used for\nsegmentation, with Adam optimizer for training. Dropout was used in\nthe two low-resolution levels and the dropout ratio was 0.5. For both\ndatasets, Z-score Normalization was applied during preprocessing. For\nthe Fetal MRI dataset, random flipping and random cropping were used\nfor data augmentation. The learning rate was initialized as 0.001 and\nreduced by half for every 150 epochs. The batch size was 4 and the\ntotal epoch number was 800. The maximum iteration number for ramp-\nup ğ‘¡\nğ‘šğ‘ğ‘¥ \nwas 5000. For the ACDC dataset, we used random flipping\nand random cropping for data augmentation. The ReduceLROnPlateau\nlearning rate strategy was used to adjust the learning rate online. The\nbatch size was 4 and the total epoch number was 600. The maximum\niteration number for ramp-up ğ‘¡\nğ‘šğ‘ğ‘¥ \nwas 15000. Based on the best\nperformance on the validation set, the hyper-parameter setting was:\nğœ\nğ‘šğ‘ğ‘¥ \n= 5, ğœ\nğ‘šğ‘–ğ‘› \n= 2, ğ›¼ = 0.01, ğ¶\nâ€² \n= 16, ğ›½ = 1, ğ· = 3, ğ›¾\n1 \n= 0.5, ğ›¾\n2 \n= 0.3,\nand ğ›¾\n3 \n= 0.2. Following Obukhov et al. (2019), the parameters for CRF\nloss were ğœ” = 1, ğœ\nğ» \n= 5 and ğœ\nğ¼ \n= 0.1. For quantitative evaluation of\nthe 3D segmentation results, we measured the Dice score and Average\nSymmetric Surface Distance (ASSD).\n4.3. Comparison with state-of-the-art methods\nOur method was compared with seven existing methods: (1) only\npCE, (2) Dynamically Mixed Pseudo Labels Supervision (DMPLS) (Luo\net al., 2022a), (3) Entropy Minimization (EM) (Grandvalet and Bengio,\n2004), (4) Mumfordâ€“Shah Loss (MLoss) (Kim and Ye, 2019), (5) Total\nVariation (TV) (Luo et al., 2022b) for regularization, (6) USTM (Liu\net al., 2022) and (7) ScribSD that denotes our previous method (Qu\net al., 2023). Note that ScribSD uses prediction distillation only for\nthe highest resolution level, i.e., single-scale, and it maximizes the\nfeature similarity between the teacher and the student, rather than\nusing class-wise contrastive regularization.\n4.3.1. Results on the fetal MRI dataset\nWe compared our method with the above-mentioned methods on\nthe fetal MRI dataset, and the results are shown in Table 1. Using\npCE loss only obtained an average Dice of 77.73% and 87.06% for\nthe placenta and fetal brain, respectively. Our method achieved the\nbest performance and improved it to 83.81% and 92.95% for the\ntwo classes, respectively. Compared with USTM (Liu et al., 2022), our\nmethod significantly improved the average Dice by 3.53 percent points\nand reduced the ASSD by 1.37 mm, with ğ‘-value < 0.05 according to\na paired t-test. Fig. 3 presents a visual comparison of segmentation\nresults, where red and green colors show the placenta and fetal brain,\nrespectively. It shows that the segmentation result obtained by our\nproposed method was the closest to the ground truth. In contrast,\nthe other methods obtained more under-segmented or over-segmented\nregions, as highlighted by yellow arrows. This is due to the design\nof our Ms-PD and Ms-CR, which enables the network to learn fine\nclass-wise feature information, thereby improving the distinctiveness\nof different classes for better performance.\n4.3.2. Results on the ACDC dataset\nTable 2 presents a quantitative comparison on the ACDC dataset.\nUsing pCE loss, the average Dice score was only 72.76% and the ASSD\nis as high as 38.78 mm, mainly due to a lot of over-segmentations. Our\nmethod achieved the best performance on all two evaluation metrics,\nimproving the average Dice score to 88.45% and decreasing the av-\nerage ASSD to 2.52 mm, significantly outperforming other methods.\nCompared with DMPLS (Luo et al., 2022a), our method significantly\nimproved the average Dice by 2.03 percent points and reduced the\nASSD by 3.76 mm. It can be seen from the visual comparison of seg-\nmentation results shown in Fig. 4 that the segmentation result obtained\nby our proposed method was more accurate in the boundary regions\nand was the closest to the ground truth. The segmentation results of\nother methods suffered from over-segmentation or under-segmentation,\nespecially in boundary regions.\n4.4. Ablation study\nWe introduced the multi-scale prediction distillation and class-wise\ncontrastive regularization strategies in the network to improve seg-\nmentation performance in the proposed method. This section will first\nintroduce ablation experiments to evaluate the effectiveness of different\nsupervision strategies on our framework. Then, we explored the sensi-\ntivity of hyper-parameters ğ·, ğœ\nğ‘šğ‘ğ‘¥\n, ğœ\nğ‘šğ‘–ğ‘›\n, ğ›½ respectively. All experiments\nwere performed on the fetal MRI dataset.\n4.4.1. Effectiveness of different supervision strategies\nTo investigate the effectiveness of each of the supervision terms\nused in our method, we set pCE as the baseline, and gradually added\nthe CRF loss, Ms-PD loss, and Ms-CR loss, respectively. The PD loss\nand CR loss refer to single-scale (for the highest resolution level only)\nprediction distillation and contrastive regularization respectively. The\nPD loss was also compared with an MSE loss applied to the probability\nComputerized Medical Imaging and Graphics 116 (2024) 102416\n5\n\nY. Qu et al.\nFig. 3. Qualitative comparison of our proposed method and several existing methods on the fetal MRI dataset.\nTable 2\nComparison between our method and existing weakly-supervised methods with the same backbone of UNet (Ronneberger et al., 2015) on the ACDC dataset.\nMethod Dice (%)â†‘ ASSD (mm)â†“\nRV Myo LV Average RV Myo LV Average\npCE 79.20 Â± 10.16 64.95 Â± 10.27 74.12 Â± 16.60 72.76 Â± 8.73 20.02 Â± 23.88 46.47 Â± 18.18 49.84 Â± 27.21 38.78 Â± 15.03\nMLoss (Kim and Ye, 2019) 69.87 Â± 9.73 62.22 Â± 8.58 72.09 Â± 15.83 68.06 Â± 8.86 43.96 Â± 22.77 45.41 Â± 18.39 53.56 Â± 27.01 47.64 Â± 18.10\nTV (Luo et al., 2022b) 79.00 Â± 9.40 75.22 Â± 7.28 87.91 Â± 6.78 80.71 Â± 5.97 23.06 Â± 16.57 22.79 Â± 13.07 15.31 Â± 14.38 20.39 Â± 11.89\nUSTM (Liu et al., 2022) 84.99 Â± 7.37 76.77 Â± 6.83 88.22 Â± 6.12 83.33 Â± 4.15 7.48 Â± 5.04 19.82 Â± 13.28 10.15 Â± 10.90 12.48 Â± 7.94\nEM (Grandvalet and Bengio, 2004) 85.22 Â± 7.30 81.76 Â± 5.83 89.75 Â± 6.50 85.58 Â± 4.98 7.93 Â± 9.98 8.81 Â± 8.50 8.82 Â± 12.48 8.52 Â± 8.88\nDMPLS (Luo et al., 2022a) 86.44 Â± 6.91 82.22 Â± 6.23 90.62 Â± 6.32 86.42 Â± 4.39 4.74 Â± 4.39 7.51 Â± 9.08 6.61 Â± 8.54 6.28 Â± 6.63\nScribSD (Qu et al., 2023) 86.62 Â± 6.78 84.02 Â± 5.96 92.05 Â± 4.20 87.56 Â± 3.77 3.88 Â± 5.83 5.05 Â± 9.03 4.01 Â± 7.64 4.31 Â± 6.55\nScribSD+ 87.33 Â± 6.54 85.39 Â± 5.53* 92.64 Â± 4.23* 88.45 Â± 3.97* 2.56 Â± 3.03* 2.63 Â± 7.68* 2.37 Â± 6.61* 2.52 Â± 5.47*\n* Denotes significantly higher performance than the existing methods (ğ‘-value < 0.05 based a paired t-test).\nTable 3\nAblation study of different supervision strategies on the fetal MRI dataset. PD and CR refer to single-scale prediction distillation and contrastive regularization respectively. MSE\ncorresponds to the typical mean teacher method.\nLoss functions Dice (%)â†‘ ASSD (mm)â†“\npCE CRF MSE PD CR Ms-PD Ms-CR Placenta Brain Average Placenta Brain Average\nâœ“ 81.24 Â± 6.45 83.81 Â± 9.46 82.53 Â± 6.37 4.83 Â± 2.71 7.07 Â± 8.92 5.95 Â± 4.64\nâœ“ âœ“ 79.34 Â± 11.10 88.26 Â± 5.28 83.80 Â± 6.14 9.07 Â± 12.24 2.62 Â± 1.33 5.85 Â± 6.00\nâœ“ âœ“ âœ“ 79.87 Â± 11.36 87.15 Â± 7.29 83.51 Â± 6.49 8.04 Â± 12.55 2.77 Â± 1.95 5.40 Â± 6.09\nâœ“ âœ“ âœ“ 83.66 Â± 4.76 87.53 Â± 6.03 85.59 Â± 4.31 3.75 Â± 1.56 2.70 Â± 1.42 3.22 Â± 0.96\nâœ“ âœ“ âœ“ âœ“ 86.38 Â± 3.96 90.15 Â± 6.13 88.27 Â± 4.34 3.18 Â± 1.46 2.45 Â± 1.90 2.81 Â± 1.23\nâœ“ âœ“ âœ“ âœ“ 86.60 Â± 3.59 92.16 Â± 3.59 89.38 Â± 3.02 3.03 Â± 1.40 1.64 Â± 0.86 2.33 Â± 0.86\nTable 4\nAblation study of different setting of hyper-parameter ğ· and the corresponding ğ›¾\nğ‘‘ \non the fetal MRI dataset.\nğ· ğ›¾\nğ‘‘ \nDice (%)â†‘ ASSD (mm)â†“\nğ›¾\n1 \nğ›¾\n2 \nğ›¾\n3 \nğ›¾\n4 \nğ›¾\n5 \nPlacenta Brain Average Placenta Brain Average\n1 1 86.38 Â± 3.96 90.15 Â± 6.13 88.27 Â± 4.34 3.18 Â± 1.46 2.45 Â± 1.90 2.81 Â± 1.23\n2 0.7 0.3 85.63 Â± 4.44 90.96 Â± 5.56 88.29 Â± 4.07 3.16 Â± 1.51 1.92 Â± 1.35 2.54 Â± 0.90\n3 0.5 0.3 0.2 86.60 Â± 3.59 92.16 Â± 3.59 89.38 Â± 3.02 3.03 Â± 1.40 1.64 Â± 0.86 2.33 Â± 0.86\n4 0.4 0.3 0.2 0.1 85.96 Â± 4.02 90.79 Â± 5.63 88.38 Â± 4.08 2.97 Â± 1.28 1.95 Â± 1.36 2.46 Â± 0.86\n5 0.3 0.3 0.2 0.1 0.1 85.14 Â± 5.05 90.77 Â± 5.33 87.96 Â± 4.55 3.25 Â± 1.51 1.91 Â± 1.20 2.58 Â± 0.96\npredictions obtained by standard Softmax, which corresponds to the\ntypical mean teacher method (Tarvainen and Valpola, 2017). The\nresults of different settings of the training loss on the validation set are\nshown in Table 3. It can be observed that pCE only obtained an average\nDice of 82.53% and introducing the CRF loss improved it to 83.80%.\nApplying single-scale prediction distillation (PD) further improved it\nto 85.59%, while replacing PD with MSE led to a lower performance\n(83.51%). On this basis, applying single-scale contrastive regularization\nComputerized Medical Imaging and Graphics 116 (2024) 102416\n6\n\nY. Qu et al.\nFig. 4. Qualitative comparison of our proposed method and several existing methods on the ACDC dataset.\nTable 5\nAblation study of ğœ with different setting strategies on the fetal MRI dataset.\nSetting strategies of ğœ Dice (%)â†‘ ASSD (mm)â†“\nPlacenta Brain Average Placenta Brain Average\nFixed value \nğœ = 2 86.37 Â± 3.68 89.17 Â± 7.24 87.77 Â± 4.80 2.92 Â± 1.32 3.49 Â± 4.04 3.21 Â± 2.20\nğœ = 5 85.33 Â± 4.42 90.68 Â± 5.31 88.00 Â± 4.24 3.12 Â± 1.28 1.91 Â± 1.26 2.51 Â± 0.88\nCosine ramp down \nğœ\nğ‘šğ‘ğ‘¥ \n= 5, ğœ\nğ‘šğ‘–ğ‘› \n= 1 86.33 Â± 4.02 91.03 Â± 5.07 88.68 Â± 3.80 3.10 Â± 1.48 2.00 Â± 1.30 2.55 Â± 1.10\nğœ\nğ‘šğ‘ğ‘¥ \n= 5, ğœ\nğ‘šğ‘–ğ‘› \n= 2 86.60 Â± 3.59 92.16 Â± 3.59 89.38 Â± 3.02 3.03 Â± 1.40 1.64 Â± 0.86 2.33 Â± 0.86\nFig. 5. Sensitivity analysis of hyper-parameters ğœ\nğ‘šğ‘ğ‘¥ \nand ğ›½ respectively on the fetal MRI dataset.\n(CR) further improved average Dice to 88.27%. Finally, compared with\nsingle-scale PD and CR, combining CRF loss with our Ms-PD and Ms-CR\nled to the best average Dice of 89.38% with ASSD of 2.33 mm, showing\nthe effectiveness of the proposed multi-scale prediction distillation and\nclass-wise contrastive regularization.\n4.4.2. Sensitivity analysis of hyper-parameters\nIn order to explore the impact of multi-scale PD and CR on model\nsegmentation performance, we set the value of ğ· from 1 to 5 to conduct\nablation experiments. For each value of ğ·, ğ›¾\nğ‘‘ \nis the weight at scale ğ‘‘,\nand its sum is 1.0. As a smaller ğ‘‘ corresponds to a higher resolution,\nComputerized Medical Imaging and Graphics 116 (2024) 102416\n7\n\nY. Qu et al.\nwe made ğ›¾\nğ‘‘ \ngradually decrease as ğ‘‘ increases empirically. The results\nof different settings of the hyper-parameter ğ· on the validation set\nare shown in Table 4. ğ· = 2, 3, 4 performed better than ğ· = 1,\nshowing that supervision at multiple scales helps the model learn more\ncomprehensive information. The best performance was obtained when\nğ· = 3. And ğ· = 5, the model segmentation performed the worst. This\nmay be due to that the bottleneck does not include sufficient spatial\ndetails for segmentation tasks.\nFor the ablation experiment of ğœ, we first investigated different\nmethods for setting ğœ: (1) Setting it as a fixed value of 2 and 5\nrespectively, and (2) Cosine ramp down that gradually deceases ğœ from\nğœ\nğ‘šğ‘ğ‘¥ \n=5 to ğœ\nğ‘šğ‘–ğ‘› \n= 1 and ğœ\nğ‘šğ‘–ğ‘› \n= 2, respectively. The results shown in\nTable 5 demonstrate that our method using the cosine ramp down\nstrategy performed better than using a fixed ğœ value. This is because\nthe teacherâ€™s performance is poor in the early stage of training, and\nsoft labels obtained through a large ğœ are more robust to noise. As\ntraining proceeds, the teacherâ€™s performance gets better, and the soft\nlabels obtained with smaller ğœ at this time contain more accurate class\nmember information. Table 5 also shows that ğœ\nğ‘šğ‘–ğ‘› \n= 2 performed better\nthan ğœ\nğ‘šğ‘–ğ‘› \n= 1, which is mainly due to that ğœ\nğ‘šğ‘–ğ‘› \n= 1 corresponds to a\nstandard Softmax that is less noise-robust for knowledge distillation.\nThen we investigated the effect of ğœ\nğ‘šğ‘ğ‘¥ \nby setting ğœ = ğœ\nğ‘šğ‘ğ‘¥ \nfor different\nğœ\nğ‘šğ‘ğ‘¥ \nvalues. Fig. 5(a) shows that the performance gradually improved\nwhen ğœ\nğ‘šğ‘ğ‘¥ \nchanges from 1.0 to 5.0, demonstrating that distillation with\na softer prediction based on ğœ-Softmax is more effective than a standard\nSoftmax (ğœ = 1) considering the noise in the teacherâ€™s output. However,\nwhen ğœ > 5.0, the performance deteriorated, which is mainly because\nan extremely soft (i.e., highly uncertain) prediction is less informative\nabout the class membership of pixels.\nWe then investigated the sensitivity of ğ›½ that controls ğ›½\nğ‘¡ \nin Eq. (11).\nFig. 5(b) shows that the highest Dice was achieved when ğ›½ = 1.\nNote that poor performance was obtained when setting ğ›½ to very large\nvalues, which enforces the student to over-fit the teacher and limits its\nperformance.\n5. Discussion and conclusion\nOur ScribSD+ framework is an end-to-end method for scribble-\nsupervised medical image segmentation that combines the advantages\nof multi-scale prediction distillation and class-wise contrastive regu-\nlarization. First, for multi-scale prediction distillation, we use the soft\npredictions obtained by the teacher to supervise the training of the\nstudent at multiple scales because soft predictions are more robust to\nnoise and more informative. It is superior to some methods that directly\nuse hard pseudo labels to supervise training without considering the\ninfluence of noise. For example, DMPLS (Luo et al., 2022a) generated\nhard pseudo labels by randomly mixing the predictions of two decoders,\nwhich will limit the learning process due to the influence of noise. Sec-\nondly, for multi-scale class-wise contrastive regularization, we extract\nclass-wise features from feature maps and their corresponding soft pre-\ndictions, constructing positive and negative pairs between the teacher\nand student based on different classes. We then supervise the studentâ€™s\ntraining using contrastive loss, aiming to learn a more compact and\ndistinctive class feature distribution. In addition, compared to classical\nKD methods (Hinton et al., 2015; Xu et al., 2020) that usually require\npre-training a teacher model, we employ a self-distillation approach\nthat effectively updates the teacher in real-time using the EMA of the\nmodel parameters.\nThis work still has some limitations that could be addressed in\nthe future. First, in terms of computational efficiency, due to the\nuse of a teacher and a student network with multi-scale predictions,\nour method has more computational cost and memory consumption\nduring training than methods using a single model. For example, the\ntraining time of our method was 42 min on the fetal MRI dataset,\nwhile that for pCE was 25 min. The GPU memory consumption of\nour method and pCE was 9655 MB and 5629 MB on the fetal MRI\ndataset, respectively. However, at the inference stage, our method only\nuses the student model and obtains predictions only for the highest\nresolution level, which makes it have the same inference time as that\nfor pCE. The average inference time across a volume was 3.2 s and\n2.1 s on the fetal MRI and ACDC datasets, respectively. Secondly, the\nvalue of ğœ in the ğœ-Softmax needs to be determined based on the effect\nof the ablation experiments on the validation set, which is relatively\ncumbersome. It would be more efficient to explore the value of ğœ that\nchanges adaptively as training progresses based on available informa-\ntion (e.g., features or predictions) (Xu et al., 2020). In addition, we\nemployed 2D UNet for slice-level segmentation in this work. Actually,\nour proposed framework is not dependent on any specific backbone\nnetwork. It is of interest to use other types of backbone networks, such\nas 3D UNet and TransUNet, for segmentation tasks.\nIn summary, we proposed a self-distillation method (ScribSD+)\nfor weakly supervised medical image segmentation based on scribble\nannotations. The proposed KD strategy combines multi-scale prediction\ndistillation based on soft predictions to deal with noisy pseudo labels\nand multi-scale class-wise contrastive regularization that enables the\nmodel to learn more comprehensive and compact feature distribution\ninformation through class-wise contrastive loss. Our method was eval-\nuated on the public ACDC dataset for heart structure segmentation\nand a fetal brain and placenta segmentation dataset. Experimental\nresults demonstrated the superiority of our method over five state-\nof-the-art scribble-supervised methods. It suggested that our method\nhas the potential for achieving accurate segmentation models based on\nscribbles, which can largely reduce the annotation cost for developing\ndeep learning models in computer-aided diagnosis systems.\nCRediT authorship contribution statement\nYijie Qu: Writing â€“ original draft. Tao Lu: Writing â€“ review &\nediting. Shaoting Zhang: Writing â€“ review & editing. Guotai Wang:\nWriting â€“ review & editing.\nDeclaration of competing interest\nThe authors declare that they have no known competing finan-\ncial interests or personal relationships that could have appeared to\ninfluence the work reported in this paper.\nData availability\nData will be made available on request.\nAcknowledgments\nThis work was supported by the National Natural Science Founda-\ntion of China under Grant 62271115.\nReferences\nAmmar, A., Bouattane, O., Youssfi, M., 2021. Automatic cardiac cine MRI segmentation\nand heart disease classification. Comput. Med. Imaging Graph. 88, 101864.\nBernard, O., Lalande, A., Zotti, C., Cervenansky, F., Yang, X., Heng, P.-A., Cetin, I.,\nLekadir, K., Camara, O., Ballester, M.A.G., et al., 2018. Deep learning techniques for\nautomatic MRI cardiac multi-structures segmentation and diagnosis: is the problem\nsolved? IEEE Trans. Med. Imaging 37 (11), 2514â€“2525.\nChaitanya, K., Erdil, E., Karani, N., Konukoglu, E., 2020. Contrastive learning of\nglobal and local features for medical image segmentation with limited annotations.\nNeurIPS 33, 12546â€“12558.\nChen, J., Fang, Z., Zhang, G., Ling, L., Li, G., Zhang, H., Wang, L., 2021a. Automatic\nbrain extraction from 3D fetal MR image with deep learning-based multi-step\nframework. Comput. Med. Imaging Graph. 88, 101848.\nChen, Q., Hong, Y., 2022. Scribble2D5: Weakly-supervised volumetric image segmenta-\ntion via scribble annotations. In: MICCAI. Springer Nature Switzerland, Cham, pp.\n234â€“243.\nChen, T., Kornblith, S., Norouzi, M., Hinton, G., 2020. A simple framework for\ncontrastive learning of visual representations. In: International Conference on\nMachine Learning. PMLR, pp. 1597â€“1607.\nComputerized Medical Imaging and Graphics 116 (2024) 102416\n8\n\nY. Qu et al.\nChen, J., Lu, Y., Yu, Q., Luo, X., Adeli, E., Wang, Y., Lu, L., Yuille, A.L., Zhou, Y., 2021b.\nTransunet: Transformers make strong encoders for medical image segmentation.\narXiv preprint arXiv:2102.04306.\nÃ‡iÃ§ek, Ã–., Abdulkadir, A., Lienkamp, S.S., Brox, T., Ronneberger, O., 2016. 3D U-\nNet: learning dense volumetric segmentation from sparse annotation. In: MICCAI.\nSpringer, pp. 424â€“432.\nDolz, J., Desrosiers, C., Ayed, I.B., 2021. Teach me to segment with mixed supervision:\nConfident students become masters. In: IPMI. Springer, pp. 517â€“529.\nDorent, R., Joutard, S., Shapey, J., Kujawa, A., Modat, M., Ourselin, S., Vercauteren, T.,\n2021. Inter extreme points geodesics for end-to-end weakly supervised image\nsegmentation. In: MICCAI. Springer, pp. 615â€“624.\nGrandvalet, Y., Bengio, Y., 2004. Semi-supervised learning by entropy minimization.\nNeurIPS 17, 281â€“296.\nGu, R., Zhang, J., Wang, G., Lei, W., Song, T., Zhang, X., Li, K., Zhang, S., 2022.\nContrastive semi-supervised learning for domain adaptive segmentation across\nsimilar anatomical structures. IEEE Trans. Med. Imaging 42 (1), 245â€“256.\nHadsell, R., Chopra, S., LeCun, Y., 2006. Dimensionality reduction by learning an\ninvariant mapping. In: CVPR. Vol. 2, IEEE, pp. 1735â€“1742.\nHe, K., Fan, H., Wu, Y., Xie, S., Girshick, R., 2020. Momentum contrast for unsupervised\nvisual representation learning. In: CVPR. pp. 9729â€“9738.\nHinton, G., Vinyals, O., Dean, J., 2015. Distilling the knowledge in a neural network.\nIn: NeurIPS. pp. 1â€“9.\nIsensee, F., Jaeger, P.F., Kohl, S.A., Petersen, J., Maier-Hein, K.H., 2021. nnU-Net: a\nself-configuring method for deep learning-based biomedical image segmentation.\nNature Methods 18 (2), 203â€“211.\nKang, G., Jiang, L., Yang, Y., Hauptmann, A.G., 2019. Contrastive adaptation network\nfor unsupervised domain adaptation. In: CVPR. pp. 4893â€“4902.\nKervadec, H., Dolz, J., Tang, M., Granger, E., Boykov, Y., Ayed, I.B., 2019.\nConstrained-CNN losses for weakly supervised segmentation. Med. Image Anal. 54,\n88â€“99.\nKim, B., Ye, J.C., 2019. Mumfordâ€“Shah loss functional for image segmentation with\ndeep learning. IEEE Trans. Image Process. 29, 1856â€“1866.\nKushnure, D.T., Talbar, S.N., 2021. MS-UNet: A multi-scale UNet with feature recalibra-\ntion approach for automatic liver and tumor segmentation in CT images. Comput.\nMed. Imaging Graph. 89, 101885.\nLee, H., Jeong, W.-K., 2020. Scribble2label: Scribble-supervised cell segmentation via\nself-generating pseudo-labels with consistency. In: MICCAI. Springer, pp. 14â€“23.\nLei, W., Xu, W., Gu, R., Fu, H., Zhang, S., Zhang, S., Wang, G., 2021. Contrastive\nlearning of relative position regression for one-shot object localization in 3D\nmedical images. In: MICCAI. Springer, pp. 155â€“165.\nLin, D., Dai, J., Jia, J., He, K., Sun, J., 2016. Scribblesup: Scribble-supervised\nconvolutional networks for semantic segmentation. In: CVPR. pp. 3159â€“3167.\nLiu, X., Yuan, Q., Gao, Y., He, K., Wang, S., Tang, X., Tang, J., Shen, D., 2022.\nWeakly supervised segmentation of COVID19 infection with scribble annotation\non CT images. Pattern Recognit. 122, 108341.\nLuo, X., Hu, M., Liao, W., Zhai, S., Song, T., Wang, G., Zhang, S., 2022a. Scribble-\nsupervised medical image segmentation via dual-branch network and dynamically\nmixed pseudo labels supervision. In: MICCAI. Springer, pp. 528â€“538.\nLuo, X., Liao, W., Xiao, J., Chen, J., Song, T., Zhang, X., Li, K., Metaxas, D.N., Wang, G.,\nZhang, S., 2022b. WORD: A large scale dataset, benchmark and clinical applicable\nstudy for abdominal organ segmentation from CT image. Med. Image Anal. 82,\n102642.\nMilletari, F., Navab, N., Ahmadi, S.-A., 2016. V-net: Fully convolutional neural\nnetworks for volumetric medical image segmentation. In: 2016 Fourth International\nConference on 3D Vision. 3DV, IEEE, pp. 565â€“571.\nMÃ¼ller, R., Kornblith, S., Hinton, G.E., 2019. When does label smoothing help? In:\nNeurIPS. pp. 1â€“10.\nObukhov, A., Georgoulis, S., Dai, D., Van Gool, L., 2019. Gated CRF loss for weakly\nsupervised semantic image segmentation. In: NeurIPS. pp. 1â€“9.\nOu, Y., Huang, S.X., Wong, K.K., Cummock, J., Volpi, J., Wang, J.Z., Wong, S.T., 2023.\nBBox-Guided Segmentor: Leveraging expert knowledge for accurate stroke lesion\nsegmentation using weakly supervised bounding box prior. Comput. Med. Imaging\nGraph. 107, 102236.\nQu, Y., Zhao, Q., Wei, L., Lu, T., Zhang, S., Wang, G., 2023. ScribSD: Scribble-\nsupervised fetal MRI segmentation based on simultaneous feature and prediction\nself-distillation. In: Workshop on Medical Image Learning with Limited and Noisy\nData. Springer, pp. 14â€“23.\nRajchl, M., Lee, M.C., Oktay, O., Kamnitsas, K., Passerat-Palmbach, J., Bai, W.,\nDamodaram, M., Rutherford, M.A., Hajnal, J.V., Kainz, B., et al., 2016. Deepcut:\nObject segmentation from bounding box annotations using convolutional neural\nnetworks. IEEE Trans. Med. Imaging 36 (2), 674â€“683.\nRonneberger, O., Fischer, P., Brox, T., 2015. U-Net: Convolutional networks for\nbiomedical image segmentation. In: MICCAI. Springer, pp. 234â€“241.\nTang, M., Djelouah, A., Perazzi, F., Boykov, Y., Schroers, C., 2018a. Normalized cut\nloss for weakly-supervised CNN segmentation. In: CVPR. pp. 1818â€“1827.\nTang, M., Perazzi, F., Djelouah, A., Ben Ayed, I., Schroers, C., Boykov, Y., 2018b. On\nregularized losses for weakly-supervised CNN segmentation. In: ECCV. pp. 507â€“522.\nTarvainen, A., Valpola, H., 2017. Mean teachers are better role models: Weight-\naveraged consistency targets improve semi-supervised deep learning results. In:\nNeurIPS. pp. 1195â€“1204.\nValvano, G., Leo, A., Tsaftaris, S.A., 2021. Learning to segment from scribbles\nusing multi-scale adversarial attention gates. IEEE Trans. Med. Imaging 40 (8),\n1990â€“2001.\nWang, G., Luo, X., Gu, R., Yang, S., Qu, Y., Zhai, S., Zhao, Q., Li, K., Zhang, S.,\n2023. PyMIC: A deep learning toolkit for annotation-efficient medical image\nsegmentation. Comput. Methods Programs Biomed. 231, 107398.\nWu, K., Du, B., Luo, M., Wen, H., Shen, Y., Feng, J., 2019. Weakly supervised brain\nlesion segmentation via attentional representation learning. In: MICCAI. Springer,\npp. 211â€“219.\nXu, K., Rui, L., Li, Y., Gu, L., 2020. Feature normalized knowledge distillation for image\nclassification. In: ECCV. Springer, pp. 664â€“680.\nYou, C., Zhao, R., Staib, L.H., Duncan, J.S., 2022. Momentum contrastive voxel-\nwise representation learning for semi-supervised volumetric medical image\nsegmentation. In: MICCAI. Springer, pp. 639â€“652.\nZhai, S., Wang, G., Luo, X., Yue, Q., Li, K., Zhang, S., 2023. PA-Seg: Learning from point\nannotations for 3D medical image segmentation using contextual regularization and\ncross knowledge distillation. IEEE Trans. Med. Imaging 42 (8), 2235â€“2246.\nZhang, K., Zhuang, X., 2022. ShapePU: A new PU learning framework regularized\nby global consistency for scribble supervised cardiac segmentation. In: MICCAI.\nSpringer, pp. 162â€“172.\nZhao, X., Qi, Z., Wang, S., Wang, Q., Wu, X., Mao, Y., Zhang, L., 2024. RCPS: Rectified\ncontrastive pseudo supervision for semi-supervised medical image segmentation.\nIEEE J. Biomed. Health Inf. 28 (1), 251â€“261. http://dx.doi.org/10.1109/JBHI.2023.\n3322590.\nZhou, M., Xu, Z., Zhou, K., Tong, R.K.-y., 2023. Weakly supervised medical image\nsegmentation via superpixel-guided scribble walking and class-wise contrastive\nregularization. In: MICCAI. Springer, pp. 137â€“147.\nZou, B., Zhou, Z., Han, Y., Li, K., Wang, G., 2023. Multi-scale convolution-transformer\nfusion network for endoscopic image segmentation. In: 2023 IEEE 20th Inter-\nnational Symposium on Biomedical Imaging. ISBI, pp. 1â€“5. http://dx.doi.org/10.\n1109/ISBI53787.2023.10230738.\nComputerized Medical Imaging and Graphics 116 (2024) 102416\n9",
    "version": "5.3.31"
  },
  {
    "numpages": 12,
    "numrender": 12,
    "info": {
      "PDFFormatVersion": "1.7",
      "Language": "en-US",
      "EncryptFilterName": null,
      "IsLinearized": true,
      "IsAcroFormPresent": false,
      "IsXFAPresent": false,
      "IsCollectionPresent": false,
      "IsSignaturesPresent": false,
      "CreationDate": "D:20221110184749+00'00'",
      "Creator": "Elsevier",
      "Custom": {
        "GTS_PDFA1Version": "PDF/A-1b:2005",
        "PTEX.Fullbanner": "This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2"
      },
      "ModDate": "D:20221110184749+00'00'",
      "Producer": "pdfTeX",
      "Subject": "Knowledge-Based Systems, 258 (2022) 109988. doi:10.1016/j.knosys.2022.109988",
      "Trapped": { "name": "False" }
    },
    "metadata": {
      "dc:format": "application/pdf",
      "dc:title": "Volume-awareness and outlier-suppression co-training for weakly-supervised MRI breast mass segmentation with partial annotations",
      "dc:creator": [
        "Xianqi Meng",
        "Jingfan Fan",
        "Hongwei Yu",
        "Jinrong Mu",
        "Zongyu Li",
        "Aocai Yang",
        "Bing Liu",
        "Kuan Lv",
        "Danni Ai",
        "Yucong Lin",
        "Hong Song",
        "Tianyu Fu",
        "Deqiang Xiao",
        "Guolin Ma",
        "Jian Yang",
        "Ying Gu"
      ],
      "dc:subject": [
        "Breast mass",
        "Weakly-supervised learning",
        "Image segmentation",
        "Partial annotation"
      ],
      "dc:description": "Knowledge-Based Systems, 258 (2022) 109988. doi:10.1016/j.knosys.2022.109988",
      "prism:aggregationtype": "journal",
      "prism:publicationname": "Knowledge-Based Systems",
      "prism:copyright": "Â© 2022 Published by Elsevier B.V.",
      "dc:publisher": "Elsevier B.V.",
      "prism:issn": "0950-7051",
      "prism:volume": "258",
      "prism:coverdisplaydate": "22 December 2022",
      "prism:coverdate": "2022-12-22",
      "prism:pagerange": "109988",
      "prism:startingpage": "109988",
      "prism:doi": "10.1016/j.knosys.2022.109988",
      "prism:url": "https://doi.org/10.1016/j.knosys.2022.109988",
      "dc:identifier": "doi:10.1016/j.knosys.2022.109988",
      "jav:journal_article_version": "VoR",
      "crossmark:majorversiondate": "2010-04-23",
      "crossmark:crossmarkdomainexclusive": "true",
      "crossmark:doi": "10.1016/j.knosys.2022.109988",
      "crossmark:crossmarkdomains": "elsevier.comsciencedirect.com",
      "fci27nt-pmt_8otagy9uknlnnytf8lwj9y9mgn978yt-jz9z7mmiso9eqn9iknm2nnmetma": "",
      "pdfx:doi": "10.1016/j.knosys.2022.109988",
      "pdfx:robots": "noindex",
      "pdfx:crossmarkmajorversiondate": "2010-04-23",
      "pdfx:crossmarkdomainexclusive": "true",
      "pdfx:crossmarkdomains": "sciencedirect.comelsevier.com",
      "xap:creatortool": "Elsevier",
      "xaprights:marked": "True"
    },
    "text": "Knowledge-Based Systems 258 (2022) 109988\nContents lists available at ScienceDirect\nKnowledge-Based Systems\njournal homepage: www.elsevier.com/locate/knosys\nVolume-awareness and outlier-suppression co-training for\nweakly-supervised MRI breast mass segmentation with partial\nannotations\nXianqi Meng \na\n, Jingfan Fan \na,\nâˆ—\n, Hongwei Yu \nb\n, Jinrong Mu \na\n, Zongyu Li \nc\n, Aocai Yang \nb\n,\nBing Liu \nb\n, Kuan Lv \nb\n, Danni Ai \na\n, Yucong Lin \nc\n, Hong Song \nd\n, Tianyu Fu \nc\n, Deqiang Xiao \na\n,\nGuolin Ma \nb,\nâˆ—\n, Jian Yang \na\n, Ying Gu \ne,\nâˆ—\na \nBeijing Engineering Research Center of Mixed Reality and Advanced Display, School of Optics and Photonics, Beijing Institute of\nTechnology, Beijing 100081, China\nb \nDepartment of Radiology, China-Japan Friendship Hospital, Beijing 100029, China\nc \nSchool of Medical Technology, Beijing Institute of Technology, Beijing 100081, China\nd \nSchool of Computer Science & Technology, Beijing Institute of Technology, Beijing 100081, China\ne \nDepartment of Laser Medicine, the First Medical Centre, Chinese PLA General Hospital, Beijing 100853, China\na r t i c l e i n f o\nArticle history:\nReceived 20 July 2022\nReceived in revised form 4 October 2022\nAccepted 4 October 2022\nAvailable online 12 October 2022\nKeywords:\nBreast mass\nWeakly-supervised learning\nImage segmentation\nPartial annotation\na b s t r a c t\nSegmenting breast mass from magnetic resonance imaging (MRI) scans is an important step in the\nbreast cancer diagnostic procedure for physicians and computer-aided diagnosis systems. Sufficient\nhigh-quality annotation is essential for establishing an automatic segmentation model, particularly for\nMRI breast masses with complex backgrounds and various sizes. In this study, we have proposed a\nnovel approach for training an MRI breast mass segmentation network with partial annotations and\nreinforcing it with two weakly supervised constraint losses. Specifically, following three user-friendly\npartial annotation methods were designed to alleviate annotation costs: single-slice, orthogonal slice,\nand interval slice annotations. With the guidance of partial annotations, we first introduced a volume\nawareness loss that supports the additional constraint for masses with various scales. Moreover, to\nreduce false-positive predictions, we proposed an end-to-end differentiable outlier-suppression loss\nto suppress noise activation outside the target during training. We validated our method on 140\npatients. The Dice similarity coefficient (DSC) of the proposed three partial annotation methods are\n0.674, 0.835, and 0.837 respectively. Quantitative and qualitative evaluations demonstrate that our\nmethod can achieve competitive performance compared to state-of-the-art methods with complete\nannotations.\nÂ© 2022 Published by Elsevier B.V.\n1. Introduction\nBreast cancer is one of the most prevalent cancers affect-\ning womenâ€™s health. According to the latest statistics from the\nAmerican Cancer Society, breast cancer has become the second\nleading cause of death from cancer among females, with an\nannual growth rate of approximately 0.5% [1]. Clinical studies\nsuggest that early detection and treatment before tumor metas-\ntasis are extremely valuable for improving the quality of life,\nprognosis, and survival of patients with breast cancer [2,3]. Rapid\nadvances in imaging technologies have facilitated the screen-\ning and monitoring of breast cancer. The most prevalent breast\nimaging techniques include mammography, ultrasonography, and\nâˆ— \nCorresponding authors.\nE-mail addresses: maguolin1007@qq.com (G. Ma), fjf@bit.edu.cn (J. Fan),\nguyinglaser301@163.com (Y. Gu).\nmagnetic resonance imaging (MRI) [4]. Among imaging methods,\ndynamic contrast-enhanced MRI (DCE-MRI) is widely regarded\nas the most sensitive imaging method. DCE-MRI can provide\nmulti-view visualization of the breast region and has capabil-\nity of high-resolution soft tissue imaging. Recent studies have\nshown that DCE-MRI is increasingly used in the supplementary\nexamination and clinical analysis of breast cancer [5].\nDCE-MRI breast lesion segmentation has been extensively\nused in clinical image analysis, which can help radiologists de-\nscribe the shape and location features in radiology reports and\nmake surgical plans before surgical treatment [6]. In addition,\nsome studies have performed diagnosis [7,8], prognosis [9], and\ngenomic analysis [10] based on segmentation results. However,\nDCE-MRI breast lesion segmentation still faces several challenges,\nas shown in Fig. 1. Existing studies based on strong prior con-\nstraints boost the segmentation performance for normal tis-\nsues [11â€“14], but variable sizes and uncertain locations hamper\nhttps://doi.org/10.1016/j.knosys.2022.109988\n0950-7051/Â© 2022 Published by Elsevier B.V.\n\nX. Meng, J. Fan, H. Yu et al. Knowledge-Based Systems 258 (2022) 109988\nFig. 1. Major challenges in breast lesion segmentation: (a) and (d) breast lesions\nhave variable sizes and uncertain locations, (b) and (e) the contrast between\nlesions and their background is inconsistent among different patients, (c) and\n(f) BPE leads to the enhancement of non lesion areas, where the non lesion\nenhancement area are highlighted by red arrows.\ndevelopment of effective segmentation constraint rules for breast\nlesions. In addition, breast gland tissue, which is affected by\nage and menstrual cycle, varies considerably among patients.\nFinally, background parenchymal enhancement (BPE), which may\nenhance the glands and blood vessels surrounding the lesion\nregion after the injection of a contrast agent [15], frequently\nresults in false positives in the segmentation results.\nResearchers have developed active contour-based [16,17],\ngraph cut-based [18,19], and superpixel-based [20] breast lesion\nsegmentation methods. These methods achieved promising re-\nsults for small-scale datasets. However, most of these methods\nrequire manual intervention and multistep processes; thus, they\ncannot meet the demands of large-scale dataset segmentation\nand clinical applications. Deep learning with convolutional neural\nnetworks (CNNs) has achieved excellent performance in many\nimage-analysis tasks in recent years [21â€“26]. The success of\na fully supervised learning segmentation framework depends\nheavily on the large number of precise annotations for all pixels\nin the training images. By comparing the differences between\npredictions and complete annotations, the model can learn high-\nlevel abstract features and generalization capabilities for unseen\ntesting images. Zhang et al. [27] proposed a mask-guided hierar-\nchical learning (MHL) framework for breast lesion segmentation\nusing a fully convolutional network. Wang et al. [28] designed\na tumor-sensitive synthesis module for deep networks to sup-\npress false-positive breast tumors. Wang et al. [29] proposed a\nmixed convolutional network with multi-scale context, which\ncombines the advantages of 2D and 3D networks to improve\nthe accuracy of MRI breast lesion segmentation. However, one of\nthe common challenges in these methods is the need for a large\namount of complete annotated data. A 3D medical image typi-\ncally contains hundreds of slices; hence, pixel-level annotation\nis time-consuming and labor-intensive. Moreover, even expe-\nrienced radiologists may have different annotation standards,\nleading to inter- and intra-observer variabilities.\nTo address this problem, weakly supervised image segmenta-\ntion techniques have received extensive attention. Various weak\nannotation methods have been proposed, such as image-level [30,\n31], point [11,32], bounding box [33,34], and scribbles [35â€“37],\nto guide the training process. These approaches can effectively\nreduce the time required for manual annotations. In natural im-\nage segmentation, using class activation mapping (CAM) coarse\nlocalization for guiding network training has become a successful\nparadigm for image-level weakly supervised segmentation [30,\n31]. However, breast mass segmentation is a binary classification\nFig. 2. Different forms of annotation used in our proposed partial annotation.\nFor a given image, radiologists can annotate (a) a cross-section slice, (b) the\ntypical orthogonal slices, or (c) a set of interval slices. (a) and (c) are performed\non the axial view, whereas (b) is performed on the three orthogonal views.\nRed indicates the sagittal view annotation, green indicates the coronal view\nannotation, yellow indicates the axial view annotation, black indicates the\nbackground, and gray indicates the unannotated voxels.\nproblem, wherein the targets are small and prone to errors in\nthe CAM localization step. A constrained CNN was proposed to\nguide network training using prior knowledge of organ size [11].\nZhang et al. [35] proposed a scribble-based segmentation method\ncomprising mixed augmentation and cycle consistency. These\nweakly supervised methods have achieved good performance\nin organ segmentation. However, managing breast masses of\nvariable sizes and locations is difficult. Supervising network train-\ning with pseudo-masks generated from weak annotations is a\npopular practice [32,33] and can be effectively applied to le-\nsion segmentation tasks [36,37]. Ji et al. [36] adopted the Graph\nCut [38] algorithm results and introduced CRF loss to supervise\nbrain lesion segmentation, where Graph Cut was initialized by\nscribble annotations. Zheng [37] utilized weak annotations in\nthe first training stage to generate pseudo annotations and de-\nsigned uncertainty-based boundary maps to refine boundaries in\nthe second training stage. However, these methods are suscep-\ntible to errors in pseudo-labels and may reinforce errors during\niterations, making convergence difficult. Furthermore, the coarse-\nto-fine segmentation method increases computational cost and\ntraining difficulty. Thus, we explored how to address the chal-\nlenge of breast mass segmentation with complex backgrounds\nand various sizes and proposed an end-to-end scheme, aiming for\nnearly fully supervised segmentation performance, but requiring\nthe least manual annotation effort.\nIn this study, we proposed a weakly supervised method for\nMRI breast mass segmentation with partial annotations. In con-\ntrast to complete image annotation, radiologists only need to\nannotate a few sampled 2D slices from 3D masses in the proposed\npartial annotation. To verify the effect of annotation amount on\nsegmentation performance, we proposed three partial annotation\nmethods for breast masses, which annotate voxel increase in\nturn: single-slice, orthogonal slice, and interval slice annotations\n(Fig. 2). Considering that bridging the gap between complete and\npartial annotations requires additional constraints, we proposed\ntwo loss functions to jointly enforce segmentation constraints.\nSpecifically, given that breast masses are mostly spheres or el-\nlipsoids, we proposed a volume-awareness loss that can fit mass\nvolumes of different sizes based on partial annotation features.\nFor example, the long and short axes of an annotated mass slice\ncan be used to estimate ellipsoids. Volume-awareness loss can\nassist partial cross-entropy loss to enhance the target area recog-\nnition ability of the model and reduce the effect of the unlabeled\narea on model training. Moreover, to address the difficulty in\n2\n\nX. Meng, J. Fan, H. Yu et al. Knowledge-Based Systems 258 (2022) 109988\nFig. 3. A schematic illustration of the proposed method.\ndistinguishing breast masses from BPE, we estimated the possible\nareas of masses based on partial annotations and designed an\noutlier suppression loss. When the prediction point is outside\nthe estimated area, this loss leads to high penalty values. This\napproach is not limited by the number of masses in a case and\nis trained in an end-to-end manner. Therefore, false positives are\nsuppressed without increasing the network scale or needing post-\nprocessing. The proposed weakly supervised training framework\nis illustrated in Fig. 3. The major contributions and novelties of\nthis study are summarized as follows:\nâ€¢ We designed three different partial annotation methods for\nMRI breast mass and the corresponding weakly supervised\ntraining methods, which help reduce labeling costs and fa-\ncilitate data collection in applications.\nâ€¢ We introduced a volume-awareness loss that is performed\nbetween the estimated volume using partial annotation and\npredicted volume. This loss enables the learning process to\nintegrate the global information, obtaining better feature\nrepresentations and boosting overall accuracy.\nâ€¢ We proposed an outlier-suppression loss based on the esti-\nmated mass region by partial annotation, which effectively\nreduces the false positives in segmentation resulting from\nBPE and helps improve the confidence of volume-awareness\nloss.\nâ€¢ We experimentally verified the feasibility of different partial\nannotation methods to train a breast mass segmentation\nmodel and analyze the segmentation effects. In the case of\npartial annotation, our method achieved comparable seg-\nmentation performance to models trained with complete\nannotation.\n2. Methods\nIn this study, we proposed an automatic MRI breast mass\nsegmentation method based on partial image annotation. Impo-\nsition of prior knowledge on a network output in the form of loss\nterms is a valid approach of bridging the gap between weakly\nand fully supervised segmentation. To this end, we designed\nvolume-awareness and outlier-suppression losses to improve the\nsegmentation performance based on the characteristics of partial\nannotations. Fig. 3 shows a schematic of the proposed framework.\nThe major components are presented in detail in the following\nsections.\n2.1. Partial annotation methods\nAs shown in Fig. 2, we designed three partial annotation\nmethods to collect image annotations for the network training.\nFor single-slice annotation, radiologists can annotate the most\nconfident slice for each mass; generally, a slice approximately in\nthe center of the mass is the best choice. This annotation provides\npartially precise texture and location information of the breast\nmass. Orthogonal slice annotation requires the labeling of three\nviews of each mass. Similarly, labeling close to the center of the\nmass is an easy and recommended way. Although orthogonal\nslice annotation is three times costlier than single-slice annota-\ntion, it helps obtain the depth of the mass and richer texture\ninformation. The interval slice annotation further increases the\namount of annotation. This labeling method allows radiologists to\nannotate multiple slices at equal intervals in one direction. These\nannotation manners reduce the restrictions and requirements for\ndata collection and deliver valuable information to our weakly\nsupervised method.\n2.2. Dealing with partial annotations\nThe loss function is generally proposed based on the im-\nage and specific task characteristics. Dice loss [39] demonstrated\ngood performance in the case of medical image class imbalance\nsegmentation. The typical cross-entropy loss is widely used in\nfully supervised segmentation, which optimizes the distribution\ndifference between the ground truth (GT) and prediction [40].\nAll partial annotations of breast masses provide an accurate fore-\nground, whereas the unannotated regions consist of background\nand unannotated foreground. Therefore, partial cross-entropy loss\ncan only be utilized to supervise credible information [41].\nFor any training image X âˆˆ R\nÎ© \n, with pixel-level label Y âˆˆ\n{0, 1}\nÎ© \n, the partial annotation region was defined as Î©\np \nâŠ‚ Î©.\n3\n\nX. Meng, J. Fan, H. Yu et al. Knowledge-Based Systems 258 (2022) 109988\nFig. 4. Partial annotation of the single slice and associated volume estimation\nmethods. (a) Partial annotation is the single axial slice of the mass. Yellow\ndenotes the partial annotation, and white denotes the GT. (b) Mass volume\nestimation is calculated based on the sphere and partial annotation minimum\ncircumscribing circle diameter. (c) Mass volume estimation is calculated based\non major and minor axes of the minimum circumscribed ellipse (a > b). Left:\noblate ellipsoid. Right: prolate ellipsoid.\nOur goal was to train a deep convolutional neural network f (X ; Î¸)\nparameterized by Î¸. Partial cross-entropy is defined as follows:\nL\nPCE \n= \nâˆ‘\npâˆˆÎ©\np\nâˆ’ log f\np\n(X ) (1)\nwhere f\np\n(X ) âˆˆ [0, 1] is a vector of probabilities generated by the\nnetwork for each pixel p. When only the foreground participates\nin supervised training, the model tends to classify the entire\nimage as foreground. Thus, additional constraints are required to\nimprove the segmentation performance.\n2.2.1. Volume-awareness constraint\nPrevious work has suggested a prior hypothesis that the size of\nthe human organ should be within a reasonable range. Under this\ncriterion, Kervadec et al. [11] implemented a weakly supervised\norgan segmentation approach based on partial point annotation.\nThe constraint term can be expressed as follows:\nlower â‰¤ V\ns \nâ‰¤ upper (2)\nwhere V\ns \n= \nâˆ‘\npâˆˆÎ© \nf\np\n(X ). Priors upper, lower denote the given\nupper and lower bounds for the volume of the organ, respectively.\nFollowing two different upper and lower bound constraints\nwere compared in [11]: (1) all images have the same upper\nand lower bounds, and (2) each image has individual bounds\nbased on its true volume. Experiments showed that the second\nmethod exhibited a pronounced performance improvement over\nthe first method. However, obtaining an accurate target volume\nfor each image is not achievable using only partial annotations.\nThe challenge faced because of breast masses of various sizes\nalso hampers the application of the first method in our task. To\naddress these challenges, we designed a volume-awareness loss\nfor the three proposed partial annotation methods.\nThe single-slice annotation method for MRI breast masses is\nshown in Fig. 4(a). We proposed three methods for determining\nthe mass volume under this annotation. One method assumed\nthat the mass is spherical, as shown in Fig. 4(b). We first fitted a\nminimum circumscribing circle of the single-slice annotation and\nthen calculated the mass volume based on its diameter r:\nV\ne \n= \nÏ€r\n3\n6 \n(3)\nA previous study [42] showed that the ellipsoid volume can\nbe better used as an alternative measure of mass volume than a\nsphere. However, the mass depth was unavailable in this annota-\ntion method. Only the major and minor axes of the mass can be\nobtained by determining the minimum circumscribed ellipse. We\nFig. 5. Partial annotation of the orthogonal slice and associated volume es-\ntimation method. (a) Partial annotation in three directions, axial annotation\n(yellow), sagittal annotation (red), coronal annotation (green). (b) 3D view of\npartial annotation, GT (white) (c) Mass volume estimation is calculated as an\nellipsoid.\nthen estimated the mass volume based on the oblate and prolate\nellipsoid volume calculations (Fig. 4(c)):\nV\ne \n= \nÏ€a\n2\nb\n6 \n(4)\nV\ne \n= \nÏ€ab\n2\n6 \n(5)\nwhere a and b are the major and minor axes of the minimum\ncircumscribed ellipse, respectively.\nCompared to single-slice annotation, the orthogonal slice an-\nnotation method expands the annotation in two other directions.\nThis provides the axis length of the mass in three directions,\nwhich are expressed as a, b, and c. From this, the ellipsoid approx-\nimation, as shown in Fig. 5, provides a more accurate estimate of\nthe mass volume:\nV\ne \n= \nÏ€abc\n6 \n(6)\nIn the interval slice annotation method, we assumed that the\ntarget size of the unannotated slice was similar to that of the\nnearest-neighbor annotated slice. As shown in Fig. 6, the mass\nvolume can be determined based on the annotation interval:\nV\ne \n= V\nl \nâˆ— Î· (7)\nwhere V\nl \nis the partial annotation volume, and Î· is the annotation\ninterval.\nIn Eq. (2), the upper and lower bounds of mass can be ob-\ntained from the estimated volume V\ne \nand scale factor Î³ . The\nvolume-awareness constraint is formulated as follows:\nV\ne\n/Î³ â‰¤ V\ns \nâ‰¤ V\ne \nâˆ— Î³ (8)\n2.2.2. Outlier-suppression constraint\nOne of the challenges in breast mass segmentation is that\nnormal tissues and vessels may render enhanced contrast in a\n4\n\nX. Meng, J. Fan, H. Yu et al. Knowledge-Based Systems 258 (2022) 109988\nFig. 6. Partial annotation of the interval slice and associated volume estimation\nmethod. (a) Partial annotation is axial interval slice of the mass, and annotation\ninterval is 3. Yellow denotes partial annotation, and white denotes GT. (b) Mass\nvolume estimation is calculated as four times of the interval annotation.\nsimilar manner as masses, leading to segmentation difficulties\nand high reading variability, even among human experts. The\nresulting false positive predictions may cause the learned mass\nfeatures to be incorrect, even if the network satisfies volume\nconstraints. To address the false positive problem, we proposed\nan outlier-suppression loss that enables the network to focus on\nlearning mass area features. The mass area is determined by the\npartial annotation center and estimated radius because complete\nannotation is unavailable. When the network generates outlier\npredictions, high penalty values are generated. Assuming that an\nimage has k masses, the distance from any predicted point to the\ncenter of any mass is expressed as:\nd\npk \n= âˆ¥l\np \nâˆ’ c\nk\nâˆ¥ (9)\nwhere l\np \nis the position of each prediction voxel and c\nk \nis the\ncenter coordinate of the mass k.\nConsidering that the number of breast masses in a case can\nbe more than one, we proposed a strategy wherein the outlier-\nsuppression loss is determined with its closer mass center. In this\nway, misidentification of the mass as a false positive was avoided.\nAs illustrated in Fig. 7, the prediction points in R\n1 \nand R\n2 \nwere\nclose to the partial annotation centers c\n1 \nand c\n2\n, respectively;\ntherefore, no loss was incurred. The false-positive prediction was\ninside the circle with a yellow dashed line. It was closer to the\npartial annotation centers c\n1 \n(d\n1 \n< d\n2\n), and d\n1 \n> R\n1\n; then, the\nloss was calculated using the outlier and partial annotation center\nc\n1\n.Outlier-suppression constraint is defined as follows:\nmin\nk\nâˆ‘\npâˆˆÎ©\n(f\np\n(X ) âˆ— d\npk \nâˆ’ R\nk\n) â‰¤ 0 (10)\nwhere R\nk \n= 1.1 âˆ— r\nk\n. r\nk \nis the estimated radius of the mass deter-\nmined by annotation. In single-slice annotation and spherical vol-\nume estimation, r\nk \n= r, whereas in ellipsoid volume estimation,\nr\nk \n= a/2. Orthogonal slice annotation r\nk \n= max(a/2, b/2, c/2).\nInterval slice annotation r\nk \n= (V\nl \nâˆ— Î·/Ï€)\n1/3\n.\nFig. 7. Illustration of outlier-suppression loss. This case has two masses. The\npartial annotation centers are c\n1 \nand c\n2 \n. The estimated radii of the breast masses\nare r\n1 \nand r\n2 \n, and the actual constraints are expanded to R\n1 \nand R\n2 \n. The yellow\ndashed line indicates outlier prediction, which is closer to the partial annotation\ncenter c\n1 \n(d\n1 \n< d\n2 \n), and d\n1 \n> R\n1 \n. Thus, the center c\n1 \nparticipates in loss\ncalculation.\n2.3. Weakly supervised learning\nIn our implementation, we selected the state-of-the-art 3D\nnnU-Net [43] planning network model. The detailed architec-\nture is shown in Fig. 8. The architecture has five encoders and\ndecoders, each block containing two convolutional layers, fol-\nlowed by instance normalization and leaky rectified linear unit\n(Leaky ReLU) activation. The final layer with the Softmax ac-\ntivation function was used to gather information and generate\npredictions.\nKervadec et al. [44] suggested that a log-barrier extension\ncan further stabilize the training when a network has multiple\ncompeting constraints. For an inequality constraint in the form\nof z â‰¤ 0, the log-barrier extension can be defined as follows:\nËœ\nÏˆ\nt \n(z) =\n{\nâˆ’ \n1\nt \nlog(âˆ’z) if z â‰¤ âˆ’ \n1\nt\n2\ntz âˆ’ \n1\nt \nlog\n( \n1\nt\n2\n)\n+ \n1\nt \notherwise \n(11)\nwhere t is a parameter that raises the barrier over time.\nAccording to Eqs. (8) and (10), we defined the breast mass\nvolume-awareness loss L\nVA \nand outlier-suppression loss L\nOS \nas:\nL\nVA \n= ËœÏˆ\nt \n(V\ne \nâˆ— Î³ âˆ’ V\ns\n) + ËœÏˆ\nt \n(V\ns \nâˆ’ V\ne\n/Î³ ) (12)\nL\nOS \n= ËœÏˆ\nt \n(min\nk\nâˆ‘\npâˆˆÎ©\n(R\nk \nâˆ’ f\np\n(X ) âˆ— d\npk\n)) (13)\nThe model was trained by optimizing the following end-to-\nend loss:\nL = L\nPCE \n+ Î»(L\nVA \n+ L\nOS \n) (14)\nwhere Î» is the coefficient of the volume-awareness loss and the\noutlier-suppression loss which achieves a balance between the\nsegmentation loss and constraint loss.\n3. Experiments and results\n3.1. Dataset and preprocessing\nClinical DCE-MRI scans of 140 patients collected from the\nChina-Japan Friendship Hospital were used for the experiments.\nAll DCE-MRI scans were acquired on the axial plane using a\n5\n\nX. Meng, J. Fan, H. Yu et al. Knowledge-Based Systems 258 (2022) 109988\nFig. 8. Detailed architecture of the segmentation network.\nTable 1\nQuantitative results and comparison of different annotation methods on the\ntraining set. The best weakly supervised results are highlighted in bold.\nAnnotation DSCâ†‘ PRâ†‘ SEâ†‘ HD95â†“ ASSDâ†“ RVDâ†“ NUMâ†“\nA\nS \n0.597 0.528 0.765 4.741 1.813 0.711 1\nA\nPE \n0.624 0.560 0.779 4.305 1.716 0.628 1\nA\nOE \n0.727 0.666 0.838 2.792 1.301 0.382 1\nO\nSE \n0.880 0.828 0.944 1.528 0.537 0.157 3\nInterval 0.896 0.845 0.957 1.487 0.462 0.145 6\nFS\n3D \n0.923 0.928 0.918 1.703 0.383 0.056 22\nâ†‘ denote â€˜higher is betterâ€™, â†“ denote â€˜lower is betterâ€™.\n3.0T GE Healthcare scanner in the prone position with TR =\n4.2 ms and TE = 1.7 ms. All included studies used T1-weighted\nfat-suppressed sequences that contained six post-contrast im-\nages. In this study, the third sequence was used as input to\nthe network. The image size ranged from 512 Ã— 512 Ã— 300 to\n384 Ã— 384 Ã— 104, with the resolution of 0.88 mm Ã— 0.88 mm Ã—\n1.5 mm to 0.7 mm Ã— 0.7 mm Ã— 0.6 mm. We performed following\ntwo types of image normalization on each image: intensity and\nspatial normalizations. For intensity normalization, we performed\nthe z-score using the mean Î¼ = 0 and standard deviation\nÏƒ = 1. Spline interpolation was used for spatial normalization.\nEach image was resampled to the same resolution as spacing\n1.4 mmÃ—1.4 mmÃ—1.2 mm. After cropping the background region\nusing automatic thresholding, the input image size was unified to\n256 Ã— 128 Ã— 128. Breast masses were manually annotated by a\nsenior radiologist. The dataset was divided into 98 training, 14\nvalidation, and 28 test images.\n3.2. Implementation details and evaluation metrics\nThe proposed weakly supervised learning framework was im-\nplemented in PyTorch with an Intel\nÂ®\nXeon\nÂ®\nGold 5118 2.30 GHz\nand one GeForce RTX 3090 graphics card. Both 2D- and 3D-\nbased state-of-the-art segmentation methods were compared.\nThe batch size was 1, and the learning rate was initialized as 5 Ã—\n10\nâˆ’4 \nfor all 3D-based methods, while 128 and 1Ã—10\nâˆ’2 \nfor all 2D-\nbased methods. We adopted the learning rate warm-up strategy\nand poly decay function initial_lr âˆ—(1 âˆ’epoch/max_epoch)\n0.9\n. Data\naugmentation was not performed. The patch size was the same\nas the input image size 256 Ã— 128 Ã— 128 for the 3D-based and\n256 Ã— 128 for 2D-based methods. Hyperparameter t was initially\nset to 5. A total of 300 epochs were run for each method. The\nmodel that obtained the best DSC for the validation set was used\nfor evaluation.\nSix popular metrics in volume segmentation were used for\nevaluation: dice similarity coefficient (DSC), precision (PR), sensi-\ntivity (SE), 95% Hausdorff distance (HD95, mm), average symmet-\nric surface distance (ASSD, mm), and relative volume difference\nTable 2\nQuantitative results and comparison of different annotation methods for breast\nmass segmentation. The best weakly supervised results are highlighted in bold.\nPartial annotation Complete annotation\nA\nS \nA\nPE \nA\nOE \nO\nSE \nInterval FS\n2D \nFS\n3D\nDSCâ†‘ 0.566 0.594 0.674 0.835 0.837 0.556 0.838\nPRâ†‘ 0.671 0.672 0.737 0.862 0.895 0.459 0.900\nSEâ†‘ 0.586 0.623 0.681 0.833 0.809 0.882 0.810\nHD95â†“ 18.015 24.5618 14.861 11.977 20.678 138.277 10.533\nASSDâ†“ 4.131 9.380 3.080 2.777 3.164 31.119 2.605\nRVDâ†“ 0.509 0.490 0.362 0.201 0.174 2.964 0.171\nNUMâ†“ 1 1 1 3 6 1 22\nâ†‘ denote â€˜higher is betterâ€™, â†“ denote â€˜lower is betterâ€™.\n(RVD). Higher DSC, PR, and SE values indicate better perfor-\nmance, whereas lower RVD, ASD, and HD95 values indicate better\nperformance.\n3.3. Comparison between different partial annotation\nIn this section, we evaluated the performance of different\npartial annotation methods and constraint methods:\nâ€¢ A\nS \n, A\nOE \n, and A\nPE \n, which employ the single slice annotation.\nThe annotation amount of each mass was one slice, and the\napproximate breast masses as spheres, oblate ellipsoids, and\nprolate ellipsoids, respectively.\nâ€¢ O\nSE \n, which employs the orthogonal slice annotation. The\nannotation amount for each mass was three slices, and the\napproximate breast mass was a scalene ellipsoid.\nâ€¢ Interval, annotated one slice for every three slices in the\naxial plane. The annotation amount of each mass was ap-\nproximately six slices. The mass volume was estimated to\nbe four times that of the annotated value.\nUsing partial annotations instead of high-quality voxel-wise\nannotations can considerably reduce costs in terms of both fi-\nnance and human effort. However, this effort will not be useful in\nreal applications if the loss in segmentation accuracy is consider-\nably high. Therefore, we verified the segmentation performance\nof our method by comparing it with fully supervised approaches.\nâ€¢ FS\n2D\n, which uses the single-slice annotation to train a fully\nsupervised segmentor based on the 2D nnU-Net.\nâ€¢ FS\n3D\n, which is trained using 3D nnU-Net based on high qual-\nity voxel-wise annotations and delivers a ceiling accuracy.\nWe first reported the accuracy of different volume estimation\nmethods compared to the actual volume on the training set.\nFig. 9(a) shows the distributions and concordance correlation\ncoefficients (CCC) of the actual and estimated volumes. CCC is\na common statistical criterion, with a value of one indicating\nperfect agreement. Statistics indicate that Interval has the highest\nagreement with the actual volume, with a CCC of 0.999. O\nSE\nexhibits a similar performance (CCC = 0.991). For single-slice\nannotation, the three volume estimation methods have the best\neffect of A\nOE \n, followed by A\nPE \n, and the worst effect of A\nS \n. Fig. 9(b)\ndepicts the linear regression fit line of the different methods. The\ncloser it is to the 45\nâ—¦ \nline through the origin, the better it is. The\nsame conclusion was drawn as the CCC analysis.\nDuring the weakly supervised training phase, supervision was\nperformed between the prediction and weak annotations,\nwhereas the consistency between the prediction and GT was\nuncertain. To verify the fitting ability of the proposed method\non unlabeled regions in the training dataset, we tested the seg-\nmentation performance of the training dataset using weakly\nsupervised models with different annotation methods. As can be\n6\n\nX. Meng, J. Fan, H. Yu et al. Knowledge-Based Systems 258 (2022) 109988\nFig. 9. Comparison between masses actual and estimated volumes of different methods. (a) Boxplots and CCCs of actual and estimated volumes. (b) Scatter plot and\nthe fitted curves of actual and estimated volumes.\nFig. 10. Visual comparison of different partial annotation methods for MRI breast mass segmentation.\nobserved from Table 1, the interval slice annotation achieved the\nhighest performance with 0.896 DSC, which is comparable to the\ncomplete annotation training model. O\nSE \nalso yielded good perfor-\nmance, with only approximately 1/7 of the complete annotation.\nThe results confirm that the proposed method can effectively\nlocate the unlabeled region and learn the correct breast mass\nfeature representation under weak supervision. Moreover, to\ndemonstrate the importance of volume estimation accuracy in\nthe weakly supervised process, we compared the segmentation\neffects of different volume estimation methods under single-slice\nannotation on the training dataset. The first three rows of Table 1\nshow that the more accurate volume estimation method (A\nOE \n>\nA\nPE \n> A\nS \n) obtained a smaller RVD and higher DSC, indicating that\nthe training process was positively impacted by more accurate\nvolume estimation.\n7\n\nX. Meng, J. Fan, H. Yu et al. Knowledge-Based Systems 258 (2022) 109988\nFig. 11. Segmentation performance with different Î³ values for our volume-awareness loss L\nVA \n.\nThe results of the different annotation methods for the testing\nset are summarized in Table 2. Under single-slice annotation, we\nobserved that the segmentation performance was improved by\nmore accurate volume estimation (A\nOE \n> A\nPE \n> A\nS \n). Fewer\ntraining slices and absence of 3D information limit the perfor-\nmance of FS\n2D\n. Replacing the use of an entire 3D image, our\nthree weakly annotated methods exploited 3D information, and\nall of them outperformed FS\n2D\n. Compared with single-slice anno-\ntation, the addition of annotation information could effectively\nimprove the segmentation results. Interval slice annotation pro-\nvided more mass texture information and achieved a 16% DSC\nimprovement over A\nOE \n. Use of orthogonal slice annotation slightly\ndecreased DSC by 0.2% compared to Interval but considerably re-\nduced the annotation cost. The comparison with FS\n3D \nshows that\nboth O\nSE \nand Interval achieved an approximately fully supervised\nsegmentation performance.\nFig. 10 shows the segmentation results of the CNNs trained\non different annotation methods. FS\n2D \nis difficult to distinguish\nbetween breast masses and other enhanced areas, which yields\na high false-positive rate. The qualitative results of A\nS \n, A\nPE \n, and\nA\nOE \nindicated significant dependence of the segmentation per-\nformance on the accuracy of the volume estimation. O\nSE \nand\nInterval prominently improved the segmentation performance\nand approached FS\n3D\n. Stable segmentation results were obtained\nfor masses at different scales. Comparatively, we considered or-\nthogonal slice annotation as the best method in our approach\nbecause annotating mass center slices was more confident for\ndoctors and the annotation cost was not excessively high.\n3.4. Effects of weighting parameters on loss function\nThe segmentation network was optimized by the joint loss\nfunction defined in Eq. (14). The predefined parameter Î» was used\nto balance the weights of the segmentation and constraint losses.\nHere, we adopted orthogonal slice annotation for supervision and\nset the weighting parameter Î» as 0.0001, 0.001, 0.01, 0.1, and 1 to\ntrain the network. The experimental results are listed in Table 3.\nFor a negligible weight of the constraint term, the segmentation\nperformance decreased, indicating the criticality of the constraint\nterm for network training. However, a significantly large weight\nof constraint term may draw more attention to the gradient\nconvergence in the direction that minimizes the loss of constraint\nrather than the segmentation loss. In our setup Î» = 0.01 achieved\nthe best performance.\n3.5. Sensitivity analysis of hyper-parameter Î³\nWe verified the effectiveness of the loose common constraint\nof mass volume. L\nVA \nconstraint range was [57,15948], as de-\nrived from the training set statistics. L\nPCE \nand L\nOS \nwere calculated\nbased on orthogonal slice annotation. Under this setting, we\nobtained poor segmentation results with DSC = 0.549. This find-\ning indicates that, owing to the varying sizes of breast masses,\nloose constraints barely help optimize the performance of weakly\nsupervised segmentation.\nTable 3\nQuantitative comparison results of segmentation loss functions with different\nweights. The best results are highlighted in bold.\nÎ» DSCâ†‘ PRâ†‘ SEâ†‘ HD95â†“ ASSDâ†“ RVDâ†“\nÎ» = 0.0001 0.814 0.808 0.855 20.929 3.339 0.259\nÎ» = 0.001 0.828 0.852 0.836 17.587 3.388 0.203\nÎ» = 0.01 0.835 0.862 0.833 11.977 2.777 0.201\nÎ» = 0.1 0.830 0.853 0.843 15.642 2.823 0.207\nÎ» = 1 0.815 0.829 0.838 18.814 3.827 0.224\nâ†‘ denote â€˜higher is betterâ€™, â†“ denote â€˜lower is betterâ€™.\nTable 4\nQuantitative results and comparison of different losses for breast mass\nsegmentation. The best results are highlighted in bold.\nL\nPCE \nL\nVA \nL\nOS \nDSCâ†‘ PRâ†‘ SEâ†‘ HD95â†“ ASSDâ†“ RVDâ†“\nâˆš \n0.007 0.003 0.999 178.1821 53.225 1830.513\nâˆš âˆš \n0.716 0.682 0.837 21.123 3.485 0.497\nâˆš âˆš \n0.793 0.858 0.783 27.303 3.972 0.238\nâˆš âˆš âˆš \n0.835 0.862 0.833 11.977 2.777 0.201\nâ†‘ denote â€˜higher is betterâ€™, â†“ denote â€˜lower is betterâ€™.\nIn addition, we investigated the optimal value of the hyperpa-\nrameter Î³ âˆˆ [1.0, 1.25, 1.5, 1.75, 2.0] for the proposed loss L\nVA\nin Eq. (12). Fig. 11 shows the evolution of the segmentation per-\nformance set when Î³ changes from 1.0 to 2.0. We observed that\nincrease in the Î³ from 1.0 to 1.5 improved the performance, and\nfor Î³ > 1.5, the segmentation performance decreased gradually.\nThis phenomenon suggests that a smaller Î³ introduced incor-\nrect supervision information owing to volume estimation errors,\nwhereas a larger Î³ weakened the effect of volume awareness loss\non network training. We set Î³ = 1.5 for the volume awareness\nloss in the following experiments.\n3.6. Effectiveness of constraint losses\nTo investigate the effectiveness of the proposed volume-\nawareness and outlier-suppression strategy, we conducted ab-\nlation experiments under orthogonal slice annotation, and the\nresults are listed in Table 4. We observed that since L\nPCE \nsu-\npervised only a few foreground voxels, the correct breast mass\nfeatures could not be efficiently learned. The combination of L\nPCE\nand L\nOS \npositively affected segmentation, but still had consider-\nable scope for improvement. L\nPCE \nand L\nVA \ndecreased in all metric\nvalues than combined three losses. These evidences show that the\ntwo proposed constraint losses combined with segmentation loss\ncan effectively improve the model accuracy without increasing\nthe testing phase cost.\nTo illustrate the effectiveness of the co-training losses, we\npresent two examples with and without BPE to compare the\nvariations in the segmentation results. As shown in Fig. 12, using\nL\nPCE \nas the only supervision loss, the network tended to classify\nall voxels as foreground, thus requiring additional loss to assist in\nsupervision. The combination of L\nPCE \nand L\nVA \nyielded satisfactory\nresults on the image without BPE but tended to confuse masses\nand enhanced non-target regions on images with BPE. When only\n8\n\nX. Meng, J. Fan, H. Yu et al. Knowledge-Based Systems 258 (2022) 109988\nFig. 12. Visual comparison of different loss combinations for MRI breast mass segmentation.\nTable 5\nQuantitative results and comparison of different methods for breast mass segmentation. The best\nweakly supervised results are highlighted in bold.\nMethod Annotation DSCâ†‘ PRâ†‘ SEâ†‘ HD95â†“ ASSDâ†“\n3D-UNet [45] Sparse Slice 0.718 0.791 0.770 10.468 3.120\nConstrained-CNN [11] Partial Point 0.647 0.793 0.589 34.201 6.593\nJi et al. [36] Scribble 0.680 0.693 0.732 51.650 8.631\nUSTM-Net [46] Scribble 0.683 0.647 0.795 87.396 14.844\nKervadec et al. [47] Bounding Box 0.613 0.695 0.646 19.578 3.553\nWang et al. [48] Bounding Box 0.730 0.771 0.724 26.523 13.716\nOurs Orthogonal Slice 0.835 0.862 0.833 11.977 2.777\nnnU-Net [43] Full 0.838 0.900 0.810 10.533 2.605\nâ†‘ denote â€˜higher is betterâ€™, â†“ denote â€˜lower is betterâ€™.\nL\nOS \nwas added, a boundary larger than GT was obtained. We can\nobserve that integration of the three loss functions can not only\nobtain finer boundaries but also suppress false positives. Notably,\nthe presented data had more than one mass, but the outlier-\nsuppression loss did not result in false negatives. This result\nillustrates that the proposed losses play a complementary role\nduring the training phase. If a more accurate mass area estimation\nmethod is used, as in Section 2.2.1, instead of focusing on the\ndistance to the annotation center, better performance may be\nrealized. Our implementation is an efficient alternative that can\nsignificantly reduce the computational burden.\n3.7. Comparison with state-of-the-art approaches\nTo demonstrate the effectiveness of the proposed method, we\nevaluated the proposed method by comparing it with six weakly\nsupervised learning frameworks that have achieved considerable\nsuccess in segmentation under different weak annotations, as\nfollows:\nâ€¢ Sparse slice annotation: 3D-UNet [45] sparsely annotates\nmultiple slices in three directions as supervision informa-\ntion, and partial cross-entropy is the only supervised loss. In\ncontrast to our proposed interval annotation, this annotation\nis approximately three times that of ours at the same sam-\npling spacing. In this implementation, the sampling spacing\nwas set to four.\nâ€¢ Partial point annotation: Prior knowledge of the statistical\norgan size was used to constrain the target within a cer-\ntain range in Constrained-CNN [11]. We used the method\nproposed in this study to randomly select a point in the\nbreast mass and expand the radius to four to generate weak\nannotations. Note that we adopted the true volume of each\nmass as a constraint, although this process is not achievable\nin practical weakly supervised applications.\nâ€¢ Scribble annotation: The initial weak annotation of [36,46]\nis the foreground and background scribble. In [36], actual\nsupervision is the pseudo-label generated based on scribbles\nusing the graph cut algorithm. Partial cross-entropy and\nCRF loss jointly supervise network training. Meanwhile, [46]\ndesigned a scribble-supervised uncertainty-aware transfor-\nmation consistency model without preprocessing scribbles.\nâ€¢ Bounding box annotation: In [47] and [48], weak annotation\nis the bounding box of the target. For the method proposed\nby Kervadec et al. [47], the background outside the bounding\nbox and size were the key information for training. Wang\net al. [48] presented a generalized MIL formulation and\nsmooth maximum approximation to integrate the bounding\nbox tightness prior to the segmentation network.\n9\n\nX. Meng, J. Fan, H. Yu et al. Knowledge-Based Systems 258 (2022) 109988\nFig. 13. Visual comparison of different methods for MRI breast mass segmentation.\nTable 5 provides a quantitative comparison of these networks\nfor the entire test set. We observed that Constrained-CNN based\non partial point annotation exhibits lower performance owing\nto barren supervision information. In contrast, the performance\nof 3D-UNet benefits from its highest annotation cost among\nthe compared methods. Both scribble-based methods performed\npoorly, mainly because of the complex background of breast MRI.\nFor bounding box-based methods, the method proposed by Wang\net al. [48] exhibited a significant improvement over that pro-\nposed by Kervadec et al. [47], but still falls short of our method.\nMoreover, this method requires tight bounding box annotation of\neach 2D slice, which is still strict and time-consuming. Based on\northogonal slice annotation, our method achieved the best DSC,\nPR, and SE, indicating that our method improved breast mass\nsegmentation without introducing considerable interference. The\nsegmentation performance of our weakly supervised method was\nclose to that of fully supervised models with the same network\nstructure.\nFig. 13 shows the performance of these methods for cases with\ndifferent contrasts. We can observe the existence of false-positive\npredictions in sparse slices, partial points, and scribble annota-\ntions. In particular, this issue was further exacerbated in low-\ncontrast images. The bounding box annotation-based method [47]\ntends to segment the mass into a box, resulting in false bound-\naries. In addition, method [48] also fails to distinguish boundaries\nin low-contrast images. In contrast, our method achieved a more\naccurate location and boundary with high robustness to different\nmass contrasts and was even comparable with fully supervised\nlearning.\n4. Conclusion\nIn this study, we proposed a novel, weakly supervised\ntraining method for breast mass segmentation in MRI. For the\nchallenge of annotating voxel-level labels, we designed three\ndifferent partial annotation methods and validated their effective-\nness for supervised breast mass segmentation. We proposed two\nconstraint losses, volume-awareness loss and outlier-suppression\nloss, which effectively improved the segmentation challenges\ncaused by various sizes of masses and complex backgrounds, re-\nspectively. We tested our method on a large-scale DCE-MRI image\ndataset and further compared it with state-of-the-art methods.\nSegmentation performance is comparable to full supervision and\ndemonstrates the potential clinical value of this method.\nCRediT authorship contribution statement\nXianqi Meng: Conceptualization, Methodology, Software, Vi-\nsualization, Writing â€“ original draft. Jingfan Fan: Conceptual-\nization, Methodology, Writing â€“ review & editing, Supervision,\n10\n\nX. Meng, J. Fan, H. Yu et al. Knowledge-Based Systems 258 (2022) 109988\nFunding acquisition. Hongwei Yu: Data curation, Writing â€“ re-\nview & editing. Jinrong Mu: Formal analysis, Writing â€“ review\n& editing. Zongyu Li: Data curation, Validation. Aocai Yang: Re-\nsources. Bing Liu: Resources. Kuan Lv: Resources. Danni Ai:\nWriting â€“ review & editing, Supervision. Yucong Lin: Writing â€“\nreview & editing, Supervision. Hong Song: Writing â€“ review &\nediting. Tianyu Fu: Writing â€“ review & editing. Deqiang Xiao:\nWriting â€“ review & editing. Guolin Ma: Data curation, Funding\nacquisition. Jian Yang: Writing â€“ review & editing, Supervision.\nYing Gu: Writing â€“ review & editing, Supervision.\nDeclaration of competing interest\nThe authors declare that they have no known competing finan-\ncial interests or personal relationships that could have appeared\nto influence the work reported in this paper.\nData availability\nThe authors do not have permission to share data.\nAcknowledgments\nThis work was supported by the National Natural Science\nFoundation of China (62171039, 62025104, 82172027, 61901031,\n81971585), Beijing Nova Program, China (Z201100006820004)\nfrom Beijing Municipal Science & Technology Commission, Beijing\nMunicipal Science and Technology Project (Z211100003521009),\nGuangzhou Science and Technology Planning Project (2021030\n10001)\nReferences\n[1] R.L. Siegel, K.D. Miller, H.E. Fuchs, A. Jemal, Cancer statistics, 2021., CA\nCancer J. Clin. 71 (1) (2021) 7â€“33.\n[2] R.M. Mann, C.K. Kuhl, L. Moy, Contrast-enhanced MRI for breast cancer\nscreening, J. Magn. Reson. Imaging 50 (2) (2019) 377â€“390.\n[3] D.A. Sippo, K.S. Burk, S.F. Mercaldo, G.M. Rutledge, C. Edmonds, Z. Guan,\net al., Performance of screening breast MRI across women with different\nelevated breast cancer risk indications, Radiology 292 (1) (2019) 51â€“59.\n[4] S.H. Jafari, Z. Saadatpour, A. Salmaninejad, F. Momeni, M. Mokhtari, J.S. Na-\nhand, et al., Breast cancer diagnosis: Imaging techniques and biochemical\nmarkers, J. Cell. Physiol. 233 (7) (2018) 5200â€“5213.\n[5] B. Reig, L. Heacock, K.J. Geras, L. Moy, Machine learning in breast MRI, J.\nMagn. Reson. Imaging 52 (4) (2020) 998â€“1018.\n[6] M.A. Schmidt, G.S. Payne, Radiotherapy planning using MRI, Phys. Med.\nBiol. 60 (22) (2015) R323.\n[7] D. Leithner, L. Moy, E.A. Morris, M.A. Marino, T.H. Helbich, K. Pinker,\nAbbreviated MRI of the breast: does it provide value? J. Magn. Reson.\nImaging 49 (7) (2019) e85â€“e100.\n[8] Y. Jiang, A.V. Edwards, G.M. Newstead, Artificial intelligence applied to\nbreast MRI for improved diagnosis, Radiology 298 (1) (2021) 38â€“46.\n[9] A.G. Bitencourt, D.S. EugÃªnio, J.A. Souza, J.O. Souza, F.B. Makdissi, E.F.\nMarques, et al., Prognostic significance of preoperative MRI findings in\nyoung patients with breast cancer, Sci. Rep. 9 (1) (2019) 1â€“6.\n[10] Y. Zhang, J.-H. Chen, Y. Lin, S. Chan, J. Zhou, D. Chow, et al., Prediction of\nbreast cancer molecular subtypes on DCE-MRI using convolutional neural\nnetwork with transfer learning between two centers, Eur. Radiol. 31 (4)\n(2021) 2559â€“2567.\n[11] H. Kervadec, J. Dolz, M. Tang, E. Granger, Y. Boykov, I.B. Ayed, Constrained-\nCNN losses for weakly supervised segmentation, Med. Image Anal. 54\n(2019) 88â€“99.\n[12] D. Ai, Z. Zhao, J. Fan, H. Song, X. Qu, J. Xian, et al., Spatial proba-\nbilistic distribution map-based two-channel 3D U-net for visual pathway\nsegmentation, Pattern. Recognit. Lett. 138 (2020) 601â€“607.\n[13] C. Zotti, Z. Luo, A. Lalande, P.-M. Jodoin, Convolutional neural network with\nshape prior applied to cardiac MRI segmentation, IEEE J. Biomed. Health\nInform. 23 (3) (2018) 1119â€“1128.\n[14] Q. Guo, H. Song, J. Fan, D. Ai, Y. Gao, X. Yu, J. Yang, Portal vein and hepatic\nvein segmentation in multi-phase MR images using flow-guided change\ndetection, IEEE Trans. Image Process. 31 (2022) 2503â€“2517.\n[15] G.J. Liao, L.C. Henze Bancroft, R.M. Strigel, R.D. Chitalia, D. Kontos, L.\nMoy, et al., Background parenchymal enhancement on breast MRI: a\ncomprehensive review, J. Magn. Reson. Imaging 51 (1) (2020) 43â€“61.\n[16] H. Liu, Y. Liu, Z. Zhao, L. Zhang, T. Qiu, A new background distribution-\nbased active contour model for three-dimensional lesion segmentation in\nbreast DCE-MRI, Med. Phys. 41 (8Part1) (2014) 082303.\n[17] S. Yang, C. Hsu, 3D tumor segmentation in breast MRIs using 3D modified\nactive contour method, Int. J. Comput. Softw. Eng. 2 (119) (2017) 8.\n[18] Y. Zheng, S. Baloch, S. Englander, M.D. Schnall, D. Shen, Segmentation and\nclassification of breast tumor using dynamic contrast-enhanced MR images,\nin: Proc. Int. Conf. Med. Image Comput. Comput.-Assisted Intervention,\n2007, pp. 393â€“401.\n[19] D. McClymont, A. Mehnert, A. Trakic, D. Kennedy, S. Crozier, Fully auto-\nmatic lesion segmentation in breast MRI using mean-shift and graph-cuts\non a region adjacency graph, J. Magn. Reson. Imaging 39 (4) (2014)\n795â€“804.\n[20] X. Xi, H. Shi, L. Han, T. Wang, H.Y. Ding, G. Zhang, et al., Breast tumor\nsegmentation with prior knowledge learning, Neurocomputing 237 (2017)\n145â€“157.\n[21] O. Ronneberger, P. Fischer, T. Brox, U-net: Convolutional networks for\nbiomedical image segmentation, in: Proc. Int. Conf. Med. Image Comput.\nComput.-Assisted Intervention, 2015, pp. 234â€“241.\n[22] G. Litjens, T. Kooi, B.E. Bejnordi, A.A.A. Setio, F. Ciompi, M. Ghafoorian, et\nal., A survey on deep learning in medical image analysis, Med. Image Anal.\n42 (2017) 60â€“88.\n[23] J. Fan, X. Cao, Q. Wang, P.-T. Yap, D. Shen, Adversarial learning for mono-\nor multi-modal registration, Med. Image Anal. 58 (2019) 101545.\n[24] J. Fan, X. Cao, P.-T. Yap, D. Shen, Birnet: Brain image registration using\ndual-supervised fully convolutional networks, Med. Image Anal. 54 (2019)\n193â€“206.\n[25] A. Wu, S. Zhao, C. Deng, W. Liu, Generalized and discriminative few-shot\nobject detection via SVD-dictionary enhancement, in: Proc. Adv. Neural Inf.\nProcess. Syst. Vol. 34, 2021, pp. 6353â€“6364.\n[26] A. Wu, Y. Han, L. Zhu, Y. Yang, Instance-invariant domain adaptive object\ndetection via progressive disentanglement, IEEE Trans. Pattern Anal. Mach.\nIntell. 44 (8) (2022) 4178â€“4193.\n[27] J. Zhang, A. Saha, Z. Zhu, M.A. Mazurowski, Hierarchical convolutional\nneural networks for segmentation of breast tumors in MRI with application\nto radiogenomics, IEEE Trans. Med. Imaging 38 (2) (2019) 435â€“447.\n[28] S. Wang, K. Sun, L. Wang, L. Qu, F. Yan, Q. Wang, D. Shen, Breast tumor\nsegmentation in DCE-MRI with tumor sensitive synthesis, IEEE Trans.\nNeural Netw. Learn. Syst. (2021).\n[29] H. Wang, J. Cao, J. Feng, Y. Xie, D. Yang, B. Chen, Mixed 2D and 3D\nconvolutional network with multi-scale context for lesion segmentation\nin breast DCE-MRI, Biomed. Signal Process. Control 68 (2021) 102607.\n[30] A. Kolesnikov, C.H. Lampert, Seed, expand and constrain: Three principles\nfor weakly-supervised image segmentation, in: Proc. Eur. Conf. Comput.\nVis., 2016, pp. 695â€“711.\n[31] Z. Huang, X. Wang, J. Wang, W. Liu, J. Wang, Weakly-supervised semantic\nsegmentation network with Deep Seeded Region growing, in: Proc. IEEE\nConf. Comput. Vis. Pattern Recog., 2018, pp. 7014â€“7023.\n[32] H. Qu, P. Wu, Q. Huang, J. Yi, Z. Yan, K. Li, G.M. Riedlinger, S. De, S. Zhang,\nD.N. Metaxas, Weakly supervised deep nuclei segmentation using partial\npoints annotation in histopathology images, IEEE Trans. Med. Imaging 39\n(11) (2020) 3655â€“3666.\n[33] M. Rajchl, M.C. Lee, O. Oktay, K. Kamnitsas, J. Passerat-Palmbach, W. Bai,\net al., Deepcut: Object segmentation from bounding box annotations using\nconvolutional neural networks, IEEE Trans. Med. Imaging 36 (2) (2016)\n674â€“683.\n[34] S. Wang, Q. Wang, Y. Shao, L. Qu, C. Lian, J. Lian, et al., Iterative label de-\nnoising network: Segmenting male pelvic organs in CT from 3D bounding\nbox annotations, IEEE. Trans. Biomed. Eng. 67 (10) (2020) 2710â€“2720.\n[35] K. Zhang, X. Zhuang, Cyclemix: A holistic strategy for medical image\nsegmentation from scribble supervision, in: Proc. IEEE Conf. Comput. Vis.\nPattern Recog., 2022, pp. 11656â€“11665.\n[36] Z. Ji, Y. Shen, C. Ma, M. Gao, Scribble-based hierarchical weakly supervised\nlearning for brain tumor segmentation, in: Proc. Int. Conf. Med. Image\nComput. Comput.-Assisted Intervention, 2019, pp. 175â€“183.\n[37] H. Zheng, Z. Zhuang, Y. Qin, Y. Gu, J. Yang, G.-Z. Yang, Weakly supervised\ndeep learning for breast cancer segmentation with coarse annotations, in:\nProc. Int. Conf. Med. Image Comput. Comput.-Assisted Intervention, 2020,\npp. 450â€“459.\n[38] Y.Y. Boykov, M.-P. Jolly, Interactive graph cuts for optimal boundary &\nregion segmentation of objects in ND images, in: Proc. IEEE Int. Conf.\nComput. Vis. Vol. 1, 2001, pp. 105â€“112.\n11\n\nX. Meng, J. Fan, H. Yu et al. Knowledge-Based Systems 258 (2022) 109988\n[39] F. Milletari, N. Navab, S.-A. Ahmadi, V-net: Fully convolutional neural\nnetworks for volumetric medical image segmentation, in: In Proc. 4th Int.\nConf. 3D Vis., 2016, pp. 565â€“571.\n[40] V. Badrinarayanan, A. Kendall, R. Cipolla, SegNet: A deep convolutional\nencoder-decoder architecture for image segmentation, IEEE Trans. Pattern\nAnal. Mach. Intell. 39 (12) (2017) 2481â€“2495.\n[41] M. Tang, A. Djelouah, F. Perazzi, Y. Boykov, C. Schroers, Normalized cut\nloss for weakly-supervised cnn segmentation, in: Proc. IEEE Conf. Comput.\nVis. Pattern Recog., 2018, pp. 1818â€“1827.\n[42] S.H. Tirumani, A.B. Shinagare, A.C. Oâ€™Neill, M. Nishino, M.H. Rosenthal,\nN.H. Ramaiya, Accuracy and feasibility of estimated tumour volumetry\nin primary gastric gastrointestinal stromal tumours: validation using\nsemiautomated technique in 127 patients, Eur. Radiol. 26 (1) (2016)\n286â€“295.\n[43] F. Isensee, P.F. Jaeger, S.A. Kohl, J. Petersen, K.H. Maier-Hein, nnU-Net:\na self-configuring method for deep learning-based biomedical image\nsegmentation, Nature Methods 18 (2) (2021) 203â€“211.\n[44] H. Kervadec, J. Dolz, J. Yuan, C. Desrosiers, E. Granger, I.B. Ayed, Constrained\ndeep networks: Lagrangian optimization via log-barrier extensions, 2019,\narXiv preprint arXiv:1904.04205.\n[45] Ã–. Ã‡iÃ§ek, A. Abdulkadir, S.S. Lienkamp, T. Brox, O. Ronneberger, 3D U-\nnet: learning dense volumetric segmentation from sparse annotation, in:\nProc. Int. Conf. Med. Image Comput. Comput.-Assisted Intervention, 2016\npp. 424â€“432.\n[46] X. Liu, Q. Yuan, Y. Gao, K. He, S. Wang, X. Tang, J. Tang, D. Shen, Weakly\nsupervised segmentation of COVID19 infection with scribble annotation on\nCT images, Pattern Recognit. 122 (2022) 108341.\n[47] H. Kervadec, J. Dolz, S. Wang, E. Granger, I.B. Ayed, Bounding boxes\nfor weakly supervised segmentation: Global constraints get close to\nfull supervision, in: Proc. Int. Conf. Medical Imaging Deep Learn., 2020,\npp. 365â€“381.\n[48] J. Wang, B. Xia, Bounding box tightness prior for weakly supervised image\nsegmentation, in: Proc. Int. Conf. Med. Image Comput. Comput.-Assisted\nIntervention, Vol. 12902, 2021, pp. 526â€“536.\n12",
    "version": "5.3.31"
  },
  {
    "numpages": 10,
    "numrender": 10,
    "info": {
      "PDFFormatVersion": "1.7",
      "Language": "en-US",
      "EncryptFilterName": null,
      "IsLinearized": true,
      "IsAcroFormPresent": false,
      "IsXFAPresent": false,
      "IsCollectionPresent": false,
      "IsSignaturesPresent": false,
      "Creator": "Elsevier",
      "Custom": {
        "CrossMarkDomains[1]": "elsevier.com",
        "CrossmarkMajorVersionDate": "2010-04-23",
        "CreationDate--Text": "31st May 2024",
        "ElsevierWebPDFSpecifications": "7.0",
        "robots": "noindex",
        "doi": "10.1016/j.engappai.2024.108059",
        "CrossMarkDomains[2]": "sciencedirect.com",
        "CrossmarkDomainExclusive": "true"
      },
      "ModDate": "D:20240531114923Z",
      "Author": "Ziyang Wang",
      "Title": "MixSegNet: Fusing multiple mixed-supervisory signals with multiple views of networks for mixed-supervised medical image segmentation",
      "Keywords": "Vision transformer,Medical image segmentation,Mixed-supervised learning",
      "CreationDate": "D:20240531113817Z",
      "Producer": "Acrobat Distiller 8.1.0 (Windows)",
      "Subject": "Engineering Applications of Artificial Intelligence, 133 (2024) 108059. doi:10.1016/j.engappai.2024.108059"
    },
    "metadata": {
      "crossmark:crossmarkdomains": "elsevier.comsciencedirect.com",
      "crossmark:crossmarkdomainexclusive": "true",
      "crossmark:doi": "10.1016/j.engappai.2024.108059",
      "crossmark:majorversiondate": "2010-04-23",
      "dc:format": "application/pdf",
      "dc:identifier": "10.1016/j.engappai.2024.108059",
      "dc:publisher": "Elsevier Ltd",
      "dc:description": "Engineering Applications of Artificial Intelligence, 133 (2024) 108059. doi:10.1016/j.engappai.2024.108059",
      "dc:subject": [
        "Vision transformer",
        "Medical image segmentation",
        "Mixed-supervised learning"
      ],
      "dc:title": "MixSegNet: Fusing multiple mixed-supervisory signals with multiple views of networks for mixed-supervised medical image segmentation",
      "dc:creator": ["Ziyang Wang", "Chen Yang"],
      "jav:journal_article_version": "VoR",
      "pdf:creationdate--text": "31st May 2024",
      "pdf:producer": "Acrobat Distiller 8.1.0 (Windows)",
      "pdf:keywords": "Vision transformer,Medical image segmentation,Mixed-supervised learning",
      "pdfx:creationdate--text": "31st May 2024",
      "pdfx:crossmarkdomains": "sciencedirect.comelsevier.com",
      "pdfx:crossmarkdomainexclusive": "true",
      "pdfx:crossmarkmajorversiondate": "2010-04-23",
      "tprdsnpynndj7yt6gntn-ylnnyt2nlwiknmqgzweompmnmwj-ndioo9eqnmakmmqmmtetma": "",
      "pdfx:doi": "10.1016/j.engappai.2024.108059",
      "pdfx:robots": "noindex",
      "prism:aggregationtype": "journal",
      "prism:copyright": "Â© 2024 Elsevier Ltd. All rights reserved.",
      "prism:coverdate": "2024-07-01",
      "prism:coverdisplaydate": "1 July 2024",
      "prism:doi": "10.1016/j.engappai.2024.108059",
      "prism:issn": "0952-1976",
      "prism:number": "PA",
      "prism:pagerange": "108059",
      "prism:publicationname": "Engineering Applications of Artificial Intelligence",
      "prism:startingpage": "108059",
      "prism:url": "https://doi.org/10.1016/j.engappai.2024.108059",
      "prism:volume": "133",
      "tdm:policy": "https://www.elsevier.com/tdm/tdmrep-policy.json",
      "tdm:reservation": "1",
      "xmp:createdate": "2024-05-31T11:38:17",
      "xmp:creatortool": "Elsevier",
      "xmp:metadatadate": "2024-05-31T11:49:23",
      "xmp:modifydate": "2024-05-31T11:49:23",
      "xmpmm:documentid": "uuid:1b499bed-4ce8-4c0c-b682-0d58baae1cbe",
      "xmpmm:instanceid": "uuid:b6ed5266-03cb-4855-90c0-636283357f42",
      "xmprights:marked": "True"
    },
    "text": "Engineering Applications of Artificial Intelligence 133 (2024) 108059\nAvailable online 24 February 2024\n0952-1976/Â© 2024 Elsevier Ltd. All rights reserved.\nContents lists available at ScienceDirect\nEngineering Applications of Artificial Intelligence\njournal homepage: www.elsevier.com/locate/engappai\nResearch paper\nMixSegNet: Fusing multiple mixed-supervisory signals with multiple views of\nnetworks for mixed-supervised medical image segmentation\nZiyang Wang \na,1,\nâˆ—\n, Chen Yang \nb,1\na \nDepartment of Computer Science, University of Oxford, Wolfson Building, Parks Road, Oxford, OX1 3QD, United Kingdom\nb \nDepartment of Computer Science, Beijing University of Chemical Technology, 15 BeiSanhuan East Road, Beijing, 100029, China\nA R T I C L E I N F O\nMSC:\n0000\n1111\nKeywords:\nVision transformer\nMedical image segmentation\nMixed-supervised learning\nA B S T R A C T\nDeep learning has driven remarkable advancements in medical image segmentation. The requirement for\ncomprehensive annotations, however, poses a significant challenge due to the labor-intensive and expensive\nnature of expert annotation. Addressing this challenge, we introduce a multiple mixed-supervisory signals\nlearning (MSL) strategy, MixSegNet, that synergistically harnesses the benefits of Fully-Supervised (FSL),\nWeakly-Supervised (WSL), and Semi-Supervised Learning (SSL). This approach enables the utilization of various\ndata-efficient annotations for network training, promoting efficient medical image segmentation within realistic\nclinical scenarios. MixSegNet concurrently trains networks with a combination of limited dense labels, a larger\nproportion of cost-efficient sparse labels, and unlabeled data. The networks utilized in this system comprise\nVision Transformer (ViT) and Convolutional Neural Networks (CNN), which work together via an effective\nstrategy including network self-ensembling and label dynamic-ensembling. This strategy adeptly handles the\ntraining challenges arising from datasets with limited or absent supervisory signals. We validated MixSegNet on\na public Magnetic Resonance Imaging (MRI) cardiac segmentation benchmark dataset. It demonstrated superior\nperformance compared to 21 other SSL or WSL baseline methods under similar labeling-cost conditions, as\nsupported by comprehensive evaluation metrics, and slightly outperform classical FSL methods. The code\nfor MixSegNet, all baseline methods, and the data pre-processing techniques with the datasets for different\nannotation situations are available at https://github.com/ziyangwang007/MixSegNet.\n1. Introduction\nMedical image segmentation is crucial for a wide range of health-\ncare applications, including the diagnosis, treatment planning, and\nmonitoring of various medical conditions. It involves the classification\nof each pixel in an input image, such as a CT scan, ultrasound, or MRI,\nto determine whether each pixel belongs to organs, tumors, or other\nregions of interest (Bernard et al., 2018; Menze et al., 2014; Wang et al.,\n2021).\nConvolutional Neural Networks (CNN), a type of deep learning ap-\nproach, have demonstrated substantial efficacy in segmentation tasks,\noften surpassing traditional techniques (He et al., 2016; Long et al.,\n2015; Ronneberger et al., 2015; Chen et al., 2017). Typically, a CNN-\nbased segmentation network, named UNet, employs an encoder for\nfeature extraction and a decoder for spatial dimension recovery, using\nskip connections to facilitate the transfer of ample semantic feature\ninformation (Ronneberger et al., 2015). Further advancements such as\nattention mechanisms (Woo et al., 2018), residual learning (He et al.,\nâˆ— \nCorresponding author.\nE-mail addresses: ziyang.wang@cs.ox.ac.uk (Z. Wang), chenyang@buct.edu.cn (C. Yang).\n1 \nZiyang Wang and Chen Yang contributed equally to this work.\n2016), and dense connections (Huang et al., 2017) have been incor-\nporated into various CNN-based segmentation networks to enhance\nperformance (Wang et al., 2017; Chen et al., 2018; Guan et al., 2019).\nRecently, Vision Transformers (ViT), a self-attention-based architec-\nture (Vaswani et al., 2017), has proven to outdo CNN in handling larger\ndatasets due to its ability to model long-range dependencies (Dosovit-\nskiy et al., 2020). Notable efforts like SegFormer (Xie et al., 2021),\nSwin-Transformer (Liu et al., 2021, 2022a), and Segmenter (Strudel\net al., 2021) have yielded promising results in dense prediction tasks\nsuch as image segmentation. For medical image segmentation, most\nViT-related research, including TransUNet (Chen et al., 2021b), Swin-\nUNet (Cao et al., 2022), SwinUNet3+ (Wang et al., 2023), and Hybrid-\nUNet (Wang et al., 2022b), draws inspiration from the traditional\nCNN-based U-shape encoderâ€“decoder UNet (Ronneberger et al., 2015).\nYet, the performance of both ViT- and CNN-based networks heavily\ndepends on the availability of a substantial volume of annotated data,\nwhich is often limited in the medical field due to the necessity for\nhttps://doi.org/10.1016/j.engappai.2024.108059\nReceived 5 December 2023; Received in revised form 22 January 2024; Accepted 5 February 2024\n\nEngineering Applications of Artificial Intelligence 133 (2024) 108059\n2\nZ. Wang and C. Yang\nFig. 1. The proposed mixed-supervised learning framework for medical image\nsegmentation.\nexpertise and the time-consuming nature of manual annotation (You\net al., 2022; Wu et al., 2022; Chen et al., 2021a).\nTo address the scarcity of annotated data, numerous strategies such\nas Weakly-Supervised Learning (WSL) (Tang et al., 2018; Liu et al.,\n2022b; Luo et al., 2022a) and Semi-Supervised Learning (SSL) (Chen\net al., 2021a; Zhang et al., 2017; Wang et al., 2022d) have been pro-\nposed. These approaches attempt to alleviate the data shortage problem\nby leveraging limited labeled data in conjunction with unlabeled or\npartially labeled data. For example, a MRI cardiac raw images from\nACDC (Bernard et al., 2018), the corresponding ground truth annotated\non each pixel, and sparse annotation via scribble are briefly sketched\nin Fig. 2. For SSL and WSL tasks, networks are typically initiated\nwith a limited supervision signal, then unlabeled or sparse annotation\nis expanded into dense labels (pseudo labels) using prior knowledge.\nThese pseudo labels can subsequently be used to expand the dataset and\niteratively train the network. Consistency regularization, also known\nas consistency-aware training, is a key concept for SSL and WSL and it\naims to ensure the network maintains similar predictions under various\ndata and network perturbations (Laine and Aila, 2016; Tarvainen and\nValpola, 2017). However, these strategies often fail to fully utilize\nthe complementary information available across different supervision\nlevels, particularly in practical clinical scenarios.\nIn this study, we introduce a novel fused multiple supervisory\nsignal learning approach, MixSegNet, which integrates FSL, WSL, and\nSSL techniques, as illustrated in Fig. 1. The MixSegNet also combines\nthe strengths of both CNN- and ViT-based segmentation networks to\nefficiently segment medical images. With CNN capturing local spatial\ninformation and ViT focusing on global contextual features, MixSegNet\neffectively encapsulates both local and global features. To enhance the\nrobustness of our approach, we propose three advanced schemes for\nMSL purpose: (i) Network self-ensembling, which separately deployed\nfor CNN- and ViT-based networks via Exponential Moving Average\n(EMA) (Tarvainen and Valpola, 2017) under feature perturbation to\nimprove robustness; (ii) Label dynamic-ensembling, facilitating mutual\nbenefits via multi-view learning between CNN- and ViT-based networks\nthrough jointly contributed pseudo labels; (iii) Scribble label learning,\nallowing both networks to be trained using sparse labels. These ad-\nvanced learning strategies allow MixSegNet to be properly trained with\nvarious types of labeled data.\nWe evaluate MixSegNetâ€™s efficacy using a public MRI cardiac seg-\nmentation dataset (Bernard et al., 2018) with novel data pre-processing\ntechniques. Our experimental results reveal that our approach delivers\nstate-of-the-art performance, outperforming existing weakly-supervised\nand semi-supervised learning techniques under equivalent annota-\ntion cost conditions, and even achieving slightly higher performance\nagainst conventional fully-supervised methods under the same hyper-\nparameter setting. To the best of our knowledge, this is the first\nFig. 2. Example MRI cardiac images with corresponding ground truth and scribble\nannotations.\nwork to harness the feature learning capabilities of ViT and CNN for\nmedical image segmentation under this specific data-efficient situation,\ni.e., MSL. The contributions of MixSegNet are fivefold:\n1. We present a novel, challenging, yet practical data situation for\nmulti-supervised learning within the realm of medical image\nsegmentation. MixSegNet is trained using a small amount of\nfully labeled data, a large volume of unlabeled data, and a\nconsiderable amount of data labeled with scribbles, which is a\nmore feasible annotation method for clinicians.\n2. We utilize a network self-ensembling technique separately im-\nplemented for CNN and ViT networks. Within each pair of\nnetworks, one network is updated using EMA and guides the\ngroup of networks under the principle of consistency aware-\nness. This enhances the networkâ€™s robustness, particularly under\nconditions of limited-signal supervision.\n3. We propose label dynamic-ensembling, applicable to both CNN\nand ViT. This strategy allows all networks in the group to\njointly contribute to pseudo-label generation. Consequently, the\ndataset can be expanded with fully annotated data, supervising\neach network in turn. This mutual learning strategy allows the\nnetworks to benefit from each other, thus enhancing the overall\nsegmentation performance.\n4. We put forward a scribble label learning scheme that allows\nthe training of segmentation networks with scribbles, a type of\nsparse label. Given that scribble annotation is a less precise yet\nmore feasible method for clinicians, our approach adds practical\nvalue by accommodating this form of annotation.\n5. We rigorously assess the performance of MixSegNet against 21\nbaseline methods under the same annotation cost and hyper-\nparameter conditions including: fully-supervised learning meth-\nods (Ronneberger et al., 2015; Cao et al., 2021), semi-supervised\n(Wang et al., 2022b; Chen et al., 2021a; Zhang et al., 2017; Wang\net al., 2022d; Tarvainen and Valpola, 2017; Vu et al., 2019; Yu\net al., 2019; Verma et al., 2022; Wang and Voiculescu, 2022;\nQiao et al., 2018; Wang et al., 2022a,c; Wang and Voiculescu,\n2023a), weakly-supervised learning methods (Tang et al., 2018;\nLiu et al., 2022b; Javanmardi et al., 2016; Lee and Jeong,\n2020; Kim and Ye, 2019; Wang and Voiculescu, 2023b). MixSeg-\nNet achieves state-of-the-art results, highlighting its superiority.\nMixSegNet, all baseline methods, and our modified dataset for\nMSL studies are made to publicly available.\n\nEngineering Applications of Artificial Intelligence 133 (2024) 108059\n3\nZ. Wang and C. Yang\nFig. 3. The framework of multiple supervisory signal learning with multiple networks for medical image segmentation.\n2. Related work\n2.1. Fully supervised image segmentation\nMedical image segmentation has seen significant advancements in\nrecent years, with numerous deep learning architectures proposed for\nimproving the accuracy and robustness of semantic segmentation tasks.\nAt the heart of these advances have been CNN-based architectures such\nas Fully Convolutional Networks (FCN) (Long et al., 2015), UNet (Ron-\nneberger et al., 2015), V-Net (Milletari et al., 2016), and DeepLab (Chen\net al., 2017). These networks have revolutionized the field of medical\nimage segmentation, demonstrating significant performance improve-\nments on a variety of tasks involving different imaging modalities, such\nas CT, MRI, and ultrasound images (Ronneberger et al., 2015; Wang\net al., 2022b; Milletari et al., 2016). To further improve the perfor-\nmance of these networks, additional strategies have been investigated.\nThese include the use of 3D convolutions to capture volumetric context\nin images (Ã‡iÃ§ek et al., 2016), atrous convolutions to control the field-\nof-view (Wang and Voiculescu, 2021), residual connections to tackle\nvanishing gradient problem (He et al., 2016), attention mechanisms to\nenhance feature extraction (Oktay et al., 2018), and densely connected\nlayers for feature reuse and better gradient flow (Huang et al., 2017).\nMore recently, the self-attention mechanism, a key component of\nthe Transformer architecture (Vaswani et al., 2017), has been inte-\ngrated into image processing tasks. The ViT (Dosovitskiy et al., 2020),\nwhich leverages the Transformer architecture, has shown remarkable\nperformance in image classification tasks. Encouraged by this success,\nresearchers have extended the application of ViT to dense prediction\ntasks such as image segmentation, leading to architectures like the\nSwin Transformer (Liu et al., 2021), SegFormer (Xie et al., 2021),\nSegmenter (Strudel et al., 2021), and nnformer (Zhou et al., 2021).\nIn the realm of medical image analysis, efforts have been made to\nincorporate the merits of both CNN and ViT. This has resulted in\nhybrid architectures such as TransUNet (Chen et al., 2021b), Un-\netr (Hatamizadeh et al., 2022), SwinUNet (Cao et al., 2022), and\nSwinUNet3+ (Wang et al., 2023), which aim to exploit the local feature\ncapturing capability of CNN and the long-range dependency modeling\npower of Transformers. Our proposed MixSegNet addresses this need by\nintegrating CNN- and ViT-based Encoder-Decoder style segmentation\nnetworks in a unified framework.\n2.2. Image segmentation with limited annotations\nMedical image segmentation often faces challenges due to the\nscarcity of fully annotated data, which makes supervised learning ap-\nproaches indispensable in this domain. Semi-/weakly-supervised learn-\ning typically encompasses strategies to train networks with sparse\nsignals. This can be achieved through SSL (Zhang et al., 2017; Vu\net al., 2019; Yu et al., 2019; Verma et al., 2022), which addresses\nscenarios with a small amount of labeled data complemented by a large\nvolume of raw unlabeled data, or through WSL (Luo et al., 2022a;\nWang and Voiculescu, 2023b; ReiÃŸ et al., 2021; Lin et al., 2016; Zhang\nand Zhuang, 2022), which copes with scenarios where annotations are\nsparse or low-cost, such as bounding boxes, points, or scribbles.\nA dominant strategy in limited-supervised learning is consistency\nregularization, which posits that a networkâ€™s predictions for a limited\nsignal should remain consistent across various perturbations. Such per-\nturbations are generally categorized into two types: data perturbations\nand network perturbations. Data perturbations involve applying data\naugmentation techniques to the input data, which is expected to yield\nconsistent predictions. This strategy is exemplified by methods such as\nFixMatch, Mean Teacher, and MixUp (Tarvainen and Valpola, 2017;\nVerma et al., 2022; Sohn et al., 2020). On the other hand, network\nperturbations leverage multiple decoders to enforce consistency in their\noutput predictions. This strategy forms the core of methods like cross\npseudo supervision, and multi-view learning (Chen et al., 2021a; Wang\nand Voiculescu, 2022; Wang et al., 2022a).\nSome of prior research has delved into the amalgamation of diverse\nlabeling levels. For instance, Wicaksana et al. (2022) introduced a\nsemi-supervised learning (SSL) approach utilizing bounding box level\nsupervision within a federated learning framework. Similarly, Pan\net al. (2022) developed a label-efficient, hybrid-supervised learning\nmodel with an integrated regularization strategy specifically tailored\nfor medical image segmentation. Despite the significant contributions\nof these methods, they often operate under specific data situations, lim-\niting their practicality in real-world clinical settings where various data\nsituations may exist simultaneously. Following the previous work and\nabove concern, we propose MixSegNet that efficiently integrates the\nstrengths of both CNN- and ViT-based architectures and accommodates\nvarious data situations including SSL and WSL concurrently. MixSegNet\nemploys a series of innovative schemes that ensure a synergistic inter-\nplay between the group of networks, enabling them to benefit from one\nanother in handling different data situations.\n\nEngineering Applications of Artificial Intelligence 133 (2024) 108059\n4\nZ. Wang and C. Yang\n3. Approach\nIn the context of MSL for medical image segmentation, we designate\nthe dataset ğ‹, ğ”, ğ’, and ğ“ as the labeled training dataset, unlabeled\ntraining dataset, scribble-labeled training dataset, and testing dataset,\nrespectively. We denote a batch of labeled training data as (ğ‘¿\nl\n, ğ’€\ngt \n) âˆˆ ğ‹,\nand a batch of labeled testing data as (ğ‘¿\nt \n, ğ’€\ngt \n) âˆˆ ğ“, both with their\ncorresponding ground truth labels. A batch of unlabeled data is denoted\nas (ğ‘¿\nu\n) âˆˆ ğ”. We also introduce a batch of scribble-labeled data denoted\nas (ğ‘¿\ns\n, ğ’€\ns\n) âˆˆ ğ’, where ğ’€\ns \nrepresents the scribble annotations provided.\nğ‘¿ âˆˆ R\nâ„Ã—ğ‘¤ \nrepresents a 2D gray-scale image, ğ’€\ngt \nâˆˆ [0, 1, 2, 3]\nâ„Ã—ğ‘¤ \nsignifies\na dense label with 4 classes annotated, and ğ’€\ns \nâˆˆ [0, 1, 2, 3, ğ‘ğ‘œğ‘›ğ‘’]\nâ„Ã—ğ‘¤\nrepresents a scribble label, with ğ‘ğ‘œğ‘›ğ‘’ indicating the absence of annota-\ntion information on some corresponding pixels. We use a segmentation\nnetwork ğ‘“ (ğœƒ) âˆ¶ ğ‘¿ â†¦ ğ’€\np\n, where ğœƒ are the networkâ€™s parameters, to\ngenerate a dense prediction map ğ’€\np \ngiven ğ‘¿.\nIn MixSegNet, we introduce a group of networks, ğ‘“\ncnn\n(ğœƒ), ğ‘“\ncnn\n(\nğœƒ),\nğ‘“\nvit \n(ğœƒ), ğ‘“\nvit \n(ğœƒ), involving both CNN- and ViT-based segmentation net-\nworks. The predictions of some networks can serve as a batch of pseudo\nlabels for unlabeled data, denoted as (ğ‘¿\nu\n, ğ’€\npseudo\n) âˆˆ ğ”, or it can be re-\nfined for scribble-labeled data, denoted as (ğ‘¿\ns\n, ğ’€\npseudo\n) âˆˆ ğ’ for retraining\nnetworks. The final evaluation is based on the differences between ğ‘Œ\np\nand ğ‘Œ\ngt \nof ğ“. The training objective in the MSL scenario is to minimize\nthe supervision loss ğ¿ğ‘œğ‘ ğ‘ \nsup \nusing labeled data, the semi-supervision\nloss ğ¿ğ‘œğ‘ ğ‘ \nsemi \nusing unlabeled data, and the scribble-supervision loss\nğ¿ğ‘œğ‘ ğ‘ \nweak \nusing scribble-labeled data. These loss functions vary by the\ntype of training data used, i.e., ğ‘¿\nl\n, ğ‘¿\nu\n, or ğ‘¿\ns\n, thereby ensuring that\nthe MSL framework adapts and learns efficiently and concurrently from\neach data scenario. The details of MixSegNet in MSL tasks, depicted in\nFig. 3, are discussed in the subsequent Sections 3.1, 3.2, 3.3, and 3.4,\nrespectively.\n3.1. Training objective\nAs demonstrated in Fig. 3, the MixSegNet framework strives to opti-\nmize the cumulative loss, which encompasses the supervision loss îˆ¸\nsup\n,\nthe semi-supervision loss îˆ¸\nsemi\n, and the weakly-supervision loss îˆ¸\nweak \n.\nEach type of loss is calculated by comparing the networkâ€™s prediction\nwith a dense label, pseudo label, or sparse label, respectively. This cu-\nmulative loss is computed separately for ViT and CNN, resulting in îˆ¸\nğ‘‰ ğ‘–ğ‘‡\nand îˆ¸\nğ¶ğ‘ğ‘ \n. This approach allows MixSegNet to concurrently optimize\nthe group of networks under different data conditions, harnessing the\nunique strengths of each network type. The overarching optimization\nobjective is to minimize the cumulative loss îˆ¸, as outlined below:\nîˆ¸ = îˆ¸\nğ‘‰ ğ‘–ğ‘‡\nsup \n+ îˆ¸\nğ¶ğ‘ğ‘\nsup\nâŸâââââââŸâââââââŸ\nğ‘ ğ‘¢ğ‘\n+ îˆ¸\nViT\nsemi \n+ îˆ¸\nCNN\nsemi\nâŸâââââââŸâââââââŸ\nğ‘ ğ‘’ğ‘šğ‘–\n+ îˆ¸\nğ‘‰ ğ‘–ğ‘‡\nweak \n+ îˆ¸\nğ¶ğ‘ğ‘\nweak\nâŸââââââââŸââââââââŸ\nğ‘¤ğ‘’ğ‘ğ‘˜\n(1)\n3.2. Network self-ensembling\nTo enhance the feature regularization of the networks in situations\nwhere there is limited supervision, ğ‘“\ncnn\n(ğœƒ) and ğ‘“\nvit \n(ğœƒ) are refined using a\nself-ensembling approach. This approach follows EMA procedure (Tar-\nvainen and Valpola, 2017), resulting in ğ‘“\ncnn\n(\nğœƒ) and ğ‘“\nvit \n(ğœƒ), which is\nrepresented by the red dashed arrow â¤ in Fig. 3. This strategy serves\ntwo main purposes: (i) it encourages consistency in learning across the\ntwo networks despite feature perturbations, and (ii) it fosters a multi-\nview learning approach by allowing the two networks to collaboratively\ngenerate pseudo labels, thereby benefiting each other. We achieve\nthis by applying Gaussian noise to the input data. The function ğ‘“ (ğœƒ)â€™s\ninference is represented in Eq. (2):\nğ’€\np \n= ğ‘“ (ğ‘¿ + ğ‘ğ‘œğ‘–ğ‘ ğ‘’; ğœƒ) (2)\nwhere ğ‘¿ âˆˆ ğ‘¿u, and the prediction ğ’€\np \nis generated either by ğ‘“\ncnn\n(ğœƒ)\nor ğ‘“\nvit \n(ğœƒ). In the EMA procedure, ğ‘“ (ğœƒ) learns directly from data with\nannotations (ğ‘¿\nl\n, ğ’€\ngt \n) and (ğ‘¿s, ğ’€ s), while ğ‘“ (ğœƒ) updates its parameters via\nEMA, as illustrated in Eq. (3).\nğœƒ\nğ‘– \n= ğ›¼ğœƒ\nğ‘– \n+ (1 âˆ’ ğ›¼)ğœƒ\nğ‘–âˆ’1 \n(3)\nwhere ğ›¼ âˆˆ [0, 1] is the balance weight for updating, and ğ‘– indicates the\nğ‘–th step in the whole training process. The prediction of ğ‘“ (\nğœƒ) is more\naccurate than ğ‘“ (ğœƒ) and is, therefore, well-suited for use as a pseudo\nlabel (Tarvainen and Valpola, 2017).\n3.3. Label dynamic-ensembling\nTo fully utilize the strength brought by network self-ensembling and\nto allow networks to benefit each other, we further introduce label\ndynamic-ensembling for generating pseudo labels. The output of ğ‘“\ncnn\n(ğœƒ)\nand ğ‘“\nvit \n(ğœƒ) is designed to ensemble pseudo labels to iteratively supervise\nğ‘“\ncnn\n(ğœƒ) and ğ‘“\nvit \n(ğœƒ). This scheme is represented by the black arrow, â†’, in\nFig. 3. The dense pseudo label, which provides a full-signal supervision,\nis formulated as:\nğ‘¦\nğ‘ğ‘ ğ‘’ğ‘¢ğ‘‘ğ‘œ \n= ğ‘ğ‘Ÿğ‘”ğ‘šğ‘ğ‘¥[ğ›½ğ‘“\ncnn\n(ğ‘¿; ğœƒ) + (1 âˆ’ ğ›½)ğ‘“\nvit \n(ğ‘¿; ğœƒ)], (4)\nwhere ğ‘¿ âˆˆ ğ‘¿\nu\n, ğ‘“\nvit \n(ğ‘¿; ğœƒ), ğ‘“\ncnn\n(ğ‘¿; ğœƒ) refer to the inference by the CNN-\nand ViT-based networks using EMA. ğ›½ âˆˆ [0, 1] is a randomly gener-\nated value, viewed as a kind of dynamic enhanced data perturbation\nmotivated by Luo et al. (2022a) and Wang and Voiculescu (2023b).\nThe pseudo labels ğ‘¦\npseudo \nserve to expand the limited-labeled data and\nsupervise ğ‘“\ncnn\n(ğœƒ), ğ‘“\nvit \n(ğœƒ) in an iterative manner.\n3.4. Scribble label learning\nTo harness scribble, the form of limited signal, for supervision, we\nadapt the strategy of Partial Cross-Entropy (ğ‘ğ¶ğ¸) for training (Lin et al.,\n2016). Compared with CrossEntropy (ğ¶ğ¸), a loss function is applied to\nall pixels in the image, aiming to minimize the discrepancy between\nthe predicted and ground truth labels in FSL segmentation tasks, we\nmodify the ğ¶ğ¸ to a ğ‘ğ¶ğ¸, which is applied only to those pixels where\nscribble annotations are available, while ignoring the unlabeled pixels,\nas illustrated in Eq. (5).\nîˆ¸\npCE\n(ğ‘¦\np\n, ğ‘¦\ns\n) = âˆ’ \nâˆ‘\nğ‘–âˆˆğœ”\nğ¿\nâˆ‘\nğ‘˜\nğ‘¦\ns\n[ğ‘–, ğ‘˜] log(ğ‘¦\np\n[ğ‘–, ğ‘˜]), (5)\nwhere ğ‘– denotes the ğ‘–th pixel, and ğœ”\nğ¿ \nrefers to the set of pixels\nannotated with scribble labels. The variable ğ‘˜ denotes the ğ‘˜th class, and\n[ğ‘–, ğ‘˜] indicates the probability of the ğ‘–th pixel belonging to the ğ‘˜th class.\nThe loss function îˆ¸\npCE \ncalculates the negative sum of the logarithm of\nthe predicted probabilities for the correct class at each labeled pixel,\nthereby pushing the network to increase these probabilities and hence,\nimprove the accuracy of its predictions.\n4. Experiments\n4.1. Dataset\nOur evaluations are conducted using the publicly accessible ACDC\n(Automated Cardiac Diagnosis Challenge) dataset from the MICCAI\nChallenge 2017 (Bernard et al., 2018). This dataset includes multi-\nclass annotations for 100 patients and encompasses the right ven-\ntricle, endocardial, and epicardial walls of the left ventricle. It fea-\ntures diverse information distributed across five subgroups: normal,\nmyocardial infarction, dilated cardiomyopathy, hypertrophic cardiomy-\nopathy, and abnormal right ventricle. To ensure compatibility with\nthe input requirements of ViT, we resize all images to a resolution\nof 224Ã—224. We generate the scribble annotations, as shown in Fig. 2,\nbased on the original ground truth, following previous work (Wang and\nVoiculescu, 2023b; Valvano et al., 2021). The data is randomly selected\nand pre-processed as dense labeled, unlabeled, scribble labeled data for\nMixSegNet training without further modification.\n\nEngineering Applications of Artificial Intelligence 133 (2024) 108059\n5\nZ. Wang and C. Yang\nTable 1\nDefinition and summary of different supervision settings.\nSupervision Definition Limitations\nFSL (Fully Supervised\nLearning)\nFSL where each image in the dataset is\nannotated with dense labels.\nHigh annotation cost and time-consuming;\nrequires extensive expert involvement.\nSSL (Semi-Supervised\nLearning)\nSSL where a portion of the dataset (e.g.,\n10%) is densely labeled and the rest remains\nunlabeled.\nMay lead to less accurate models compared\nto FSL due to limited labeled data.\nWSL (Weakly-Supervised\nLearning)\nWSL where the entire dataset is annotated\nwith less informative labels, such as\nscribbles.\nScribble, bounding box, points annotations\nmay provide insufficient information for\ncomplex tasks; difficult to handle with\nvarious types of signal.\nMSL (Mixed-Supervised\nLearning)\nMSL combining FSL, SSL, and WSL, with\ndifferent portions of the dataset receiving\ndifferent types of annotations.\nBalances annotation cost and model\naccuracy; however, it requires complex\ndesign to ensure effective learning.\n4.2. Data annotation constraints\nTo facilitate a fair comparison between different learning including\nSSL, WSL and MSL, the data annotation cost is considered, which is\nessential in the realm of medical image segmentation. A brief summary\nof each supervision scheme is detailed in Table 1. Our experiment\naims to assess the performance of our proposed method, MixSegNet\nwith MSL, in comparison to traditional SSL and WSL methods, under\nthe assumption of equivalent annotation costs. We make a practical\nassumption that the time taken to annotate 100% of the dataset with\nscribble labels is similar to the time taken to annotate 10% of the\ndataset with dense labels. This supposition is in line with earlier studies\nthat indicate scribble labeling typically requires less time than dense\nlabeling (Luo et al., 2022a; Wang and Voiculescu, 2023b; Valvano et al.,\n2021). This setup ensures a balanced comparison among SSL, WSL,\nand MSL methods under equal annotation cost conditions. Specifically,\nfor SSL, we provide ground truth for 10% of the dataset, with the\nremaining 90% treated as unlabeled data. For WSL, we apply weak\nannotations to the entire dataset using scribble labels. In the case of\nMSL, 5% of the dataset is annotated with ground truth, 50% with\nscribble labels, and the remaining 45% is left unlabeled. We ensure\nthere is no overlap between the densely labeled training set, the weakly\nlabeled training set, the unlabeled training set, the validation set, and\nthe testing set. All these sets are randomly selected from the original\nACDC dataset, maintaining the distinctness of each dataset.\n4.3. Implementation details\nOur implementation was carried out on an Ubuntu 20.04 system\nusing Python 3.8.8, PyTorch 1.10, and CUDA 11.3. The hardware\nspecifications included a single Nvidia GeForce RTX 3090 GPU and\nan Intel Core i9-10900K processor. On average, the entire process, en-\ncompassing data transfer, network training, and inference, took around\n5 h. The data was processed specifically for 2D image segmentation. All\nnetworks were trained for a total of 30,000 iterations with a batch size\nof 24. We utilized the Stochastic Gradient Descent (SGD) optimizer with\nan initial learning rate of 0.01, momentum of 0.9, and a weight decay of\n0.0001 (Robbins and Monro, 1951). Validation was conducted on the\nvalidation set every 200 iterations. The network weights were saved\nonly when the validation performance exceeded the previous best. The\nnetwork with the best performance on validation set is used for final\ntesting. It is noteworthy that these settings were uniformly applied to all\nother baseline methods without any modifications. In addition to these\nmethods, we also incorporated 5-fold cross validation was employed\nfor a more robust evaluation of our network.\n4.4. Segmentation loss\nThe loss function employed in MixSegNet is a comprehensive com-\nbination as depicted in Eq. (1). The losses îˆ¸\nsup\n, îˆ¸\nsemi \ncomprise a mix of\nDice- and Cross-Entropy (CE)-based losses. This fusion offers a balanced\nperformance by addressing class imbalance and emphasizing pixel-level\naccuracy. For the losses îˆ¸\nweak \n, we utilize the pCE loss (Lin et al., 2016),\nas formulated in Eq. (5). As demonstrated in Fig. 3, the supervised\npart is marked by a blue line, calculating the loss by comparing dense\nlabels with the predictions from ğ‘“ vit(ğœƒ), ğ‘“\ncnn\n(ğœƒ). The weakly-supervised\nsegment, illustrated with a green line, computes the loss using sparse\nlabels against the predictions from ğ‘“\nvit \n(ğœƒ), ğ‘“\ncnn\n(ğœƒ). The semi-supervised\nsection, indicated with a yellow line, calculates the loss by contrast-\ning the pseudo labels, generated by the networks ğ‘“\ncnn\n(\nğœƒ) and ğ‘“\nvit \n(ğœƒ),\nwith the predictions from ğ‘“\nvit \n(ğœƒ), ğ‘“\ncnn\n(ğœƒ). This combined loss strategy\nharnesses the benefits of different supervision types, ensuring effective\nMSL, which consequently enhances the segmentation performance of\nMixSegNet.\n4.5. Evaluation metrics\nWe utilize a range of evaluation metrics to thoroughly assess and\ncompare the performance of MixSegNet against other baseline meth-\nods. Given the multi-class segmentation nature of our dataset, we\nreport the mean values for each metric across all classes. Additionally,\ngiven our use of 5-fold validation, we also provide both the average\nand standard deviation for each metric. Similarity measures including\nthe Dice Coefficient (Dice), Intersection over Union (IoU), Accuracy\n(Acc), Precision (Pre), Sensitivity (Sen), and Specificity (Spe) have been\nused, where higher values indicate better performance. On the other\nhand, difference measures, which perform better with lower values,\nare employed to calculate the discrepancy between the predicted and\nactual segmentations. We use the 95th percentile Hausdorff Distance\n(HD95) and Average Surface Distance (ASD) as difference measures,\nproviding a measure of the spatial distance between the boundaries\nof the predicted and ground truth segmentations. For a comprehensive\nevaluation, we also present results from a fully labeled setting (100%\ndense label training set) with CNN- and ViT-based networks. Inter-\nestingly, our proposed MixSegNet method has demonstrated superior\nperformance compared to these traditional FSL methods.\n4.6. Segmentation backbone network\nThroughout our experiments, we deploy both CNN and ViT as\nthe segmentation backbone networks for MSL, WSL, and SSL train-\ning strategy. We specifically employ two well-known architectures,\nUNet (Ronneberger et al., 2015) as the representative of CNN-based\nnetworks and SwinUNet (Cao et al., 2021) to represent ViT-based\nnetworks. Both architectures encompass two layers, either CNN or Swin\nTransformers, working synergistically to achieve effective segmenta-\ntion, and are sketched in Fig. 4. The inclusion of skip connections\naids in preserving spatial information during encoding and decoding,\nbolstering performance. For ViT, the patch size is set to 4, and the\nwindow size is set to 7. The embedded dimension is set to 96, the\nnumber of multi-head attention is set to 3, 6, 12, 24 for each level\nof encoder and decoder. No dropout techniques is utilized for ViT.\n\nEngineering Applications of Artificial Intelligence 133 (2024) 108059\n6\nZ. Wang and C. Yang\nFig. 4. The CNN- and ViT-based U-shape segmentation network.\nFig. 5. Qualitative inference results of MixSegNet and all baseline methods compared against the ground truth under the same data annotation cost conditions.\nFor CNN, we use 3 Ã— 3 convolutional operations, and the number of\nCNN filter is set to 16, 32, 64, 128, 256 for each level of encoder and\ndecoder. The dropout is utilized and set to 0.05, 0.1, 0.2, 0.3, 0.5 for\neach level of encoder and decoder. This consistent setup across various\nstrategies ensures fair and balanced comparisons.\n4.7. Baseline methods\nTo provide a comprehensive comparison, we include several FSL,\nSSL and WSL baseline methods in our study(seen in Table 3), each\npaired with a specific backbone network for segmentation. For SSL,\nwe compare against the following methods: DAN (Zhang et al., 2017),\nADVENT (Vu et al., 2019), UAMT (Yu et al., 2019), ICT (Verma\net al., 2022), MT (Tarvainen and Valpola, 2017), CPS (Chen et al.,\n2021a), TVL (Wang and Voiculescu, 2022), and DCN (Qiao et al.,\n2018). These use a CNN backbone for segmentation. S4CV (Wang\net al., 2022b) uses both CNN and ViT. For methods utilizing a ViT\nbackbone, we include UAMTViT (Wang et al., 2022d), CESSViT (Wang\net al., 2022a), CAAViT (Wang et al., 2022c), and EST (Wang and\nVoiculescu, 2023a). For WSL, we incorporate the following methods:\npCE (Tang et al., 2018), TV (Javanmardi et al., 2016), USTM (Liu et al.,\n2022b), Scribble2Label (S2L) (Lee and Jeong, 2020), Mumford (Kim\nand Ye, 2019), and CRF (Tang et al., 2018), which employ a CNN\nbackbone. CHNet (Wang and Voiculescu, 2023b) uses both CNN and\nViT as backbones. Additionally, we provide FSL performance including\nCNN- and ViT-based segmentation network. For a fair comparsion, we\nutilized either 2D CNN-based UNet or 2D ViT-based UNet for all FSL,\nTable 2\nThe computational cost of segmentation backbone network.\nNetwork CNN-based UNet ViT-based UNet\nParameters 1,813,764 27,168,420\nSSL, WSL and MSL framework and the total parameters of each network\nis reported in Table 2.\n4.8. Qualitative results\nFig. 5 displays a series of inference outcomes from MixSegNet as\nwell as from all the baseline methods. These are applied to a randomly\nselected raw image from the test set and are compared against the\nestablished ground truth. In each instance, the boundaries of the re-\ngions of interest in the inference images are marked in blue, green, and\nred lines, with varying shades of gray indicating the actual network\nprediction. From these visual results, it is evident that MixSegNet,\nalong with the FSL methods, generate high-quality segmentation. These\nvisual comparisons provide an intuitive understanding of the superior\nperformance of our proposed MixSegNet relative to other learning\nmethodologies.\n4.9. Quantitative results\nOur quantitative assessment further emphasizes the effectiveness of\nMixSegNet. As exhibited in Table 3, we present a detailed comparison\n\nEngineering Applications of Artificial Intelligence 133 (2024) 108059\n7\nZ. Wang and C. Yang\nFig. 6. The distribution and comparison of IoU scores across different SSL, WSL, and MSL approaches.\nTable 3\nThe performance of all baseline methods and mixsegnet on mri cardiac test set under same annotation cost situation.\nFramework Diceâ†‘ IoUâ†‘ Accâ†‘ Preâ†‘ Senâ†‘ Speâ†‘ HD95â†“ ASDâ†“\nDAN (Zhang et al., 2017) 0.7794\n(0.06) \n0.6457\n(0.08) \n0.9942\n(0.00) \n0.8060\n(0.08) \n0.7659\n(0.05) \n0.9976\n(0.00) \n20.0674\n(7.36) \n5.6077\n(3.00)\nADVENT (Vu et al., 2019) 0.8152\n(0.04) \n0.6922\n(0.06) \n0.9953\n(0.00) \n0.8578\n(0.03) \n0.7834\n(0.06) \n0.9983\n(0.00) \n16.7614(\n(10.31) \n) 4.1916\n(3.22)\nICT (Verma et al., 2022) 0.7866\n(0.07) \n0.6560\n(0.09) \n0.9946\n(0.00) \n0.8011\n(0.09) \n0.7811\n(0.04) \n0.9974\n(0.00) \n21.7181\n(20.26) \n7.5004\n(8.84)\nMT (Tarvainen and Valpola, 2017) 0.8160\n(0.04) \n0.6932\n(0.05) \n0.9952\n(0.00) \n0.8515\n(0.03) \n0.7881\n(0.04) \n0.9982\n(0.00) \n15.5061\n(4.99) \n3.9793\n(1.44)\nUAMT (Yu et al., 2019) 0.8102\n(0.05) \n0.6871\n(0.07) \n0.9952\n(0.00) \n0.8634\n(0.02) \n0.7728\n(0.07) \n0.9985\n(0.00) \n15.5428\n(5.34) \n3.9619\n(1.89)\nCPS (Chen et al., 2021a) 0.8512\n(0.02) \n0.7446\n(0.04) \n0.9961\n(0.00) \n0.8832\n(0.01) \n0.8255\n(0.03) \n0.9986\n(0.00) \n9.7329\n(1.80) \n2.4255\n(0.51)\nTVL (Wang and Voiculescu, 2022) 0.8550\n(0.02) \n0.7502\n(0.03) \n0.9962\n(0.00) \n0.8849\n(0.01) \n0.8307\n(0.03) \n0.9986\n(0.00) \n8.6624\n(2.37) \n2.1301\n(0.61)\nS4CV (Wang et al., 2022b) 0.8758\n(0.01) \n0.7814\n(0.02) \n0.9966\n(0.00) \n0.8632\n(0.02) \n0.8927\n(0.02) \n0.9981\n(0.00) \n6.3765\n(2.04) \n1.7915\n(0.62)\nUAMViT (Wang et al., 2022d) 0.7576\n(0.07) \n0.6171\n(0.09) \n0.9938\n(0.00) \n0.7932\n(0.05) \n0.7337\n(0.08) \n0.9975\n(0.00) \n17.4453\n(8.25) \n5.4357\n(2.89)\nCAAViT (Wang et al., 2022c) 0.8724\n(0.01) \n0.7759\n(0.02) \n0.9965\n(0.00) \n0.8564\n(0.02) \n0.8935\n(0.02) \n0.9980\n(0.00) \n7.8172\n(1.25) \n2.1444\n(0.44)\nCESSV (Wang et al., 2022a) 0.7819\n(0.06) \n0.6482\n(0.07) \n0.9944\n(0.00) \n0.8235\n(0.03) \n0.7530\n(0.07) \n0.9979\n(0.00) \n13.7458\n(5.03) \n4.2125\n(1.92)\nEST (Wang and Voiculescu, 2023a) 0.7857\n(0.05) \n0.6528\n(0.07) \n0.9945\n(0.00) \n0.8220\n(0.02) \n0.7584\n(0.08) \n0.9979\n(0.00) \n14.3315\n(5.65) \n4.5105\n(2.24)\npCE (Tang et al., 2018) 0.6507\n(0.04) \n0.4875\n(0.05) \n0.9859\n(0.00) \n0.5174\n(0.05) \n0.8973\n(0.04) \n0.9874\n(0.00) \n176.4831\n(15.52) \n79.1208\n(11.78)\nTV (Javanmardi et al., 2016) 0.8668\n(0.03) \n0.7682\n(0.04) \n0.9959\n(0.00) \n0.8369\n(0.02) \n0.9053\n(0.03) \n0.9974\n(0.00) \n38.6166\n(9.90) \n9.9671\n(2.82)\nUSTM (Liu et al., 2022b) 0.8249\n(0.05) \n0.7072\n(0.08) \n0.9944\n(0.00) \n0.7767\n(0.07) \n0.8876\n(0.04) \n0.9962\n(0.00) \n80.0150\n(37.48) \n24.2134\n(11.38)\nS2L (Lee and Jeong, 2020) 0.8411\n(0.04) \n0.7299\n(0.06) \n0.9953\n(0.00) \n0.8383\n(0.03) \n0.8485\n(0.05) \n0.9977\n(0.00) \n49.6515\n(19.09) \n12.7441\n(5.02)\nMumford (Kim and Ye, 2019) 0.8767\n(0.02) \n0.7830\n(0.03) \n0.9963\n(0.00) \n0.8801\n(0.02) \n0.8779\n(0.03) \n0.9983\n(0.00) \n10.4209\n(4.06) \n3.4598\n(1.20)\nCRF (Tang et al., 2018) 0.8833\n(0.02) \n0.7938\n(0.03) \n0.9965\n(0.00) \n0.8738\n(0.02) \n0.9008\n(0.03) \n0.9981\n(0.00) \n7.6152\n(4.37) \n2.2352\n(1.26)\nCHNet (Wang and Voiculescu, 2023b) 0.8789\n(0.01) \n0.7864\n(0.02) \n0.9966\n(0.00) \n0.8543\n(0.02) \n0.9110\n(0.02) \n0.9979\n(0.00) \n10.7101\n(4.69) \n2.8359\n(0.99)\nMixSegNet 0.9169\n(0.01) \n0.8480\n(0.02) \n0.9976\n(0.00) \n0.9159\n(0.01) \n0.9187\n(0.02) \n0.9988\n(0.00) \n3.9909\n(1.44) \n1.0228\n(0.45)\nCNN (Ronneberger et al., 2015) 0.9162\n(0.02) \n0.8469\n(0.03) \n0.9976\n(0.00) \n0.9129\n(0.01) \n0.9200\n(0.02) \n0.9987\n(0.00) \n4.7912\n(2.05) \n1.3681\n(0.68)\nViT (Cao et al., 2022) 0.8987\n(0.01) \n0.8180\n(0.02) \n0.9973\n(0.00) \n0.8966\n(0.01) \n0.9011\n(0.01) \n0.9986\n(0.00) \n5.2223\n(1.94) \n1.3904\n(0.57)\nof MixSegNet with other SSL, WSL, and FSL segmentation networks.\nThese results comprise both similarity and difference measures for each\nmethod under equivalent annotation costs, ensuring a fair and compre-\nhensive performance evaluation of each approach. All values reported\nin the table are mean values computed over five-fold cross-validation,\nwith the standard deviation also included to illustrate performance\nvariability in bracket. The best-performing method for each metric is\nhighlighted in red to clearly exhibit the comparative outcomes. The\ntable vividly underscores the consistent superiority of MixSegNet over\nthe other methods, reinforcing its robustness in providing high-quality\nsegmentation results.\nIn our comprehensive evaluation, we employed both line chart and\nbox plots to analyze the segmentation performance of various models,\nwhich is illustrated in Fig. 6. The line charts, plotted with IoU thresh-\nolds on the ğ‘¥-axis and the number of images achieving these thresholds\non the ğ‘¦-axis, provided a visual depiction of each modelâ€™s ability to\nreach varying levels of segmentation precision. Models demonstrating\na pronounced curvature towards higher IoU thresholds were indicative\nof superior segmentation capabilities. The box plots further augmented\nour analysis by offering insights into the statistical distribution of\nthe IoU scores for each method. These plots highlighted the median,\nquartiles, and potential outliers in the IoU scores, offering a succinct\nstatistical summary of each modelâ€™s performance. The combination of\nline chart and box plots allowed for a nuanced understanding of the\nmodelsâ€™ performance, capturing both the general trends in segmenta-\ntion accuracy and the variability within each methodâ€™s results. This\ndual-method approach underscored the differences in segmentation\nefficacy among the models, providing a robust basis for comparing their\nperformance characteristics.\n\nEngineering Applications of Artificial Intelligence 133 (2024) 108059\n8\nZ. Wang and C. Yang\nFig. 7. The different frameworks of combinations of proposed contributions with segmentation networks for ablation study.\nTable 4\nThe quantitative results of the ablation study of different combinations of proposed contributions .\nFramework Diceâ†‘ IoUâ†‘ Accâ†‘ Preâ†‘ Senâ†‘ Speâ†‘ HD95â†“ ASDâ†“ ğ‘ƒ ğ‘ğ‘Ÿ\n10\n6\na 0.8918\n(0.01) \n0.8070\n(0.02) \n0.9969\n(0.00) \n0.8999\n(0.02) \n0.8857\n(0.03) \n0.9986\n(0.00) \n4.9941\n(1.33) \n1.1703\n(0.47) \n108.7\nb 0.9050\n(0.01) \n0.8284\n(0.02) \n0.9973\n(0.00) \n0.9070\n(0.01) \n0.9039\n(0.02) \n0.9987\n(0.00) \n4.4197\n(1.88) \n1.0277\n(0.55) \n81.5\nc 0.9101\n(0.01) \n0.8368\n(0.01) \n0.9974\n(0.00) \n0.9116\n(0.01) \n0.9093\n(0.01) \n0.9987\n(0.00) \n3.9583\n(1.43) \n0.9159\n(0.44) \n54.3\nd 0.9095\n(0.01) \n0.8358\n(0.02) \n0.9974\n(0.00) \n0.9098\n(0.01) \n0.9106\n(0.02) \n0.9987\n(0.00) \n4.0697\n(1.23) \n0.9600\n(0.32) \n7.3\ne 0.9064\n(0.02) \n0.8306\n(0.03) \n0.9973\n(0.00) \n0.9074\n(0.01) \n0.9074\n(0.03) \n0.9986\n(0.00) \n4.8492\n(1.65) \n1.2179\n(0.45) \n5.4\nf 0.9147\n(0.01) \n0.8441\n(0.02) \n0.9975\n(0.00) \n0.9092\n(0.01) \n0.9211\n(0.01) \n0.9987\n(0.00) \n4.4462\n(1.33) \n1.0475\n(0.37) \n3.6\ng(Ours) 0.9169\n(0.01) \n0.8480\n(0.02) \n0.9976\n(0.00) \n0.9159\n(0.01) \n0.9187\n(0.02) \n0.9988\n(0.00) \n3.9909\n(1.44) \n1.0228\n(0.45) \n57.9\nh 0.9099\n(0.01) \n0.8365\n(0.01) \n0.9974\n(0.00) \n0.9163\n(0.01) \n0.9042\n(0.02) \n0.9988\n(0.00) \n4.0726\n(0.98) \n0.9085\n(0.37) \n30.8\ni 0.9141\n(0.01) \n0.8432\n(0.02) \n0.9975\n(0.00) \n0.9158\n(0.01) \n0.9136\n(0.03) \n0.9988\n(0.00) \n4.1002\n(1.45) \n1.0073\n(0.45) \n56.1\nj 0.9083\n(0.01) \n0.8335\n(0.02) \n0.9973\n(0.00) \n0.9078\n(0.01) \n0.9093\n(0.01) \n0.9987\n(0.00) \n4.4500\n(1.69) \n1.0711\n(0.53) \n29.0\n4.10. Ablation study\nIn order to rigorously evaluate the efficacy of MixSegNet and its\nconstituent components, an extensive ablation study was undertaken.\nThis study involved testing a variety of configurations that integrate\nCNN and ViT in diverse arrangements, as delineated in Fig. 7. Mul-\ntiple iterations of MixSegNet were examined, each characterized by\na distinct amalgamation of CNN and ViT elements. The primary ob-\njective of this exploration was to discern the key design aspects that\nsubstantially contribute to the enhanced performance of MixSegNet.\nFor a qualitative assessment, example inference results are showcased\nin Fig. 8, while quantitative outcomes for these configurations are\nsystematically presented in Table 4. Furthermore, the computational\nexpenditure for each variant was quantified in terms of the networkâ€™s\nparameters, denoted as ğ‘ƒ ğ‘ğ‘Ÿ\n10\n6 \n. This juxtaposition of results enables a\ncomprehensive understanding of the most effective combination and\nscheme pertinent to our specific context.\n5. Conclusion\nThis paper introduced a novel MSL method for medical image\nsegmentation, named MixSegNet, which amalgamates the benefits of\nsupervised, semi-supervised, and weakly-supervised learning strategies.\nOur approach was developed with real-world clinical scenarios in mind,\n\nEngineering Applications of Artificial Intelligence 133 (2024) 108059\n9\nZ. Wang and C. Yang\nFig. 8. Qualitative inference results of different combinations of proposed contributions of MixSegNet for ablation study.\nwhere procuring dense labels is expensive and expert time is limited.\nThe versatility of MixSegNet to learn from varying degrees and types\nof annotations makes it a potent solution for practical deployment in\nthe medical imaging domain. Moreover, our network exhibited state-\nof-the-art performance against fully-supervised, semi-supervised, and\nweakly-supervised learning methods under equivalent annotation cost,\ndemonstrating the effectiveness of our multi-supervised strategy. In\nthe future, we plan to broaden the capabilities of our network to\naccommodate more diverse types of annotations, such as point and\nbounding box annotations. This enhancement will make MixSegNet\neven more flexible and robust, meeting the ever-evolving requirements\nof medical image segmentation tasks.\nCRediT authorship contribution statement\nZiyang Wang: Conceptualization, Data curation, Formal analysis,\nInvestigation, Methodology, Project administration, Resources, Soft-\nware, Supervision, Validation, Visualization, Writing â€“ original draft,\nWriting â€“ review & editing. Chen Yang: Conceptualization, Formal\nanalysis, Investigation, Methodology, Writing â€“ review & editing.\nDeclaration of competing interest\nThe authors declare that they have no known competing finan-\ncial interests or personal relationships that could have appeared to\ninfluence the work reported in this paper.\nData availability\nAvailable with a GitHub project page provided.\nReferences\nBernard, O., Lalande, A., Zotti, C., Cervenansky, F., Yang, X., Heng, P.-A., Cetin, I.,\nLekadir, K., Camara, O., Ballester, M.A.G., et al., 2018. Deep learning techniques for\nautomatic MRI cardiac multi-structures segmentation and diagnosis: is the problem\nsolved? IEEE Trans. Med. Imaging 37 (11), 2514â€“2525.\nCao, H., Wang, Y., Chen, J., Jiang, D., Zhang, X., Tian, Q., Wang, M., 2022. Swin-\nunet: Unet-like pure transformer for medical image segmentation. In: European\nConference on Computer Vision. Springer, pp. 205â€“218.\nCao, H., et al., 2021. Swin-unet: Unet-like pure transformer for medical image\nsegmentation. arXiv preprint arXiv:2105.05537.\nChen, H., Dou, Q., Yu, L., Qin, J., Heng, P.-A., 2018. VoxResNet: Deep voxelwise\nresidual networks for brain segmentation from 3D MR images. NeuroImage 170,\n446â€“455.\nChen, L.-C., Papandreou, G., Kokkinos, I., Murphy, K., Yuille, A.L., 2017. Deeplab:\nSemantic image segmentation with deep convolutional nets, atrous convolution,\nand fully connected crfs. IEEE Trans. Pattern Anal. Mach. Intell. 40 (4), 834â€“848.\nChen, X., Yuan, Y., Zeng, G., Wang, J., 2021a. Semi-supervised semantic segmentation\nwith cross pseudo supervision. In: Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition. pp. 2613â€“2622.\nChen, J., et al., 2021b. Transunet: Transformers make strong encoders for medical\nimage segmentation. arXiv preprint arXiv:2102.04306.\nÃ‡iÃ§ek, Ã–., Abdulkadir, A., Lienkamp, S.S., Brox, T., Ronneberger, O., 2016. 3D U-net:\nlearning dense volumetric segmentation from sparse annotation. In: Medical Image\nComputing and Computer-Assisted Interventionâ€“MICCAI 2016: 19th International\nConference, Athens, Greece, October 17-21, 2016, Proceedings, Part II 19. Springer,\npp. 424â€“432.\nDosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T.,\nDehghani, M., Minderer, M., Heigold, G., Gelly, S., et al., 2020. An image is\nworth 16x16 words: Transformers for image recognition at scale. arXiv preprint\narXiv:2010.11929.\nGuan, S., Khan, A.A., Sikdar, S., Chitnis, P.V., 2019. Fully dense UNet for 2-D sparse\nphotoacoustic tomography artifact removal. IEEE J. Biomed. Health Inform. 24 (2),\n568â€“576.\nHatamizadeh, A., Tang, Y., Nath, V., Yang, D., Myronenko, A., Landman, B., Roth, H.R.,\nXu, D., 2022. Unetr: Transformers for 3d medical image segmentation. In: Proceed-\nings of the IEEE/CVF Winter Conference on Applications of Computer Vision. pp.\n574â€“584.\nHe, K., Zhang, X., Ren, S., Sun, J., 2016. Deep residual learning for image recog-\nnition. In: Proceedings of the IEEE Conference on Computer Vision and Pattern\nRecognition. pp. 770â€“778.\nHuang, G., Liu, Z., Van Der Maaten, L., Weinberger, K.Q., 2017. Densely connected\nconvolutional networks. In: Proceedings of the IEEE Conference on Computer Vision\nand Pattern Recognition. pp. 4700â€“4708.\nJavanmardi, M., Sajjadi, M., Liu, T., Tasdizen, T., 2016. Unsupervised total variation\nloss for semi-supervised deep learning of semantic segmentation. arXiv preprint\narXiv:1605.01368.\nKim, B., Ye, J.C., 2019. Mumfordâ€“shah loss functional for image segmentation with\ndeep learning. IEEE Trans. Image Process. 29, 1856â€“1866.\nLaine, S., Aila, T., 2016. Temporal ensembling for semi-supervised learning. arXiv\npreprint arXiv:1610.02242.\n\nEngineering Applications of Artificial Intelligence 133 (2024) 108059\n10\nZ. Wang and C. Yang\nLee, H., Jeong, W.-K., 2020. Scribble2label: Scribble-supervised cell segmentation via\nself-generating pseudo-labels with consistency. In: Medical Image Computing and\nComputer Assisted Interventionâ€“MICCAI 2020: 23rd International Conference, Lima,\nPeru, October 4â€“8, 2020, Proceedings, Part I 23. Springer, pp. 14â€“23.\nLin, D., Dai, J., Jia, J., He, K., Sun, J., 2016. Scribblesup: Scribble-supervised\nconvolutional networks for semantic segmentation. In: Proceedings of the IEEE\nConference on Computer Vision and Pattern Recognition. pp. 3159â€“3167.\nLiu, Z., Hu, H., Lin, Y., Yao, Z., Xie, Z., Wei, Y., Ning, J., Cao, Y., Zhang, Z., Dong, L., et\nal., 2022a. Swin transformer v2: Scaling up capacity and resolution. In: Proceedings\nof the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp.\n12009â€“12019.\nLiu, Z., Lin, Y., Cao, Y., Hu, H., Wei, Y., Zhang, Z., Lin, S., Guo, B., 2021. Swin\ntransformer: Hierarchical vision transformer using shifted windows. In: Proceedings\nof the IEEE/CVF International Conference on Computer Vision. pp. 10012â€“10022.\nLiu, X., Yuan, Q., Gao, Y., He, K., Wang, S., Tang, X., Tang, J., Shen, D., 2022b.\nWeakly supervised segmentation of COVID19 infection with scribble annotation on\nCT images. Pattern Recogn. 122, 108341.\nLong, J., Shelhamer, E., Darrell, T., 2015. Fully convolutional networks for semantic\nsegmentation. In: Proceedings of the IEEE Conference on Computer Vision and\nPattern Recognition. pp. 3431â€“3440.\nLuo, X., Hu, M., Liao, W., Zhai, S., Song, T., Wang, G., Zhang, S., 2022a. Scribble-\nsupervised medical image segmentation via dual-branch network and dynamically\nmixed pseudo labels supervision. In: International Conference on Medical Image\nComputing and Computer-Assisted Intervention. Springer, pp. 528â€“538.\nMenze, B.H., Jakab, A., Bauer, S., Kalpathy-Cramer, J., Farahani, K., Kirby, J.,\nBurren, Y., Porz, N., Slotboom, J., Wiest, R., et al., 2014. The multimodal brain\ntumor image segmentation benchmark (BRATS). IEEE Trans. Med. Imaging 34 (10),\n1993â€“2024.\nMilletari, F., Navab, N., Ahmadi, S.-A., 2016. V-net: Fully convolutional neural\nnetworks for volumetric medical image segmentation. In: 2016 Fourth International\nConference on 3D Vision. 3DV, Ieee, pp. 565â€“571.\nOktay, O., et al., 2018. Attention u-net: Learning where to look for the pancreas. arXiv\npreprint arXiv:1804.03999.\nPan, J., Bi, Q., Yang, Y., Zhu, P., Bian, C., 2022. Label-efficient hybrid-supervised\nlearning for medical image segmentation. In: Proceedings of the AAAI Conference\non Artificial Intelligence. pp. 2026â€“2034.\nQiao, S., Shen, W., Zhang, Z., Wang, B., Yuille, A., 2018. Deep co-training for semi-\nsupervised image recognition. In: Proceedings of the European Conference on\nComputer Vision (Eccv). pp. 135â€“152.\nReiÃŸ, S., Seibold, C., Freytag, A., Rodner, E., Stiefelhagen, R., 2021. Every annotation\ncounts: Multi-label deep supervision for medical image segmentation. In: Proceed-\nings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp.\n9532â€“9542.\nRobbins, H., Monro, S., 1951. A stochastic approximation method. Ann. Math. Statist.\n400â€“407.\nRonneberger, O., Fischer, P., Brox, T., 2015. U-net: Convolutional networks for biomed-\nical image segmentation. In: Medical Image Computing and Computer-Assisted\nInterventionâ€“MICCAI 2015: 18th International Conference, Munich, Germany,\nOctober 5-9, 2015, Proceedings, Part III 18. Springer, pp. 234â€“241.\nSohn, K., Berthelot, D., Carlini, N., Zhang, Z., Zhang, H., Raffel, C.A., Cubuk, E.D.,\nKurakin, A., Li, C.-L., 2020. Fixmatch: Simplifying semi-supervised learning with\nconsistency and confidence. Adv. Neural Inf. Process. Syst. 33, 596â€“608.\nStrudel, R., Garcia, R., Laptev, I., Schmid, C., 2021. Segmenter: Transformer for\nsemantic segmentation. In: Proceedings of the IEEE/CVF International Conference\non Computer Vision. pp. 7262â€“7272.\nTang, M., Perazzi, F., Djelouah, A., Ben Ayed, I., Schroers, C., Boykov, Y., 2018. On\nregularized losses for weakly-supervised cnn segmentation. In: Proceedings of the\nEuropean Conference on Computer Vision. ECCV, pp. 507â€“522.\nTarvainen, A., Valpola, H., 2017. Mean teachers are better role models: Weight-\naveraged consistency targets improve semi-supervised deep learning results. Adv.\nNeural Inf. Process. Syst. 30.\nValvano, G., Leo, A., Tsaftaris, S.A., 2021. Learning to segment from scribbles\nusing multi-scale adversarial attention gates. IEEE Trans. Med. Imaging 40 (8),\n1990â€“2001.\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.N., Kaiser, Å.,\nPolosukhin, I., 2017. Attention is all you need. Adv. Neural Inf. Process. Syst. 30.\nVerma, V., Kawaguchi, K., Lamb, A., Kannala, J., Solin, A., Bengio, Y., Lopez-Paz, D.,\n2022. Interpolation consistency training for semi-supervised learning. Neural Netw.\n145, 90â€“106.\nVu, T.-H., Jain, H., Bucher, M., Cord, M., PÃ©rez, P., 2019. Advent: Adversarial entropy\nminimization for domain adaptation in semantic segmentation. In: Proceedings\nof the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp.\n2517â€“2526.\nWang, Z., Dong, N., Voiculescu, I., 2022a. Computationally-efficient vision transformer\nfor medical image semantic segmentation via dual pseudo-label supervision.\nIn: 2022 IEEE International Conference on Image Processing. ICIP, IEEE, pp.\n1961â€“1965.\nWang, F., Jiang, M., Qian, C., Yang, S., Li, C., Zhang, H., Wang, X., Tang, X.,\n2017. Residual attention network for image classification. In: Proc IEEE CVPR.\npp. 3156â€“3164.\nWang, Z., Li, T., Zheng, J.-Q., Huang, B., 2022b. When cnn meet with vit: Towards\nsemi-supervised learning for multi-class medical image semantic segmentation. In:\nEuropean Conference on Computer Vision. Springer, pp. 424â€“441.\nWang, Z., Su, M., Zheng, J.-Q., Liu, Y., 2023. Densely connected swin-unet for\nmultiscale information aggregation in medical image segmentation. In: 2023 IEEE\nInternational Conference on Image Processing. ICIP, IEEE, pp. 940â€“944.\nWang, Z., Voiculescu, I., 2021. Quadruple augmented pyramid network for multi-class\nCOVID-19 segmentation via CT. In: 2021 43rd Annual International Conference of\nthe IEEE Engineering in Medicine & Biology Society. EMBC, IEEE, pp. 2956â€“2959.\nWang, Z., Voiculescu, I., 2022. Triple-view feature learning for medical image seg-\nmentation. In: MICCAI Workshop on Resource-Efficient Medical Image Analysis.\nSpringer, pp. 42â€“54.\nWang, Z., Voiculescu, I., 2023a. Exigent examiner and mean teacher: An advanced 3D\nCNN-based semi-supervised brain tumor segmentation framework. In: Workshop on\nMedical Image Learning with Limited and Noisy Data. Springer, pp. 181â€“190.\nWang, Z., Voiculescu, I., 2023b. Weakly supervised medical image segmentation\nthrough dense combinations of dense pseudo-labels. In: MICCAI Workshop on Data\nEngineering in Medical Imaging. Springer, pp. 1â€“10.\nWang, Z., Zhang, Z., Voiculescu, I., 2021. RAR-U-net: a residual encoder to attention\ndecoder by residual connections framework for spine segmentation under noisy\nlabels. In: 2021 IEEE International Conference on Image Processing. ICIP, IEEE,\npp. 21â€“25.\nWang, Z., Zhao, W., Ni, Z., 2022c. Adversarial vision transformer for medical image\nsemantic segmentation with limited annotations. In: 33rd British Machine Vision\nConference 2022. BMVC 2022, London, UK, November 21-24, 2022, BMVA Press,\nURL https://bmvc2022.mpi-inf.mpg.de/1002.pdf.\nWang, Z., Zheng, J.-Q., Voiculescu, I., 2022d. An uncertainty-aware transformer for\nMRI cardiac semantic segmentation via mean teachers. In: Annual Conference on\nMedical Image Understanding and Analysis. Springer, pp. 494â€“507.\nWicaksana, J., Yan, Z., Zhang, D., Huang, X., Wu, H., Yang, X., Cheng, K.-T., 2022.\nFedMix: Mixed supervised federated learning for medical image segmentation. IEEE\nTrans. Med. Imaging.\nWoo, S., et al., 2018. CBAM: Convolutional block attention module. In: Proc. pp. 3â€“19.\nWu, H., Wang, Z., Song, Y., Yang, L., Qin, J., 2022. Cross-patch dense contrastive\nlearning for semi-supervised segmentation of cellular nuclei in histopathologic\nimages. In: Proceedings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition. pp. 11666â€“11675.\nXie, E., Wang, W., Yu, Z., Anandkumar, A., Alvarez, J.M., Luo, P., 2021. SegFormer:\nSimple and efficient design for semantic segmentation with transformers. Adv.\nNeural Inf. Process. Syst. 34, 12077â€“12090.\nYou, C., Zhou, Y., Zhao, R., Staib, L., Duncan, J.S., 2022. Simcvd: Simple con-\ntrastive voxel-wise representation distillation for semi-supervised medical image\nsegmentation. IEEE Trans. Med. Imaging 41 (9), 2228â€“2237.\nYu, L., Wang, S., Li, X., Fu, C.-W., Heng, P.-A., 2019. Uncertainty-aware self-\nensembling model for semi-supervised 3D left atrium segmentation. In: Medical\nImage Computing and Computer Assisted Interventionâ€“MICCAI 2019: 22nd Inter-\nnational Conference, Shenzhen, China, October 13â€“17, 2019, Proceedings, Part II\n22. Springer, pp. 605â€“613.\nZhang, Y., Yang, L., Chen, J., Fredericksen, M., Hughes, D.P., Chen, D.Z., 2017.\nDeep adversarial networks for biomedical image segmentation utilizing unannotated\nimages. In: Medical Image Computing and Computer Assisted Intervention- MICCAI\n2017: 20th International Conference, Quebec City, QC, Canada, September 11-13,\n2017, Proceedings, Part III 20. Springer, pp. 408â€“416.\nZhang, K., Zhuang, X., 2022. Cyclemix: A holistic strategy for medical image segmen-\ntation from scribble supervision. In: Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition. pp. 11656â€“11665.\nZhou, H.-Y., Guo, J., Zhang, Y., Yu, L., Wang, L., Yu, Y., 2021. Nnformer: Interleaved\ntransformer for volumetric segmentation. arXiv preprint arXiv:2109.03201.",
    "version": "5.3.31"
  },
  {
    "numpages": 13,
    "numrender": 13,
    "info": {
      "PDFFormatVersion": "1.7",
      "Language": "English",
      "EncryptFilterName": null,
      "IsLinearized": true,
      "IsAcroFormPresent": false,
      "IsXFAPresent": false,
      "IsCollectionPresent": false,
      "IsSignaturesPresent": false,
      "Creator": "Elsevier",
      "Custom": {
        "CrossMarkDomains[1]": "elsevier.com",
        "CrossmarkMajorVersionDate": "2010-04-23",
        "CreationDate--Text": "26th October 2025",
        "ElsevierWebPDFSpecifications": "7.0.1",
        "robots": "noindex",
        "doi": "10.1016/j.dsp.2025.105494",
        "CrossMarkDomains[2]": "sciencedirect.com",
        "CrossmarkDomainExclusive": "true"
      },
      "ModDate": "D:20251026010132Z",
      "Author": "Hongxu Guo",
      "Title": "Semi-supervised cardiac MRI image segmentation via learning consistency under differential perturbations",
      "Keywords": "Semi-supervised learning,Medical image segmentation,Consistency regularization,Pseudo-labeling,Cardiac MRI image",
      "CreationDate": "D:20251025122934Z",
      "Producer": "Acrobat Distiller 8.1.0 (Windows)",
      "Subject": "Digital Signal Processing, 168 (2026) 105494. doi:10.1016/j.dsp.2025.105494"
    },
    "metadata": {
      "crossmark:crossmarkdomains": "elsevier.comsciencedirect.com",
      "crossmark:crossmarkdomainexclusive": "true",
      "crossmark:doi": "10.1016/j.dsp.2025.105494",
      "crossmark:majorversiondate": "2010-04-23",
      "dc:format": "application/pdf",
      "dc:identifier": "10.1016/j.dsp.2025.105494",
      "dc:publisher": "Elsevier Inc.",
      "dc:description": "Digital Signal Processing, 168 (2026) 105494. doi:10.1016/j.dsp.2025.105494",
      "dc:subject": [
        "Semi-supervised learning",
        "Medical image segmentation",
        "Consistency regularization",
        "Pseudo-labeling",
        "Cardiac MRI image"
      ],
      "dc:title": "Semi-supervised cardiac MRI image segmentation via learning consistency under differential perturbations",
      "dc:creator": [
        "Hongxu Guo",
        "Ying Li",
        "Xianzhe Wang",
        "Renjie He",
        "Jie Quan",
        "Lingyue Wang",
        "Lei Guo"
      ],
      "jav:journal_article_version": "VoR",
      "pdf:creationdate--text": "26th October 2025",
      "pdf:producer": "Acrobat Distiller 8.1.0 (Windows)",
      "pdf:keywords": "Semi-supervised learning,Medical image segmentation,Consistency regularization,Pseudo-labeling,Cardiac MRI image",
      "pdfx:creationdate--text": "26th October 2025",
      "pdfx:crossmarkdomains": "sciencedirect.comelsevier.com",
      "pdfx:crossmarkdomainexclusive": "true",
      "pdfx:crossmarkmajorversiondate": "2010-04-23",
      "dpairmmf9mmipnt2godyrz8nnnpiqlt__nmqgnt2ongyqnd-szwmko9eqn9iknm-rotetma": "",
      "pdfx:doi": "10.1016/j.dsp.2025.105494",
      "pdfx:robots": "noindex",
      "prism:aggregationtype": "journal",
      "prism:copyright": "Â© 2025 Published by Elsevier Inc.",
      "prism:coverdate": "2026-01-01",
      "prism:coverdisplaydate": "1 January 2026",
      "prism:doi": "10.1016/j.dsp.2025.105494",
      "prism:issn": "1051-2004",
      "prism:number": "PA",
      "prism:pagerange": "105494",
      "prism:publicationname": "Digital Signal Processing",
      "prism:startingpage": "105494",
      "prism:url": "https://doi.org/10.1016/j.dsp.2025.105494",
      "prism:volume": "168",
      "tdm:policy": "https://www.elsevier.com/tdm/tdmrep-policy.json",
      "tdm:reservation": "1",
      "xmp:createdate": "2025-10-25T12:29:34",
      "xmp:creatortool": "Elsevier",
      "xmp:metadatadate": "2025-10-26T01:01:32",
      "xmp:modifydate": "2025-10-26T01:01:32",
      "xmpmm:documentid": "uuid:1b499bed-4ce8-4c0c-b682-0d58baae1cbe",
      "xmpmm:instanceid": "uuid:b6ed5266-03cb-4855-90c0-636283357f42",
      "xmprights:marked": "True"
    },
    "text": "Semi-supervised cardiac MRI image segmentation via learning consistency\nunder differential perturbations\nHongxu Guo \na \n, Ying Li \na,b,* \n, Xianzhe Wang \na\n, Renjie He \nc\n, Jie Quan \nd\n, Lingyue Wang \na,b\n,\nLei Guo \na,b\na \nHebei Key Laboratory of Bioelectromagnetics and Neural Engineering, School of Health Sciences and Biomedical Engineering, Hebei University of Technology, Tianjin,\n300131, China\nb \nTianjin Key Laboratory of Bioelectricity and Intelligent Health, School of Health Sciences and Biomedical Engineering, Hebei University of Technology, Tianjin, 300131,\nChina\nc \nDepartment of Radiation Oncology, MD Anderson Cancer Center, Houston, 77030, USA\nd \nShenzhen Mindray Bio-Medical Electronics Co.,Ltd, Shenzhen, Guangdong, 518052, China\nA R T I C L E I N F O\nKeywords:\nSemi-supervised learning\nMedical image segmentation\nConsistency regularization\nPseudo-labeling\nCardiac MRI image\nA B S T R A C T\nProblem: Fully-supervised learning-based methods have achieved remarkable success in cardiac MRI image\nsegmentation due to the availability of large-scale labeled datasets. However, obtaining sufficient cardiac an-\nnotations not only requires expert participation, but is also costly and time-consuming, which limits the appli-\ncation of fully supervised learning methods.\nAim: To reduce the dependence of the training of segmentation models on a large amount of labeled data, this\nstudy proposes an improved semi-supervised method based on UniMatch, which aims to achieve accurate seg-\nmentation of cardiac MRI images with limited labeled data.\nMethod: Two modules named Differential Perturbation Pool and Consistency Regularization Module (DPPCRM)\nand Dual-dimensional Feature Perturbation Module (DFPM) are proposed and integrated into UniMatch to\nenhance its performance in cardiac MRI image segmentation tasks. In the DPPCRM, In DPPCRM, two differen-\ntiated perturbation pools are first designed to increase the relevance of the strongly perturbed images for the\ncardiac segmentation task and to explore a wider perturbation space. Then, a consistency regularization method\nis constructed in DPPCRM to mine and exploit the feature hidden in the strongly perturbed images, which forces\nthe model to better extract features from samples generated by the newly designed perturbation pools. While in\nthe DFPM, perturbations are performed on the features extracted from weakly perturbed images in both channel\nand spatial dimensions, which enriches the types of feature perturbations and allows the model to learn more\nrobust feature representations.\nResults: Extensive experiments on the Automated Cardiac Diagnosis Challenge (ACDC) 2017 dataset and the\nMulti-Sequence Cardiac MR Segmentation Challenge (MSCMR) 2019 challenge dataset show that the proposed\nimproved-UniMatch outperforms the state-of-the-art semi-supervised methods, which demonstrates its effec-\ntiveness for cardiac MRI image segmentation. On the ACDC dataset, the DSC and HD\n95 \nobtained by proposed\nmethod with 5% and 10% labeled data are 0.880 and 4.025 mm, 0.898 and 2.527 mm, respectively. On the\nMSCMR dataset, the DSC and HD\n95 \nobtained by proposed method with 5% and 10% labeled data are 0.726 and\n12.688 mm, 0.748 and 8.706 mm, respectively. Furthermore, ablation studies prove that the above two modules\nare all effective and essential for proposed method to achieve this very superior performance.\nConclusion: The proposed improved-UniMatch framework effectively addresses the limitations of the original\nUniMatch by introducing enhanced perturbation strategies and leveraging feature-level consistency. The results\nconfirm that the proposed method can achieve excellent cardiac segmentation accuracy with limited labeled\ndata.\n* Corresponding author at: School of Health Sciences and Biomedical Engineering, Hebei University of Technology, Tianjin, CA 300131, China.\nE-mail address: yli@hebut.edu.cn (Y. Li).\nContents lists available at ScienceDirect\nDigital Signal Processing\njournal homepage: www.elsevier.com/locate/dsp\nhttps://doi.org/10.1016/j.dsp.2025.105494\nDigit. Signal Process. 168 (2026) 105494\nAvailable online 19 July 2025\n1051-2004/Â© 2025 Published by Elsevier Inc.\n\n1. Introduction\nCardiovascular diseases (CVDs) are the leading cause of mortality\nglobally. In 2021 alone, CVDs accounted for 20.5 million deaths,\ncomprising approximately one-third of all global deaths [1]. Cardiac\nmagnetic resonance imaging (MRI) is considered as the \"gold standard\"\nfor non-invasive characterization of cardiac function and viability [2].\nSegmentation of cardiac anatomical structures in MRI images is a pre-\nrequisite for automatic diagnosis and prognosis of CVDs [3]. In recent\nyears, with the development of deep learning techniques, many seg-\nmentation models have been proposed and have led to significant im-\nprovements in the accuracy of automatic cardiac image segmentation\n[4]. However, the superior performance of these segmentation models\ngenerally relies on training with large-scale finely labeled datasets.\nUnlike the natural image domain, acquiring such datasets in medical\nimage domain is often difficult because manually annotating medical\nimages (e.g., volumetric CT or MRI scans) at the pixel/voxel level re-\nquires specialized knowledge and clinical experience, and is a tedious\nand costly process. Considering that unlabeled data is relatively easier to\ncollect from the clinic sites, semi-supervised learning has become\nincreasing active in medical image segmentation due to its ability to\nimprove model generalization by utilizing a large amount of unlabeled\ndata and a limited amount of labeled data [5].\nThe challenge of semi-supervised learning is how to extract addi-\ntional and useful training signals from unlabeled images. Consistency\nregularization [6â€“8] and pseudo-labeling [9â€“11] are two powerful\ntechniques for exploiting unlabeled data, and have been widely used in\nsemi-supervised learning algorithms [12â€“14]. The advantage of consis-\ntency regularization is that it can effectively enhance the robustness and\ngeneralization performance of the model by constraining the consis-\ntency of the modelâ€™s predictions for different perturbed versions of the\ngiven data. The advantage of the pseudo-labeling approach is that it can\neffectively utilize unlabeled data to expand the training data through\nmodel-generated pseudo-labels, thus improving the performance of the\nmodel. In recent years, several researchers have proposed to fuse\npseudo-labeling and consistency regularization into a framework to\ncombine the advantages of two methods. Among them, FixMtach [15] is\nthe pioneer study of these fusion methods, which is applied to the\nsemi-supervised image classification task. FixMatch proposes to apply\nstrongly and weakly image perturbation strategies to unlabeled data and\nto use the modelâ€™s prediction of weakly perturbed images as\npseudo-labels to supervise the modelâ€™s prediction of strongly perturbed\nimages. In addition, FixMatch filters noise in pseudo-labels by setting a\nconfidence threshold to avoid the model from receiving inaccurate la-\nbels. Due to the excellent performance of FixMatch, some researchers\nhave migrated it to the field of semi-supervised image segmentation and\nimproved it to make it more suitable for image segmentation tasks.\nAmong these, UniMatch [16] is a representative work. UniMatch in-\nherits the idea of image perturbation proposed by FixMatch, which en-\ncourages the model to predict strongly perturbed images consistently\nwith weakly perturbed images. But different with FixMatch, UniMatch\nputs more emphasis on the impact of image perturbation on the effect of\nconsistency regularization, and proposes a two-stream image perturba-\ntion strategy to further explore the perturbation space at the image level.\nIn addition, to overcome the limitation of perturbing only from the\nimage level, UniMatch introduces feature perturbation by randomly\ndiscarding features from half of the channels. In this way, UniMatch has\nachieved excellent results in several semi-supervised image segmenta-\ntion fields.\nAlthough UniMatch has achieved significant success, its framework\nstill exhibits certain limitations when applied to the task of cardiac MRI\nimage segmentation. Firstly, for cardiac MRI image segmentation, due to\nthe significant differences in the cardiac structures of the patients and\nthe unique image characteristics of different devices during imaging, a\ntargeted image perturbation strategy is required to enrich the pertur-\nbation sample range as much as possible to improve the generalization\nperformance of the model. However, the image perturbation strategy of\nUniMatch is not specifically optimized for the cardiac MRI image seg-\nmentation task, which limits its performance in the corresponding task.\nSecondly, UniMatch employs randomly discarding half of the features as\na feature perturbation, which is not only in a single form but also leads to\nincomplete information in the feature space, thus affecting the perfor-\nmance of the model. To optimize the aspect of image and feature\nperturbation of UniMatch and enhance its adaptability to cardiac MRI\nimage segmentation, two modules are proposed and integrated into the\noriginal framework of UniMatch to construct an improved version of\nUniMatch.\nTo improve the suitability of image perturbation of UniMatch for\ncardiac segmentation tasks and to better utilize the information hidden\nin strongly perturbed image features, Differential Perturbation Pool and\nConsistency Regularization Module (DPPCRM) is proposed. Firstly, two\ndifferentiated perturbation pools are constructed in the DPPCRM, which\ngenerates perturbed images using two complementary sets of image\nperturbation strategies to enrich the perturbation space at the image\nlevel and to enhance the modelâ€™s adaptability to the cardiac segmenta-\ntion task. Then, a consistency regularization method is constructed in\nDPPCRM by utilizing the extracted features of strongly perturbed im-\nages, which mine the information hidden in the features of the strongly\nperturbed images and force the model to extract more effective features\nfrom the samples generated by the newly designed perturbation pools.\nTo extract more robust feature representations from weakly perturbed\nimages and to compensate the shortcomings of UniMatch in feature\nperturbation, Dual-dimensional Feature Perturbation Module (DFPM) is\nproposed. In the DFPM, multiple perturbations in both channel and\nspatial dimensions are applied to the features extracted from weakly\nperturbed images, and consistency constraints are imposed on the pre-\ndictions of the original and perturbed features. In this way, the pertur-\nbation space at the feature level is broadened, and more robust feature\nrepresentations are enabled to be learned by the model.\nOverall, the contributions of this paper can be summarized as\nfollows:\nâ€¢ Two shortcomings of UniMatch are identified, and DPPCRM and\nDFPM are proposed to address these issues, aiming to improve the\nperformance of the UniMatch framework for semi-supervised cardiac\nsegmentation. In the DPPCRM, differential perturbation pools are\nfirst constructed by utilizing multiple perturbation strategies to\ngenerate strongly perturbed images, which increases data diversity\nand explores a wider perturbation space. Then, a consistency regu-\nlarization method are constructed by utilizing the extracted features\nof strongly perturbed images, which mines and exploits the infor-\nmation hidden in the features of the strongly perturbed images and\nenhances the ability of the model to extract features of strongly\nperturbed images. While in the DFPM, the features extracted from\nweakly perturbed images are perturbed in both the channel and\nspatial dimensions, which enriches the types of feature perturbations\nand allows the model to learn more robust feature representations.\nâ€¢ An improved-UniMatch framework is proposed by integrating the\ntwo aforementioned modules. Extensive experimental studies are\nconducted on two public cardiac MRI datasets, and the results show\nthe following: (1) The proposed improved-UniMatch significantly\noutperforms the state-of-the-art semi-supervised image segmenta-\ntion methods in the cardiac MRI image segmentation tasks in terms\nof all metrics. (2) The proposed module of DPPCRM and DFPM are\nboth effective and essential for the proposed model to achieve su-\nperior performances.\n2. Related works\n2.1. Cardiac MRI image segmentation in the supervised setting\nAccurate segmentation of cardiac magnetic resonance images is an\nH. Guo et al. \nDigital Signal Processing 168 (2026) 105494\n2\n\nimportant pre-requisite in clinical practice to reliably diagnose and\nassess cardiovascular diseases [17]. In recent years, many deep\nlearning-based segmentation models have been proposed and improved\nthe accuracy of CMR image segmentation significantly. Among them,\none of the most popular frameworks is the encoder-decoder architecture\nbased on fully convolutional network (FCN) [18] with representative\nmodel such as U-Net [19]. To further enhance the segmentation per-\nformance, many variants of U-Net [20â€“25] have been designed by\nredesigning new network structures and introducing new modules.\nFurthermore, with the rise of ViT [26], some recent research [27â€“30]\nhas attempted to merge the merits of Transformer and CNN. In terms of\nCMR segmentation under supervised settings, although these newly\ndesigned models have achieved impressive results, their limitation is\nthat their outstanding performance usually relies on training with a\nlarge amount of high-quality annotated data, which restricts their\napplication in a wider range. Therefore, it is a challenge to reduce the\nnumber of labeled data for training these models while maintaining\nacceptable performance.\n2.2. Semi-supervised medical image segmentation\nTo reduce the burden of annotation costs, a number of semi-\nsupervised medical image segmentation methods have been proposed,\nwhere consistency regularization [31â€“34] and pseudo-labeling [35â€“38]\nbased methods have arisen as the two main streams. Consistency regu-\nlarization methods aim to enforce consistency between the prediction\nresults of perturbed unlabeled images, where the perturbation can be\nimposed on the input image [36,39,40], the features of the encoder\noutput [41â€“43], and the network itself [44â€“46]. Among these, Tarvai-\nnen et al. [39] add noise to the input images of the teacher and student\nmodels and forced two models to make consistent predictions. Ouali\net al. [43] introduces feature-level perturbations and enforces consis-\ntency between the predictions of different decoders. Ke et al. [46] en-\nforces network perturbations by using two segmentation models with\ndifferent initializations and promotes consistency between the pre-\ndictions of the two models. While pseudo-labeling based methods first\nuse pre-trained model to make predictions on a large number of unla-\nbeled images then these predictions are in turn used as (pseudo) ground\ntruth to fine-tune the network. The key to pseudo-labeling methods is\nhow to generate reliable pseudo-labels. The most straightforward way is\nto use a high-confidence threshold to filter the noise in the\npseudo-labels. In addition, Zhang et al. [47] considers model perfor-\nmance at different training stages and uses adaptive threshold to select\npseudo-labels. Yu et al. [48] generates reliable pseudo-labels through\nuncertainty estimation. Mendel et al. [49] and Kwon et al. [50] intro-\nduce an additional trainable correction network to further polish the\npseudo-labels. Overall, consistency regularization enforces the model to\nproduce consistent predictions for perturbed inputs, which enhances the\ngeneralization ability of the model. The pseudo-labeling method uses\nthe modelâ€™s own predictions to label the unlabeled data, increasing the\ntraining data effectively, which is helpful in improving the performance\nof the model.\nTo combine the advantages of consistency regularization and\npseudo-labeling, several recent studies have proposed to integrate these\ntwo approaches into a unified framework with excellent results. Among\nthese, as one of pioneering works, FixMatch [15] first generates a\npseudo-label from the modelâ€™s prediction on the weakly augmented\nimage and then encourages the prediction on the strongly augmented\nimage to follow the pseudo-label. The success of FixMatch has led to a\nseries of studies such as SIM-FixMatch [51], SegMatch [52], FullMatch\n[53] and UniMatch [16]. Among them, UniMatch is the closest work to\nthis study, which utilizes unlabeled data by constructing image and\nfeature perturbations and combining them with consistency constraints\nto improve model performance. However, there are two differences in\nthe proposed method compared to UniMatch: (1) This work emphasizes\nthe role of strongly perturbed images in consistency regularization and\nconstructs differentiated perturbation pools and a consistency regulari\n-\nzation method for exploiting priori informations embedded in strongly\nperturbed images to mitigate confirmation bias. (2) Feature perturba-\ntions in both channel and spatial dimensions are built into the proposed\nmethod, which achieves richer perturbations and enables the model to\nextract more robust feature representations from weakly perturbed\nimages.\n2.3. Data augmentation\nData augmentation is an effective technique for expanding the data\nspace and generating perturbed samples, which is crucial for semi-\nsupervised learning algorithms to achieve superior performance.\nCommonly employed methods include affine transformation [54],\nelastic deformation [55], blur [56], color jitter [57], gamma correction\n[58], CutMix [59], and so on. For example, Yang et al. [60] uses color\njitter and blur as strong data augmentation to enable the student model\nto learn additional information. French et al. [61] reveals that applying\nCutMix to input images is crucial for the success of consistency regula-\nrization in segmentation. Sun et al. [62] uses affine transformation,\ngamma correction, and elastic deformation as augmentation strategies\nto improve the performance of their proposed semi-supervised seg-\nmentation algorithm. However, as mentioned in [63], if data augmen-\ntation is used haphazardly, it will over distort the unlabeled data and\ncorrupt the data distribution, which in turn leads to degradation of\nmodel performance. To overcome this problem, this work first catego-\nrizes multiple data augmentation strategies into two groups, which are\ncomplementary to each other, and then assigns them to two branches of\nthe proposed framework to generate perturbed samples, respectively. In\nthis way, the perturbed data are more diverse and less likely to overly\ndisrupt the data distribution.\n3. Methods\n3.1. The framework of UniMatch\nUniMatch involves two image perturbed operations and introduces a\nfeature perturbed branch additionally, the basic structure is shown in\nFig. 1. The process of UniMatch for unlabeled data is as follows: Firstly,\neach unlabeled image x\nu \nis perturbed by weak perturbation pool (rotate\nFig. 1. Schematic representation of the structure of the UniMatch. The dashed\ncurves represent supervision.\nH. Guo et al. \nDigital Signal Processing 168 (2026) 105494\n3\n\nand flip) A\nw \nto obtain x\nw\n, and then x\nw \nis perturbed by the strong\nperturbation pool (color jitter, blur and CutMix) A\ns \nto obtain x\ns\n1 \nand x\ns\n2 \n.\nIt should be noted that the data augmentation strategies in the pertur-\nbation pool are executed sequentially in a fixed order and randomly,\nsuch that x\ns\n1 \nand x\ns\n2 \nare not exactly identical. Secondly, x\nw \nis fed into the\nencoder g to generate the encoded feature g(x\nw\n), and g(x\nw\n) is processed\nby nn.Dropout2d in PyTorch to generate the perturbed feature D(g(x\nw\n)).\nMeanwhile, x\ns\n1 \nand x\ns\n2 \nare fed into the encoder g and generate the\ncorresponding features g(x\ns\n1 \n) and g(x\ns\n2 \n). Thirdly, the decoder h decodes\nthe features and outputs the predictions p\nw \n(the pseudo-label generated\nby the prediction model F for the weakly perturbed image), p\nw\nD \n(the\nprediction result of the feature perturbation branch), p\ns\n1 \nand p\ns\n2 \n(the\nprediction results of the two strongly perturbed images). Finally, the\nprediction p\nw \nfor x\nw \nis used to supervise the predictions of the feature\nperturbation branch and the two strong perturbation branches.\nThe overall objective function of UniMatch is a combination of su-\npervised loss L \ns \nand unsupervised loss L \nu \nas:\nL = \n1\n2 \n(L \ns \n+ L \nu\n) \n(1)\nTypically, the supervised term L \ns \nis the cross-entropy loss between\nmodel predictions and groundtruth labels. And the unsupervised loss L \nu\ncan be formulated as:\nL \nu \n= \n1\nB\nu\nâˆ‘ \n1(max(p\nw\n) â‰¥ \nÏ„\n)â‹…\n(\nÎ·H(p\nw\n, p\nw\nD \n) + \nÎ¼\n2 \n(H(p\nw\n, p\ns\n1 \n) + H(p\nw\n, p\ns\n2 \n))\n)\n(2)\nwhere, B\nu \nis the batch size for unlabeled data and Ï„ is a pre-defined\nconfidence threshold to filter noisy labels, H is a regular cross-entropy\nloss used to minimize the entropy between two probability distribu-\ntions, \nÎ· and Î¼ are the loss weights corresponding to the feature pertur-\nbation branch and the image strong perturbation branches, both set to\n0.5.\n3.2. Differential perturbation pooling and consistency regularization\nmodule (DPPCRM) for strongly perturbed images\nImage-level data augmentation is the simplest and most direct way to\ngenerate perturbed samples for consistency regularization methods. To\nallow the generated strongly perturbed images to be more suitable for\nthe task of cardiac MRI image segmentation, different perturbation pools\nis designed for the two strong perturbation branches, as shown in Fig. 2.\nThe perturbation pool A\ns \nof the first branch is the same with the original\ndesign of UniMatch which consists of color jitter, blur and CutMix [59],\nand the perturbation pool of the second branch A\ns\nÊ¹ \nconsists of gamma\ncorrection, elastic deformation and CutMix. The two perturbation pools\nA\ns \nand A\ns\nÊ¹ \ncomplement each other, where color jitter and gamma\ncorrection are used to jointly explore the color perturbation space,\nwhich facilitates the model to adapt to the grayscale distribution of MRI\nimages; while blur and elastic deformation are used to jointly explore\nthe perturbation space of edges and contours, which allows the model to\nbetter capture the contour features of the cardiac structure. CutMix is\nretained in both perturbation pools as a strategy for introducing local\nperturbations to further enhance the generalization performance of the\nmodel. Under the modifications, the weakly perturbed image x\nw \nis\ntransformed to x\ns\nand x\ns\nÊ¹ \n, respectively.\nIn addition, some researchers point out that the output of the model\nencoder exists the low-density regions, which makes the encoder output\na suitable place to apply consistency regularization [43]. So, a consis-\ntency regularization method was constructed by using the extracted\nfeatures of strongly perturbed images to better mine and utilize the in-\nformation hidden in the feature maps of strongly perturbed images and\nto improve the feature extraction capability of the model. Specifically,\nafter the processing of encoder g, the corresponding features g(x\ns\n) and\ng(x\ns\nÊ¹ \n) are extracted from images x\ns \nand x\ns\nÊ¹ \n, and Mixup operation [64] is\nperformed on these two features, where Mixup can be denoted as:\nMix\nÎ»\n(a, b) = Î»â‹…a + (1 \u0000 Î»)â‹…b (3)\nwhere a and b represent two target samples that need to be mixed, and Î»\nis the mixture scale factor, which is used to control the mixture weight of\nthe two target objects. On each update, a Î» was randomly sampled from\nBeta(\nÎ±, Î±), where Î± is a hyperparameter. Subsequently, the mixed\nfeature Mix\nÎ» \n(g(x\ns\n), g(x\ns\nÊ¹ \n)) is fed into the decoder h to get the prediction\nresult p\nmix \n= h(Mix\nÎ» \n(g(x\ns\n), g(x\ns\nÊ¹ \n))). Then the consistency regularization\nis applied between p\nmix \nand the mixture of the modelâ€™s predictions p\ns \nand\np\ns\nÊ¹ \nfor x\ns\nand x\ns\nÊ¹ \n, which is denoted as:\nl\n\u0000 \np\nmix\n, Mix\nÎ»\n(p\ns\n, p\ns\nÊ¹ \n)\n) \n(4)\nwhere l denotes the mean square error. The above improvements is\nintegrated into the framework of UniMatch and name it Differential\nPerturbation Pool and Consistency Regularization Module (DPPCRM).\nThe unsupervised loss L \nu\nDPPCRM \nof DPPCRM is formulated as:\nL \nu\nDPPCRM \n= \n1\nB\nu\n( âˆ‘\n \n1(max(p\nw\n) â‰¥ \nÏ„\n).(H(p\nw\n, p\ns\n) + H(p\nw\n, p\ns\nÊ¹ \n))\n+ w(t)â‹…l \n\u0000 \np\nmix\n, Mix\nÎ»\n(p\ns\n, p\ns\nÊ¹ \n)\n)\n)\n(5)\nwhere w(t) is a ramp function used to increases the importance of the\nconsistency regularization term l \n\u0000 \np\nmix\n,Mix\nÎ»\n(p\ns\n,p\ns\nÊ¹ \n)\n) \nafter each iteration.\n3.3. Dual-dimensional feature perturbation module (DFPM) for weakly\nperturbed images\nTo explore the feature perturbation space more comprehensively and\nhelp the model to learn more robust feature representations from the\nfeature level, DFPM is proposed, as shown in Fig. 3.\nThe DFTM performs the perturbation on the extracted features of\nweakly perturbed image x\nw \nin both the channel and spatial dimensions,\nand then uses predictions of the unperturbed features to supervise the\nprediction of those perturbed features. Among them, the perturbation of\nthe channel dimension D(â‹…) is a simple channel dropout (nn.Dropout2d\nin PyTorch), which is the same as UniMatch. For the perturbation of the\nspatial dimension, two operations W(â‹…) and M(â‹…)is introduced, which are\nadding white noise and feature masking respectively.\nFig. 2. Schematic representation of the structure of the DPPCRM. (The feature\nperturbation branch of UniMatch is removed as a non-essential item of\nthe module.).\nH. Guo et al. \nDigital Signal Processing 168 (2026) 105494\n4\n\nFor white noise perturbation, a noise tensor N is generated by\nrandomly sampling from a uniform distribution U(\u0000 0.3, 0.3) and ensure\nthat it has the same size as the tensor g(x\nw\n). Then, the noise tensor N is\nmultiplied element by element with the tensor g(x\nw\n) to adjust the\nmagnitude of the noise. Finally, the newly generated noise is added to\ntensor g(x\nw\n) to obtain (g(x\nw\n)) = (g(x\nw\n) âŠ™ N) + g(x\nw\n) . In this way, the\nnoise will be proportional to each activation in the output tensor.\nFor feature masking, a threshold Î³ is randomly sampled from a uni-\nform distribution U(0.1, 0.3), and the feature map g(x\nw\n) is summed and\nnormalized in the channel dimension to obtain g\nÊ¹\n(x\nw\n). Then, a perturbed\nversion M(g(x\nw\n)) = g\nÊ¹\n(x\nw\n) âŠ™ M is obtained by generating a conditional\nmask M = {g\nÊ¹\n(x\nw\n) < Î³}\n1 \nand applying this mask to g\nÊ¹\n(x\nw\n). In this way,\n10 % to 30 % of the relatively inactive regions of the feature map are\nmasked.\nIn addition, to prevent the degradation of modelâ€™s learning ability for\nfeatures caused by the multiple perturbations mixing in the same\nbranch, different types of feature perturbations are assigned to multiple\nindependent branches, so that the model can achieve goal consistency\nmore directly in each branch. The unsupervised loss L \nu\nDFPM \nof DFPM is\nformulated as:\nL \nu\nDFPM \n= \n1\nB\nu\nâˆ‘ \n1(max(p\nw\n) â‰¥ \nÏ„\n)â‹…\n(\nÎ»\n1\nH(p\nw\n, p\nw\nD \n) + \nÎ»\n2\n2 \n(H(p\nw\n, p\nw\nw \n)\n+ H(p\nw\n, p\nw\nM \n))\n)\n(6)\nWhere Î»\n1 \nand Î»\n2 \nrepresent the weights of the channel and spatial\ndimension perturbations, respectively, which are both set to 0.5.\n3.4. The holistic framework\nOverall, two key modules, namely DPPCRM and DFPM, were pro-\nposed to extract more effective supervised signals from strongly and\nweakly perturbed images, respectively. These two modules were inte-\ngrated into an overall framework to obtain the improved-UniMatch, as\nshown in Fig. 4. The corresponding pseudocode of improved-UniMatch\nis provided in Algorithm 1.\nThe final unsupervised lossL \nu \nof improved-UniMatch is computed\nas:\nL \nu \n= Î¾L \nu\nDPPCRM \n+ ÎµL \nu\nDFPM \n(7)\nSince DPPCRM and DFPM process strongly and weakly perturbed\nimages in different ways, respectively, and the two processes aim to\nachieve different goals, their corresponding loss weights Î¾ and \nÎµ are\nequally set to 0.5.\n4. Experiments and results\n4.1. Datasets and evaluation metrics\nTo evaluate the performance of the proposed improved-UniMatch in\nsegmentation tasks, extensive experiments were conducted on two\npublicly available cardiac MRI datasets: ACDC [65] and MSCMR [66] .\nACDC: The MICCAIâ€™17 Automatic Cardiac Diagnosis Challenge\ndataset comprises of short-axis cardiac MRI images of 100 patients from\n5 groups: one group with 20 healthy controls and 20 subjects in each\nremaining group with four different abnormalities. Manual segmenta-\ntion results are provided for the end-diastolic (ED) and end-systolic (ES)\ncardiac phases for left ventricle (LV), right ventricle (RV) and\nFig. 3. Schematic representation of the structure of the DFPM. The D(â‹…) denotes\nchannel dropout, W(â‹…) denotes adding white noise, M(â‹…) denotes masking\nof features.\nFig. 4. Schematic representation of the structure of the improved-UniMatch.\nH. Guo et al. \nDigital Signal Processing 168 (2026) 105494\n5\n\nmyocardium (MYO). Because of the wide inter-slice interval, it is more\nappropriate to use 2D segmentation instead of direct 3D segmentation\n[67]. The data of 100 patients were randomly divided into training,\nvalidation, and test sets by case, with a ratio of 7:1:2.\nMSCMR: The MICCAIâ€™19 Multi-Sequence Cardiac MR Segmentation\nChallenge dataset contains late gadolinium enhancement (LGE) MRI\nimages collected from 45 patients underwent cardiomyopathy. Gold\nstandard segmentation of LV, MYO, RV of these images has also been\nreleased by the organizers. Compared to the ACDC dataset, the seg-\nmentation task on the MSCMR dataset is more challenging. This is pri-\nmarily because the training set for MSCMR is smaller, and the\nsegmentation of LGE cardiac MRI images is inherently more complex.\nSimilar to the ACDC dataset, the images from 45 patients were divided in\na 7:1:2 ratio, with 31 for training, 5 for validation, and 9 for testing.\nFor the pre-processing of the ACDC and MSCMR datasets, all the\nimages were resized to 256 Ã— 256 pixels, and the intensity of each image\nwas rescaled to the range [0,1]. To avoid overfitting, conventional data\naugmentation strategies were applied, including random rotation and\nrandom flipping.\nTwo widely used evaluation metrics in the field of medical image\nsegmentation were utilized to quantitatively assess the performance of\nthe proposed model: Dice Similarity Coefficient (DSC) and 95 % Haus-\ndorff distance (HD\n95\n). Their calculation expression is as follows:\nDSC(X, Y) = \n2|X âˆ© Y|\n|X| + |Y| \n(8)\nHD\n95\n(X, Y) = max\n(\nmax\nxâˆˆX\n(\nmin\nyâˆˆY \n(x, y)\n)\n, max\nyâˆˆY\n(\nmin\nxâˆˆX \n(y, x)\n))\nÃ— 95% (9)\nwhere X and Y indicate the original segmentation masks and the pre-\ndicted segmentation results, respectively. The DSC evaluate the area of\noverlap between prediction segmentation and the ground truth seg-\nmentation. The HD\n95 \nmeasures the maximum distance from the pre-\ndicted segmentation to the nearest point on the ground truth\nsegmentation. A higher score of DSC, and lower score of HD\n95 \nindicate\nbetter segmentation performance of the metrics.\n4.2. Experimental setup\nImplementation Details. PyTorch was used for the implementation\nof all methods, and all experiments were run on an Ubuntu desktop with\na GTX1080TI GPU. To ensure a fair comparison with previous works, U-\nNet [19] was adopted as the segmentation model. The model was\ntrained using the SGD optimizer with a batch size of 24, consisting of 12\nlabeled samples and 12 unlabeled samples. The total number of itera-\ntions was set to 30,000. The initial learning rate was set to 0.01 and the\npoly learning rate strategy was used to adjust the learning rate.\nFollowing [68], the consistency coefficient w(t) was gradually increased\nfrom its initial value of 0.0 to 10, reaching this maximum at one-fourth\nof the total iterations. Regarding the data augmentation strategy in\nproposed method, the same settings was adopted for A\ns \nand A\nw\nas\nUniMatch. For A\ns\nÊ¹ \n, color jitter was substituted with gamma correction\nand blur was replaced with elastic transform, based on A\ns\n. The in-\ntensities of the elastic deformation and gamma correction were followed\n[69].\nCompared Methods. To demonstrate the effectiveness of the pro-\nposed method, several classical and recently proposed semi-supervised\nsegmentation methods were selected for comparison, including Deep\nAdversarial Networks(DNA)[70], Mean Teacher(MT) [39], Deep\nCo-Training(DCT) [70], Uncertainty Aware Mean Teacher(UA-MT)\n[48], Entropy Minimization (EM) [71], Cross Consistency Training\n(CCT) [43], FixMatch [15], Interpolation Consistency Training(ICT)\n[68], Uncertainty Rectified Pyramid Consistency (URPC) [72], Cross\nPseudo Supervision(CPS) [73], Cross Teaching between CNN and\nTransformer(CTCT) [74] and UniMatch [16]. Except for FixMatch,\nUniMatch and proposed method, which use specific image augmenta-\ntion strategies, all methods were trained with the same hyperparameter\nsettings to ensure fairness in the evaluation process. Additionally, they\nwere implemented under the same environment and backbone.\n4.3. Performance on the ACDC dataset\nTo demonstrate the performance of the proposed method on regular-\nsized datasets, experiments were conducted on the ACDC dataset. In\nexperiments on the ACDC dataset, the performance of various models\nwas examined using only 5 % (3 cases) and 10 % (7 cases) of labeled\nAlgorithm 1\nPseudocode of Improved-UniMatch in a PyTorch-like style.\n# network F is composed of an encoder g and a decoder h\n#aug_w: weak perturbation pool A\nw \n(rotate and flip)\n# aug_s1: strong perturbation pool A\ns \n(color jitter, blur and CutMix)\n# aug_s2: strong perturbation pool A\ns\nÊ¹ \n(gamma correction, elastic deformation and CutMix)\n#ce: cross entropy loss, mse: mse loss()\n#D: channel dropout, W: white noise, M: mask\n#w_t: ramp function, mixup: fusion function\nfor x in loader_u:\n#Step1: generate perturbed images by weak/strong perturbation pool\nx_w=aug_w(x)\nx_s1,x_s2=aug_s1(x_w),aug_s2(x_w)\n#Step2: generate features of perturbed images by encoder g\nfeat_w=g(x_w)\nfeat_s1, feat_s2= g(x_s1), g(x_s2)\n#Step3: generate perturbed features\nfeat_wd, feat_ww, feat_wm =D(feat_w), W(feat_w), M(feat_w)\nfeat_s_mix=mixup(feat_s1, feat_s2)\n#Step4: generate prediction for all features by decoder h\np_w=h(feat_w)\np_wd, p_ww, p_wm = h(feat_wd), h(feat_ww), h(feat_wm)\np_s1, p_s2, p_mix = h(feat_s1), h(feat_s2), h(feat_s_mix)\n#Step5: generate pseudo label\nmask_w=p_w.argmax(dim=1).detach()\n# Step6: hhhcalculate unsupervised losses\nloss_dfpm=0.5*ce(p_wd,mask_w)+0.25*(ce(p_ww,mask_w)+ ce(p_wm,mask_w))\nloss_dppcrm= ce(p_s1, mask_w)+ ce(p_s2, mask_w)+w_t*mse(p_mix, mixup(p_s1, p_s2))\nloss_u=0.5*(loss_dfpm+ loss_dppcrm)\nH. Guo et al. \nDigital Signal Processing 168 (2026) 105494\n6\n\ndata. Firstly, the proposed method was compared to the fully supervised\nmethod (denoted as Sup). The results in Table 1 indicate that the pro-\nposed method significantly improved in all metrics on this dataset.\nSpecifically, with 5 % labeled data, the proposed method achieved 0.880\nand 3.025 mm on the DSC and HD\n95\n, respectively, while the fully su-\npervised learning approach only reached 0.456 and 31.440 mm on these\nmetrics. With 10 % labeled data, the proposed approach surpassed the\nfully supervised learning approach by 0.119 on the DSC and reduced by\n6.962mm on the HD\n95\n. Furthermore, a comparison was also conducted\nbetween the proposed model trained with 10 % labeled data and the\nfully supervised method using 100 % labeled data. The results show that\neven with much less labeled data, the proposed model is only 0.014\nlower than the fully supervised method on the DSC and the gap on the\nHD\n95 \nis merely 0.125 mm These demonstrate the effectiveness of pro-\nposed approach in leveraging a large amount of unlabeled data to ach-\nieve significant performance improvements of deep models. Secondly, a\nquantitative comparison was conducted between the proposed method\nand other advanced semi-supervised segmentation methods, with the\nresults also presented in Table 1. In terms of DSC and HD\n95\n, the proposed\nmethod significantly outperforms all the most advanced semi-supervised\nmethods in all cases, proving the superior performance of this work in\nmedical image segmentation tasks. Finally, to further qualitatively\ndemonstrate the superior performance of the proposed method, seg-\nmentation results obtained with 5 % and 10 % labeled data were visu-\nalized and compared with other advanced semi-supervised\nsegmentation methods, as shown in Fig. 5. From the visualized results, it\ncan be seen that the proposed method makes more accurate predictions\ncompared to other methods, which further proves the effectiveness of\nimproved-UniMatch.\nTable 1\nQuantitative comparison of different methods on ACDC dataset with 5 % and 10 % labeled data in terms of DSC and HD\n95\n. Mean and standard variance (in parentheses)\nare presented in the table.\nLabeled Method LV MYO RV Mean\nDSCâ†‘ HD\n95 \nâ†“ DSCâ†‘ HD\n95 \nâ†“ DSCâ†‘ HD\n95 \nâ†“ DSCâ†‘ HD\n95 \nâ†“\n5 % sup 0.345(0.017) 62.928(3.192) 0.482(0.011) 13.602(2.671) 0.540(0.028) 17.789\n(1.438)\n0.456(0.015) 31.440\n(1.095)\nDAN 0.506(0.013) 37.629\n(13.891)\n0.551(0.026) 27.464(8.557) 0.623(0.040) 22.833\n(5.266)\n0.560(0.024) 29.309\n(8.567)\nMT 0.475(0.027) 43.405\n(10.367)\n0.555(0.023) 10.665(5.463) 0.678(0.028) 8.785(6.018) 0.569(0.016) 20.952\n(2.983)\nDCT 0.503(0.029) 40.643(9.780) 0.581(0.012) 11.421(2.900) 0.639(0.019) 10.823\n(4.075)\n0.574(0.012) 20.962\n(3.528)\nUAMT 0.493(0.038) 39.292(5.431) 0.604(0.014) 17.098(2.588) 0.717(0.009) 16.394\n(5.322)\n0.605(0.012) 24.261\n(2.306)\nEM 0.471(0.028) 39.187(5.796) 0.583(0.024) 14.549(3.504) 0.665(0.051) 11.794\n(5.292)\n0.573(0.015) 21.843\n(4.269)\nCCT 0.484(0.014) 24.635(4.703) 0.590(0.015) 8.037(1.717) 0.691(0.013) 12.417\n(2.434)\n0.588(0.011) 15.030\n(1.840)\nFixmatch 0.413(0.053) 26.022(6.660) 0.634(0.041) 13.723(3.710) 0.748(0.017) 17.974\n(2.092)\n0.598(0.021) 19.240\n(1.507)\nICT 0.471(0.015) 13.842(2.637) 0.622(0.016) 10.144(3.357) 0.680(0.008) 12.486\n(2.610)\n0.591(0.009) 12.157\n(1.766)\nURPC 0.479(0.024) 26.784(3.264) 0.613(0.044) 9.726(1.508) 0.731(0.010) 12.766\n(1.713)\n0.608(0.010) 16.425\n(1.233)\nCPS 0.457(0.044) 16.962(1.972) 0.626(0.025) 13.066(1.829) 0.732(0.020) 14.691\n(2.980)\n0.605(0.014) 14.906\n(1.499)\nCTCT 0.640(0.040) 9.962(3.765) 0.606(0.039) 5.964(1.718) 0.744(0.016) 9.051(1.936) 0.663(0.012) 8.326(1.838)\nUniMatch 0.867(0.013) 4.976(1.244) 0.847(0.009) 5.112(1.290) 0.901(0.006) 9.827(2.594) 0.872(0.007) 6.638(1.674)\nImproved-\nUniMatch\n0.873\n(0.009)\n3.501(0.904) 0.855\n(0.006)\n3.893(0.751) 0.911\n(0.003)\n4.682(1.063) 0.880\n(0.005)\n4.025(0.732)\n10 % sup 0.692(0.021) 7.802(1.511) 0.786(0.009) 11.097(5.752) 0.860(0.007) 9.569(1.830) 0.779(0.009) 9.489(2.207)\nDAN 0.758(0.041) 6.650(3.096) 0.766(0.008) 8.093(1.121) 0.830(0.005) 12.390\n(3.181)\n0.785(0.015) 9.044(1.920)\nMT 0.764(0.026) 6.886(4.496) 0.796(0.009) 11.298(8.336) 0.866(0.010) 12.522\n(2.491)\n0.809(0.013) 10.235\n(2.016)\nDCT 0.787(0.022) 5.461(1.220) 0.800(0.011) 13.769\n(10.930)\n0.857(0.013) 12.879\n(3.947)\n0.815(0.014) 10.703\n(4.272)\nUAMT 0.773(0.017) 8.847(3.064) 0.805(0.011) 14.754\n(10.313)\n0.869(0.004) 15.011\n(3.119)\n0.816(0.007) 12.871\n(3.566)\nEM 0.767(0.023) 4.224(1.273) 0.784(0.013) 14.800(9.992) 0.843(0.009) 13.735\n(1.955)\n0.798(0.012) 10.920\n(4.151)\nCCT 0.797(0.011) 3.933(1.461) 0.816(0.009) 7.053(2.987) 0.867(0.006) 14.493\n(3.237)\n0.827(0.008) 8.493(1.383)\nFixmatch 0.772(0.020) 12.991(3.615) 0.805(0.007) 17.260(4.249) 0.880(0.012) 18.377\n(6.112)\n0.819(0.006) 16.209\n(3.746)\nICT 0.816(0.009) 4.797(1.156) 0.814(0.009) 9.109(3.493) 0.867(0.013) 14.029\n(2.598)\n0.832(0.009) 9.312(1.395)\nURPC 0.771(0.016) 3.751(0.990) 0.789(0.013) 4.096(1.372) 0.848(0.011) 19.639\n(4.431)\n0.803(0.011) 9.162(2.748)\nCPS 0.822(0.010) 4.000(1.036) 0.828(0.010) 6.356(2.607) 0.877(0.007) 12.958\n(2.225)\n0.842(0.008) 7.771(2.647)\nCTCT 0.871(0.011) 6.562(1.287) 0.848(0.008) 4.357(1.828) 0.905(0.006) 7.784(1.359) 0.875(0.007) 6.234(2.186)\nUniMatch 0.889(0.009) 3.110(1.161) 0.863(0.008) 3.762(0.924) 0.922(0.004) 7.045(1.691) 0.891(0.006) 4.639(1.084)\nImproved-\nUniMatch\n0.893\n(0.006)\n1.713(0.648) 0.872\n(0.005)\n1.750(0.614) 0.929\n(0.002)\n4.117(1.302) 0.898\n(0.003)\n2.527(0.706)\n100 % sup 0.904(0.005) 1.601(0.366) 0.887(0.002) 2.107(0.179) 0.946(0.001) 3.499(0.193) 0.912(0.002) 2.402(0.228)\nH. Guo et al. \nDigital Signal Processing 168 (2026) 105494\n7\n\nFig. 5. Visualized segmentation results on the ACDC and MSCMR datasets with 5 % and 10 % labeled data. The first four rows of visualization samples are from the\nACDC dataset. Case1 and Case2 utilized 5 % labeled data, while Case3 and Case4 utilized 10 % labeled data. The last four rows of visualization samples are from the\nMSCMR dataset. Case5 and Case6 utilized 5 % labeled data, while Case7 and Case8 utilized 10 % labeled data.\nTable 2\nQuantitative comparison of different methods on MSCMR dataset with 5 % and 10 % labeled data in terms of DSC and HD\n95\n. Mean and standard variance (in pa-\nrentheses) are presented in the table.\nLabeled Method LV MYO RV Mean\nDSCâ†‘ HD\n95 \nâ†“ DSCâ†‘ HD\n95 \nâ†“ DSCâ†‘ HD\n95 \nâ†“ DSCâ†‘ HD\n95 \nâ†“\n5 % sup 0.237(0.029) 99.681(4.319) 0.444(0.014) 72.616(6.235) 0.638(0.009) 39.260(3.183) 0.440(0.016) 70.519(3.820)\nDAN 0.341(0.019) 91.164(12.707) 0.443(0.019) 56.225(7.933) 0.611(0.018) 43.933(3.391) 0.465(0.006) 63.774(6.096)\nMT 0.306(0.014) 107.015\n(10.302)\n0.485(0.023) 85.197(5.974) 0.602(0.023) 73.762(4.445) 0.464(0.019) 88.658(4.057)\nDCT 0.334(0.031) 94.345(7.591) 0.527(0.011) 69.350(8.183) 0.645(0.014) 47.734(4.671) 0.502(0.017) 70.476(6.928)\nUAMT 0.347(0.029) 101.323(2.469) 0.493(0.019) 72.826(4.271) 0.618(0.015) 68.834(3.248) 0.486(0.013) 80.994(7.801)\nEM 0.335(0.019) 99.234(6.667) 0.509(0.017) 64.400(3.554) 0.647(0.012) 42.099(5.400) 0.497(0.009) 68.578(4.317)\nCCT 0.387(0.014) 97.910(2.684) 0.448(0.013) 74.256(6.982) 0.615(0.010) 59.728(4.545) 0.483(0.011) 77.298(3.508)\nFixmatch 0.325(0.037) 63.391(14.689) 0.571(0.022) 49.588\n(17.643)\n0.739(0.019) 37.157(4.390) 0.545(0.044) 50.045(5.466)\nICT 0.324(0.026) 102.811(6.948) 0.486(0.009) 83.085(4.935) 0.632(0.012) 69.596(8.715) 0.481(0.015) 85.164(3.087)\nURPC 0.426(0.011) 44.757(4.073) 0.496(0.011) 55.257(5.286) 0.598(0.014) 59.446(3.808) 0.507(0.011) 53.153(4.364)\nCPS 0.343(0.017) 78.355(3.636) 0.532(0.009) 61.414(3.784) 0.607(0.017) 71.354(3.861) 0.494(0.008) 70.374(3.822)\nCTCT 0.512(0.015) 59.348(6.230) 0.606(0.008) 35.381(2.646) 0.742(0.011) 31.743(4.671) 0.620(0.011) 42.157(2.365)\nUniMatch 0.702(0.009) 11.576(3.686) 0.649(0.007) 17.339(3.904) 0.801(0.008) 15.800(3.304) 0.717(0.007) 14.905(2.950)\nImproved-\nUniMatch\n0.711\n(0.006)\n9.880(1.778) 0.654\n(0.003)\n15.502(2.710) 0.812\n(0.003)\n12.681(2.691) 0.726\n(0.005)\n12.688\n(1.978)\n10 % sup 0.446(0.017) 74.697(4.831) 0.532(0.011) 60.237(3.376) 0.655(0.013) 55.830(5.655) 0.544(0.013) 63.588(2.559)\nDAN 0.451(0.019) 79.999(3.266) 0.541(0.007) 63.453(2.857) 0.677(0.014) 61.315(6.918) 0.556(0.008) 68.256(2.949)\nMT 0.506\n((0.025)\n76.756(8.667) 0.558(0.008) 67.297(5.577) 0.648(0.006) 81.958(7.623) 0.571(0.009) 75.337(3.101)\nDCT 0.526(0.031) 69.947(5.197) 0.600(0.012) 58.175(6.568) 0.709(0.009) 55.001(4.319) 0.612(0.014) 61.041(3.854)\nUAMT 0.509(0.019) 77.253(4.172) 0.600(0.018) 60.629\n(10.831)\n0.712(0.020) 58.854\n(13.989)\n0.607(0.015) 65.579(6.924)\nEM 0.513(0.015) 68.791(2.784) 0.614(0.009) 49.237(7.442) 0.717(0.011) 43.690\n(10.696)\n0.615(0009) 53.906(3.855)\nCCT 0.448(0.015) 74.838(6.645) 0.576(0.016) 51.170(6.542) 0.727(0.015) 31.305(2.915) 0.584(0.011) 52.438(4.125)\nFixmatch 0.375(0.021) 74.183(8.130) 0.632(0.009) 44.712(5.432) 0.762(0.028) 44.749(6.358) 0.590(0.007) 54.548(2.851)\nICT 0.505(0.016) 77.450(6.204) 0.587(0.011) 67.571(6.289) 0.696(0.009) 72.165\n(11.364)\n0.596(0.012) 72.395(4.225)\nURPC 0.545(0.013) 27.398(3.957) 0.589(0.012) 36.551(3.664) 0.688(0.011) 45.844(4.198) 0.607(0.011) 36.598(2.909)\nCPS 0.561(0.024) 50.271(2.732) 0.632(0.010) 41.873(3.968) 0.721(0.014) 47.908(4.728) 0.638(0.007) 46.684(3.441)\nCTCT 0.572(0.011) 50.710(2.119) 0.654(0.009) 26.550(3.297) 0.774(0.010) 27.870(3.270) 0.667(0.009) 35.043(2.876)\nUniMatch 0.729(0.008) 11.043(1.724) 0.675(0.005) 12.169(2.273) 0.816(0.006) 10.996(3.231) 0.740(0.006) 11.403(2.284)\nImproved-\nUniMatch\n0.737\n(0.006)\n7.379(1.381) 0.681\n(0.004)\n10.186(1.513) 0.825\n(0.003)\n8.554(2.203) 0.748\n(0.004)\n8.706(1.947)\n100 % sup 0.799(0.004) 4.347(0.425) 0.779(0.003) 4.527(1.387) 0.873(0.001) 4.539(2.457) 0.817(0.003) 4.471(1.547)\nH. Guo et al. \nDigital Signal Processing 168 (2026) 105494\n8\n\n4.4. Performance on the MSCMR dataset\nTo further validate the performance of the proposed method on\nsmall-scale datasets and complex scenarios, experiments were con-\nducted on the ACDC dataset.The results in Table 2 shows the quantita-\ntive comparative performance of the proposed method with the fully\nsupervised method on the MSCMR dataset. Due to the unique properties\nof LGE CMR images and the relatively small number of training samples\nin the MSCMR dataset, the image segmentation task on the MSCMR\ndataset is more challenging and complex than ACDC dataset. When\nusing 100 % labeled data, the DSC and HD\n95 \nof the fully supervised\nmethod are 0.817 and 4.417 mm, respectively. However, when trained\nusing only with 5 % (2 case) and 10 % (3 case) labeled data, the DSC of\nthe fully supervised baseline method decreases significantly to 0.440\nand 0.544, while the HD\n95 \nincreases significantly to 70.519 mm and\n63.588 mm. In contrast, the proposed method performs better with these\ntwo limited labeled data, achieving higher DSC (0.726 and 0.748) and\nlower HD\n95 \n(12.688 mm and 8.706 mm), respectively. In addition, the\nquantitative comparison of proposed method with other state-of-the-art\nsemi-supervised methods is also exhibited in Table 2, where it can be\nseen that proposed method provides the best segmentation results for\nRV, My, and LV organs regardless of whether 5 % or 10 % labeled data is\nused, with the highest mean values in DSC and the lowest mean values in\nHD\n95\n. Furthermore, the proposed model possesses the lowest standard\nerror in all cases, which indicates that the proposed method not only\nperforms well but is also more robust and stable. The qualitative com-\nparison results are shown in Fig. 5. From these figures, it can be seen that\nthe results of other methods tend to have more over-segmented or\nunder-segmented regions compared to proposed method, while pro-\nposed method better describes the target region with more accurate\nboundaries. This demonstrates that the proposed method can more\neffectively utilize the large amount of unlabeled data to improve model\naccuracy.\n4.5. Ablation study\nTo validate the effectiveness of the modules in the proposed method,\nablation experiments were performed on the ACDC and MSCMR datasets\nwith 5 % and 10 % labeled data, and the corresponding results are\nshown in \nTable 3 and Table 4. The framework of proposed method\ncontains two main components: DPPCRM and DFPM. To investigate the\neffectiveness of each component, ablation studies were conducted by\nadding both components one by one to UniMatch. The abbreviations\nâ€œUniMatch-DPPCRMâ€ and â€œUniMatch-DFPMâ€ in the table represent\nUniMatch with DPPCRM and UniMatch with DFPM, respectively.\nFirstly, the effectiveness of the DPPCRM was verified. As shown in\nthe second row of Table 3 and Table 4, UniMatch-DPPCR achieves\nhigher DSC and lower HD\n95 \nthan UniMatch on both datasets, which\nproves the effectiveness of the DPPCRM. The reasons for the effective-\nness of DPPCRM can be summarized as follows: (1) In the DPPCRM,\nmultiple perturbation strategies are used to construct two complemen-\ntary perturbation pools to generate strongly perturbed images, which\nenhances the diversity of the perturbation samples and allows the model\nto understand the images from different perspectives for better feature\nextraction. (2) The proposed consistency regularization method effec\n-\ntively exploits and utilizes the information hidden in the features of the\nstrongly perturbed images and enables the model to extract more robust\nfeature representations from the images generated by the newly\ndesigned perturbation pool.\nThen, the effectiveness of the DFPM was verified. As shown in the\nthird row of Table 3 and Table 4, compared to UniMatch, UniMatch-\nDFPM exhibits superior performance on both datasets, which proves\nthe effectiveness of the DFPM. The reason for the effectiveness of DFPM\nis that the features extracted from weakly perturbed images were per-\nturbed in both channel and spatial dimensions, which enriches the di-\nversity of feature perturbations and helps the model to learn more robust\nfeature representations from the weakly perturbed images.\nFurthermore, it can be observed from the results that the segmen-\ntation performance of UniMatch-DFPM and UniMatch-DPCRM show\ndifferent characteristics. In most cases, UniMatch-DPCRM performs\nbetter in HD\n95\n, while UniMatch-DFPM has better DSC performance,\nwhich suggests that the proposed DPPCRM and DFPM are complemen-\ntary to some extent. This view is confirmed by the last rows of Table 3\nand Table 4. Compared with UniMatch which integrates only DPPCRM\nor DFPM, the model that integrates both DPPCRM and DFPM in UniM-\natch obtains the highest DSC and the lowest HD\n95\n, which suggests that\nthe proposed two modules are not only effective individually, but also\ncomplement each other to further improve the segmentation perfor-\nmance when they are applied in a framework.\n4.6. Supplementary experiment\n4.6.1. Effectiveness of differential strong perturbation pools\nTo verify the reasonableness of the design of the differential\nperturbation pool in the DPPCRM, complementary experiments were\nconducted on the ACDC dataset with 5 % and 10 % labeled data. Spe-\ncifically, based on the UniMatch, the perturbation pools for the two\nbranches of strongly perturbed images were modified, and experiments\nwere conducted to explore the impact of different perturbation pools on\nmodel performance. Furthermore, besides the previously mentioned A\ns\nand A\ns\nÊ¹ \n, a new perturbation operator A\nss\nÊ¹ \nwas introduced to represent a\nmore complex perturbation pool that combines all the perturbation\nstrategies of A\ns \nand A\ns\nÊ¹ \n. For convenience, combinations of these\nperturbation operators are used to represent the perturbation pools of\nthe two perturbation branches, respectively. The experimental results\nare shown in Fig. 6.\nFrom the results, it is clear that different perturbation pool strategies\nhave a significant impact on model performance. Specifically, under the\nsetting with 5 % and 10 % labeled data, the model with the lowest\nperformance is UniMatch with A\nss\nÊ¹ \n-A\nss\nÊ¹ \n, the model with the best perfor-\nmance is UniMatch with A\ns\n-A\ns\nÊ¹ \n,and the performance of UniMatch with\nA\ns\n-A\ns \n(UniMatch) and UniMatch with A\ns\nÊ¹ \n-A\ns\nÊ¹ \nhave similar performance\nand lie in the middle. From this, the following three conclusions can be\nTable 3\nAblation studies of the proposed method on ACDC dataset with 5 % and 10 % labeled data in terms of DSC and HD\n95\n. Mean and standard variance (in parentheses) are\npresented in the table.\nLabeled Method LV MYO RV Mean\nDSCâ†‘ HD\n95 \nâ†“ DSCâ†‘ HD\n95 \nâ†“ DSCâ†‘ HD\n95 \nâ†“ DSCâ†‘ HD\n95 \nâ†“\n5 % UniMatch 0.867(0.013) 4.976(1.244) 0.847(0.009) 5.112(1.290) 0.901(0.006) 9.827(2.594) 0.872(0.007) 6.638(1.674)\nUniMatch-DPPCRM 0.872(0.008) 3.927(0.968) 0.850(0.007) 4.676(0.802) 0.910(0.003) 5.208(1.175) 0.877(0.006) 4.604(0.816)\nUniMatch-DFPM 0.871(0.010) 4.478(1.139) 0.854(0.008) 4.387(1.066) 0.908(0.005) 5.879(1.409) 0.878(0.008) 4.915(0.913)\nImproved-UniMatch 0.873(0.009) 3.501(0.904) 0.855(0.006) 3.893(0.751) 0.911(0.003) 4.682(1.063) 0.880(0.005) 4.025(0.732)\n10 % UniMatch 0.889(0.009) 3.110(1.161) 0.863(0.008) 3.762(0.924) 0.922(0.004) 7.045(1.691) 0.891(0.006) 4.639(1.084)\nUniMatch- DPPCRM 0.893(0.005) 2.226(0.880) 0.866(0.005) 2.780(0.809) 0.927(0.003) 5.084(1.563) 0.895(0.003) 3.363(1.082)\nUniMatch-DFPM 0.891(0.008) 2.608(1.035) 0.871(0.006) 2.593(0.871) 0.925(0.006) 5.541(1.609) 0.896(0.006) 3.581(1.163)\nImproved-UniMatch 0.893(0.006) 1.713(0.648) 0.872(0.005) 1.750(0.614) 0.929(0.002) 4.117(1.302) 0.898(0.003) 2.527(0.706)\nH. Guo et al. \nDigital Signal Processing 168 (2026) 105494\n9\n\ndrawn: firstly, if the image-level data augmentation strategy is too\ncomplex and cumbersome, such as A\nss\nÊ¹ \n, it will not only fail to improve\nmodel performance but may also generate overly distorted samples and\ndamage the data distribution, resulting in a decrease in model seg-\nmentation performance. Secondly, the performance of the UniMatch\nwith A\ns\n-A\ns\nand UniMatch with A\ns\nÊ¹ \n-A\ns\nÊ¹ \nare close to each other, which in-\ndicates that the proposed perturbation pool A\ns\nÊ¹ \ncan effectively replace the\noriginal perturbation pool A\ns \nin UniMatch without degrading the model\nperformance, which to a certain extent proves the validity and reason-\nableness of the proposed perturbation pool A\ns\nÊ¹ \n. Finally, UniMatch with\nA\ns\n-A\ns\nÊ¹ \nachieves the best performance, which proves the effectiveness of\nthe proposed differential perturbation pool and shows that differential\nperturbation pool allows the model to effectively learn image features\nfrom more perspectives and improves the modelâ€™s generalization ability.\n4.6.2. Hyperparameter \nÎ± analysis\nAfter the effectiveness of the differential perturbation pool in\nDPPCRM was verified, supplementary experiments were also conducted\nTable 4\nAblation studies of the proposed method on MSCMR dataset with 5 % and 10 % labeled data in terms of DSC and HD\n95\n. Mean and standard variance (in parentheses) are\npresented in the table.\nLabeled Method LV MYO RV Mean\nDSCâ†‘ HD\n95 \nâ†“ DSCâ†‘ HD\n95 \nâ†“ DSCâ†‘ HD\n95 \nâ†“ DSCâ†‘ HD\n95 \nâ†“\n5 % UniMatch 0.702(0.009) 11.576\n(3.686)\n0.649(0.007) 17.339(3.904) 0.801(0.008) 15.800(3.304) 0.717(0.007) 14.905(2.950)\nUniMatch-DPPCRM 0.708(0.006) 10.403\n(1.906)\n0.654(0.003) 15.985(2.851) 0.809(0.004) 13.409(2.407) 0.724(0.004) 13.267(2.159)\nUniMatch-DFPM 0.710(0.009) 10.824\n(2.003)\n0.653(0.005) 16.791(3.128) 0.809(0.006) 14.087(2.981) 0.724(0.005) 13.901(2.312)\nImproved-\nUniMatch\n0.711\n(0.006)\n9.880(1.778) 0.654\n(0.003)\n15.502\n(2.710)\n0.812\n(0.003)\n12.681\n(2.691)\n0.726\n(0.003)\n12.688\n(1.978)\n10 % UniMatch 0.729(0.008) 11.043\n(1.724)\n0.675(0.005) 12.169(2.273) 0.816(0.006) 10.996(3.231) 0.740(0.006) 11.403(2.284)\nUniMatch-DPPCRM 0.735(0.006) 7.832(1.293) 0.680(0.005) 10.508(1.682) 0.823(0.004) 8.996(2.772) 0.746(0.006) 9.112(1.983)\nUniMatch-DFPM 0.736(0.008) 8.379(1.509) 0.680(0.005) 11.142(1.706) 0.824(0.007) 9.683(2.971) 0.747(0.007) 9.735(2.143)\nImproved-\nUniMatch\n0.737\n(0.006)\n7.379(1.381) 0.681\n(0.004)\n10.186\n(1.513)\n0.825\n(0.003)\n8.554(2.203) 0.748\n(0.005)\n8.706(1.947)\nFig. 6. DSC performance of different strong perturbation pool strategies on the ACDC dataset.\nFig. 7. DSC performance under different Î± on the ACDC dataset.\nH. Guo et al. \nDigital Signal Processing 168 (2026) 105494\n10\n\nto investigate the impact of the hyperparameter Î± in the DPPCRM\nmodule. As described in Section 3.2 , Followed [68], on each update, a Î»\nis randomly sampled from Beta(\nÎ±, Î±\n). Further, as shown in Eq. (3), the\nparameter Î» is used to control the fusion ratio of the extracted features of\ntwo strongly perturbed images, which is crucial for the proposed con-\nsistency regularization method. Therefore, the value of the parameter \nÎ±\nwill influence the value of Î» directly and affect the model performance\nindirectly. In view of this , base on UniMatch- DPPCRM, four values of \nÎ±\nwere selected: 0.1, 0.5, 1.0, and 10 to investigate their effects on the\nmodel performance. The experimental results are shown in Fig. 7.\nFrom the results, it can be observed that the best performance is\nachieved when the value of \nÎ± is set to 0.5. The reason for this is further\nanalyzed. When the value of \nÎ± is small (e.g., Î± = 0.1), the Beta distri-\nbution tends to generate Î» values close to 0 or 1, causing the fused\nfeatures to be more inclined towards the features from a single input\nimage. Therefore, this extreme fusion ratio may not fully exploit the\neffects of regularization. When \nÎ± = 1, the Beta distribution degenerates\ninto a uniform distribution, and the fusion ratio Î» is random between\n[0.1]. Although this fusion strategy enhances the intensity of feature\nfusion to a certain extent, it may not be conducive to the model pro-\nducing stable segmentation results due to the excessively random fusion.\nConsequently, it does not achieve the optimal model performance. When\nÎ± > 1 (e.g., Î± = 10), the Beta distribution is concentrated around 0.5,\nand the fusion ratio Î» tends to take values close to 0.5. While this\nstrategy ensures a relatively balanced fusion of the two input features,\nan overly strong fusion may blur the features thus reduce the accuracy of\nthe model segmentation results. However, in the case of \nÎ± = 0.5, the Î»\nsampled from the Beta distribution can maintain a certain level of\nrandomness while avoiding extreme values. Therefore, the effect of\nproposed regularization scheme is the best in this case, and the model\nachieves the optimal performance.\n4.6.3. Effect of different feature perturbation strategies\nTo explore the impact of different feature perturbation strategies on\nmodel performance, supplementary experiments were conducted based\non the UniMatch-DFPM. Notably, considering the critical role of channel\ndropout in the original framework of UniMatch, it was set as a fixed\nperturbation strategy for one branch in the DFPM and a detailed study\nwas performed on the feature perturbation strategies for two newly\nintroduced. For convenience, the combination of three letters â€œDâ€, â€œWâ€,\nand â€œMâ€ were utilized to denote the perturbation strategies of the three\nperturbation branches of the UniMatch-DFPM model, where â€œDâ€, â€œWâ€,\nand â€œMâ€ denote channel dropout, adding white noise and masking\nfeature map, respectively. The experimental results are shown in Fig. 8.\nFrom the results, it can be observed that under the setting with 5 %\nand 10 % labeled data, all improved versions achieve higher perfor-\nmance compared to the DSC of UniMatch (0.872, 0.891). This indicates\nthat introducing and enriching feature perturbations can effectively\nimprove the model performance when data labels are scarce. Further-\nmore, comparing the performance of different models reveals that other\nmodels achieve better results than UniMatch-DFPM with DDD in most\ncases. The reason for this is that UniMatch-DFPM with DDD only in-\ncludes feature perturbations in the channel dimension, which leads to its\nrelatively poor performance. In contrast, other models incorporate\nfeature perturbations in both channel and spatial dimensions, leading\nthem to learn more robust feature representations and achieve better\nperformance. Finally, UniMatch-DFPM with DNM achieves the best\nperformance, which proves the effectiveness and reasonableness of\nproposed method and shows that diverse perturbation strategies allow\nthe model to understand and capture data features from more perspec-\ntives and enable it to learn more effective feature representations.\n5. Conclusion\nTo address the challenge of accurate cardiac MRI image segmenta-\ntion under limited labeled data, this study proposes a novel semi-\nsupervised segmentation framework based on UniMatch. The\nimproved UniMatch framework consists of two components named\nDPPCRM and DFPM. In DPPCRM, two differentiated perturbation pools\nare designed to enrich the diversity and specificity of the perturbed\nimages, which enable the model to improve its generalization perfor-\nmance to cardiac segmentation under the constraint of consistency\nregularization. In addition, a consistent regularization method based on\nthe mixup strategy is constructed in DPPCRM, which further exploits the\nfeature information in strongly perturbed images and enhances the\nrobustness of the model. In the DFPM module, perturbations are per-\nformed on the features extracted from weakly perturbed images in both\nthe channel and spatial dimensions which allows the model to learn\nmore robust feature representations.\nExtensive experiments on two public cardiac MRI image datasets\ndemonstrate that the proposed improved-UniMatch achieves superior\nimage segmentation performance using only a small number of anno-\ntations compared to the current state-of-the-art methods, which dem-\nonstrates the advantages of the proposed method in the field of cardiac\nMRI image segmentation.\nAlthough the proposed method has achieved excellent performance\nin cardiac MRI images segmentation tasks, there are still areas that can\nbe further optimized and improved. Firstly, the proposed method, along\nwith UniMatch and FixMatch, uses a threshold-based approach to filter\nincorrect pseudo-labels, which may cause some pixels to be without\nFig. 8. DSC performance of different feature perturbation strategies on the ACDC dataset.\nH. Guo et al. \nDigital Signal Processing 168 (2026) 105494\n11\n\nlabels, thus affecting the modelâ€™s segmentation accuracy for these pixels.\nFurthermore, the prediction of pseudo-labels and perturbed samples in\nthe current approach is performed in the same framework, which is\nprone to overfitting errors in its own prediction, thus affecting the\nperformance of the model.\nIn summary, future research can focus on two aspects:(1) Exploring\nthe category-based adaptive thresholding strategy to adaptively adjust\nthe label thresholds according to the prediction accuracy of different\ncategories, to further improve the prediction accuracy of the model for\ndifferent structures of the cardiac. (2) Migrating existing studies to the\nteacher-student framework for triage processing of pseudo-label gener-\nation and perturbed sample prediction to further enhance model\nperformance.\nFunding\nThis work was supported by the National Natural Science Foundation\nof China [grant number 52077056].\nCredit author statement\nHongxu Guo: Conceptualization, Methodology, Software, Writing -\nOriginal Draft.\nYing Li: Project administration, Validation, Writing - Review &\nEditing.\nXianzhe Wang: Formal analysis, Visualization.\nRenjie He: Supervision, Review.\nJie Quan: Software, Validation.\nLingyue Wang: Investigation.\nLei Guo: Project administration, Supervision.\nDeclaration of competing interest\nThe authors declare that they have no known competing financial\ninterests or personal relationships that could have appeared to influence\nthe work reported in this paper.\nData availability\nData will be made available on request.\nReferences\n[1] M. Lindstrom, N. DeCleene, H. Dorsey, V. Fuster, C.O. Johnson, K.E. LeGrand, G.\nA. Mensah, C. Razo, J.Varieur Turco B.Stark, Global burden of cardiovascular\ndiseases and risks collaboration, 1990-2021, J. Am. Coll. Cardiol. 80 (2022)\n2372â€“2425.\n[2] A.D. Piersson, Essentials of cardiac MRI in clinical practice, J. Cardiovasc. Magn.\nReson. 18 (2016) T10.\n[3] J. Sander, B.D. de Vos, I. I\nË‡\nsgum, Automatic segmentation with detection of local\nsegmentation failures in cardiac MRI, Sci. Rep. 10 (2020) 21769.\n[4] V.M. Campello, P. Gkontra, C. Izquierdo, C. Martin-Isla, A. Sojoudi, P.M. Full,\nK. Maier-Hein, Y. Zhang, Z. He, J. Ma, Multi-centre, multi-vendor and multi-disease\ncardiac segmentation: the M&Ms challenge, IEEe Trans. Med. ImAging 40 (2021)\n3543â€“3554.\n[5] Q. Zeng, Y. Xie, Z. Lu, M. Lu, Y. Xia, Discrepancy matters: learning from\ninconsistent decoder features for consistent semi-supervised medical image\nsegmentation, arXiv preprint arXiv:2309.14819, (2023).\n[6] P. Bachman, O. Alsharif, D. Precup, Learning with pseudo-ensembles, Adv. Neural\nInf. Process. Syst. 27 (2014) 3365â€“3373.\n[7] S. Laine, T. Aila, Temporal ensembling for semi-supervised learning, arXiv preprint\narXiv:1610.02242, (2016).\n[8] M. Sajjadi, M. Javanmardi, T. Tasdizen, Regularization with stochastic\ntransformations and perturbations for deep semi-supervised learning, Adv. Neural\nInf. Process. Syst. 29 (2016).\n[9] D.-H. Lee, Pseudo-label: the simple and efficient semi-supervised learning method\nfor deep neural networks. Workshop On Challenges in Representation Learning,\nICML, Atlanta, 2013, p. 896.\n[10] B. Zhang, Y. Wang, W. Hou, H. Wu, J. Wang, M. Okumura, T. Shinozaki,\nFlexmatch: boosting semi-supervised learning with curriculum pseudo labeling,\nAdv. Neural Inf. Process. Syst. 34 (2021) 18408â€“18419.\n[11] P. Cascante-Bonilla, F. Tan, Y. Qi, V. Ordonez, Curriculum labeling: revisiting\npseudo-labeling for semi-supervised learning, in: Proceedings of the AAAI\nconference on artificial intelligence, 2021, pp. 6912â€“6920.\n[12] D. Berthelot, N. Carlini, I. Goodfellow, N. Papernot, A. Oliver, C.A. Raffel,\nMixmatch: a holistic approach to semi-supervised learning, Adv. Neural Inf.\nProcess. Syst. (2019) 32.\n[13] Q. Xie, Z. Dai, E. Hovy, T. Luong, Q. Le, Unsupervised data augmentation for\nconsistency training, Adv. Neural Inf. Process. Syst. 33 (2020) 6256â€“6268.\n[14] A. Rasmus, M. Berglund, M. Honkala, H. Valpola, T. Raiko, Semi-supervised\nlearning with ladder networks, Adv. Neural Inf. Process. Syst. (2015) 28.\n[15] K. Sohn, D. Berthelot, N. Carlini, Z. Zhang, H. Zhang, C.A. Raffel, E.D. Cubuk,\nA. Kurakin, C.-L. Li, Fixmatch: simplifying semi-supervised learning with\nconsistency and confidence, Adv. Neural Inf. Process. Syst. 33 (2020) 596â€“608.\n[16] L. Yang, L. Qi, L. Feng, W. Zhang, Y. Shi, Revisiting weak-to-strong consistency in\nsemi-supervised semantic segmentation, in: Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition, 2023, pp. 7236â€“7246.\n[17] C. Martin-Isla, V.M. Campello, C. Izquierdo, Z. Raisi-Estabragh, B. BaeÃŸler, S.\nE. Petersen, K. Lekadir, Image-based cardiac diagnosis with machine learning: a\nreview, Front. Cardiovasc. Med. 7 (2020) 1.\n[18] J. Long, E. Shelhamer, T. Darrell, Fully convolutional networks for semantic\nsegmentation, in: Proceedings of the IEEE conference on computer vision and\npattern recognition, 2015, pp. 3431â€“3440.\n[19] O. Ronneberger, P. Fischer, T. Brox, U-net: convolutional networks for biomedical\nimage segmentation, in: Medical Image Computing And Computer-Assisted\nInterventionâ€“MICCAI 2015: 18th International Conference, Munich, Germany,\nOctober 5-9, 2015, Proceedings, Part III 18, Springer, 2015, pp. 234â€“241.\n[20] H. Huang, L. Lin, R. Tong, H. Hu, Q. Zhang, Y. Iwamoto, X. Han, Y.-W. Chen, J. Wu,\nUnet 3+: a full-scale connected unet for medical image segmentation, in: ICASSP\n2020-2020 IEEE International Conference On Acoustics, Speech And Signal\nProcessing (ICASSP), IEEE, 2020, pp. 1055â€“1059.\n[21] X. Li, H. Chen, X. Qi, Q. Dou, C.-W. Fu, P.-A. Heng, H-DenseUNet: hybrid densely\nconnected UNet for liver and tumor segmentation from CT volumes, IEEe Trans.\nMed. ImAging 37 (2018) 2663â€“2674.\n[22] W. Bai, H. Suzuki, C. Qin, G. Tarroni, O. Oktay, P.M. Matthews, D. Rueckert,\nRecurrent neural networks for aortic image sequence segmentation with sparse\nannotations, in: Medical Image Computing and Computer Assisted\nInterventionâ€“MICCAI 2018: 21st International Conference, Granada, Spain,\nSeptember 16-20, 2018, Proceedings, Part IV 11, Springer, 2018, pp. 586â€“594.\n[23] O. Oktay, J. Schlemper, L.L. Folgoc, M. Lee, M. Heinrich, K. Misawa, K. Mori, S.\nMcDonagh, N.Y. Hammerla, B. Kainz, Attention u-net: learning where to look for\nthe pancreas, arXiv preprint arXiv:1804.03999, (2018).\n[24] X. Chen, R. Zhang, P. Yan, Feature fusion encoder decoder network for automatic\nliver lesion segmentation, in: 2019 IEEE 16th international symposium on\nbiomedical imaging (ISBI 2019), IEEE, 2019, pp. 430â€“433.\n[25] L. Chen, P. Bentley, K. Mori, K. Misawa, M. Fujiwara, D. Rueckert, DRINet for\nmedical image segmentation, IEEe Trans. Med. ImAging 37 (2018) 2453â€“2462.\n[26] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner, M.\nDehghani, M. Minderer, G. Heigold, S. Gelly, An image is worth 16x16 words:\ntransformers for image recognition at scale, arXiv preprint arXiv:2010.11929,\n(2020).\n[27] H.-Y. Zhou, J. Guo, Y. Zhang, L. Yu, L. Wang, Y. Yu, nnformer: interleaved\ntransformer for volumetric segmentation, arXiv preprint arXiv:2109.03201,\n(2021).\n[28] J. Chen, Y. Lu, Q. Yu, X. Luo, E. Adeli, Y. Wang, L. Lu, A.L. Yuille, Y. Zhou,\nTransunet: transformers make strong encoders for medical image segmentation,\narXiv preprint arXiv:2102.04306, (2021).\n[29] Y. Xie, J. Zhang, C. Shen, Y. Xia, Cotr: efficiently bridging cnn and transformer for\n3d medical image segmentation, in: Medical Image Computing and Computer\nAssisted Interventionâ€“MICCAI 2021: 24th International Conference, Strasbourg,\nFrance, September 27â€“October 1, 2021, Proceedings, Part III 24, Springer, 2021,\npp. 171â€“180.\n[30] H. Cao, Y. Wang, J. Chen, D. Jiang, X. Zhang, Q. Tian, M. Wang, Swin-unet: Unet-\nlike pure transformer for medical image segmentation, in: European conference on\ncomputer vision, Springer, 2022, pp. 205â€“218.\n[31] X. Luo, W. Liao, J. Chen, T. Song, Y. Chen, S. Zhang, N. Chen, G. Wang, S. Zhang,\nEfficient semi-supervised gross target volume of nasopharyngeal carcinoma\nsegmentation via uncertainty rectified pyramid consistency, in: Medical Image\nComputing and Computer Assisted Interventionâ€“MICCAI 2021: 24th International\nConference, Strasbourg, France, September 27â€“October 1, 2021, Proceedings, Part\nII 24, Springer, 2021, pp. 318â€“329.\n[32] X. Cao, H. Chen, Y. Li, Y. Peng, S. Wang, L. Cheng, Uncertainty aware temporal-\nensembling model for semi-supervised abus mass segmentation, IEEe Trans. Med.\nImAging 40 (2020) 431â€“443.\n[33] Y. Wang, Y. Zhang, J. Tian, C. Zhong, Z. Shi, Y. Zhang, Z. He, Double-uncertainty\nweighted method for semi-supervised learning, in: Medical Image Computing and\nComputer Assisted Interventionâ€“MICCAI 2020: 23rd International Conference,\nLima, Peru, October 4â€“8, 2020, Proceedings, Part I 23, Springer, 2020,\npp. 542â€“551.\n[34] Y. Wu, M. Xu, Z. Ge, J. Cai, L. Zhang, Semi-supervised left atrium segmentation\nwith mutual consistency training, in: Medical Image Computing And Computer\nAssisted Interventionâ€“MICCAI 2021: 24th International Conference, Strasbourg,\nFrance, September 27â€“October 1, 2021, Proceedings, Part II 24, Springer, 2021,\npp. 297â€“306.\n[35] S. ReiÃŸ, C. Seibold, A. Freytag, E. Rodner, R. Stiefelhagen, Every annotation counts:\nmulti-label deep supervision for medical image segmentation, in: Proceedings of\nH. Guo et al. \nDigital Signal Processing 168 (2026) 105494\n12\n\nthe IEEE/CVF Conference On Computer Vision And Pattern Recognition, 2021,\npp. 9532â€“9542.\n[36] Q. Xie, M.-T. Luong, E. Hovy, Q.V. Le, Self-training with noisy student improves\nimagenet classification, in: Proceedings of the IEEE/CVF Conference On Computer\nVision And Pattern Recognition, 2020, pp. 10687â€“10698.\n[37] R. Ke, A.I. Aviles-Rivero, S. Pandey, S. Reddy, C.-B. Sch\nÂ¨\nonlieb, A three-stage self-\ntraining framework for semi-supervised semantic segmentation, IEEe Trans. Image\nProcess. 31 (2022) 1805â€“1815.\n[38] Y. Xia, D. Yang, Z. Yu, F. Liu, J. Cai, L. Yu, Z. Zhu, D. Xu, A. Yuille, H. Roth,\nUncertainty-aware multi-view co-training for semi-supervised medical image\nsegmentation and domain adaptation, Med. Image Anal. 65 (2020) 101766.\n[39] A. Tarvainen, H. Valpola, Mean teachers are better role models: weight-averaged\nconsistency targets improve semi-supervised deep learning results, Adv. Neural Inf.\nProcess. Syst. 30 (2017).\n[40] Y. Wu, Z. Wu, Q. Wu, Z. Ge, J. Cai, Exploring smoothness and class-separation for\nsemi-supervised medical image segmentation, in: International Conference On\nMedical Image Computing And Computer-Assisted Intervention, Springer, 2022,\npp. 34â€“43.\n[41] Y. Liu, Y. Tian, Y. Chen, F. Liu, V. Belagiannis, G. Carneiro, Perturbed and strict\nmean teachers for semi-supervised semantic segmentation, in: Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern Recognition, 2022,\npp. 4258â€“4267.\n[42] M.-C. Xu, Y.-K. Zhou, C. Jin, S.B. Blumberg, F.J. Wilson, M. deGroot, D.\nC. Alexander, N.P. Oxtoby, J. Jacob, Learning morphological feature perturbations\nfor calibrated semi-supervised segmentation, in: International Conference on\nMedical Imaging with Deep Learning, PMLR, 2022, pp. 1413â€“1429.\n[43] Y. Ouali, C. Hudelot, M. Tami, Semi-supervised semantic segmentation with cross-\nconsistency training, in: Proceedings of the IEEE/CVF Conference On Computer\nVision And Pattern Recognition, 2020, pp. 12674â€“12684.\n[44] Z. Feng, Q. Zhou, Q. Gu, X. Tan, G. Cheng, X. Lu, J. Shi, L. Ma, Dmt: dynamic\nmutual training for semi-supervised learning, Pattern. Recognit. 130 (2022)\n108777.\n[45] Z. Ke, D. Wang, Q. Yan, J. Ren, R.W. Lau, Dual student: breaking the limits of the\nteacher in semi-supervised learning, in: Proceedings of the IEEE/CVF International\nConference On Computer Vision, 2019, pp. 6728â€“6736.\n[46] Z. Ke, D. Qiu, K. Li, Q. Yan, R.W. Lau, Guided collaborative training for pixel-wise\nsemi-supervised learning, in: Computer Visionâ€“ECCV 2020: 16th European\nConference, Glasgow, UK, August 23â€“28, 2020, Proceedings, Part XIII 16, Springer,\n2020, pp. 429â€“445.\n[47] W. Zhang, L. Zhu, J. Hallinan, S. Zhang, A. Makmur, Q. Cai, B.C. Ooi, Boostmis:\nboosting medical image semi-supervised learning with adaptive pseudo labeling\nand informative active annotation, in: Proceedings of the IEEE/CVF conference on\ncomputer vision and pattern recognition, 2022, pp. 20666â€“20676.\n[48] L. Yu, S. Wang, X. Li, C.-W. Fu, P.-A. Heng, Uncertainty-aware self-ensembling\nmodel for semi-supervised 3D left atrium segmentation, in: Medical Image\nComputing And Computer Assisted Interventionâ€“MICCAI 2019: 22nd International\nConference, Shenzhen, China, October 13â€“17, 2019, Proceedings, Part II 22,\nSpringer, 2019, pp. 605â€“613.\n[49] R. Mendel, L.A. De Souza, D. Rauber, J.P. Papa, C. Palm, Semi-supervised\nsegmentation based on error-correcting supervision, in: Computer Visionâ€“ECCV\n2020: 16th European Conference, Glasgow, UK, August 23â€“28, 2020, Proceedings,\nPart XXIX 16, Springer, 2020, pp. 141â€“157.\n[50] D. Kwon, S. Kwak, Semi-supervised semantic segmentation with error localization\nnetwork, in: Proceedings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition, 2022, pp. 9957â€“9967.\n[51] Z. Lai, C. Wang, Z. Hu, B.N. Dugger, S.-C. Cheung, C.-N. Chuah, A semi-supervised\nlearning for segmentation of gigapixel histopathology images from brain tissues,\nin: 2021 43rd Annual International Conference of the IEEE Engineering in\nMedicine & Biology Society (EMBC), IEEE, 2021, pp. 1920â€“1923.\n[52] M. Wei, C. Budd, L.C. Garcia-Peraza-Herrera, R. Dorent, M. Shi, T. Vercauteren,\nSegMatch: a semi-supervised learning method for surgical instrument\nsegmentation, arXiv preprint arXiv:2308.05232, (2023).\n[53] Z. Peng, S. Tian, L. Yu, D. Zhang, W. Wu, S. Zhou, Semi-supervised medical image\nclassification with adaptive threshold pseudo-labeling and unreliable sample\ncontrastive loss, Biomed. Signal. Process. Control 79 (2023) 104142.\n[54] O. Ecabert, J. Peters, H. Schramm, C. Lorenz, J. von Berg, M.J. Walker, M. Vembar,\nM.E. Olszewski, K. Subramanyan, G. Lavi, Automatic model-based segmentation of\nthe heart in CT images, IEEe Trans. Med. ImAging 27 (2008) 1189â€“1201.\n[55] S.K. Hasan, C.A. Linte, A modified U-Net convolutional network featuring a\nnearest-neighbor re-sampling-based elastic-transformation for brain tissue\ncharacterization and segmentation, in: 2018 IEEE Western New York Image and\nSignal Processing Workshop (WNYISPW), IEEE, 2018, pp. 1â€“5.\n[56] R. Hemelings, B. Elen, I. Stalmans, K. Van Keer, P. De Boever, M.B. Blaschko,\nArteryâ€“vein segmentation in fundus images using a fully convolutional network,\nComput. Med. Imaging Graph. 76 (2019) 101636.\n[57] A. Wibowo, S.R. Purnama, P.W. Wirawan, H. Rasyidi, Lightweight encoder-\ndecoder model for automatic skin lesion segmentation, Inform. Med. Unlocked. 25\n(2021) 100640.\n[58] M. Agarwal, R. Mahajan, Medical images contrast enhancement using quad\nweighted histogram equalization with adaptive gama correction and homomorphic\nfiltering, Procedia Comput. Sci. 115 (2017) 509â€“517.\n[59] S. Yun, D. Han, S.J. Oh, S. Chun, J. Choe, Y. Yoo, Cutmix: regularization strategy to\ntrain strong classifiers with localizable features, in: Proceedings of the IEEE/CVF\nInternational Conference On Computer Vision, 2019, pp. 6023â€“6032.\n[60] L. Yang, W. Zhuo, L. Qi, Y. Shi, Y. Gao, St++: make self-training work better for\nsemi-supervised semantic segmentation, in: Proceedings of the IEEE/CVF\nConference On Computer Vision And Pattern Recognition, 2022, pp. 4268â€“4277.\n[61] G. French, T. Aila, S. Laine, M. Mackiewicz, G. Finlayson, Semi-supervised\nsemantic segmentation needs strong, high-dimensional perturbations, (2019).\n[62] M. Sun, Y. Jiang, H. Guo, Semi-supervised detection, identification and\nsegmentation for abdominal organs, in: MICCAI Challenge on Fast and Low-\nResource Semi-supervised Abdominal Organ Segmentation, Springer, 2022,\npp. 35â€“46.\n[63] J. Yuan, Y. Liu, C. Shen, Z. Wang, H. Li, A simple baseline for semi-supervised\nsemantic segmentation with strong data augmentation, in: Proceedings of the\nIEEE/CVF International Conference on Computer Vision, 2021, pp. 8229â€“8238.\n[64] H. Zhang, M. Cisse, Y.N. Dauphin, D. Lopez-Paz, mixup: beyond empirical risk\nminimization, arXiv preprint arXiv:1710.09412, (2017).\n[65] O. Bernard, A. Lalande, C. Zotti, F. Cervenansky, X. Yang, P.-A. Heng, I. Cetin,\nK. Lekadir, O. Camara, M.A.G. Ballester, Deep learning techniques for automatic\nMRI cardiac multi-structures segmentation and diagnosis: is the problem solved?\nIEEe Trans. Med. ImAging 37 (2018) 2514â€“2525.\n[66] X. Zhuang, Multivariate mixture model for myocardial segmentation combining\nmulti-source images, IEEe Trans. Pattern. Anal. Mach. Intell. 41 (2018)\n2933â€“2946.\n[67] W. Bai, O. Oktay, M. Sinclair, H. Suzuki, M. Rajchl, G. Tarroni, B. Glocker, A. King,\nP.M. Matthews, D. Rueckert, Semi-supervised learning for network-based cardiac\nMR image segmentation, in: Medical Image Computing and Computer-Assisted\nIntervention\u0000 MICCAI 2017: 20th International Conference, Quebec City, QC,\nCanada, September 11-13, 2017, Proceedings, Part II 20, Springer, 2017,\npp. 253â€“260.\n[68] V. Verma, K. Kawaguchi, A. Lamb, J. Kannala, A. Solin, Y. Bengio, D. Lopez-Paz,\nInterpolation consistency training for semi-supervised learning, Neural Netw. 145\n(2022) 90â€“106.\n[69] F. Isensee, P.F. Jaeger, S.A. Kohl, J. Petersen, K.H. Maier-Hein, nnU-Net: a self-\nconfiguring method for deep learning-based biomedical image segmentation, Nat.\nMethods 18 (2021) 203â€“211.\n[70] J. Peng, G. Estrada, M. Pedersoli, C. Desrosiers, Deep co-training for semi-\nsupervised image segmentation, Pattern. Recognit. 107 (2020) 107269.\n[71] T.-H. Vu, H. Jain, M. Bucher, M. Cord, P. P\nÂ´\nerez, Advent: adversarial entropy\nminimization for domain adaptation in semantic segmentation, in: Proceedings of\nthe IEEE/CVF conference on computer vision and pattern recognition, 2019,\npp. 2517â€“2526.\n[72] X. Luo, G. Wang, W. Liao, J. Chen, T. Song, Y. Chen, S. Zhang, D.N. Metaxas,\nS. Zhang, Semi-supervised medical image segmentation via uncertainty rectified\npyramid consistency, Med. Image Anal. 80 (2022) 102517.\n[73] X. Chen, Y. Yuan, G. Zeng, J. Wang, Semi-supervised semantic segmentation with\ncross pseudo supervision, in: Proceedings of the IEEE/CVF Conference On\nComputer Vision And Pattern Recognition, 2021, pp. 2613â€“2622.\n[74] X. Luo, M. Hu, T. Song, G. Wang, S. Zhang, Semi-supervised medical image\nsegmentation via cross teaching between cnn and transformer, in: International\nConference On Medical Imaging With Deep Learning, PMLR, 2022, pp. 820â€“833.\nH. Guo et al. \nDigital Signal Processing 168 (2026) 105494\n13",
    "version": "5.3.31"
  },
  {
    "numpages": 9,
    "numrender": 9,
    "info": {
      "PDFFormatVersion": "1.7",
      "Language": null,
      "EncryptFilterName": null,
      "IsLinearized": true,
      "IsAcroFormPresent": false,
      "IsXFAPresent": false,
      "IsCollectionPresent": false,
      "IsSignaturesPresent": false,
      "Custom": {
        "CrossmarkDomainExclusive": "true",
        "CrossMarkDomains[2]": "elsevier.com",
        "ElsevierWebPDFSpecifications": "6.5",
        "CrossmarkMajorVersionDate": "2010-04-23",
        "CrossMarkDomains[1]": "sciencedirect.com",
        "robots": "noindex",
        "AuthoritativeDomain[1]": "sciencedirect.com",
        "doi": "10.1016/j.media.2020.101688",
        "AuthoritativeDomain[2]": "elsevier.com"
      },
      "CreationDate": "D:20200408091153+05'30'",
      "Author": "Dennis Bontempi",
      "Creator": "Elsevier",
      "Subject": "Medical Image Analysis, 62 (2020) 101688. doi:10.1016/j.media.2020.101688",
      "ModDate": "D:20200408091235+05'30'",
      "Keywords": "Brain MRI segmentation; Convolutional neural networks; Weakly supervised learning; 3D Image analysis",
      "Title": "CEREBRUM: a fast and fully-volumetric Convolutional Encoder-decodeR for weakly-supervised sEgmentation of BRain strUctures from out-of-the-scanner MRI",
      "Producer": "Acrobat Distiller 10.1.10 (Windows)"
    },
    "metadata": {
      "xmp:createdate": "2020-04-08T09:11:53+05:30",
      "xmp:creatortool": "Elsevier",
      "xmp:modifydate": "2020-04-08T09:12:35+05:30",
      "xmp:metadatadate": "2020-04-08T09:12:35+05:30",
      "pdf:producer": "Acrobat Distiller 10.1.10 (Windows)",
      "pdf:keywords": "Brain MRI segmentation,Convolutional neural networks,Weakly supervised learning,3D Image analysis",
      "dc:format": "application/pdf",
      "dc:identifier": "doi:10.1016/j.media.2020.101688",
      "dc:publisher": "Elsevier B.V.",
      "dc:description": "Medical Image Analysis, 62 (2020) 101688. doi:10.1016/j.media.2020.101688",
      "dc:subject": [
        "Brain MRI segmentation",
        "Convolutional neural networks",
        "Weakly supervised learning",
        "3D Image analysis"
      ],
      "dc:title": "CEREBRUM: a fast and fully-volumetric Convolutional Encoder-decodeR for weakly-supervised sEgmentation of BRain strUctures from out-of-the-scanner MRI",
      "dc:creator": [
        "Dennis Bontempi",
        "Sergio Benini",
        "Alberto Signoroni",
        "Michele Svanera",
        "Lars Muckli"
      ],
      "xmpmm:documentid": "uuid:fd8cfc10-4791-4a8b-8025-00abb91efa0c",
      "xmpmm:instanceid": "uuid:7c67b6df-3f59-41a9-b6a6-acb2065382c7",
      "prism:aggregationtype": "journal",
      "prism:copyright": "Â© 2020 The Authors. Published by Elsevier B.V.",
      "prism:publicationname": "Medical Image Analysis",
      "prism:issn": "1361-8415",
      "prism:volume": "62",
      "prism:doi": "10.1016/j.media.2020.101688",
      "prism:url": "https://doi.org/10.1016/j.media.2020.101688",
      "prism:startingpage": "101688",
      "prism:pagerange": "101688",
      "prism:coverdisplaydate": "May 2020",
      "6miuqowv-nm6qndqgnm2kzsnnz9-olt-mywugz9ijzt6podajy9uso9eqn9iknm-qnd6tma": "",
      "pdfx:crossmarkmajorversiondate": "2010-04-23",
      "pdfx:crossmarkdomainexclusive": "true",
      "pdfx:doi": "10.1016/j.media.2020.101688",
      "pdfx:robots": "noindex",
      "pdfx:authoritativedomain": "sciencedirect.comelsevier.com",
      "pdfx:crossmarkdomains": "sciencedirect.comelsevier.com",
      "jav:journal_article_version": "VoR",
      "crossmark:crossmarkdomainexclusive": "true",
      "crossmark:majorversiondate": "2010-04-23",
      "crossmark:doi": "10.1016/j.media.2020.101688",
      "crossmark:crossmarkdomains": "sciencedirect.comelsevier.com",
      "xmprights:marked": "True",
      "ali:license_ref": "http://creativecommons.org/licenses/by/4.0/"
    },
    "text": "Medical Image Analysis 62 (2020) 101688\nContents lists available at ScienceDirect\nMedical Image Analysis\njournal homepage: www.elsevier.com/locate/media\nCEREBRUM: a fast and fully-volumetric Convolutional\nEncoder-decodeR for weakly-supervised sEgmentation of BRain\nstrUctures from out-of-the-scanner MRI\nDennis Bontempi \na\n, Sergio Benini \na\n, Alberto Signoroni \na\n, Michele Svanera \nb,1,âˆ—\n, Lars Muckli \nb,1\na \nDepartment of Information Engineering, University of Brescia, Brescia, Italy\nb \nInstitute of Neuroscience and Psychology, University of Glasgow, Glasgow, United Kingdom\na r t i c l e i n f o\nArticle history:\nReceived 30 September 2019\nRevised 7 March 2020\nAccepted 12 March 2020\nAvailable online 24 March 2020\nKeywords:\nBrain MRI segmentation\nConvolutional neural networks\nWeakly supervised learning\n3D Image analysis\na b s t r a c t\nMany functional and structural neuroimaging studies call for accurate morphometric segmentation of\ndifferent brain structures starting from image intensity values of MRI scans. Current automatic (multi-\n) atlas-based segmentation strategies often lack accuracy on difficult-to-segment brain structures and,\nsince these methods rely on atlas-to-scan alignment, they may take long processing times. Alternatively,\nrecent methods deploying solutions based on Convolutional Neural Networks (CNNs) are enabling the\ndirect analysis of out-of-the-scanner data. However, current CNN-based solutions partition the test vol-\nume into 2D or 3D patches, which are processed independently. This process entails a loss of global\ncontextual information, thereby negatively impacting the segmentation accuracy. In this work, we design\nand test an optimised end-to-end CNN architecture that makes the exploitation of global spatial infor-\nmation computationally tractable, allowing to process a whole MRI volume at once. We adopt a weakly\nsupervised learning strategy by exploiting a large dataset composed of 947 out-of-the-scanner (3 Tesla\nT1-weighted 1mm isotropic MP-RAGE 3D sequences) MR Images. The resulting model is able to pro-\nduce accurate multi-structure segmentation results in only a few seconds. Different quantitative measures\ndemonstrate an improved accuracy of our solution when compared to state-of-the-art techniques. More-\nover, through a randomised survey involving expert neuroscientists, we show that subjective judgements\nfavour our solution with respect to widely adopted atlas-based software.\nÂ© 2020 The Authors. Published by Elsevier B.V.\nThis is an open access article under the CC BY license. (http://creativecommons.org/licenses/by/4.0/)\n1. Introduction\nThe segmentation of various brain structures from MRI scans\nis an essential process in several non-clinical and clinical analy-\nses, such as the comparison at various stages of normal brain, or\ndisease development of neurodegenerative processes, neurological\ndiseases, and psychiatric disorders. The morphometric approach is\nespecially helpful in pathological situations for confirming the di-\nagnosis, defining the prognosis, and selecting the best treatment.\nMoreover, brain structure segmentation is an early step in func-\ntional MRI (fMRI) study pipelines, as neuroscientists need to iso-\nlate specific brain structures before analysing the spatiotemporal\npatterns of activity within them.\nâˆ— \nCorresponding author.\nE-mail address: michele.svanera@glasgow.ac.uk (M. Svanera).\n1 \nShared authorship.\nManual segmentation, although considered to be the gold stan-\ndard in terms of accuracy, is time consuming (Zhan et al., 2018).\nTherefore, neuroscience studies began to exploit computer vi-\nsion to process data from increasingly performing MRI scanners\nand ease the interpretation of brain data, intrinsically charac-\nterised by a strong inter-subject variability. Different fully auto-\nmated pipelines have been developed in recent years (Despotovi \nÂ´\nc\net al., 2015), moving from techniques based only on image fea-\ntures to ones that make also use of a-priori statistical knowledge\nabout the neuroanatomy. The vast majority of the available tools\napply a (multi-) atlas-based segmentation strategy (Cabezas et al.,\n2011), in which the segmentation of the target volume is inferred\nfrom one or several templates built from manual annotations. In\norder to make this inference phase possible, a time consuming\nand computationally intensive (FreeSurfer, 2008) non-rigid subject-\nto-atlas alignment is necessary. Due to the aforementioned high\ninter-subject brain variability, such registration procedures often\nintroduce errors that yield a decrease in segmentation accuracy on\nhttps://doi.org/10.1016/j.media.2020.101688\n1361-8415/Â© 2020 The Authors. Published by Elsevier B.V. This is an open access article under the CC BY license. (http://creativecommons.org/licenses/by/4.0/)\n\n2 D. Bontempi, S. Benini and A. Signoroni et al. / Medical Image Analysis 62 (2020) 101688\nbrain structure or tissue boundaries (Klein et al., 2017; Lerch et al.,\n2017).\nIn recent years, Deep Learning (DL) techniques have emerged\nas one of the most powerful ways to combine statistical mod-\nelling of the data with pattern recognition for decision making\nand classification (Voulodimos et al., 2018), and their develop-\nment is impacting various medical imaging domains (Hamidinekoo\net al., 2018; Litjens et al., 2017). Provided that they are trained\non a sufficient amount of data embodying the observable variabil-\nity, DL models are able to generalise well to previously unseen\ndata. Furthermore, they can work directly with out-of-the-scanner\nimages, removing the need for the expensive scan-to-atlas align-\nment phase. Numerous DL-based algorithms proposed for brain\nMRI segmentation match or even improve the accuracy of atlas-\nbased segmentation tools (Akkus et al., 2017; Rajchl et al., 2018;\nRoy et al., 2019; Wachinger et al., 2018). Due to the scarcity of\ntraining data and to hardware limitations, approaching this task\nusing DL commonly requires the volume to be processed consid-\nering 2D (Roy et al., 2019) or 3D-patches (Fedorov et al., 2017; Ra-\njchl et al., 2018; Dolz et al., 2019; Wachinger et al., 2018; Li et al.,\n2017) at a time. Although this method simplifies the process from\na technical point of view, it introduces significant limitations in the\nanalysis: since each 2D or 3D patch is segmented independently\nfrom the others, these models mostly exploit local spatial infor-\nmation - ignoring â€œglobalâ€ cues, such as the absolute and relative\npositions of different brain structures - which makes them sub-\noptimal. Different works have considered the potential improve-\nments of removing said volume partitioning (McClure et al., 2018;\nWachinger et al., 2018). Solutions that exploit such fully-volumetric\napproach have already been applied to prostate (Milletari et al.,\n2016), heart atrium (Savioli et al., 2019), and proximal femur MRI\nsegmentation (Deniz et al., 2018), but not yet to brain MRI seg-\nmentation - where this strategy could prove particularly useful\ngiven the complex geometry and the variety of structures charac-\nterising the brain anatomy. Here, we discuss how both hardware\nlimitations and the scarcity of hand-labelled ground truth (GT)\ndata can be overcome. First, we tackle the former by customis-\ning and simplifying the model architecture. Second, the latter is\ncoped with by training our model on segmentation masks obtained\nexploiting atlas-based techniques, in what can be considered a\nweakly supervised fashion - more precisely what (Zhou, 2017) and\n(Li et al., 2019) describe as â€œinaccurate supervisionâ€. Hence, even\nthough CEREBRUM is trained exploiting labelling which is not ex-\nempt from errors, we demonstrate that the statistical reliability of\natlas-based segmentation is enough to guarantee good generalisa-\ntion capability of the DL models trained on such imperfect ground\ntruth.\n2. Existing methods for whole brain MRI segmentation and\nhow to advance them\n2.1. Atlas-based methods\nIn the last twenty years, several atlas-based segmentation\nmethods have been developed. However, only a few of them\nare completely automatic, and thus pertinent to our discus-\nsion: FreeSurfer, FSLâ€™s FAST and FMRIB, and fMRIprep. FreeSurfer\n(Fischl, 2012) is an open-source software package that contains a\ncompletely automated pipeline for tissue and sub-cortical brain\nstructure segmentation. FSLâ€™s FAST (FMRIBâ€™s Automated Segmen-\ntation Tool, Zhang et al., 2001) and FIRST (FMRIBâ€™s Integrated\nRegistration and Segmentation Tool, Patenaude et al., 2011) are\npart of the Oxfordâ€™s open-source library of analysis tools for MRI\nand fMRI data. FAST segments different tissue types in already\nskull-stripped brain scans, while FIRST deals with the segmen-\ntation of sub-cortical brain structures. fMRIprep (Esteban et al.,\n2019) is a recently published preprocessing software for MRI\nscans that combines tools from widely used open-source neu-\nroimaging packages (e.g., the above mentioned FSL and FreeSurfer).\nIt implements a brain tissues segmentation pipeline, provid-\ning the user with both soft (i.e., probability maps) and hard\nsegmentation.\nThese methods are widely used in neuroscience, since they\nproduce consistent results with little human intervention. Never-\ntheless, they are all atlas-based and not learning-based - hence,\nthe only way to improve their accuracy is to manually produce\nnew atlases. Furthermore, since they implement a long processing\npipeline together with the atlas-based labelling strategy, the seg-\nmentation operation is time consuming (FreeSurfer, 2008). Limita-\ntions of these approaches, such as the lack of accuracy on various\nbrain structure boundaries, have been documented (Ellingsen et al.,\n2016; Wenger et al., 2014; Weier et al., 2012; Cabezas et al., 2011).\n2.2. Deep learning methods\nMany of the state-of-the-art methods based on deep learning\nexploit multi-modal MRI data (Ã‡iÃ§ek et al., 2016; Chen et al., 2018;\nDolz et al., 2019; Andermatt et al., 2016). Yet, in real-case scenar-\nios and due to time constraints, the acquisition of different MRI\nsequences for anatomical analysis is rarely done: in most studies a\nsingle sequence is used - with T1\nw \nbeing the most popular proto-\ncol. Various alternatives have been proposed to obtain whole brain\nsegmentation from T1\nw \nonly. QuickNAT (Roy et al., 2019) lever-\nages a 2D based approach to efficiently segment brain MRI, ex-\nploiting a paradigm that aggregates the predictions of three dif-\nferent encoder-decoder models by averaging the probability maps -\neach model trained to segment a single slice at a time along one of\nthe three principal axes (longitudinal, sagittal, and coronal). Mesh-\nNet (Fedorov et al., 2017; McClure et al., 2018) is a feedforward\nCNN based on 3D dilated convolutions, whose structure guarantees\ngood results while keeping the number of parameters low. Neu-\nroNet (Rajchl et al., 2018) is an encoder-multi-decoder CNN, trained\nto replicate segmentation results obtained with multiple state-of-\nthe-art neuroimaging tools. DeepNAT (Wachinger et al., 2018) is\ncomposed of a cascade of two CNNs. It breaks the segmentation\ntask into two hierarchical operations - the foreground-background\nseparation, and the labelling of each voxel as belonging to the fore-\nground - implemented by the first and the second network, respec-\ntively. Finally, the solution presented in Li et al. (2017) makes use\nof various refinements, such as residual connections and dilated\nconvolution, to favour the learning of 3D representation and in-\ncrease the compactness of the proposed model. Such modifications\nare furthermore at the centre of the extensive analysis conducted\nby the authors in an effort to explain how the former impact the\nmodel performance.\nHowever, a common trait of these methods is that they do not\nfully exploit the 3D spatial nature of MRI data. Although Quick-\nNAT tries to integrate spatial information by averaging the proba-\nbility maps computed with respect to different views, it is slice-\nbased. DeepNAT exploits an intrinsic parameterisation of the brain\n(through the Laplace-Beltrami operator) trying to introduce some\nspatial context, but as with MeshNet it is trained on small non-\noverlapping 3D-patches. Finally, NeuroNet is trained on random\ncrops of the MR volume, and so is the high-resolution compact\nCNN presented in Li et al. (2017).\n2.3. Aims and contributions\nAiming to exploit both local and global spatial information\ncontained in MRI data, we introduce CEREBRUM: a fast and\nfully-volumetric Convolutional Encoder-decodeR for weakly super-\nvised sEgmentation of BRain strUctures from out-of-the-scanner\n\nD. Bontempi, S. Benini and A. Signoroni et al. / Medical Image Analysis 62 (2020) 101688 3\nFig. 1. Overview of the proposed segmentation method. The model is trained on 900 T1\nw \nvolumes and the associated relabelled FreeSurfer segmentation, while testing is\nperformed by feeding NIfTI data to the model.\nMRI. To the best of our knowledge, CEREBRUM is the first DL\nmodel designed to tackle the brain MRI segmentation task in\nsuch a fully-volumetric fashion. This is accomplished exploiting\nan end-to-end encoding-decoding structure, where only convo-\nlutional blocks are used. This delivers a whole brain MRI seg-\nmentation in just âˆ¼ 5â€“10 s on a desktop GPU. The model ar-\nchitecture and the proposed learning framework are shown in\nFig. 1.\nSince in most real case scenarios, to save scanner time, only\nsingle-modal MR images are collected, we develop and test our\nmethod on a large set of data (composed by 947 MRI scans) ac-\nquired using a T1-weighted (T1\nw \n) 1mm isotropic MPRAGE proto-\ncol. Neither registration nor filtering is applied to these data, so\nthat CEREBRUM learns to segment out-of-the-scanner volumes. Fo-\ncusing on the requirements of a real case scenario (fMRI stud-\nies), we train the model to segment the classes of interest in the\nMICCAI challenge (Mendrik et al., 2015) i.e., gray matter (GM),\nwhite matter (WM), cerebrospinal fluid (CSF), ventricles, cerebel-\nlum, brainstem, and basal ganglia. Since manually annotating such\na large body of data would require a prohibitive amount of human\nhours, we train our model on automatic segmentations obtained by\nFreeSurfer (Fischl, 2012) - relabelled to obtain the aforementioned\nset of seven classes.\nWe compare the proposed method with other CNN-based solu-\ntions: the well-known 2D-patch-based U-Net (Ronneberger et al.,\n2015), its 3D variant (Ã‡iÃ§ek et al., 2016), and the state-of-the-art\narchitecture QuickNAT (Roy et al., 2019) - which leverages the ag-\ngregation of three slightly modified U-Net architectures (trained\non coronal, sagittal, and axial MRI slices, respectively). To ensure a\nfair comparison, we train these models by conducting an extensive\nhyperparameter selection process. Results are quantitatively eval-\nuated exploiting the same metrics used in the MICCAI MR Brain\nSegmentation challenge, i.e., the Dice Similarity Coefficient, the\n95th Hausdorff Distance, and the Volumetric Similarity Coefficient\n(Taha and Hanbury, 2015), utilising FreeSurfer as GT reference. In\naddition, to assess the generalisation capability of the proposed\nmodel, we compare the obtained results against the FreeSurfer seg-\nmentation we used for training. To do so, we design a survey in\nwhich five expert neuroscientists (with more than five years of ex-\nperience in MRI analysis) are asked to choose the most accurate\nsegmentation between the two aforementioned ones. This qualita-\ntive test covers different areas of interest in neuroimaging studies,\ni.e., the early visual cortex (EVC), the high-level visual areas (HVC),\nthe motor cortex (MCX), the cerebellum (CER), the hippocampus\n(HIP), the early auditory cortex (EAC), the brainstem (BST) and the\nbasal ganglia (BGA).\nAll the code necessary to train CEREBRUM and run the survey\nis available at the projectâ€™s GitHub page.\n2\n3. Material and methods\n3.1. Data\nTo speed up research and promote reproducibility, numerous\nlarge-scale neuroimaging experiments make the collected data\navailable to all researchers (Marcus et al., 2007; Van Essen et al.,\n2013; Oxtoby et al., 2019; Miller et al., 2016; Bellec et al., 2017).\nHowever, none of these studies provide manual annotations, as car-\nrying out the operation on such large databases would prove ex-\nceptionally time-consuming.\nFor this reason, most of the studies investigating the applica-\ntion of DL architectures for brain MRI segmentation make use of\nautomatically produced GT for training purposes (Roy et al., 2019;\nMcClure et al., 2018; Fedorov et al., 2017; Rajchl et al., 2018) -\nwith some of them reporting the latter can be exploited to train\nmodels that perform the same (Rajchl et al., 2018), or even better\n(Roy et al., 2019), than the automated pipeline itself. Motivated by\nthis rationale, we train and test the proposed model using both\na large collection of out-of-the-scanner MR images and the re-\nsults of the FreeSurfer (Fischl, 2012) cortical reconstruction process,\nrecon-all, as reference GT. As anticipated in Section 1, we rela-\nbel this result preserving seven among the most important classes\nof interest in most of fMRI studies (see Section 2.3 and Fig. 1).\nThe database, collected from the Centre for Cognitive Neu-\nroimaging (the University of Glasgow) in more than 10 years of\nmachine activity, consists of 947 MR images - 900 of which are\nused for training, 11 for validation, and 36 for testing. All the vol-\numes are out-of-the-scanner, i.e, obtained directly from a set of DI-\nCOM images using dcm2niix (Li et al., 2016), whose auto-crop\noption is exploited to make sizes consistent across all the dataset\n(i.e., 192 Ã— 256 Ã— 170 for sagittal, coronal, and longitudinal axis, re-\nspectively) without any other pre-processing of the data. Given the\nnumber of available scans for training, and since no registration is\nperformed, the variability in shape, rotation, position, and anatom-\nical size is such that no data augmentation is needed to avoid the\nrisk of overfitting. The first two columns of Fig. 2(a) and (b) show\ndetailed views from some selected slices of the out-of-the-scanner\nT1\nw \nand the corresponding relabelled FreeSurfer segmentation, re-\nspectively. The main characteristics of the dataset are summarised\nin Table 1. As the data have been collected under different ethics\n2 \nhttps://github.com/denbonte/CER3BRUM.\n\n4 D. Bontempi, S. Benini and A. Signoroni et al. / Medical Image Analysis 62 (2020) 101688\nFig. 2. Out-of-the-scanner (contrast enhanced) T1\nw \nscan (left), FreeSurfer segmentation (middle), and the result produced by our model (right). Fig. (a) depicts slices of test\nSubject 1, while (b) slices of test Subject 4 (sagittal, coronal, and longitudinal view, respectively). Cases of white matter over-segmentation are highlighted by yellow circles,\nwhile cases of white matter under-segmentation \nare highlighted by turquoise circles (best viewed in electronic format).\nTable 1\nDatasets details. MR Images acquired at the Centre for Cognitive Neuroimaging\n(University of Glasgow, UK).\nParameter Value\nSequence used T1\nw \nMPRAGE\nField strenght 3 Tesla\nVoxel size 1mm-isotropic\nOriginal volume sizes 192 Ã— 256 Ã— 256\nTraining volume sizes 192 Ã— 256 Ã— 170\nb\nTraining 900 volumes\nValidation 11 volumes\nTesting 36 volumes\na\na \n7 of which are publicly available.\nb \nout-of-the-scanner data, neck cropping only.\napplications, we are not able to make the whole database publicly\navailable. However, 7 out of 36 volumes used for testing are col-\nlected under the approval of the local ethics committee of the Col-\nlege of Science & Engineering (ethics #30 0170 016) and shared on-\nline after anonymisation,\n3 \nfor comparison and research purposes,\nalong with the segmentation masks resulting from CEREBRUM and\nFreeSurfer (See Fig. 2 and Section 4.2).\n3.2. Proposed model\nTo make the complexity of managing our 192 Ã— 256 Ã— 170 vox-\nels data tractable, we carefully optimise the model architecture\nso as to implicitly deal with GPU memory constraints. Further-\nmore we exploit, for training purposes, a machine equipped with\n4 GeForceÂ® GTX 1080 Ti - distributing different parts of the model\non different GPUs.\nInspired by Ronneberger et al. (2015) and Ã‡iÃ§ek et al. (2016), we\npropose a deep encoder-decoder model with six 3D convolutional\n3 \nhttps://openneuro.org/datasets/ds002207/versions/1.0.0.\nblocks, which are arranged in increasing number on three layers.\nSince a whole volume is considered as an input, the feature maps\nextracted by such convolutional blocks are not limited to patches\nbut span across the entire volume. As each block captures the con-\ntent of the whole brain MRI, this enables the learning of both local\nand global spatial features by leveraging the spatial context which\nis propagated to each subsequent block. The capability of CERE-\nBRUM to learn both local and global features is coherent with the\nlast layer units of the model having a 100x100x100 theoretical re-\nceptive field. A table reporting the complete calculation of such pa-\nrameter for each convolutional block of CEREBRUM can be found in\nthe Supplementary Material. In order to better exploit the fine de-\ntails found in 3T brain MRI data, kernels of size 3 Ã— 3 Ã— 3 are used\nas feature extractors. Instead of max-pooling, convolutions with\nstride are used as a dimensionality reduction method, thus allow-\ning the network to learn the optimal down-sampling strategy start-\ning from the extracted features. Exploiting such operations, and to\nforce the learning of more abstract (spatial) features, a factor 1: 64\ndimensionality reduction is implemented after the first layer. Fi-\nnally, skip connections are used along with tensorial sum (instead\nof concatenation, Quan et al., 2016) to improve the quality of the\nsegmented volume while greatly limiting the number of parame-\nters to âˆ¼ 5M, far less with respect to state-of-the-art models which\nare structured in a similar fashion.\nWe train the model by optimising the categorical cross-entropy\nfunction. Convergence is achieved after roughly 24 hours of train-\ning (40 epochs), using Adam (Kingma and Ba, 2014) with a learn-\ning rate of 42 Â· 10\nâˆ’5 \n, Î²\n1 \n= 0.9, and Î²\n2 \n= 0.999. Furthermore, we set\nthe batch size to 1 and thus do not implement batch normalisation\n(Ioffe and Szegedy, 2015).\n4. Results\nThe results we present in this section aim to confirm the hy-\npothesis that avoiding the partitioning of MRI data enables the\n\nD. Bontempi, S. Benini and A. Signoroni et al. / Medical Image Analysis 62 (2020) 101688 5\nCEREBRUM to learn global spatial features useful for improv-\ning segmentation. At first, in Section 4.1, we provide numerical\ncomparison with other state-of-the-art CNN architectures (U-Net,\nRonneberger et al., 2015; 3D U-Net, Ã‡iÃ§ek et al., 2016; QuickNAT,\nRoy et al., 2019). Then, in Section 4.2, we conduct a survey in-\nvolving expert neuroscientists to subjectively assess the CEREBRUM\nsegmentation accuracy. Finally, we further verify the validity of our\nassumptions by inspecting the soft-segmentation maps produced\nby the models in Section 4.3, and we demonstrate the suitability\nof our dataset by analysing the impact of the training set size on\nCEREBRUM performance in Section 4.4.\n4.1. Numerical comparison\nWe numerically assess the performance of the models, us-\ning FreeSurfer segmentation as a reference, exploiting the metrics\nutilised in the MICCAI MRBrainS18 challenge (among the most em-\nployed in the literature, Taha and Hanbury, 2015). Dice (similar-\nity) Coefficient (DC) is a measure of overlap, and a common met-\nric in segmentation tasks. The Hausdorff Distance, a dissimilarity\nmeasure, is useful to gain some insight on contours segmentation.\nSince HD is generally sensitive to outliers, a modified version (95\nth\npercentile, HD95) is generally used when dealing with medical im-\nage segmentation evaluation (Huttenlocher et al., 1993). Finally, the\nVolumetric Similarity (VS), as in Crdenes et al. (2009), evaluates\nthe similarity between two volumes.\nCEREBRUM is compared against state-of-the-art encoder-\ndecoder architectures: the well-known-2D-patch based U-Net\n(Ronneberger et al., 2015, trained on the three principal views,\ni.e., longitudinal, sagittal, and coronal), the 3D-patch based U-Net\n3D (Ã‡iÃ§ek et al., 2016 - with 3D patches sized 64 Ã— 64 Ã— 64, as in\nÃ‡iÃ§ek et al., 2016; Fedorov et al., 2017; Pawlowski et al., 2017),\nand the QuickNAT architecture (Roy et al., 2019), which imple-\nments view-aggregation starting from 2D-patch based models. We\ntrain all the models minimising the same loss for 50 epochs, us-\ning the same number of volumes, and similar learning rates (with\nchanges in those regards made to ensure the best possible valida-\ntion score). Fig. 3 shows class-wise results (DC, HD95, and VS) de-\npicting the average score (computed across all the 36 test volumes)\nand the standard deviation. We compare 2D-patch-based (longitu-\ndinal, sagittal, coronal), QuickNAT, 3D-patch-based, and CEREBRUM\n(both a max pooling and strided convolutions version). Overall,\nthe latter outperforms all the other CNN-based solutions on ev-\nery class, despite having far less parameters: when its average\nscore (computed across all the subjects) is comparable with that of\nother methods (e.g., view-aggregation, GM), it has a smaller vari-\nability (suggesting higher reliability). Moreover, we determine the\np-values for such scores computing a paired t-test using as a refer-\nence the strided-convolutions version of CEREBRUM. In Fig. 3, sta-\ntistically significant findings (p < 0.05) are highlighted with aster-\nisks, whereas the numerical results are reported in the Supplemen-\ntary Materials.\n4.2. Expertsâ€™ qualitative evaluation\nThe quantitative assessment presented in Section 4.1, though\ninformative, cannot be considered exhaustive. Indeed, using\nFreeSurfer as a reference for such evaluation makes the latter a\nranking on a relative scale - and if this highlights the value of\nthe fully-volumetric approach, it does not make a direct com-\nparison with the atlas-based method possible. Thus, we need to\nconfirm more systematically what can be inferred, for instance,\nfrom Fig. 2 - where far superior qualitative performance of CERE-\nBRUM are clear compared to FreeSurfer, as the former produces\nmore accurate segmentation masks, with far less holes and bridges.\nThis somehow surprising generalisation capability of CEREBRUM\nover its training reference, if confirmed, would prove the desired\nâ€œstrengtheningâ€ effect yielded by the adoption of a weakly super-\nvised learning approach. Moreover, quantitative assessments are\noften criticised by human experts, such as physicians and neuro-\nscientists, for they do not take into account the severity of each\nsegmentation error (Taha and Hanbury, 2015), which is of critical\nimportance in professional usage scenarios.\nFor the aforementioned reasons, we design and implement\na systematic subjective assessment by means of a PsychoPy\n(Peirce, 2007) test in which five expert neuroscientists (with more\nthan five years of expertise in MRI analysis) are asked to choose\nthe most accurate segmentation between the one produced by\nCEREBRUM and the (relabelled) FreeSurfer one. The participants\nare presented with a coronal, sagittal, or axial slice selected from a\ntest volume, and are allowed both to navigate between four neigh-\nbouring slices (two following and two preceding the displayed one)\nand to change the opacity of the segmentation mask (from 0% to\n100%) to better evaluate the latter with respect to the anatomi-\ncal data. This process is repeated seven times - one for each test\nsubject - per each of the eight brain areas of interest, i.e., early vi-\nsual cortex (EVC), the high-level visual areas (HVC), the motor cor-\ntex (MCX), the cerebellum (CER), the hippocampus (HIP), the early\nauditory cortex (EAC), the brainstem (BST), and the basal ganglia\n(BGA). The choice of the slices to present and the order in which\nthe latter are arranged is randomised. Furthermore, the neurosci-\nentists are allowed to skip as many slices as they want if they are\nunsure about the choice: such cases are reported separately. The\nsurvey interface and a run example are provided in the Supple-\nmentary Material. From the results shown in Fig. 4 it emerges that,\naccording to expert neuroscientists, CEREBRUM qualitatively out-\nperforms FreeSurfer. This proves the model superior generalisation\ncapability and provides evidence to support the adopted weakly\nsupervised approach. Moveover, such results hint at the possibil-\nity to have atlas-based methods and deep learning ones operating\ntogether in a synergistic way.\n4.3. Probability maps and entropy measures\nTo further investigate the hypothesis that a fully-volumetric ap-\nproach is advantageous with respect to other patch-based models,\nwe also conduct an assessment on the predicted probability maps\n(i.e., soft segmentation). Such evaluation could clearly reveal the\nability of the model to make use of spatial cues: for instance, a\nwell-learned model which exploits learned spatial features should\npredict the presence of cerebellum voxels only in the back of the\nbrain, where the structure is normally located.\nFig. 5(a) and (b) show two selected slices of the soft seg-\nmentation (percent probability, displayed in logarithmic scale) re-\nsulting from the best 2D-patch-based method (i.e., QuickNAT),\nthe 3D-patch-based method, and CEREBRUM - for the cerebellum\nand basal ganglia classes, respectively (superimposed to the corre-\nsponding T1\nw \nslice). Other classes are omitted for clarity.\nThe probability maps produced by the 2D and 3D-patch based\nmethods are characterised by the presence of voxels associated\nwith significant probability of belonging to the structure of inter-\nest (p > 0.2) despite their distance from the latter. This can lead to\nmisclassification errors in the hard segmentation (after the thresh-\nolding). In particular, higher uncertainty and spurious activations\ndue to views averaging can be seen in the soft segmentation maps\nproduced by QuickNAT - while blocking artefacts on the patch bor-\nders are visible in the case of the 3D-U-Net, even when the lat-\nter is trained using overlapping 3D-patches whose predictions are\nthen averaged. The soft segmentation produced by CEREBRUM, on\nthe contrary, is more coherent and closer to the reference in both\ncases, and does not present the aforementioned errors.\n\n6 D. Bontempi, S. Benini and A. Signoroni et al. / Medical Image Analysis 62 (2020) 101688\nFig. 3. Dice Coefficient, 95th percentile Hausdorff Distance, and Volumetric Similarity computed using FreeSurfer relabelled segmentation as a reference. The 2D-patch-\nbased (red, green, blue, and grey for longitudinal, sagittal, coronal, and view-aggregation, respectively), the 3D-patch-based (pink), and our model (yellow for max-pooling\nand orange for strided convolutions) are compared. The height \nof the bar indicates the mean across all the test subjects, while the error bar represents the standard\ndeviation. The asterisks below the bars highlight statistically significant results (p < 0.05), where the p-value is obtained from a paired t-test computed with respect to the\nstrided-convolutions version of \nCEREBRUM, labelled with â€œref.â€ (best viewed in electronic format).\n\nD. Bontempi, S. Benini and A. Signoroni et al. / Medical Image Analysis 62 (2020) 101688 7\nFig. 4. Outcome of the segmentation accuracy assessment test, conducted by ex-\npert neuroscientists, for the following areas: early visual cortex (EVC), the high-\nlevel visual areas (HVC), the motor cortex (MCX), the cerebellum (CER), the hip-\npocampus (HIP), the early auditory cortex (EAC), the brainstem (BST), and the basal\nganglia \n(BGA). The bars represent the number of preferences expressed by the ex-\nperts: CEREBRUM (in orange), FreeSurfer (in blue), or none of the two (in grey). (For\ninterpretation of the references to colour in this figure legend, the reader is referred\nto the web version of this article.)\nBeside such qualitative evaluations, we quantitatively compare\nthe sparseness of the predicted probability maps exploiting the av-\nerage voxel-wise entropy H\nV \n, defined as:\nH\nV \n(V, DNN ) =\nâˆ‘\nvâˆˆV \nH\nv \n(v, DNN )\n|V | \n(1)\nwhere V is an MRI volume, DNN a trained DL model, and v a single\nvoxel. Hence, |V| is the total number of voxels in the volume, and\nthe summation in Eq. (1) is computed for every voxel v in V. The\nquantity H\nv \nis the voxel-wise entropy, defined starting from the\nclassical definition in Shannon (1948):\nH\nv \n(v, DNN ) = âˆ’ \nâˆ‘\ncâˆˆC\nP\n(\nDNN(v ) âˆˆ c \n) \nln \n(\nP\n(\nDNN(v ) âˆˆ c \n)\n) \n(2)\nwhere C is the set of the segmented classes (in our case C =\n{\nGM, GANGL, . . . , BRNSTEM\n}\n), P\n(\nDNN(v ) âˆˆ c \n) \nis the probability the\nmodel assigns to the event â€œvoxel v belongs to the class câ€, and\nln \n(\nP\n(\nDNN(v ) âˆˆ c \n)\n) \nis the natural logarithm of such quantity. We\nreport the results of such test in Fig. 6(a), â€œnormalisingâ€ the\nquantity H\nV \nby the highest average voxel-wise entropy achiev-\nable H \nMAX\nV \n, i.e., H\nV \ncomputed for a voxel for which every class is\npredicted as equiprobable - so that H\nV \n/H \nMAX\nV \nâˆˆ [0, 1] for ease of\ninterpretation.\nIf entropy evaluates the sparseness of the predicted probabil-\nity maps in general, cross-entropy is able to assess the uncertainty\nof a model, once the correct predictions are known. The average\nvoxel-wise cross-entropy CH\nV \nbuilds upon the idea of the voxel-\nwise entropy CH\nv \n, defined as:\nCH\nv \n(v, GT, DNN ) = âˆ’ \nâˆ‘\ncâˆˆC\nP\n(\nGT(v ) âˆˆ c \n) \nln \n(\nP\n(\nDNN(v ) âˆˆ c \n)\n) \n(3)\nwhere GT is the ground-truth reference provided by FreeSurfer, C\nis the set of the segmented classes, P\n(\nGT(v ) âˆˆ c \n) \nis the probability\nrelated to the ground-truth event â€œvoxel v belongs to the class câ€\n(i.e., \nâ€œ1â€ for the correct class, and â€œ0â€ otherwise, since FreeSurfer\ndoes not provide class probabilities), and ln \n(\nP\n(\nDNN(v ) âˆˆ c \n)\n) \nis\nthe natural logarithm of the probability the DNN model assigns\nto the event â€œvoxel v belongs to the class câ€. We report the re-\nsults of such test in Fig. 6(b), â€œnormalisingâ€ the quantity CH\nV \nby\nthe highest average voxel-wise entropy achievable CH \nMAX\nV \n, i.e., the\ncross-entropy computed by using a random classifier - so that\nCH\nV \n/CH \nMAX\nV \nâˆˆ [0, 1].\nBoth qualitative examples illustrated in Fig. 5, and quantitative\nevaluations presented in Fig. 6, hint at the superior ability of the\nproposed model in learning both global and local spatial features.\nAdditional qualitative examples of probability maps, as well as the\ntables reporting the p-values for the tests depicted in Fig. 6, are\nprovided in the Supplementary Material.\n4.4. Number of training samples\nOne of the possible limitations of approaching the brain MRI\nsegmentation task in a fully-volumetric fashion could be the\nscarcity of training data - for in such a case each volume does\nnot yield many training samples, as for 2D and 3D-patch-based\nsolutions, but a single one. To investigate this possible draw-\nback, we evaluate the performance of CEREBRUM when trained\non smaller sub-sets of our database. In particular, we train the\nproposed model by randomly extracting 25, 50, 100, 250, 500,\n70 0, 90 0 samples from the training set. To evaluate the perfor-\nmance of the model in the first two cases (i.e., 25 and 50 MRI\nscans), we repeat the training 5 times (on randomly extracted\nyet non-overlapping subsets of the database) and average the re-\nsults. Furthermore, we evaluate the impact on the performance\nyielded by the introduction of strided convolutions (i.e., more\nlearnable parameters) when the training set size is limited by\ntraining a variation of CEREBRUM where max-pooling is used as\na dimensionality-reduction strategy. Fig. 7 shows that the perfor-\nmance variation significantly deteriorates as the training set size\nfalls below 250 samples, while substantial stability is reached over\n750 samples. This confirms that our 900 samples training set is\nproperly sized for the task, without there being any urge for data\naugmentation.\nFig. 5. Soft segmentation maps of test subject 1 cerebellum (a) and the basal ganglia (b) produced by the best 2D-patch-based model (QuickNAT), the 3D-patch-based model\n(3D U-Net), and CEREBRUM (ours). The proposed approach produces results that are spatially more coherent, and lack of false positives (highlighted in light blue). Base-10\nlogarithmic scale of percent probability is used for visualisation purposes (best viewed in electronic format).\n\n8 D. Bontempi, S. Benini and A. Signoroni et al. / Medical Image Analysis 62 (2020) 101688\nFig. 6. Normalised average voxel-wise entropy H\nV \n/H \nMAX\nV \n(a), and normalised average voxel-wise cross-entropy CH\nV \n/CH \nMAX\nV \n(b). The probability maps resulting from 2D-patch-\nbased (red, green, blue, and grey for longitudinal, sagittal, coronal, and view-aggregation, respectively), the 3D-patch-based (pink), and our models (yellow for max-pooling\nand orange for strided convolutions) are compared. The height of the bar indicates the mean across all the test subjects, while \nthe error bar represents the standard\ndeviation. The asterisks below the bars highlight statistically significant results (p < 0.05), where the p-value is obtained from a paired t-test computed with respect to the\nstrided-convolutions version of CEREBRUM, labelled with â€œref.â€ (best viewed in electronic format).\nFig. 7. Impact of the training set size on the performance - Dice Coefficient aver-\naged across all the seven classes. Results are computed on the whole test set (36\nvolumes).\n5. Conclusion\nIn this work we presented CEREBRUM, a CNN-based deep\nmodel that approaches the brain MRI segmentation problem in a\nfully-volumetric fashion. The proposed architecture is a carefully\n(architecturally) optimised encoder-decoder that, starting from a\nT1\nw \nMRI volume, produces a result in only few seconds on a desk-\ntop GPU. We evaluated the proposed model performance, compar-\ning it to state-of-the-art 2D and 3D-patch-based models with sim-\nilar structure, exploiting the Dice Coefficient, the 95th percentile\nHausdorff Distance, and the Volumetric Similarity, assessing CERE-\nBRUM superior performance. Furthermore, we conducted a survey\nof expert neuroscientists to obtain their judgements about the ac-\ncuracy of the resulting segmentation, comparing the latter with\nthe result of FreeSurfer cortical reconstruction process. According\nto the participants to such experiment, CEREBRUM achieves bet-\nter segmentation than FreeSurfer. To our knowledge, this is the\nfirst time a DL-based fully-volumetric approach for brain MRI seg-\nmentation is deployed. The results we obtained prove the poten-\ntial of this approach, as CEREBRUM outperforms 2D and 3D-patch-\nbased encoder-decoder models using far less parameters. Remov-\ning the partitioning of the volume, as hypothesised, allows the\nmodel to learn both local and spatial features. Furthermore, we\nare also the first conducting a qualitative assessment test consult-\ning expert neuroscientists: this is fundamental, as commonly used\nmetrics often fail to capture the information experts need to rely\non DL methods and exploit the latter for research.\nDeclaration of Competing Interest\nNone.\nCRediT authorship contribution statement\nDennis Bontempi: Conceptualization, Methodology, Software,\nFormal analysis, Investigation, Data curation, Writing - original\ndraft, Writing - review & editing, Visualization. Sergio Benini: Con-\nceptualization, Writing - original draft, Writing - review & edit-\ning, Supervision, Project administration. Alberto Signoroni: Writ-\ning - review & editing, Supervision, Funding acquisition. Michele\nSvanera: Conceptualization, Methodology, Validation, Formal anal-\nysis, Investigation, Data curation, Supervision, Project administra-\ntion. Lars Muckli: Resources, Funding acquisition.\nAcknowledgments\nThis project has received funding from the European Unions\nHorizon 2020 Programme for Research and Innovation under the\nSpecific Grant Agreement No. 785907 (Human Brain Project SGA2)\nawarded to LM.\nSupplementary material\nSupplementary material associated with this article can be\nfound, in the online version, at doi:10.1016/j.media.2020.101688.\nReferences\nAkkus, Z., Galimzianova, A., Hoogi, A., Rubin, D.L., Erickson, B.J., 2017. Deep learn-\ning for brain MRI segmentation: state of the art and future directions. J. Digit\nImaging 30 (4), 449â€“459. doi:10.1007/s10278- 017- 9983- 4.\n\nD. Bontempi, S. Benini and A. Signoroni et al. / Medical Image Analysis 62 (2020) 101688 9\nAndermatt, S., Pezold, S., Cattin, P., 2016. Multi-dimensional gated recurrent units\nfor the segmentation of biomedical 3d-data. In: Carneiro, G., Mateus, D., Pe-\nter, L., Bradley, A., Tavares, J.M.R.S., Belagiannis, V., Papa, J.P., Nascimento, J.C.,\nLoog, M., Lu, Z., Cardoso, J.S., Cornebise, J. (Eds.), Deep Learning and Data Label-\ning for \nMedical Applications. Springer International Publishing, Cham, pp. 142â€“\n151. doi:10.1007/978- 3- 319- 46976- 8_15.\nBellec, P., Chu, C., Chouinard-Decorte, F., Benhajali, Y., Margulies, D.S., Craddock, R.C.,\n2017. The neuro bureau ADHD-200 preprocessed repository. Neuroimage 144,\n275â€“286. doi:10.1016/j.neuroimage.2016.06.034.\nCabezas, M., Oliver, A., LladÃ³, X., Freixenet, J., \nBach Cuadra, M., 2011. A review of\natlas-based segmentation for magnetic resonance brain images. Comput. Meth-\nods Programs Biomed. 104 (3), e158â€“e177. doi:10.1016/j.cmpb.2011.07.015.\nChen, H., Dou, Q., Yu, L., Qin, J., Heng, P.-A., 2018. Voxresnet: deep voxelwise residual\nnetworks for brain segmentation from 3d mr images. Neuroimage 170, \n446â€“455.\ndoi:10.1016/j.neuroimage.2017.04.041.\nÃ‡iÃ§ek, Ã–., Abdulkadir, A., Lienkamp, S.S., Brox, T., Ronneberger, O., 2016. 3D\nU-Net: Learning dense volumetric segmentation from sparse annotation. In:\nOurselin, S., Joskowicz, L., Sabuncu, M.R., Unal, G., Wells, W. (Eds.), Medical Im-\nage Computing and Computer-Assisted Interventionâ€“MICCAI 2016. Springer In-\nternational Publishing, Cham, pp. 424â€“432.\nCrdenes, R., de Luis-Garca, R., Bach-Cuadra, M., 2009. A multidimensional segmen-\ntation evaluation for medical image data. Comput. Methods Programs Biomed.\n96 (2), 108â€“124. doi:10.1016/j.cmpb.20 09.04.0 09.\nDeniz, C.M., Xiang, S., Hallyburton, R.S., Welbeck, A., Babb, J.S., Honig, S., Cho, K.,\nChang, \nG., 2018. Segmentation of the proximal femur from mr images us-\ning deep convolutional neural networks. Sci. Rep. 8 (1), 16485. doi:10.1038/\ns41598- 018- 34817- 6.\nDespotovi \nÂ´\nc, I., Goossens, B., Philips, W., 2015. MRI Segmentation of the human\nbrain: challenges, methods, and applications. Comput. Math. Methods Med.\n2015, 450341(1â€“23). doi:10.1155/2015/450341.\nDolz, J., Gopinath, K., Yuan, J., Lombaert, H., Desrosiers, C., Ben Ayed, I., 2019.\nHyperDense-Net: a hyper-densely connected CNN for multi-modal image seg-\nmentation. IEEE Trans. Med. Imaging 38 (5), 1116â€“1126. doi:10.1109/TMI.2018.\n2878669.\nEllingsen, L.M., Roy, S., Carass, A., Blitz, A.M., Pham, D.L., Prince, \nJ.L., 2016. Segmen-\ntation and labeling of the ventricular system in normal pressure hydrocephalus\nusing patch-based tissue classification and multi-atlas labeling. In: Styner, M.A.,\nAngelini, E.D. (Eds.), Medical Imaging 2016: Image Processing. SPIE, pp. 116â€“122.\ndoi:10.1117/12.2216511. International Society for Optics and Photonics.\nEsteban, O., Markiewicz, C.J., Blair, R.W., \nMoodie, C.A., Isik, A.I., Erramuzpe, A.,\nKent, J.D., Goncalves, M., DuPre, E., Snyder, M., Oya, H., Ghosh, S.S., Wright, J.,\nDurnez, J., Poldrack, R.A., Gorgolewski, K.J., 2019. fMRIPrep: a robust prepro-\ncessing pipeline for functional MRI. Nat. Methods 16 (1), 111â€“116. doi:10.1038/\ns41592- 018- 0235- 4.\nFedorov, A., Johnson, \nJ., Damaraju, E., Ozerin, A., Calhoun, V., Plis, S., 2017. End-\nto-end learning of brain tissue segmentation from imperfect labeling. In: 2017\nInternational Joint Conference on Neural Networks (IJCNN), pp. 3785â€“3792.\ndoi:10.1109/IJCNN.2017.7966333.\nFischl, B., 2012. FreeSurfer. Neuroimage 62 (2), 774â€“781. doi:10.1016/j.neuroimage.\n2012.01.021.\nFreeSurfer, 2008. Recon-all run \ntimes. https://surfer.nmr.mgh.harvard.edu/fswiki/\nReconAllRunTimes. [Online; accessed 11-September-2019].\nHamidinekoo, A., Denton, E., Rampun, A., Honnor, K., Zwiggelaar, R., 2018. Deep\nlearning in mammography and breast histology, an overview and future trends.\nMed. Image Anal. 47, 45â€“67. doi:10.1016/j.media.2018.03.006.\nHuttenlocher, D.P., Klanderman, G.A., Rucklidge, W.J., 1993. Comparing images using\nthe hausdorff distance. IEEE \nTrans. Pattern Anal. Mach. Intell. 15 (9), 850â€“863.\ndoi:10.1109/34.232073.\nIoffe, S., Szegedy, C., 2015. Batch normalization: accelerating deep network training\nby reducing internal covariate shift. arXiv:1502.03167.\nKingma, D. P., Ba, J., 2014. Adam: a method for stochastic optimization. arXiv\npreprint arXiv:1412.6980.\nKlein, A., Ghosh, \nS.S., Stavsky, E., Lee, N., Rossa, B., Reuter, M., Neto, E.C., Kesha-\nvan, A., 2017. Mindboggling morphometry of human brains. PLoS Comput. Biol.\n13 (2), e1005350. doi:10.1371/journal.pcbi.1005350.\nLerch, J.P., van der Kouwe, A.J.W., Raznahan, A., Paus, T., Johansen-Berg, H.,\nMiller, K.L., Smith, S.M., Fischl, B., Sotiropoulos, S.N., 2017. Studying \nneu-\nroanatomy using MRI. Nat. Neurosci. 20 (3), 314â€“326. doi:10.1038/nn.4501.\nLi, W., Wang, G., Fidon, L., Ourselin, S., Cardoso, M.J., Vercauteren, T., 2017. On\nthe compactness, efficiency, and representation of 3d convolutional networks:\nBrain parcellation as a pretext task. In: Niethammer, M., \nStyner, M., Aylward, S.,\nZhu, H., Oguz, I., Yap, P.-T., Shen, D. (Eds.), Information Processing in Medical\nImaging. Springer International Publishing, Cham, pp. 348â€“360.\nLi, X., Morgan, P.S., Ashburner, J., Smith, J., Rorden, C., 2016. The first step for neu-\nroimaging data analysis: DICOM to NIfti conversion. J. Neurosci. Methods \n264,\n47â€“56. doi:10.1016/j.jneumeth.2016.03.001.\nLi, Y., Guo, L., Zhou, Z., 2019. Towards safe weakly supervised learning. IEEE Trans.\nPattern Anal. Mach. Intell. 1. Early Access.\nLitjens, G., Kooi, T., Bejnordi, B.E., Setio, A.A.A., Ciompi, F., Ghafoorian, M., van der\nLaak, J.A., van \nGinneken, B., Snchez, C.I., 2017. A survey on deep learning in\nmedical image analysis. Med. Image Anal. 42, 60â€“88. doi:10.1016/j.media.2017.\n07.005.\nMarcus, D.S., Wang, T.H., Parker, J., Csernansky, J.G., Morris, J.C., Buckner, R.L., 2007.\nOpen access series of imaging studies (OASIS): cross-sectional mri data in\nyoung, middle aged, nondemented, and demented older adults. J. Cogn. Neu-\nrosci. 19 (9), 1498â€“1507. doi:10.1162/jocn.2007.19.9.1498.\nMcClure, P., Rho, N., Lee, J. A., Kaczmarzyk, J. R., Zheng, C., Ghosh, S. S., Nielson, D.,\nThomas, A., Bandettini, P., Pereira, F., 2018. Knowing What You Know in Brain\nSegmentation Using Deep \nNeural Networks. arXiv:1812.01719 [cs, stat].\nMendrik, A.M., Vincken, K.L., Kuijf, H.J., Breeuwer, M., Bouvy, W.H., de Bresser, J.,\nAlansary, A., de Bruijne, M., Carass, A., El-Baz, A., Jog, A., Katyal, R., Khan, A.R.,\nvan der Lijn, F., Mahmood, Q., Mukherjee, R., van Opbroek, A., Paneri, S.,\nPereira, S., Persson, M., Rajchl, M., Sarikaya, D., Smedby, O., Silva, C.A.,\nVrooman, H.A., Vyas, S., Wang, C., Zhao, L., Biessels, G.J., Viergever, M.A., 2015.\nMRBrainS challenge: online evaluation framework for brain image segmentation\nin 3T MRI scans. Comput. Intell. Neurosci. 2015, 1â€“16. doi:10.1155/2015/813696.\nMiller, K.L., Alfaro-Almagro, F., Bangerter, \nN.K., Thomas, D.L., Yacoub, E., Xu, J.,\nBartsch, A.J., Jbabdi, S., Sotiropoulos, S.N., Andersson, J.L.R., Griffanti, L.,\nDouaud, G., Okell, T.W., Weale, P., Dragonu, I., Garratt, S., Hudson, S., Collins, R.,\nJenkinson, M., Matthews, P.M., Smith, S.M., 2016. Multimodal population brain\nimaging in the UK biobank prospective epidemiological study. Nat. \nNeurosci. 19.\ndoi:10.1038/nn.4393. 1523 EP â€“\nMilletari, F., Navab, N., Ahmadi, S., 2016. V-Net: Fully convolutional neural networks\nfor volumetric medical image segmentation. In: 2016 Fourth International Con-\nference on 3D Vision (3DV), pp. 565â€“571. doi:10.1109/3DV.2016.79.\nOxtoby, N. P., Ferreira, F. S., Mihalik, A., Wu, T., Brudfors, \nM., Lin, H., Rau, A., Blum-\nberg, S. B., Robu, M., Zor, C., et al., 2019. ABCD Neurocognitive Prediction Chal-\nlenge 2019: Predicting Individual Residual Fluid Intelligence Scores from Cortical\nGrey Matter Morphology. arXiv:1905.10834.\nPatenaude, B., Smith, S.M., Kennedy, D.N., Jenkinson, M., 2011. A Bayesian model of\nshape and \nappearance for subcortical brain segmentation. Neuroimage 56 (3),\n907â€“922. doi:10.1016/j.neuroimage.2011.02.046.\nPawlowski, N., Ktena, S.I., Lee, M.C.H., Kainz, B., Rueckert, D., Glocker, B., Rajchl, M.,\n2017. DLTK: state of the art reference implementations for deep learning on\nmedical images. ariXiv:1711.06853.\nPeirce, J.W., 2007. Psychopy psychophysics software \nin python. J. Neurosci. Methods\n162 (1â€“2), 8â€“13.\nQuan, T. M., Hildebrand, D. G., Jeong, W.-K., 2016. FusionNet: a deep fully resid-\nual convolutional neural network for image segmentation in connectomics.\narXiv:1612.05360.\nRajchl, M., Pawlowski, N., Rueckert, D., Matthews, P. M., Glocker, B., 2018. NeuroNet:\nFast and Robust \nReproduction of Multiple Brain Image Segmentation Pipelines.\narXiv:1806.04224.\nRonneberger, O., Fischer, P., Brox, T., 2015. U-Net: Convolutional networks for\nbiomedical image segmentation. In: Navab, N., Hornegger, J., Wells, W.M.,\nFrangi, A.F. (Eds.), Medical Image Computing and Computer-Assisted Interven-\ntionâ€“MICCAI 2015. Springer International Publishing, Cham, pp. 234â€“241\n.\nRoy, A.G., Conjeti, S., Navab, N., Wachinger, C., ADNI, 2019. QuickNAT: a fully con-\nvolutional network for quick and accurate segmentation of neuroanatomy. Neu-\nroimage 186, 713â€“727.\nSavioli, N., Montana, G., Lamata, P., 2019. V-FCNN: Volumetric fully convolution\nneural network for \nautomatic atrial segmentation. In: Pop, M., Sermesant, M.,\nZhao, J., Li, S., McLeod, K., Young, A., Rhode, K., Mansi, T. (Eds.), Statistical At-\nlases and Computational Models of the Heart. Atrial Segmentation and LV Quan-\ntification Challenges. Springer International Publishing, Cham, pp. 273â€“281.\nShannon, C.E., 1948. A mathematical theory \nof communication. Bell Syst. Tech. J. 27\n(3), 379â€“423. doi:10.1002/j.1538-7305.1948.tb01338.x.\nTaha, A.A., Hanbury, A., 2015. Metrics for evaluating 3D medical image segmenta-\ntion: analysis, selection, and tool. BMC Med. Imaging 15 (1), 29.\nVan Essen, D.C., Smith, S.M., Barch, D.M., Behrens, T.E., Yacoub, \nE., Ugurbil, K., 2013.\nThe WU-minn human connectome project: an overview. Neuroimage 80, 62â€“79.\ndoi:10.1016/j.neuroimage.2013.05.041.\nVoulodimos, A., Doulamis, N., Doulamis, A., Protopapadakis, E., 2018. Deep learning\nfor computer vision: a brief review. Comput. Intell. Neurosci. 2018, 1â€“13. doi:10.\n1155/2018/7068349.\nWachinger, C., Reuter, M., Klein, \nT., 2018. DeepNAT: Deep Convolutional Neural Net-\nwork for segmenting neuroanatomy. Neuroimage 170, 434â€“445. doi:10.1016/j.\nneuroimage.2017.02.035.\nWeier, K., Beck, A., Magon, S., Amann, M., Naegelin, Y., Penner, I.K., ThÃ¼rling, M.,\nAurich, V., Derfuss, T., Radue, E.-W., Stippich, C., Kappos, L., Timmann, D.,\nSprenger, T., 2012. Evaluation of a new \napproach for semi-automatic segmen-\ntation of the cerebellum in patients with multiple sclerosis. J. Neurol. 259 (12),\n2673â€“2680. doi:10.10 07/s0 0415- 012- 6569- 4.\nWenger, E., MÃ¥rtensson, J., Noack, H., Bodammer, N.C., KÃ¼hn, S., Schaefer, S.,\nHeinze, H.-J., DÃ¼zel, E., BÃ¤ckman, L., Lindenberger, U., LÃ¶vdÃ©n, M., 2014. Com-\nparing manual and automatic segmentation of hippocampal volumes: reliabil-\nity and validity issues in younger and older brains: comparing manual and\nautomatic segmentation of hc volumes. Hum. Brain Mapp. 35 (8), 4236â€“4248.\ndoi:10.1002/hbm.22473.\nZhan, M., Goebel, R., de Gelder, B., 2018. Ventral and dorsal pathways relate differ-\nently to \nvisual awareness of body postures under continuous flash suppression.\neNeuro 5 (1). doi:10.1523/ENEURO.0285-17.2017.\nZhang, Y., Brady, M., Smith, S., 2001. Segmentation of brain MR images through\na hidden Markov random field model and the expectation-maximization algo-\nrithm. IEEE Trans. Med. Imaging 20 (1), 45â€“57. doi:10.1109/42.906424.\nZhou, \nZ.-H., 2017. A brief introduction to weakly supervised learning. Natl. Sci. Rev.\n5 (1), 44â€“53.",
    "version": "5.3.31"
  },
  {
    "numpages": 13,
    "numrender": 13,
    "info": {
      "PDFFormatVersion": "1.7",
      "Language": "en-US",
      "EncryptFilterName": null,
      "IsLinearized": true,
      "IsAcroFormPresent": false,
      "IsXFAPresent": false,
      "IsCollectionPresent": false,
      "IsSignaturesPresent": false,
      "Creator": "Elsevier",
      "Custom": {
        "CrossMarkDomains[1]": "elsevier.com",
        "CrossmarkMajorVersionDate": "2010-04-23",
        "CreationDate--Text": "9th October 2025",
        "ElsevierWebPDFSpecifications": "7.0.1",
        "robots": "noindex",
        "doi": "10.1016/j.media.2025.103806",
        "CrossMarkDomains[2]": "sciencedirect.com",
        "CrossmarkDomainExclusive": "true"
      },
      "ModDate": "D:20251010021107Z",
      "Author": "Fangmao Ju",
      "Title": "Model-unrolled fast MRI with weakly supervised lesion enhancement",
      "Keywords": "Fast MRI,Lesion-focused,Task-oriented imaging,Weakly supervised enhancement,Model-based deep learning",
      "CreationDate": "D:20251009174027Z",
      "Producer": "Acrobat Distiller 8.1.0 (Windows)",
      "Subject": "Medical Image Analysis, 107 (2026) 103806. doi:10.1016/j.media.2025.103806"
    },
    "metadata": {
      "crossmark:crossmarkdomains": "elsevier.comsciencedirect.com",
      "crossmark:crossmarkdomainexclusive": "true",
      "crossmark:doi": "10.1016/j.media.2025.103806",
      "crossmark:majorversiondate": "2010-04-23",
      "dc:format": "application/pdf",
      "dc:identifier": "10.1016/j.media.2025.103806",
      "dc:publisher": "Elsevier B.V.",
      "dc:description": "Medical Image Analysis, 107 (2026) 103806. doi:10.1016/j.media.2025.103806",
      "dc:subject": [
        "Fast MRI",
        "Lesion-focused",
        "Task-oriented imaging",
        "Weakly supervised enhancement",
        "Model-based deep learning"
      ],
      "dc:title": "Model-unrolled fast MRI with weakly supervised lesion enhancement",
      "dc:creator": [
        "Fangmao Ju",
        "Yuzhu He",
        "Fan Wang",
        "Xianjun Li",
        "Chen Niu",
        "Chunfeng Lian",
        "Jianhua Ma"
      ],
      "jav:journal_article_version": "VoR",
      "pdf:creationdate--text": "9th October 2025",
      "pdf:producer": "Acrobat Distiller 8.1.0 (Windows)",
      "pdf:keywords": "Fast MRI,Lesion-focused,Task-oriented imaging,Weakly supervised enhancement,Model-based deep learning",
      "pdfx:creationdate--text": "9th October 2025",
      "pdfx:crossmarkdomains": "sciencedirect.comelsevier.com",
      "pdfx:crossmarkdomainexclusive": "true",
      "pdfx:crossmarkmajorversiondate": "2010-04-23",
      "tprdsndn8npeqn9qgotf7osnnmgqplt-jyt-gowj_yteoot_-zd6po9eqn9iknm-rm9etma": "",
      "pdfx:doi": "10.1016/j.media.2025.103806",
      "pdfx:robots": "noindex",
      "prism:aggregationtype": "journal",
      "prism:copyright": "Â© 2025 Elsevier B.V. All rights are reserved, including those for text and data mining, AI training, and similar technologies.",
      "prism:coverdate": "2026-01-01",
      "prism:coverdisplaydate": "1 January 2026",
      "prism:doi": "10.1016/j.media.2025.103806",
      "prism:issn": "1361-8415",
      "prism:number": "PA",
      "prism:pagerange": "103806",
      "prism:publicationname": "Medical Image Analysis",
      "prism:startingpage": "103806",
      "prism:url": "https://doi.org/10.1016/j.media.2025.103806",
      "prism:volume": "107",
      "tdm:policy": "https://www.elsevier.com/tdm/tdmrep-policy.json",
      "tdm:reservation": "1",
      "xmp:createdate": "2025-10-09T17:40:27",
      "xmp:creatortool": "Elsevier",
      "xmp:metadatadate": "2025-10-10T02:11:07",
      "xmp:modifydate": "2025-10-10T02:11:07",
      "xmpmm:documentid": "uuid:1b499bed-4ce8-4c0c-b682-0d58baae1cbe",
      "xmpmm:instanceid": "uuid:b6ed5266-03cb-4855-90c0-636283357f42",
      "xmprights:marked": "True"
    },
    "text": "Contents lists available at ScienceDirect\nMedical Image Analysis\njournal homepage: www.elsevier.com/locate/media\nModel-unrolled fast MRI with weakly supervised lesion enhancement\nFangmao Ju \na,1\n, Yuzhu He \na,1\n, Fan Wang \nb,d\n, Xianjun Li \nd \n, Chen Niu \nd\n, Chunfeng Lian \na,c ,\nâˆ—\n,\nJianhua Ma \nb,c ,\nâˆ—âˆ—\na \nSchool of Mathematics and Statistics, Xiâ€™an Jiaotong University, Xiâ€™an, China\nb \nKey Laboratory of Biomedical Information Engineering of Ministry of Education, School of Life Science and Technology, Xiâ€™an Jiaotong University, Xiâ€™an, China\nc \nResearch Center for Intelligent Medical Equipment and Devices (IMED), Xiâ€™an Jiaotong University, Xiâ€™an 710049, China\nd \nDepartment of Radiology, The First Affiliated Hospital of Xiâ€™an Jiaotong University, Xiâ€™an, China\nA R T I C L E I N F O\nKeywords:\nFast MRI\nLesion-focused\nTask-oriented imaging\nWeakly supervised enhancement\nModel-based deep learning\nA B S T R A C T\nThe utility of Magnetic Resonance Imaging (MRI) in anomaly detection and disease diagnosis is well\nrecognized. However, the current imaging protocol is often hindered by long scanning durations and a\nmisalignment between the scanning process and the specific requirements of subsequent clinical assessments.\nWhile recent studies have actively explored accelerated MRI techniques, the majority have concentrated on\nimproving overall image quality across all voxel locations, overlooking the attention to specific abnormalities\nthat hold clinical significance. To address this discrepancy, we propose a model-unrolled deep-learning method,\nguided by weakly supervised lesion attention, for accelerated MRI oriented by downstream clinical needs. In\nparticular, we construct a lesion-focused MRI reconstruction model, which incorporates customized learnable\nregularizations that can be learned efficiently by using only image-level labels to improve potential lesion\nreconstruction but preserve overall image quality. We then design a dedicated iterative algorithm to solve this\ntask-driven reconstruction model, which is further unfolded as a cascaded deep network for lesion-focused\nfast imaging. Comprehensive experiments on two public datasets, i.e., fastMRI and Stanford Knee MRI Multi-\nTask Evaluation (SKM-TEA), demonstrate that our approach, referred to as Lesion-Focused MRI (LF-MRI),\nsurpassed existing accelerated MRI methods by relatively large margins. Remarkably, LF-MRI led to substantial\nimprovements in areas showing pathology. The source code and pretrained models will be publicly available\nat https://github.com/ladderlab-xjtu/LF-MRI.\n1. Introduction\nMagnetic Resonance Imaging (MRI) plays an indispensable role\nin diagnosing and monitoring diseases by providing high-resolution\nimages of soft tissues. Despite its significance, the current MRI pro-\ntocol faces two primary challenges in clinical practice. First, the long\ntime needed to capture fully sampled images not only causes patient\ndiscomfort but also heightens the risk of motion artifacts, subsequently\ndiminishing the quality of the reconstructed images. Second, there exists\na disconnection between the scanning process and specific clinical\nrequirements. Typically, the general imaging process tends to optimize\non-average image quality across all positional locations (or voxels),\nwhich does not necessarily bring high-fidelity reconstruction of region-\nalized lesions that are of great interest to radiologists and directly\nlinked to clinical decisions.\nâˆ— \nCorresponding author at: School of Mathematics and Statistics, Xiâ€™an Jiaotong University, Xiâ€™an, China.\nâˆ—âˆ— \nCorresponding author at: Key Laboratory of Biomedical Information Engineering of Ministry of Education, School of Life Science and Technology, Xiâ€™an\nJiaotong University, Xiâ€™an, China.\nE-mail addresses: chunfeng.lian@xjtu.edu.cn (C. Lian), jhma@xjtu.edu.cn (J. Ma).\n1 \nThe two authors contribute equally to this work.\nTo accelerate MRI, varying studies have attempted to reconstruct\nhigh-fidelity images from undersampled ğ‘˜-space data. These approaches\nfall into three main categories, i.e., traditional model-based (Donoho,\n2006; Lustig et al., 2007; Pruessmann et al., 1999; Griswold et al.,\n2002), learning-based (Mardani et al., 2017; Lee et al., 2018; Quan\net al., 2018), and model-driven learning-based methods (Hammernik\net al., 2018; Schlemper et al., 2017). Traditional model-based strategies\ndevise reconstruction algorithms that combine a data-consistency term\nwith manually designed regularizations for MRI reconstruction (Lustig\net al., 2007). However, limited prior knowledge regarding the inverse\nimaging problem as well as the difficulty of precisely encoding them\nwith manually designed regularizations often results in a challenge\nto achieve high-quality outcomes. In contrast, learning-based methods\nadopt a purely data-driven strategy to deduce non-linear mappings\nhttps://doi.org/10.1016/j.media.2025.103806\nReceived 18 July 2024; Received in revised form 10 April 2025; Accepted 7 September 2025\nMedical Image Analysis 107 (2026) 103806\nAvailable online 15 September 2025\n1361-8415/Â© 2025 Elsevier B.V. All rights are reserved, including those for text and data mining, AI training, and similar technologies.\n\nF. Ju et al.\nfrom undersampled ğ‘˜-space data to fully sampled images (Hyun et al.,\n2018). Although these methods can have promising performance when\ntrained with extensive datasets, the prerequisite of substantial training\ndata is generally difficult to meet in real-world settings. Also, the\nblack-box nature of the resulting deep networks makes it difficult\nto understand how they function. Recently, there has been a surge\nin interest in hybrid model-driven learning-based techniques. These\nmethods intuitively combine the benefits of interpretable model designs\nwith the strength of non-linear data fitting. They typically unroll the\niterative optimization process of a reconstruction model into a sequence\nof cascaded networks, where regularizations and tuning parameters\nare learned empirically in a data-driven fashion (Sun et al., 2016;\nZhang and Ghanem, 2018; Aggarwal et al., 2018). This approach\nensures that the unrolled deep networks preserve the power of deep\nrepresentation learning, while the foundational iterative optimization\ntechnique aids in controlling their hypothesis space, thereby enhancing\nboth computational interpretability and reconstruction quality.\nDespite the extensive research efforts currently being devoted to\naccelerating MRI, the majority of them focus on enhancing the overall\nimage quality on a per-voxel basis, neglecting task-oriented attention\nto the specific abnormalities that are of primary interest to clini-\ncians. Recently, a few studies have tried to perform concurrent image\nreconstruction and processing/analysis, e.g., in a multi-task learning\nframework, demonstrating that downstream processing tasks could\nbring additional improvements to imaging (Razumov et al., 2022a;\nWeber et al., 2024; Wu et al., 2023; CalivÃ¡ et al., 2020). However, these\napproaches often rely on the availability of fully annotated training\nsamples (e.g., lesion masks or bounding boxes) for multi-task learning,\nwhich are challenging to acquire, particularly in the context of medical\nimaging. This hampers their applications in clinical practice.\nBuilding upon the foundational theoretical advancements in signal\ndetection and image analysis established by prior studies (Li et al.,\n2021, 2023, 2024), this paper proposes a model-based deep-learning\nmethod, guided by weakly supervised abnormality localization (Wu\net al., 2019; Lian et al., 2021), for accelerated MRI oriented by down-\nstream clinical needs. Unlike existing studies, our initiative aims not\nonly to reconstruct overall high-fidelity images but also to particularly\nenhance the imaging of areas that may present abnormalities. To\nthis end, we build a task-oriented MRI reconstruction model, which\ndistinctively integrates implicit denoising regularizations that can be\nlearned efficiently by using only image-level labels to optimize the\nquality of reconstructed images with a focus on lesions. We design a\ndedicated iterative algorithm to solve this task-driven model, which\nis further unfolded as cascaded deep networks for lesion-focused fast\nimaging. We conducted comprehensive experiments on the widely\nrecognized fastMRI and SKM-TEA datasets. Both quantitative and qual-\nitative evaluation results demonstrate that our method (referred to\nas Lesion-Focused MRI, i.e., LF-MRI) outperformed existing MRI re-\nconstruction techniques under varying acceleration factors. Notably,\nsignificant enhancements were observed within pathological regions.\nOverall, the main contributions of our work are three-fold:\n1. We propose a task-oriented, model-driven fast MRI method capa-\nble of optimizing the imaging of lesions that are directly linked\nto downstream clinical investigations, without sacrificing the\naverage image quality;\n2. By integrating weakly supervised localization into the learn-\nable denoising regularization of the reconstruction model, our\nmethod requires only image-level labels to achieve lesion en-\nhanced fast MRI. To the best of our knowledge, this is the first at-\ntempt to achieve task-oriented imaging using weakly supervised\nlearning techniques.\n3. Our method led to state-of-the-art reconstruction performance\non the public benchmarks (i.e., fastMRI and SKM-TEA) under\nvarying acceleration ratios, especially for the imaging of patho-\nlogical regions. In addition, the weakly supervised saliency maps\nfor lesion localization and enhancement could provide additional\ninformation for downstream clinical investigations, making our\nLF-MRI an initial protocol for end-to-end imaging and diagnosis.\nThe rest of the paper is organized as follows. Section 2 provides a\nbrief review of related work. Section 3 introduces our LF-MRI method.\nIn Section 4, we conduct experiments to assess the effectiveness of\nour method. Section 5 discusses the influence of key components and\npotential limitations of our method. Finally, Section 6 concludes the\npaper.\n2. Related work\n2.1. Accelerated MRI reconstruction\nTo accelerate MRI, a variety of studies have been conducted. Rep-\nresentative reconstruction methods include traditional model-based\nmethods (Donoho, 2006; Pruessmann et al., 1999; Lustig et al., 2007;\nLiang et al., 2009), learning-based methods (Mardani et al., 2017; Lee\net al., 2018; Quan et al., 2018; Jin et al., 2017; Huang et al., 2022; Guo\net al., 2023), and model-driven learning-based methods (Yang et al.,\n2017; Zhang and Ghanem, 2018; Aggarwal et al., 2018; Song et al.,\n2023). Traditional model-based methods formulate MRI reconstruction\nas an ill-posed inverse problem, incorporating data consistency and\nsparsity regularization terms, followed by corresponding optimization\nalgorithms to iteratively derive reconstructed images (Donoho, 2006;\nPruessmann et al., 1999; Lustig et al., 2007; Liang et al., 2009). As\na purely data-driven strategy, the learning-based methods typically\nlearn the direct nonlinear mappings from undersampled ğ‘˜-space data or\nundersampled images to high-quality images (Mardani et al., 2017; Lee\net al., 2018; Quan et al., 2018; Han et al., 2019; Jin et al., 2017). For\nexample, Jin et al. (2017) learned a convolutional neural network with\nthe U-Net architecture for the end-to-end generation of high-quality MR\nimages. Mardani et al. (2017) and Quan et al. (2018) employed the gen-\nerative adversarial networks to learn such nonlinear mappings. Huang\net al. (2022) trained a Swin-Transformer for MRI reconstruction. Guo\net al. (2023) proposed a Transformer model incorporated specific\nRecurrent Pyramid Transformer Layers (RPTL), which leverage in-\ntrinsic multi-scale information to improve MRI reconstruction. As a\nrepresentative model-driven learning-based method, ADMM-Net (Yang\net al., 2017) unfolded the ADMM algorithm for MRI reconstruction\ninto deep cascaded networks. Similarly, ISTA-Net (Zhang and Ghanem,\n2018) iteratively derived MR images by designing a deep network to\nlearn the proximal operator of the ISTA algorithm. Aggarwal et al.\n(2018) proposed a more general paradigm to reconstruct the MR images\nby embedding a regularization pre-learned offline into the iterative\nreconstruction process. More recently, Song et al. (2023) developed a\ndeep proximal unrolling network that integrates a memory-augmented\nproximal mapping module to ensure maximum information flow in\nintra-stage and inter-stage for high-quality MRI reconstruction.\nNotably, although these existing approaches greatly advanced the\nfast imaging performance, they primarily concentrated on enhancing\nthe overall image quality per voxel while paid few attention to specific\npathologies that are more strictly linked to downstream clinical needs,\ne.g., disease diagnosis.\n2.2. Task-oriented MRI reconstruction\nOnly a few research in the literature investigated task-oriented\nimaging. They typically conduct concurrent image reconstruction and\nprocessing under a multi-task learning setting. For example, Pramanik\nand Jacob (2021) cascaded a model-based reconstruction network with\na segmentation network for joint training. In a purely data-driven\nfashion, Razumov et al. (2022b) combined the learning of ğ‘˜-space\nsampling and down-stream segmentation to enhance the accelerated\nMedical Image Analysis 107 (2026) 103806\n2\n\nF. Ju et al.\nFig. 1. The architecture of our LF-MRI, a model-driven deep network consisting of multiple cascaded stages for lesion-focused fast MRI. Each stage includes three\nmodules: a denoising module (ğ·\nğ‘¤\n), a lesion-focused module (LF), and a data consistency module (DC).\nMRI reconstruction. In Wu et al. (2023), the authors pretrained an ini-\ntial reconstruction network, which was then coupled with downstream\ntask-heads (i.e., processing networks) for task-driven fine-tuning. We-\nber et al. (2024) devised a multi-task network featuring a cross-task\nshared encoder and task-specific decoders for joint reconstruction and\nsegmentation.\nNotably, these existing task-oriented MRI reconstruction methods\ntypically require dense labeled training samples (e.g., voxel-wise seg-\nmentation masks) for downstream task learning, which is hard to be\nsatisfied in the clinical practice. Moreover, the current methodologi-\ncal designs tend to simply cascade a reconstruction network with a\nprocessing-task head to form a multi-task learning framework, where\nimaging and downstream processing are implicitly co-regularized in a\npurely data-driven fashion to improve their performance. In contrast, in\nthis study, we attempt to construct a unified task-oriented MRI model,\nwhich is then unrolled to deduce an interpretable model-driven deep\nnetwork that explicitly learns to perform lesion/pathology-enhanced\nimaging with only image-level weak labels.\n3. Method\n3.1. General MRI model\nIn the MR imaging process, based on its physical principles, a\ncontinuous-to-discrete imaging system is used to scan an object to\nobtain discrete k-space measurements ğ‘Œ . Then, based on the acquired\nk-space measurements ğ‘Œ , the subsequent image processing process can\nbe generally modeled as the following form:\nğ‘Œ = ğ´ğ‘‹ + ğœ€, (1)\nwhere ğ´ denotes an operator that maps the image ğ‘‹ to the k-space\nmeasurements ğ‘Œ , and ğœ€ denotes the background noise. The objective\nof accelerated MRI reconstruction is to recover high-quality image ğ‘‹\nfrom partially sampled ğ‘˜-space data ğ‘Œ , which is inherently ill-posed.\nA common approach to address this issue is to incorporate additional\nregularization terms, thereby constraining/refining the hypothetical\nspace to search for desired solutions.\n3.2. Lesion-focused MRI model\nTo emphasize the specific requirements of downstream clinical\napplications, we construct a lesion-focused reconstruction model to\nenhance MR imaging of specific lesions of interest. Specifically, in\nthe multi-coil imaging scenarios, given undersampled ğ‘˜-space data\nY = {ğ‘¦\nğ‘Ÿ \nâˆˆ C\nNâˆ—N\nğ‘£ \n}\nğ‘…\nğ‘Ÿ=1 \nobtained by R receiver coils, our goal is to\nreconstruct high-fidelity image X âˆˆ C\nNâˆ—N \nfrom ğ‘Œ . Here, N\nğ‘£ \nand N\ndenote the number of sampled lines in the phase encoding direction\nfor partially sampled and fully sampled scenarios, respectively. Such\nthat the acceleration factor is defined as ğ›¼ = Nâˆ•N\nğ‘£\n. According to the\nMRI mechanism, we has such a lesion-focused reconstruction model as:\nğ‘‹\nâˆ— \n= arg min\nğ‘‹\n1\n2 \nâ€–\nğ´ğ‘‹ âˆ’ ğ‘Œ \nâ€–\n2\n2 \n+ ğœ‡\n1\nâ€–ğ‘(ğ‘‹)â€–\n2\n2 \n+ ğœ‡\n2\nâ€–ğ»(ğ‘‹)â€–\n2\n2\n. (2)\nThe first term denotes the data fidelity, ensuring the consistency\nbetween the recovered image and its corresponding ğ‘˜-space data.\nHere ğ´ is defined as ğ´ = ğ‘€ğ¹ ğ‘†, in which ğ‘† = {ğ‘ \nğ‘Ÿ\n}\nğ‘…\nğ‘Ÿ=1 \ndenote\nthe sensitivity maps, ğ¹ represents the 2-D Fourier transform, and M\ndenotes the sampling matrix. The last two terms are learnable implicit\nregularizations that confine the solution to a desired image space\ntailored for downstream clinical needs. Specifically, the second term\nutilizes an adaptive denoising regularization to minimize the global\nnoise and alias artifacts. Based on this, the third lesion-focused term\nintroduces a straightforward yet effective weakly supervised discrim-\ninative localization strategy to capture spatial information regarding\npathological regions for lesion-enhanced imaging. The details of these\nregularizations and the optimization of this model are presented below.\n3.3. Model optimization algorithm\nWe design an alternative optimization algorithm to solve the recon-\nstruction model in Eq. (2). Following MoDL (Aggarwal et al., 2018),\nthe denoising prior is defined as ğ‘(ğ‘‹) = ğ‘‹ âˆ’ ğ·\nğ‘¤\n(ğ‘‹), where ğ·\nğ‘¤\n(â‹…) is a\nCNN-based denoising network. Given ğ‘‹ = ğ‘‹\n(ğ‘›âˆ’1) \n+ ğ›¥ğ‘‹, we can use its\nTaylor expansion to obtain the approximation at the ğ‘›th iteration:\nğ·\nğ‘¤\n(ğ‘‹\n(ğ‘›âˆ’1) \n+ ğ›¥ğ‘‹) â‰ˆ ğ·\nğ‘¤\n(ğ‘‹\n(ğ‘›âˆ’1)\n) + ğ½\nğ·\nğ‘¤\n(â‹…)\nğ›¥ğ‘‹, (3)\nwhere ğ½\nğ·\nğ‘¤\n(â‹…) \nis the Jacobian matrix. Then, for the denoising prior ğ‘(ğ‘‹),\nwe can obtain its estimate as:\nâ€–ğ‘(ğ‘‹)â€–\n2\n2 \n= â€–ğ‘‹ âˆ’ ğ·\nğ‘¤\n(ğ‘‹\n(ğ‘›âˆ’1) \n+ ğ›¥ğ‘‹)â€–\n2\n2\nâ‰ˆ â€–ğ‘‹ âˆ’ ğ·\nğ‘¤\n(ğ‘‹\n(ğ‘›âˆ’1)\n)â€–\n2\n2 \n+ â€–ğ½\nğ·\nğ‘¤\n(â‹…)\nğ›¥ğ‘‹â€–\n2\n2\n. \n(4)\nDue to the typically small value of â€–ğ›¥ğ‘‹â€–\n2\n2\n, the last items can be\ndiscarded. Therefore, the denoising prior ğ‘(ğ‘‹) can be expressed as:\nâ€–ğ‘(ğ‘‹)â€–\n2\n2 \nâ‰ˆ â€–ğ‘‹ âˆ’ ğ‘\n(ğ‘›)\nâ€–\n2\n2\n, (5)\nin which ğ·\nğ‘¤\n(ğ‘‹\n(ğ‘›âˆ’1)\n) is denoted as ğ‘\n(ğ‘›)\n.\nThen, the lesion-focused regularization is optimized in the same\nmanner, which can be formulated as:\nğ»(ğ‘‹) = ğ‘‹ âˆ’ ğº\nğ‘¢\n(ğ‘ğ‘’ğ‘¡\nğ‘ \n(ğ‘‹), ğ‘‹), (6)\nwhere ğ‘ğ‘’ğ‘¡\nğ‘ \n(â‹…) is a classification-guided discriminative localization mod-\nule that provides fine-grained pathology location information in a\nweakly supervised fashion, based on which the denoising network ğº\nğ‘¢\n(â‹…)\nperforms refined denoising within the specified region of interest. For\nMedical Image Analysis 107 (2026) 103806\n3\n\nF. Ju et al.\nthe ğ‘›th iteration, given ğ‘‹ = ğ‘‹\n(ğ‘›âˆ’1) \n+ ğ›¥ğ‘‹, ğ‘ğ‘’ğ‘¡\nğ‘ \n(ğ‘‹\n(ğ‘›âˆ’1) \n+ ğ›¥ğ‘‹) can be\napproximated by its Taylor expansion:\nğ‘ğ‘’ğ‘¡\nğ‘\n(\nğ‘‹\n(ğ‘›âˆ’1) \n+ ğ›¥ğ‘‹\n) \nâ‰ˆ ğ‘ğ‘’ğ‘¡\nğ‘\n(\nğ‘‹\n(ğ‘›âˆ’1)\n) \n+ ğ½\nğ‘ğ‘’ğ‘¡\nğ‘ \n(â‹…)\nğ›¥ğ‘‹, (7)\nwhere ğ½\nğ‘ğ‘’ğ‘¡\nğ‘ \n(â‹…) \nis the Jacobian matrix. Since ğº\nğ‘¢\n(ğ‘ğ‘’ğ‘¡\nğ‘ \n(â‹…), â‹…) is a binary\nfunction, we need to estimate each variable step by step, leveraging\nthe multivariate Taylor expansion as follows:\nğº\nğ‘¢\n(ğ‘ğ‘’ğ‘¡\nğ‘ \n(ğ‘‹\n(ğ‘›âˆ’1) \n+ ğ›¥ğ‘‹), ğ‘‹\n(ğ‘›âˆ’1) \n+ ğ›¥ğ‘‹)\nâ‰ˆ ğº\nğ‘¢\n(ğ‘ğ‘’ğ‘¡\nğ‘ \n(ğ‘‹\n(ğ‘›âˆ’1)\n) + ğ½ \nğ‘‡\nğ‘ğ‘’ğ‘¡\nğ‘ \nğ›¥ğ‘‹, ğ‘‹\n(ğ‘›âˆ’1) \n+ ğ›¥ğ‘‹)\nâ‰ˆ ğº\nğ‘¢\n(ğ‘ğ‘’ğ‘¡\nğ‘ \n(ğ‘‹\n(ğ‘›âˆ’1)\n), ğ‘‹\n(ğ‘›âˆ’1)\n) + ğ½ \nğ‘‡\n(ğº\nğ‘¢\n)\nğ‘ğ‘’ğ‘¡ğ‘\nğ½ \nğ‘‡\nğ‘ğ‘’ğ‘¡\nğ‘ \nğ›¥ğ‘‹ + ğ½ \nğ‘‡\n(ğº\nğ‘¢\n)\nğ‘‹ \nğ›¥ğ‘‹,\n(8)\nwhere ğ½\n(ğº\nğ‘¢\n)\nğ‘ğ‘’ğ‘¡ğ‘\n \nand ğ½\n(ğº\nğ‘¢\n)\nğ‘‹ \nare the Jacobian matrices of the network\nğº\nğ‘¢ \nfor the variables ğ‘ğ‘’ğ‘¡\nğ‘ \n(â‹…) and ğ‘‹\n(ğ‘›âˆ’1)\n, respectively. Based on the\nconclusion of Eq. (8), we can obtain the lesion-focused regularized\noptimization estimation:\nâ€–\nğ»(ğ‘‹)\nâ€–\n2\n2 \n= â€–ğ‘‹ âˆ’ ğº\nğ‘¢\n(ğ‘ğ‘’ğ‘¡\nğ‘ \n(ğ‘‹\n(ğ‘›âˆ’1) \n+ ğ›¥ğ‘‹), ğ‘‹\n(ğ‘›âˆ’1) \n+ ğ›¥ğ‘‹)â€–\n2\n2\nâ‰ˆ â€–ğ‘‹ âˆ’ ğº\nğ‘¢\n(ğ‘ğ‘’ğ‘¡\nğ‘ \n(ğ‘‹\n(ğ‘›âˆ’1)\n), ğ‘‹\n(ğ‘›âˆ’1)\n)â€–\n2\n2\n+ â€–ğ½\n(ğº\nğ‘¢\n)\nğ‘ğ‘’ğ‘¡ğ‘\n \nğ½ \nğ‘‡\nğ‘ğ‘’ğ‘¡\nğ‘ \nğ›¥ğ‘‹â€–\n2\n2 \n+ â€–ğ½\n(ğº\nğ‘¢\n)\nğ‘‹ \nğ›¥ğ‘‹â€–\n2\n2\n.\n(9)\nDue to the typically small value of â€–ğ›¥ğ‘‹â€–\n2\n2\n, the last two items can be\ndiscarded. Consequently, we can draw the conclusion:\nâ€–ğ»(ğ‘‹)â€–\n2\n2 \nâ‰ˆ â€–ğ‘‹ âˆ’ ğº\nğ‘¢\n(ğ‘ğ‘’ğ‘¡\nğ‘ \n(ğ‘‹\n(ğ‘›âˆ’1)\n), ğ‘‹\n(ğ‘›âˆ’1)\n)â€–\n2\n2\n. (10)\nFor clarity, we define ğ¶ğ´ğ‘€\n(ğ‘›) \n= ğ‘ğ‘’ğ‘¡\nğ‘ \n(ğ‘‹\n(ğ‘›âˆ’1)\n). ğ¶ğ´ğ‘€\n(ğ‘›) \nis the class\nactivation map (CAM) that highlights the spatial locations of potential\nlesions from ğ‘‹\n(ğ‘›âˆ’1)\n. Following this inference, we can derive the nth\nalternative optimization algorithm as follows:\nâ§\nâª\nâª\nâª\nâ¨\nâª\nâª\nâª\nâ©\nğ‘\n(ğ‘›) \n= ğ·\nğ‘¤\n(ğ‘‹\n(ğ‘›âˆ’1)\n)\nğ¶ğ´ğ‘€\n(ğ‘›) \n= ğ‘ğ‘’ğ‘¡\nğ‘ \n(ğ‘‹\n(ğ‘›âˆ’1)\n), ğ‘…\n(ğ‘›) \n= ğº\nğ‘¢\n(ğ¶ğ´ğ‘€\n(ğ‘›)\n, ğ‘‹\n(ğ‘›âˆ’1)\n)\nğ‘‹\n(ğ‘›) \n= argmin\nğ‘‹\n1\n2 \nâ€–ğ´ğ‘‹ âˆ’ ğ‘Œ â€–\n2\n2 \n+ ğœ‡\n1\nâ€–\nâ€–\nâ€–\nğ‘‹ âˆ’ ğ‘\n(ğ‘›)\nâ€–\nâ€–\nâ€–\n2\n2\n+ ğœ‡\n2\nâ€–\nâ€–\nâ€–\nğ‘‹ âˆ’ ğ‘…\n(ğ‘›)\nâ€–\nâ€–\nâ€–\n2\n2 \n.\n(11)\nHere, {ğ‘¤, ğ‘, ğ‘¢, ğœ‡\n1\n, ğœ‡\n2\n} are the iterative algorithmâ€™s tuning parameters\nthat can be learned in a data-driven fashion.\n3.4. Model-unrolled LF-MRI network\nBy unrolling the iterative optimization of the reconstruction model\naccording to Eq. (11), we derive a cascaded deep network LF-MRI, as\ndisplayed in Fig. 1. Each stage of our LF-MRI includes three critical\ncomponents, i.e., denoising module (ğ·\nğ‘Š \n), lesion-focused module (LF),\nand data consistency module (DC), each of which strictly corresponding\nto the three iterative steps in Eq. (11).\n3.4.1. Denoising module\nThe denoising module takes the MR image ğ‘‹\n(ğ‘›âˆ’1) \nfrom the previous\nstage as input to produce the denoised image with globally uniform\ndenoising, as per Eq. (11):\nğ‘\n(ğ‘›) \n= ğ·\nğ‘¤\n(ğ‘‹\n(ğ‘›âˆ’1)\n). (12)\nThe denoising network ğ·\nğ‘¤ \nis designed as a ResNet-like architecture\nwith five ResBlocks as shown in Fig. 2(1), each containing two 3 Ã— 3\nconvolutional layers, which was chosen for its proven effectiveness in\nthe image denoising tasks (Lee et al., 2018).\n3.4.2. Lesion-focused module\nOur LF module takes the reconstruction results from the preceding\nstage as input and outputs the lesion-enhanced image:\nğ‘…\n(ğ‘›) \n= ğ¿ğ¹ (ğ‘‹\n(ğ‘›âˆ’1)\n). (13)\nThe LF module is carefully designed as depicted in Fig. 2(2). To\nrealize lesion-enhanced imaging, we incorporate a classification net-\nwork to provide pathology information of interest with a predicted\nCAM, which is then used to guide the denoising network ğº\nğ‘¢ \nto\nFig. 2. Details of each module of our LF-MRI. (1) Denoising Module: ğ·\nğ‘¤\nand ğº\nğ‘¢ \nhave the same network architecture while different inputs; (2)\nLesion-Focused Module: ğ‘ğ‘’ğ‘¡\nğ‘ \nis the classification-guided weakly supervised\nlocalization operation that generates the lesion-aware CAM information; (3)\nDetails of ğ‘ğ‘’ğ‘¡\nğ‘ \n, including the process of obtaining CAM and classification\nscores. (4) Data-Consistency Module.\npay more attention to the feature of the interested region during the\nreconstruction process. The denoising network ğº\nğ‘¢ \nis structured to be\nentirely consistent with ğ·\nğ‘¤\n. ğ‘ğ‘’ğ‘¡\nğ‘ \n(â‹…) is implemented using a pretrained\nVGG19 network (Simonyan and Zisserman, 2014) combined with the\nXGradCAM method (Fu et al., 2020). Specifically, XGradCAM combines\nimage features with gradient information to generate the class activa-\ntion maps for weakly supervised localization. The image features can be\ndirectly obtained from the feature extraction layers of the pre-trained\nVGG19. Since our focus is on information related to the lesion region,\nthe gradient information of the network is derived by backpropagating\nthe scores of the anomaly class. The detailed calculation process is\nillustrated in Fig. 2(3). In contrast to previous reconstruction methods,\nour method allows us to pinpoint the lesion region without the need for\npixel-level labels (only image-level labels) during the training process.\n3.4.3. Data-consistency module\nFollowing the third subequation of Eq. (11), the data consistency\nmodule is described as:\nğ‘‹\n(ğ‘›) \n= DC \n(\nğ´\nğ» \nğ‘Œ , ğ‘\n(ğ‘›)\n, ğ‘…\n(ğ‘›)\n) \n. (14)\nIt produces the reconstruction results ğ‘‹\n(ğ‘›) \nby minimizing the dis-\ntance between the solution ğ‘‹\n(ğ‘›) \nand the input (i.e., the image ğ´\nğ» \nğ‘Œ cor-\nresponding to k-space measurements ğ‘Œ , the globally denoising image\nğ‘\n(ğ‘›)\n, and the lesion-enhanced image ğ‘…\n(ğ‘›)\n) to the DC module. Although\nthe analytical solution to this problem can be derived as shown in\nFig. 2(4), it involves an inversion operation that may not always be\ncomputationally feasible. Hence, we employ the conjugate gradient\nmethod (Hestenes et al., 1952) to iteratively solve this subproblem.\nMedical Image Analysis 107 (2026) 103806\n4\n\nF. Ju et al.\nFig. 3. Comparison of different methods under 4Ã— acceleration on the fastMRI and SKM-TEA knee datasets. The reconstructed images and error maps are presented\nwith the corresponding quantitative metrics in PSNR/SSIM.\n3.5. Network training\nAccording to the model and network design, the parameters that\nneed to be learned in our LF-MRI can be summarized as ğ›© = {ğ‘¤, ğ‘, ğ‘¢, ğœ‡\n1\n,\nğœ‡\n2\n}. To effectively train such a network, the loss function is specified\nas a combination of multiple terms:\nğ¿(ğ›©) = \n1\nğ‘\nğ‘\nâˆ‘\nğ‘–=1\nâ€–\nâ€–\nâ€–\nğ‘‹\n(ğ¾)\nğ‘– \nâˆ’ ğ‘‹\nğ‘–\nâ€–\nâ€–\nâ€–\n2\n2 \n+ ğ›¼ \nâ€–\nâ€–\nâ€–\nğ‘\n(ğ¾)\nğ‘– \nâˆ’ ğ‘‹\nğ‘–\nâ€–\nâ€–\nâ€–\n2\n2\n+ ğ›½ \nâ€–\nâ€–\nâ€–\nğ¶ğ´ğ‘€\n(ğ¾)\nğ‘– \nğ‘‹\n(ğ¾)\nğ‘– \nâˆ’ ğ¶ğ´ğ‘€\n(ğ¾)\nğ‘– \nğ‘‹\nğ‘–\nâ€–\nâ€–\nâ€–\n2\n2\n+ ğ›¾\n1\nğ¿\nğ‘ğ‘™ğ‘ \n(\nğ¶\n+\nğ‘– \n, ğ¶\n(ğ¾)\nğ‘–\n)\n+ ğ›¾\n2\nğ¿\nğ‘ğ‘™ğ‘ \n(\nğ¶\nâˆ’\nğ‘– \n, ğ¶\n(ğ¾)\nğ‘–\n)\n,\n(15)\nwhere ğ‘ is the number of training samples. The first two terms quantify\nthe mean square error (MSE) between the target image ğ‘‹\nğ‘– \nand the\nreconstruction result ğ‘‹\n(ğ¾)\nğ‘– \nor the intermediate result ğ‘\n(ğ¾)\nğ‘– \n, respectively,\nensuring the overall fidelity of the reconstructed image. The third\nterm is introduced to ensure the fidelity between the target image and\nğ‘‹\n(ğ¾)\nğ‘– \non the pathological region of interest, where ğ¶ğ´ğ‘€\n(ğ¾)\nğ‘– \nis CAM\ninformation output by the network at stage ğ¾. The last two terms are\ncross-entropy losses aimed at encouraging the reconstructed images\nto drive the classification network to produce accurate pathological\ndiagnosis. ğ¶\n+\nğ‘– \nand ğ¶\nâˆ’\nğ‘– \ndenote the labels for images with lesions and\nwithout lesions, respectively. ğ¶\n(ğ¾)\nğ‘– \nrepresents the output score of the\nclassification network. Specifically, we set ğ›¾\n1 \n> ğ›¾\n2 \nto ensure that our\nnetwork prioritizes the accurate classification of images containing\nlesions due to the higher importance assigned to pathological findings\nin clinical diagnostics.\nAdditionally, since the loss terms include constraints on both the\noverall image and the lesion region, we carefully set the tuning param-\neters of the loss function to balance the preservation of other image\nfeatures while enhancing the lesion. Specifically, we set ğ›¼ = ğ›½ = 0.5 to\nmake the model focus more on the quality of the reconstructed image.\nThis prevents the model from losing other important features of the\nimage, such as tissue contrast or resolution, while overly concentrating\non the lesion region. The tuning parameters for the cross-entropy loss\nare set based on requirements and empirical experience to ğ›¾\n1 \n= 1, and\nğ›¾\n2 \n= 0.5, respectively.\nMedical Image Analysis 107 (2026) 103806\n5\n\nF. Ju et al.\nTable 1\nQuantitative comparisons on the fastMRI knee and SKM-TEA datasets under 4Ã— acceleration rate, with the performance evaluated in terms of both the overall\nmetrics, i.e., PSNR and SSIM, and Lesion-Focused metrics, i.e., LF-PSNR and LF-SSIM.\nMethods fastMRI-PDFS fastMRI-PD SKM-TEA\nPSNR LF-PSNR SSIM LF-SSIM PSNR LF-PSNR SSIM LF-SSIM PSNR LF-PSNR SSIM LF-SSIM\nZero-filled 22.29 Â± 3.78 16.88 Â± 5.37 0.731 Â± 0.06 0.741 Â± 0.06 24.48 Â± 2.43 18.53 Â± 3.60 0.766 Â± 0.04 0.846 Â± 0.05 23.44 Â± 1.57 22.41 Â± 2.62 0.631 Â± 0.02 0.618 Â± 0.04\nGRAPPA 27.28 Â± 2.39 17.42 Â± 4.57 0.733 Â± 0.07 0.725 Â± 0.07 26.28 Â± 2.14 18.94 Â± 3.01 0.836 Â± 0.04 0.859 Â± 0.06 26.18 Â± 1.32 23.99 Â± 2.40 0.690 Â± 0.02 0.684 Â± 0.05\nU-Net 28.67 Â± 1.97 21.50 Â± 4.00 0.822 Â± 0.08 0.783 Â± 0.08 31.98 Â± 2.10 24.15 Â± 2.34 0.860 Â± 0.04 0.887 Â± 0.04 27.97 Â± 1.23 25.39 Â± 1.94 0.748 Â± 0.03 0.705 Â± 0.05\nSwinMR 32.09 Â± 2.05 24.48 Â± 3.61 0.859 Â± 0.06 0.827 Â± 0.05 32.58 Â± 2.37 25.09 Â± 2.76 0.885 Â± 0.04 0.894 Â± 0.04 29.36 Â± 1.30 25.96 Â± 1.91 0.778 Â± 0.02 0.739 Â± 0.04\nISTA-Net 32.35 Â± 1.86 24.79 Â± 3.61 0.854 Â± 0.03 0.849 Â± 0.04 32.72 Â± 1.95 25.05 Â± 2.00 0.886 Â± 0.02 0.899 Â± 0.03 29.27 Â± 1.15 26.57 Â± 1.54 0.772 Â± 0.02 0.738 Â± 0.04\nMoDL 34.10 Â± 2.07 26.12 Â± 3.90 0.876 Â± 0.03 0.871 Â± 0.04 35.98 Â± 1.89 26.89 Â± 1.94 0.928 Â± 0.02 0.923 Â± 0.02 30.66 Â± 1.12 28.06 Â± 1.35 0.818 Â± 0.02 0.795 Â± 0.04\nOurs 34.86 Â± 2.12 26.67 Â± 4.01 0.897 Â± 0.03 0.885 Â± 0.03 36.94 Â± 2.16 27.64 Â± 1.89 0.935 Â± 0.02 0.929 Â± 0.02 31.37 Â± 1.29 28.63 Â± 1.48 0.837 Â± 0.02 0.812 Â± 0.04\n4. Experiments\n4.1. Datasets\nWe evaluated our LF-MRI on two publicly available benchmarks,\ni.e., fastMRI (Zbontar et al., 2018) and SKM-TEA (Desai et al., 2022) of\nknee MR images with different contrasts.\nfastMRI: Multi-coil knee MR imaging data with two different con-\ntrasts: Proton Density (PD) and Proton Density Fat-Suppressed (PDFS),\ncomprising 796 and 798 volumes, respectively. Each volume measures\n320 Ã— 320 pixels per slice, with the number of slices varying from 35\nto 45. We selected the central 30% of the consecutive slices contain-\ning complete anatomical structures from each volume. The pathology\nannotations (e.g., lesion masks or bounding boxes) provided in Zhao\net al. (2021) were leveraged to define the image-level labels for our\nlesion-focused imaging. The coil sensitivity maps were pre-estimated\nby Espirit (Uecker et al., 2014).\nSKM-TEA: The SKM-TEA dataset contains knee MR imaging data\ncollected from 155 anonymized subjects, encompassing raw ğ‘˜-space\nmeasurements, corresponding coil sensitivity maps, and bounding box\nannotations for sixteen clinically relevant pathologies. Each subjectâ€™s\nmeasurement data has the dimensions of 512 Ã— 512 Ã— 160. From each\nsubject, which comprises 160 slices in total, we specifically chose the\ncentral 30% slices that contain anatomical structures for evaluation.\nTo learn weakly supervised lesion-enhanced imaging, for both\ndatasets, we defined the binary image-level labels (i.e., 1: presence, and\n0: absence) for each slice based on whether it contained pathological\nmarkers. Finally, each dataset was split in terms of subjects into\ntraining, validation, and test sets at a ratio of 5:1:1 for subsequent\nexperiments and evaluation.\n4.2. Implementation details\nOur LF-MRI network was implemented on the PyTorch platform,\nusing the Ubuntu 20.04 operating system and an RTX 3090Ti GPU\ncard. Experiments were performed at both the 4Ã— and 8Ã— acceleration\nrates, respectively. The network was designed to have 3 stages. The\nnetwork parameters were initialized by Xavier random initialization\nand trained by Adam optimizer with a learning rate, batch size, and\nthe number of epochs setting as 1eâˆ’3, 8, and 300, respectively. The LF-\nMRI network combines the real and imaginary parts of the MR complex\nimages as two separate input channels, and the initial input ğ‘‹\n(0) \nwas\nset as ğ‘‹\n(0) \n= ğ´\nğ» \nğ‘Œ .\nMore specifically, we first fine-tuned the pre-trained classification\nnetwork on fully sampled MR images to make sure it can accurately\ndistinguish the image with and without lesions to generate precise CAM\nfor lesion localization. Notably, we only require the coarsest image-\nlevel labels to train the network to generate precise pixel-level CAM\ninformation. This weakly supervised training paradigm reduces the\ndemand for meticulously annotated data and enhances the practicality\nof our model.\n4.3. Compared methods\nWe compared our LF-MRI against five representative methods: the\ntraditional GRAPPA method, purely data-driven methods, i.e., U-Net\nand SwinMR, and model-unrolled learning-based methods, i.e., ISTA-\nNet and MoDL. All comparison methods were implemented based on\ntheir publicly accessible original codes provided by the authors. The\ndetails for these methods are summarized as follows:\n(1) GRAPPA (Griswold et al., 2002): A traditional reconstruction\nmethod that leverages the correlation between multiple par-\nallel receive channels to fill in missing k-space data. For our\nevaluation, we set the interpolation kernel size to 5 Ã— 5 and\nadjusted the number of sampled self-calibration lines based on\nthe acceleration factor (i.e., 8% and 4% of the total lines for 4x\nand 8x acceleration, respectively).\n(2) U-Net (Ronneberger et al., 2015): A conventional end-to-end\nencoderâ€“decoder model for medical imaging tasks. During train-\ning, the learning rate and the epoch were set to 0.0001 and 100,\nrespectively.\n(3) SwinMR (Huang et al., 2022): A purely data-driven approach\nthat incorporates Swin Transformer for MRI reconstruction tasks.\nThe training hyperparameters were set to the default parameters\nprovided by the authors.\n(4) ISTA-Net (Zhang and Ghanem, 2018): The ISTA network is a\nclassic model-based unfolding network inspired by the ISTA\nalgorithm, in which the sparse regularization and tuning pa-\nrameters are learned in a data-driven manner. The training\nhyperparameters were empirically configured with 5 layers, a\nlearning rate of 0.001, and a total of 100 epochs.\n(5) MoDL (Aggarwal et al., 2018): A model-based deep learning\nparadigm for solving inverse problems, where the number of\nstages, learning rate, and epoch were set to 3, 0.001, and 100,\nrespectively.\n4.4. Evaluation metrics\nTo quantitatively assess the reconstruction performance, we em-\nployed two commonly used metrics: Peak Signal-to-Noise Ratio (PSNR)\nand Structural Similarity Index Measure (SSIM). Moreover, given our\nmethodâ€™s emphasis on lesion-focused imaging, in addition to provid-\ning global PSNR and SSIM metrics, we also evaluated the PSNR and\nSSIM on specified lesion regions (denoted as LF-PSNR and LF-SSIM,\nrespectively). Specifically, based on the pathological box annotations\nprovided by the two data benchmarks, we evaluated the PSNR and\nSSIM within these pathological regions. It is worth noting that, for a fair\ncomparison across different methods, all these metrics were quantified\nafter normalizing the reconstructed images with 0â€“1 normalization to\nremove the influence of intensity ranges.\n4.5. Experimental results\nTables 1 and 2 present the quantitative reconstruction results of all\ncompeting methods on the fastMRI and SKM-TEA knee datasets under\nthe 4Ã— and 8Ã— acceleration factors, respectively. From these results, we\ncan have the following observations:\nMedical Image Analysis 107 (2026) 103806\n6\n\nF. Ju et al.\nTable 2\nQuantitative comparisons on the fastMRI knee and SKM-TEA datasets under 8Ã— acceleration rate, with the performance evaluated in terms of both the overall\nmetrics, i.e., PSNR and SSIM, and Lesion-Focused metrics, i.e., LF-PSNR and LF-SSIM.\nMethods fastMRI-PDFS fastMRI-PD SKM-TEA\nPSNR LF-PSNR SSIM LF-SSIM PSNR LF-PSNR SSIM LF-SSIM PSNR LF-PSNR SSIM LF-SSIM\nZero-filled 17.96 Â± 3.43 12.49 Â± 5.14 0.614 Â± 0.08 0.623 Â± 0.09 18.95 Â± 2.35 12.46 Â± 3.27 0.653 Â± 0.05 0.764 Â± 0.07 20.84 Â± 1.47 19.92 Â± 2.42 0.541 Â± 0.03 0.515 Â± 0.04\nGRAPPA 25.40 Â± 2.22 13.93 Â± 5.07 0.628 Â± 0.09 0.625 Â± 0.10 21.55 Â± 2.13 14.76 Â± 2.96 0.744 Â± 0.05 0.817 Â± 0.07 23.65 Â± 1.19 21.35 Â± 2.51 0.593 Â± 0.03 0.580 Â± 0.06\nU-Net 26.20 Â± 1.78 19.82 Â± 3.55 0.674 Â± 0.07 0.726 Â± 0.10 27.85 Â± 2.04 21.37 Â± 2.93 0.788 Â± 0.05 0.846 Â± 0.05 25.55 Â± 1.25 22.96 Â± 2.02 0.677 Â± 0.04 0.626 Â± 0.06\nSwinMR 28.34 Â± 2.25 21.32 Â± 3.32 0.749 Â± 0.07 0.749 Â± 0.07 27.04 Â± 2.38 20.18 Â± 3.70 0.774 Â± 0.06 0.823 Â± 0.06 26.59 Â± 1.15 22.83 Â± 1.80 0.682 Â± 0.03 0.646 Â± 0.05\nISTA-Net 27.25 Â± 2.04 21.17 Â± 3.08 0.745 Â± 0.05 0.764 Â± 0.06 26.19 Â± 2.18 19.88 Â± 2.34 0.757 Â± 0.04 0.816 Â± 0.06 26.11 Â± 1.10 23.49 Â± 1.53 0.654 Â± 0.04 0.612 Â± 0.06\nMoDL 28.81 Â± 2.45 22.49 Â± 3.37 0.755 Â± 0.04 0.792 Â± 0.05 28.98 Â± 2.32 22.64 Â± 2.27 0.811 Â± 0.03 0.855 Â± 0.05 26.40 Â± 1.17 23.89 Â± 1.61 0.689 Â± 0.03 0.654 Â± 0.05\nOurs 29.36 Â± 2.18 22.85 Â± 3.27 0.766 Â± 0.04 0.797 Â± 0.05 30.42 Â± 2.31 23.61 Â± 2.29 0.835 Â± 0.03 0.873 Â± 0.05 27.68 Â± 1.24 25.02 Â± 1.63 0.726 Â± 0.03 0.690 Â± 0.05\nTable 3\nAblation study of our LF-MRI across different numbers of stages under 4Ã— and 8Ã— acceleration rates on the fastMRI dataset.\nfastMRI 1D 4 Ã— mask 1D 8 Ã— mask\nContrast Stage PSNR LF-PSNR SSIM LF-SSIM PSNR LF-PSNR SSIM LF-SSIM\nPD\nK = 1 35.54 Â± 1.96 26.67 Â± 1.91 0.924 Â± 0.02 0.921 Â± 0.03 29.20 Â± 2.13 22.73 Â± 2.35 0.789 Â± 0.03 0.860 Â± 0.05\nK = 3 36.94 Â± 2.16 27.64 Â± 1.89 0.935 Â± 0.02 0.929 Â± 0.02 30.42 Â± 2.31 23.61 Â± 2.29 0.835 Â± 0.03 0.873 Â± 0.05\nK = 5 37.41 Â± 2.19 27.88 Â± 1.91 0.938 Â± 0.02 0.932 Â± 0.02 30.47 Â± 2.42 23.42 Â± 2.39 0.834 Â± 0.03 0.875 Â± 0.04\nPDFS\nK = 1 34.07 Â± 1.95 26.03 Â± 3.91 0.886 Â± 0.03 0.875 Â± 0.03 28.89 Â± 2.29 22.45 Â± 3.29 0.785 Â± 0.04 0.794 Â± 0.05\nK = 3 34.86 Â± 2.12 26.67 Â± 4.01 0.897 Â± 0.03 0.885 Â± 0.03 29.36 Â± 2.18 22.85 Â± 3.27 0.766 Â± 0.04 0.797 Â± 0.05\nK = 5 35.04 Â± 2.16 26.74 Â± 4.05 0.894 Â± 0.02 0.885 Â± 0.03 29.44 Â± 2.35 22.89 Â± 3.33 0.772 Â± 0.04 0.801 Â± 0.05\n(1) Overall reconstruction results: Across different datasets, imag-\ning contrasts, and acceleration factors, our LF-MRI consistently out-\nperformed all competing methods in terms of PSNR and SSIM. This\nindicates the effectiveness and robustness of our method in fast MR\nimaging.\n(2) Pathology reconstruction results: In the pathology regions,\nour method led to more remarkable reconstruction performance in\nterms of LF-PSNR and LF-SSIM, especially given a high acceleration fac-\ntor. For instance, compared to the leading method MoDL, our method\nachieved an LF-PSNR improvement of 1.13 dB (23.89 dB vs. 25.02 dB)\non SKM-TEA dataset and 0.97 dB (22.64 dB vs. 23.61 dB) on fastMRI\ndataset under the high 8Ã— acceleration rate. These improvements high-\nlight the pivotal contribution of our meticulously designed lesion-\nfocused module in enhancing the imaging of target lesions.\n(3) Visualization results: As a complement to the quantitative\nfindings, Figs. 3 and 4 depict representative reconstructed MR images\nand their corresponding error maps obtained by different methods\nunder both low (4Ã—) and high (8Ã—) acceleration scenarios across dif-\nferent datasets. Independent of the acceleration factor or dataset, our\nmethod consistently exceled in overall image reconstruction, exhibiting\nminimal reconstruction errors and the highest PSNR values. Impor-\ntantly, even under a high acceleration rate, our method maintained\nexceptional reconstruction performance.\nFigs. 5 and 6 further illustrate the magnified details of lesions\nand their corresponding residual plots of the reconstructed MR images\nunder the 4Ã— and 8Ã— acceleration rates, respectively. The pathological\nregions in the image are highlighted in red/yellow and correspond\nto specified bounding boxes annotated in each public dataset. Our\napproach demonstrates improved visual clarity with enhanced tex-\nture information and image quality, particularly in the highlighted\npathological regions.\nIn addition, in Fig. 7, we compared the CAMs of the images recon-\nstructed by our method with the ground-truth lesion bounding boxes,\nfrom which we observed a high degree of consistency regardless of the\nacceleration factors or datasets.\nOverall, the observations above confirm the efficacy of our LF-MRI\nin lesion-enhanced fast imaging. Crucially, our method operates effi-\nciently without the need for extensive pixel-level annotations, thereby\nreducing costs and improving practical applicability. The precise iden-\ntification and enhancement of pathological regions are directly linked\nto accurate diagnosis and treatment, emphasizing the clinical relevance\nof our method.\n5. Discussion\nIn this section, we conducted a series of ablation studies on the\nfastMRI dataset to more comprehensively disscuss about the efficacy\nas well as potential limitations of our LF-MRI method.\n5.1. The influence of the number of stages\nTo examine the impact of the number of stages of our LF-MRI\nnetwork on the reconstruction performance, we list the quantitative\nresults under different stages for both the 4Ã— and 8Ã— acceleration rates\nin Table 3. It can be observed that the reconstruction performance\ninitially showed a noticeable improvement with increasing stages, but\nplateauing at five iterations. Further stages increased resource con-\nsumption and time costs with limited performance gains. Therefore, in\nour implementation, we selected K = 3 for our LF-MRI network to strike\na balance between performance and resource efficiency.\n5.2. The role of classification-guided Lesion localization\nTo investigate the influence of the classification network ğ‘ğ‘’ğ‘¡\nğ‘\nwithin the lesion-focused module on the reconstruction performance,\nwe first performed an ablation experiment by removing the classifi-\ncation network from our LF-MRI network, denoted as LF-MRI/C. The\nresults, detailed in Table 4, indicate a notable performance degra-\ndation, particularly in pathological regions, as evidenced by reduced\nLF-PSNR and LF-SSIM values. These findings underscore the critical role\nof the lesion-focused module in our LF-MRI design.\nFurthermore, to evaluate whether the performance of our LF-MRI\ndepends on the specific VGG19 classification network, we integrated\nalternative classification networks (e.g., Res-Net50 He et al., 2016 and\nGoogle-Net Szegedy et al., 2015) into our reconstruction framework to\ncheck the performance changes. The experimental results in Table 4\nreveal that the reconstruction performance consistently improved un-\nder various acceleration factors when different classification networks\nwere employed in the lesion-focused module. This finding suggests that\nthe effectiveness of LF-MRI is not confined to VGG19. Specifically, both\nLF-MRI+Google-Net and LF-MRI+Res-Net50, which utilize Google-Net\nand Res-Net50 as classification networks in the lesion-focused module,\nachieved comparable reconstruction performance.\nOverall, these ablation studies confirm that the inclusion of a\nclassification-guided attention module is essential for effective lesion\nlocalization and enhanced target imaging in our LF-MRI. The integra-\ntion of the classification network significantly affects reconstruction\nperformance, underscoring the necessity for careful integration of this\ncomponent in the LF module.\nMedical Image Analysis 107 (2026) 103806\n7\n\nF. Ju et al.\nFig. 4. Comparison of different methods under 8Ã— acceleration on the fastMRI and SKM-TEA knee datasets. The reconstructed images and their error maps are\npresented with the corresponding quantitative metrics in PSNR/SSIM.\nTable 4\nThe performance of our LF-MRI on the fastMRI PD dataset in terms of using different classification networks to perform weakly\nsupervised localization for lesion-enhanced imaging.\nAcceleration factor Methods PSNR LF-PSNR SSIM LF-SSIM\n4Ã—\nLF-MRI/C 36.81 Â± 2.24 27.51 Â± 1.87 0.928 Â± 0.02 0.928 Â± 0.02\nLF-MRI+Google-Net 36.93 Â± 2.18 27.62 Â± 1.89 0.933 Â± 0.02 0.929 Â± 0.02\nLF-MRI+Res-Net50 36.93 Â± 2.19 27.64 Â± 1.89 0.932 Â± 0.02 0.929 Â± 0.02\nLF-MRI+VGG19 (Ours) 36.94 Â± 2.16 27.64 Â± 1.89 0.935 Â± 0.02 0.929 Â± 0.02\n8Ã—\nLF-MRI/C 29.81 Â± 2.04 23.20 Â± 2.26 0.805 Â± 0.03 0.864 Â± 0.05\nLF-MRI+Google-Net 30.20 Â± 2.18 23.43 Â± 2.33 0.831 Â± 0.03 0.871 Â± 0.05\nLF-MRI+Res-Net50 30.35 Â± 2.22 23.59 Â± 2.31 0.843 Â± 0.03 0.873 Â± 0.05\nLF-MRI+VGG19 (Ours) 30.42 Â± 2.31 23.61 Â± 2.29 0.835 Â± 0.03 0.873 Â± 0.05\n5.3. The efficacy of different loss design\nTo train our LF-MRI network, a combination of different loss terms\nwas employed to regulate and optimize the modelâ€™s performance. To\nevaluate the efficacy of our designed loss configurations, Table 5\npresents the results obtained by maintaining the core reconstruction\nloss while selectively removing other terms under both 4Ã— and 8Ã—\nacceleration factors: (1) â€˜â€˜No-CAMâ€™â€™, excluding the pathology-aware re-\nconstruction loss for lesion-enhanced imaging; (2) â€˜â€˜No-CLSâ€™â€™, removing\nthe last two terms related to disease diagnosis/classification with the\nreconstructed images; (3) â€˜â€˜Base-Lossâ€™â€™, eliminating both the pathology-\naware reconstruction and classification loss terms; (4) â€˜â€˜Full-Lossâ€™â€™, the\noriginal loss design including all components. Furthermore, to explore\nthe impact of loss designs in boosting lesion-enhanced imaging, Table 6\nMedical Image Analysis 107 (2026) 103806\n8\n\nF. Ju et al.\nFig. 5. Comparison of different reconstruction methods using 4Ã— mask on the fastMRI and SKM-TEA knee datasets. The reconstructed images, their magnified\ndetails of pathological regions (marked in the red/yellow boxes), and their error maps are presented with the corresponding quantitative metrics in PSNR/LF-PSNR.\n(Pathology: fastMRI-PD: Meniscus Tear; fastMRI-PDFS: Meniscus Tear; SKM-TEA: Effusion).\nTable 5\nAblation studies on our LF-MRI network under various loss configurations at the acceleration rates of 4Ã— and 8Ã—, respectively.\nfastMRI 1D 4 Ã— mask 1D 8 Ã— mask\nContrast Loss settings PSNR LF-PSNR SSIM LF-SSIM PSNR LF-PSNR SSIM LF-SSIM\nPD\nBase-loss 36.81 Â± 2.24 27.51 Â± 1.87 0.928 Â± 0.02 0.928 Â± 0.02 29.81 Â± 2.04 23.20 Â± 2.26 0.805 Â± 0.03 0.864 Â± 0.05\nNo-CAM 36.88 Â± 2.06 27.54 Â± 1.91 0.931 Â± 0.02 0.928 Â± 0.02 30.08 Â± 2.11 23.36 Â± 2.29 0.838 Â± 0.03 0.870 Â± 0.05\nNo-CLS 36.81 Â± 2.13 27.54 Â± 1.90 0.933 Â± 0.02 0.928 Â± 0.02 30.16 Â± 2.05 23.56 Â± 2.24 0.796 Â± 0.04 0.870 Â± 0.05\nFull-Loss 36.94 Â± 2.16 27.64 Â± 1.89 0.935 Â± 0.02 0.929 Â± 0.02 30.42 Â± 2.31 23.61 Â± 2.29 0.835 Â± 0.03 0.873 Â± 0.05\nPDFS\nBase-loss 34.58 Â± 2.23 26.42 Â± 3.96 0.891 Â± 0.02 0.833 Â± 0.03 28.80 Â± 2.55 22.47 Â± 3.36 0.784 Â± 0.04 0.784 Â± 0.05\nNo-CAM 34.78 Â± 2.10 26.62 Â± 3.99 0.891 Â± 0.03 0.884 Â± 0.03 29.13 Â± 2.42 22.63 Â± 3.32 0.787 Â± 0.04 0.799 Â± 0.05\nNo-CLS 34.71 Â± 2.19 26.55 Â± 3.99 0.894 Â± 0.03 0.884 Â± 0.03 28.94 Â± 2.50 22.45 Â± 3.38 0.787 Â± 0.04 0.798 Â± 0.05\nFull-Loss 34.86 Â± 2.12 26.67 Â± 4.01 0.897 Â± 0.03 0.885 Â± 0.03 29.36 Â± 2.18 22.85 Â± 3.27 0.766 Â± 0.04 0.797 Â± 0.05\nTable 6\nThe performance of our LF-MRI on the fastMRI PD dataset by including extra perceptual loss for network training.\nAcceleration factor Methods PSNR LF-PSNR SSIM LF-SSIM\n4Ã— \nFull-Loss 36.94 Â± 2.16 27.64 Â± 1.89 0.935 Â± 0.02 0.929 Â± 0.02\nExtra-Loss 36.99 Â± 2.17 27.65 Â± 1.91 0.936 Â± 0.02 0.930 Â± 0.02\n8Ã— \nFull-Loss 30.42 Â± 2.31 23.61 Â± 2.29 0.835 Â± 0.03 0.873 Â± 0.05\nExtra-Loss 30.43 Â± 2.10 23.61 Â± 2.30 0.839 Â± 0.03 0.872 Â± 0.05\npresents the outcomes of introducing an extra perceptual loss (labeled\nas â€˜â€˜Extra-Lossâ€™â€™) at the acceleration factors of 4Ã— and 8Ã—, respectively.\nThe experimental results indicate that removing each individual loss\ncomponent resulted in a drop in model performance. In particular,\nexcluding the classification loss (i.e., No-CLS) led to a pronounced\ndecrease of the image reconstruction quality, especially in the lesion\nregions. It implies that such a downstream diagnostic constraint effec-\ntively assisted the network to generate more accurate CAM information,\nMedical Image Analysis 107 (2026) 103806\n9\n\nF. Ju et al.\nFig. 6. Comparison of different reconstruction methods using 8Ã— mask on the fastMRI and SKM-TEA datasets. The reconstructed images, magnified details of\npathological regions (marked in the red/yellow boxes), and error maps are presented with the corresponding quantitative metrics in PSNR/LF-PSNR. (Pathology:\nfastMRI-PD: Cartilage - Partial Thickness loss/defect; fastMRI-PDFS: Bone-Fracture/Contusion/dislocation; SKM-TEA: Effusion).\nFig. 7. The CAMs of the images reconstructed under 4Ã— and 8Ã— acceleration\nrates are presented on the left and right sides, respectively. Our LF-MRI\nidentified and located pathological regions with high accuracy.\nthus providing crucial lesion location details for lesion-enhanced and\ntask-oriented reconstruction. Besides, we can see that the simultaneous\nremoval of the pathology-aware reconstruction loss and the classifi-\ncation loss (i.e., Base-Loss) led to the most substantial performance\nTable 7\nComparison of the diagnostic results of the images reconstructed by different\nmethods across varying datasets (imaging contrasts) and acceleration factors.\nAcceleration factor Methods fastMRI-PDFS fastMRI-PD SKM-TEA\n4Ã—\nZero-filled 0.694 0.694 0.729\nGRAPPA 0.663 0.723 0.709\nUNet 0.730 0.725 0.754\nSwinMR 0.733 0.734 0.736\nISTA-Net 0.734 0.730 0.761\nMoDL 0.742 0.743 0.769\nOurs 0.744 0.750 0.775\n8Ã—\nZero-filled 0.517 0.620 0.590\nGRAPPA 0.555 0.653 0.732\nUNet 0.699 0.697 0.725\nSwinMR 0.691 0.687 0.719\nISTA-Net 0.668 0.625 0.746\nMoDL 0.717 0.692 0.752\nOurs 0.720 0.714 0.754\ndecline, aligning with our intuitive expectations. Specifically, the re-\nmoval of any loss component has a more significant impact on the\nresults at the 8Ã— acceleration rate. We speculate that this is because,\nunder more limited k-space data, noise and artifacts become more\nMedical Image Analysis 107 (2026) 103806\n10\n\nF. Ju et al.\nTable 8\nAblation studies on our LF-MRI integrating learnable sampling patterns under 4Ã— and 8Ã— acceleration rates, respectively.\nfastMRI 1D 4 Ã— mask 1D 8 Ã— mask\nContrast Methods PSNR LF-PSNR SSIM LF-SSIM PSNR LF-PSNR SSIM LF-SSIM\nPD \nLF-MRI (Ours) 36.94 Â± 2.16 27.64 Â± 1.89 0.935 Â± 0.02 0.929 Â± 0.02 30.42 Â± 2.31 23.61 Â± 2.29 0.835 Â± 0.03 0.873 Â± 0.05\nLS-LF-MRI 36.73 Â± 2.02 27.43 Â± 2.03 0.928 Â± 0.02 0.923 Â± 0.02 34.60 Â± 2.68 25.70 Â± 2.35 0.902 Â± 0.03 0.900 Â± 0.04\nPDFS \nLF-MRI (Ours) 34.86 Â± 2.12 26.67 Â± 4.01 0.897 Â± 0.03 0.885 Â± 0.03 29.36 Â± 2.18 22.85 Â± 3.27 0.766 Â± 0.04 0.797 Â± 0.05\nLS-LF-MRI 34.61 Â± 1.78 26.60 Â± 4.20 0.877 Â± 0.02 0.871 Â± 0.04 32.13 Â± 2.43 24.74 Â± 3.95 0.821 Â± 0.04 0.824 Â± 0.05\nTable 9\nThe performance of our LF-MRI across varying imaging qualities in terms of magnetic field strength (MFS) of the scanners. The\nexperiments were conducted on the fastMRI PD dataset.\nAcceleration factor MFS PSNR LF-PSNR SSIM LF-SSIM\n4Ã—\n1.5T 36.57 Â± 2.19 27.03 Â± 1.79 0.926 Â± 0.02 0.923 Â± 0.03\n3T 37.10 Â± 2.13 27.87 Â± 1.87 0.939 Â± 0.02 0.932 Â± 0.02\nMix (Ours) 36.94 Â± 2.16 27.64 Â± 1.89 0.935 Â± 0.02 0.929 Â± 0.02\n8Ã—\n1.5T 30.31 Â± 2.33 23.34 Â± 2.32 0.825 Â± 0.03 0.870 Â± 0.05\n3T 30.46 Â± 2.30 23.69 Â± 2.28 0.840 Â± 0.03 0.875 Â± 0.05\nMix (Ours) 30.42 Â± 2.31 23.61 Â± 2.29 0.835 Â± 0.03 0.873 Â± 0.05\npronounced, making the reconstruction task inherently more challeng-\ning. In such scenarios, the carefully designed loss functions can better\nguide the network to prioritize learning features that are more critical\nfor the reconstruction task. These findings further demonstrate the\neffectiveness of our designed specialized loss functions.\nAdditionally, the experimental results in Table 6 demonstrate that\nthe inclusion of perceptual loss slightly improves the quality of the\nreconstructed images. This suggests that incorporating feature con-\nstraints, such as perceptual loss, into the reconstruction task can en-\ncourage the generation of better feature-based CAM to assist lesion-\nenhanced imaging, which further improves the overall image quality.\nIntegrating feature-based losses to train both the classification net-\nwork and the reconstruction network is a viable direction for further\nenhancing the performance of our LF-MRI. These findings highlight\nthe necessity of carefully integrating different loss terms to achieve\nhigh-fidelity, lesion-enhanced fast imaging under label-efficient weak\nsupervision.\n5.4. Application to downstream diagnostic tasks\nTo further validate the value of our method for downstream di-\nagnostic tasks, we conducted a comparative classification of images\nreconstructed by different methods across varying acceleration fac-\ntors and datasets. By using VGG19 as the classification network, the\ncorresponding experimental results are shown in Table 7. From the\nresults, it can be observed that the images reconstructed by our method\nconsistently achieve the highest diagnosis accuracy, regardless of the\nacceleration factors or datasets with different imaging contrasts. It\nsuggests that our method not only produces clear overall reconstructed\nimages but also generates detailed lesion-focused images that provide\nmore effective features for the classification model, thereby improving\nits accuracy. These results imply the practical application value of our\nmethod.\n5.5. Further improvements via learnable ğ‘˜-space sampling\nTo investigate further improvements, we examined the impact of\nintegrating learnable sampling patterns into the LF-MRI framework.\nInspired by Aggarwal and Jacob (2020), we incorporated learnable 1-D\nsampling patterns into our method for lesion-enhanced imaging.\nIn Aggarwal and Jacob (2020), the authors proposed a learnable 2D\nsampling strategy, which learns two separate one-dimensional Fourier\ntransform matrices representing the horizontal and vertical directions,\nrespectively. Considering that the slice experimental data in this study\nrequires a 1D sampling strategy, we utilize a 1D variant of their sam-\npling approach, focusing exclusively on learning the sampling location\nparameters in the vertical direction.\nThe corresponding experimental results in Table 8 demonstrate a\nsignificant enhancement in reconstruction quality by employing learn-\nable sampling patterns, particularly pronounced under high acceler-\nation rates, whereas the improvement is marginal under low accel-\neration rates. This discrepancy can be attributed to the effectiveness\nof traditional fixed sampling methods in acquiring adequate data for\nreconstruction under low acceleration conditions. However, under high\nacceleration rates, fixed sampling methods struggle to capture suf-\nficient frequency domain information, leading to diminished quality\nand clarity in the reconstructed images. In contrast, learnable sam-\npling address this challenge by optimizing sampling strategies and\nlocations, enabling the model to efficiently utilize sparse data, thereby\nboosting reconstruction performance and enhancing image fidelity in\nchallenging scenarios with high acceleration factors. As a conclusion,\ncombining our current LF-MRI implementation with learnable ğ‘˜-space\nsampling techniques could be a promising direction to further improve\ntask-oriented imaging performance under high accelerations.\n5.6. Robustness to varying imaging qualities\nMagnetic field strength (MFS) is one of the critical factors that\ndetermine the quality of MR images. First, to explore the impact of\ndata from different field strengths on our method, Table 9 presents\nthe results of our approach under various acceleration factors and field\nstrengths within the PD dataset. Notably, the fastMRI knee dataset\nincludes data scanned at either 1.5T or 3T. Therefore, in Table 9,\nâ€˜â€˜Mixedâ€™â€™ represents the entire test dataset, which is further categorized\nby field strength into â€˜â€˜1.5Tâ€™â€™ and â€˜â€˜3Tâ€™â€™ for evaluation. The experimental\nresults demonstrate that our method performs well across data of dif-\nferent magnetic field strengths, highlighting its robustness to variations\nin image quality.\nSimilarly, we further investigate the impact of data from vari-\nous scanners on our method. According to the original papers of the\ndatasets, the fastMRI knee data (Zbontar et al., 2018) were acquired\nusing Siemens medical devices operating at either 1.5T or 3T, while\nthe SKM-TEA data (Desai et al., 2022) were all scanned using a GE 3T\nmedical device. As indicated by the previously discussed experimental\nresults, our method achieves the best performance on both the SKM-\nTEA and fastMRI knee datasets. This suggests the robustness of our\nmethod across scanning devices from different vendors. Second, as\ncan be seen from the results in Table 9, our method demonstrates\nMedical Image Analysis 107 (2026) 103806\n11\n\nF. Ju et al.\nFig. 8. Comparison of CAMs generated by our LF-MRI for pathology localiza-\ntion on fastMRI dataset: The left side shows accurate CAM alongside annotated\npathological markers in red, while the right side shows failure CAM cases on\nunseen data, misaligned with the annotated markers in red.\nrobustness to different magnetic field strengths within scanning devices\nfrom the same vendor. These results highlight the robustness and gen-\neralizability of our method with respect to variations in scanners and\nimage quality. Additional evaluations under more heterogeneous clin-\nical environments in the future are desired for a more comprehensive\nevaluation of the practical usage of our LF-MRI method.\n5.7. Limitations and future work\nWhile our LF-MRI method has demonstrated promising results in ac-\ncelerated MR imaging, several limitations necessitate further research.\nFirst, we currently rely on a generic Cartesian sampling trajectory and\nhave not yet developed an algorithm to dynamically adjust the sam-\npling trajectory based on lesion information. Even if made trainable,\nour approach focuses solely on adapting the sampling lines, potentially\nlimiting its ability to optimize sampling for specific imaging tasks.\nSecond, although our model exhibits strong performance in classifying\nand localizing lesions frequently encountered during training, its ro-\nbustness diminishes when faced with unseen or rare lesion scenarios,\nas illustrated in Fig. 8. For commonly encountered lesions such as\nMeniscus Tear or Cartilage-Partial Thickness loss/defect, our model\nachieved a high overlap between CAM localization and ground-truth\nbounding box information. In contrast, less frequently encountered\nlesions, such as LCL Complex-Low-Mod Grade Sprain or Ligament-ACL\nLow Grade Sprain, showed weaker CAM localization capabilities. This\nobservation holds across different contrast types, including PD and\nPDFS. Enhancing the modelâ€™s generalization to rare lesions remains a\ncritical point for improvement.\nTo move forward, in the future, we plan to address these lim-\nitations by exploring cross-domain generalizable anomaly detection\napproaches (Fernando et al., 2021) to better detect and incorporate\nlesion-specific information into our task-oriented imaging process. By\nleveraging anomaly detection, we intend to refine our sampling trajec-\ntory strategy and improve the modelâ€™s ability to handle diverse lesion\nscenarios effectively.\n6. Conclusion\nThis paper have presented a model-unrolled deep learning approach\n(termed LF-MRI), guided by weakly supervised lesion attention tailed,\nfor accelerated multi-coil MRI to meet downstream clinical require-\nments. Specifically, we have developed a lesion-focused MRI recon-\nstruction model uniquely integrating learnable regularizations that can\nbe trained efficiently by employing only image-level labels to realize\nlesion-focused imaging while preserving overall image quality. Then,\nwe have designed an iterative optimization algorithm to solve this\nreconstruction model, deriving it into a multi-stage cascaded deep\nnetwork for lesion-enhanced imaging. Experimental results on public\ndatasets have demonstrate the superior performance of our LF-MRI\ncompared to representative fast MRI methods, particularly in the imag-\ning of specific pathology regions. These findings highlight the efficacy\nof our method in accelerated task-oriented MR imaging and its potential\nto bridge the gap between MR imaging and clinical needs.\nCRediT authorship contribution statement\nFangmao Ju: Investigation, Visualization, Validation, Methodol-\nogy, Writing â€“ original draft. Yuzhu He: Writing â€“ review & editing,\nWriting â€“ original draft, Validation, Methodology. Fan Wang: Writ-\ning â€“ review & editing, Supervision, Formal analysis. Xianjun Li:\nWriting â€“ review & editing, Validation. Chen Niu: Writing â€“ review\n& editing, Validation. Chunfeng Lian: Writing â€“ review & editing,\nSupervision, Methodology, Funding acquisition, Formal analysis, Con-\nceptualization. Jianhua Ma: Writing â€“ review & editing, Supervision,\nResources, Conceptualization.\nDeclaration of competing interest\nThe authors declare that they have no known competing finan-\ncial interests or personal relationships that could have appeared to\ninfluence the work reported in this paper.\nAcknowledgments\nThis work was partly supported by the National Key R&D Pro-\ngram of China under Grant 2022YFA1004200, Natural Science Basic\nResearch Program of Shaanxi (No. 2024JC-TBZC-09), and Shaanxi\nProvincial Key Industrial Innovation Chain Project (No. 2024SF-ZDCYL-\n02-10).\nData availability\nData will be made available on request.\nReferences\nAggarwal, H.K., Jacob, M., 2020. J-MoDL: Joint model-based deep learning for\noptimized sampling and reconstruction. IEEE J. Sel. Top. Signal Process. 14 (6),\n1151â€“1162.\nAggarwal, H.K., Mani, M.P., Jacob, M., 2018. MoDL: Model-based deep learning\narchitecture for inverse problems. IEEE Trans. Med. Imaging 38 (2), 394â€“405.\nCalivÃ¡, F., Leynes, A.P., Shah, R., Bharadwaj, U.U., Majumdar, S., Larson, P.E.,\nPedoia, V., 2020. Breaking speed limits with simultaneous ultra-fast MRI recon-\nstruction and tissue segmentation. In: Medical Imaging with Deep Learning. PMLR,\npp. 94â€“110.\nDesai, A.D., Schmidt, A.M., Rubin, E.B., Sandino, C.M., Black, M.S., Mazzoli, V.,\nStevens, K.J., Boutin, R., RÃ©, C., Gold, G.E., et al., 2022. Skm-tea: A dataset for\naccelerated mri reconstruction with dense image labels for quantitative clinical\nevaluation. arXiv preprint arXiv:2203.06823.\nDonoho, D.L., 2006. Compressed sensing. IEEE Trans. Inform. Theory 52 (4),\n1289â€“1306.\nFernando, T., Gammulle, H., Denman, S., Sridharan, S., Fookes, C., 2021. Deep learning\nfor medical anomaly detectionâ€“a survey. ACM Comput. Surv. 54 (7), 1â€“37.\nFu, R., Hu, Q., Dong, X., Guo, Y., Gao, Y., Li, B., 2020. Axiom-based grad-cam: Towards\naccurate visualization and explanation of cnns. arXiv preprint arXiv:2008.02312.\nGriswold, M.A., Jakob, P.M., Heidemann, R.M., Nittka, M., Jellus, V., Wang, J.,\nKiefer, B., Haase, A., 2002. Generalized autocalibrating partially parallel acqui-\nsitions (GRAPPA). Magn. Reson. Med.: An Off. J. the Int. Soc. Magn. Reson. Med.\n47 (6), 1202â€“1210.\nGuo, P., Mei, Y., Zhou, J., Jiang, S., Patel, V.M., 2023. ReconFormer: Accelerated MRI\nreconstruction using recurrent transformer. IEEE Trans. Med. Imaging.\nHammernik, K., Klatzer, T., Kobler, E., Recht, M.P., Sodickson, D.K., Pock, T., Knoll, F.,\n2018. Learning a variational network for reconstruction of accelerated MRI data.\nMagn. Reson. Med. 79 (6), 3055â€“3071.\nMedical Image Analysis 107 (2026) 103806\n12\n\nF. Ju et al.\nHan, Y., Sunwoo, L., Ye, J.C., 2019. K-space deep learning for accelerated MRI. IEEE\nTrans. Med. Imaging 39 (2), 377â€“386.\nHe, K., Zhang, X., Ren, S., Sun, J., 2016. Deep residual learning for image recog-\nnition. In: Proceedings of the IEEE Conference on Computer Vision and Pattern\nRecognition. pp. 770â€“778.\nHestenes, M.R., Stiefel, E., et al., 1952. Methods of conjugate gradients for solving\nlinear systems. Vol. 49, (1), NBS Washington, DC.\nHuang, J., Fang, Y., Wu, Y., Wu, H., Gao, Z., Li, Y., Del Ser, J., Xia, J., Yang, G., 2022.\nSwin transformer for fast MRI. Neurocomputing 493, 281â€“304.\nHyun, C.M., Kim, H.P., Lee, S.M., Lee, S., Seo, J.K., 2018. Deep learning for\nundersampled MRI reconstruction. Phys. Med. Biol. 63 (13), 135007.\nJin, K.H., McCann, M.T., Froustey, E., Unser, M., 2017. Deep convolutional neural\nnetwork for inverse problems in imaging. IEEE Trans. Image Process. 26 (9),\n4509â€“4522.\nLee, D., Yoo, J., Tak, S., Ye, J.C., 2018. Deep residual learning for accelerated MRI\nusing magnitude and phase networks. IEEE Trans. Biomed. Eng. 65 (9), 1985â€“1995.\nLi, K., Li, H., Anastasio, M.A., 2024. Investigating the use of signal detection information\nin supervised learning-based image denoising with consideration of task-shift. J.\nMed. Imaging 11 (5), 055501â€“055501.\nLi, K., Zhou, W., Li, H., Anastasio, M.A., 2021. A hybrid approach for approximating the\nideal observer for joint signal detection and estimation tasks by use of supervised\nlearning and markov-chain monte carlo methods. IEEE Trans. Med. Imaging 41 (5),\n1114â€“1124.\nLi, K., Zhou, W., Li, H., Anastasio, M.A., 2023. Estimating task-based performance\nbounds for image reconstruction methods by use of learned-ideal observers. In:\nMedical Imaging 2023: Image Perception, Observer Performance, and Technology\nAssessment. 12467, SPIE, pp. 120â€“125.\nLian, C., Liu, M., Wang, L., Shen, D., 2021. Multi-task weakly-supervised attention\nnetwork for dementia status estimation with structural MRI. IEEE Trans. Neural\nNetworks Learn. Syst. 33 (8), 4056â€“4068.\nLiang, D., Liu, B., Wang, J., Ying, L., 2009. Accelerating SENSE using compressed\nsensing. Magn. Reson. Med.: An Off. J. the Int. Soc. Magn. Reson. Med. 62 (6),\n1574â€“1584.\nLustig, M., Donoho, D., Pauly, J.M., 2007. Sparse MRI: The application of compressed\nsensing for rapid MR imaging. Magn. Reson. Med.: An Off. J. the Int. Soc. Magn.\nReson. Med. 58 (6), 1182â€“1195.\nMardani, M., Gong, E., Cheng, J.Y., Vasanawala, S., Zaharchuk, G., Alley, M.,\nThakur, N., Han, S., Dally, W., Pauly, J.M., et al., 2017. Deep generative adversarial\nnetworks for compressed sensing automates MRI. arXiv preprint arXiv:1706.00051.\nPramanik, A., Jacob, M., 2021. Reconstruction and segmentation of parallel MR data\nusing image domain deep-SLR. In: 2021 IEEE 18th International Symposium on\nBiomedical Imaging. ISBI, pp. 1095â€“1098. http://dx.doi.org/10.1109/ISBI48211.\n2021.9434056.\nPruessmann, K.P., Weiger, M., Scheidegger, M.B., Boesiger, P., 1999. SENSE: sensitivity\nencoding for fast MRI. Magn. Reson. Med.: An Off. J. the Int. Soc. Magn. Reson.\nMed. 42 (5), 952â€“962.\nQuan, T.M., Nguyen-Duc, T., Jeong, W.-K., 2018. Compressed sensing MRI reconstruc-\ntion using a generative adversarial network with a cyclic loss. IEEE Trans. Med.\nImaging 37 (6), 1488â€“1497.\nRazumov, A., Rogov, O.Y., Dylov, D.V., 2022a. Optimal MRI undersampling patterns\nfor pathology localization. In: Medical Image Computing and Computer Assisted\nInterventionâ€“MICCAI 2022: 25th International Conference, Singapore, September\n18â€“22, 2022, Proceedings, Part VI. Springer, pp. 768â€“779.\nRazumov, A., Rogov, O.Y., Dylov, D.V., 2022b. Optimal mri undersampling patterns for\npathology localization. In: International Conference on Medical Image Computing\nand Computer-Assisted Intervention. Springer, pp. 768â€“779.\nRonneberger, O., Fischer, P., Brox, T., 2015. U-net: Convolutional networks for biomed-\nical image segmentation. In: Medical Image Computing and Computer-Assisted\nInterventionâ€“MICCAI 2015: 18th International Conference, Munich, Germany,\nOctober 5-9, 2015, Proceedings, Part III 18. Springer, pp. 234â€“241.\nSchlemper, J., Caballero, J., Hajnal, J.V., Price, A.N., Rueckert, D., 2017. A deep\ncascade of convolutional neural networks for dynamic MR image reconstruction.\nIEEE Trans. Med. Imaging 37 (2), 491â€“503.\nSimonyan, K., Zisserman, A., 2014. Very deep convolutional networks for large-scale\nimage recognition. arXiv preprint arXiv:1409.1556.\nSong, J., Chen, B., Zhang, J., 2023. Deep memory-augmented proximal unrolling\nnetwork for compressive sensing. Int. J. Comput. Vis. 131 (6), 1477â€“1496.\nSun, J., Li, H., Xu, Z., et al., 2016. Deep ADMM-Net for compressive sensing MRI. Adv.\nNeural Inf. Process. Syst. 29.\nSzegedy, C., Liu, W., Jia, Y., Sermanet, P., Reed, S., Anguelov, D., Erhan, D., Van-\nhoucke, V., Rabinovich, A., 2015. Going deeper with convolutions. In: Proceedings\nof the IEEE Conference on Computer Vision and Pattern Recognition. pp. 1â€“9.\nUecker, M., Lai, P., Murphy, M.J., Virtue, P., Elad, M., Pauly, J.M., Vasanawala, S.S.,\nLustig, M., 2014. ESPIRiTâ€”an eigenvalue approach to autocalibrating parallel MRI:\nwhere SENSE meets GRAPPA. Magn. Reson. Med. 71 (3), 990â€“1001.\nWeber, T., Ingrisch, M., Bischl, B., RÃ¼gamer, D., 2024. Constrained Probabilistic mask\nlearning for task-specific undersampled MRI reconstruction. In: Proceedings of the\nIEEE/CVF Winter Conference on Applications of Computer Vision. pp. 7665â€“7674.\nWu, K., Du, B., Luo, M., Wen, H., Shen, Y., Feng, J., 2019. Weakly supervised brain\nlesion segmentation via attentional representation learning. In: Medical Image\nComputing and Computer Assisted Interventionâ€“MICCAI 2019: 22nd International\nConference, Shenzhen, China, October 13â€“17, 2019, Proceedings, Part III 22.\nSpringer, pp. 211â€“219.\nWu, Z., Yin, T., Sun, Y., Frost, R., van der Kouwe, A., Dalca, A.V., Bouman, K.L.,\n2023. Learning task-specific strategies for accelerated MRI. arXiv preprint arXiv:\n2304.12507.\nYang, G., Yu, S., Dong, H., Slabaugh, G., Dragotti, P.L., Ye, X., Liu, F., Arridge, S.,\nKeegan, J., Guo, Y., et al., 2017. DAGAN: deep de-aliasing generative adversarial\nnetworks for fast compressed sensing MRI reconstruction. IEEE Trans. Med. Imaging\n37 (6), 1310â€“1321.\nZbontar, J., Knoll, F., Sriram, A., Murrell, T., Huang, Z., Muckley, M.J., Defazio, A.,\nStern, R., Johnson, P., Bruno, M., et al., 2018. FastMRI: An open dataset and\nbenchmarks for accelerated MRI. arXiv preprint arXiv:1811.08839.\nZhang, J., Ghanem, B., 2018. ISTA-Net: Interpretable optimization-inspired deep net-\nwork for image compressive sensing. In: Proceedings of the IEEE Conference on\nComputer Vision and Pattern Recognition. pp. 1828â€“1837.\nZhao, R., Yaman, B., Zhang, Y., Stewart, R., Dixon, A., Knoll, F., Huang, Z., Lui, Y.W.,\nHansen, M.S., Lungren, M.P., 2021. fastMRI+: Clinical pathology annotations for\nknee and brain fully sampled multi-coil MRI data. arXiv preprint arXiv:2109.03812.\nMedical Image Analysis 107 (2026) 103806\n13",
    "version": "5.3.31"
  },
  {
    "numpages": 11,
    "numrender": 11,
    "info": {
      "PDFFormatVersion": "1.7",
      "Language": "en-US",
      "EncryptFilterName": null,
      "IsLinearized": true,
      "IsAcroFormPresent": false,
      "IsXFAPresent": false,
      "IsCollectionPresent": false,
      "IsSignaturesPresent": false,
      "Creator": "Elsevier",
      "Custom": {
        "CrossMarkDomains[1]": "elsevier.com",
        "CrossmarkMajorVersionDate": "2010-04-23",
        "CreationDate--Text": "5th September 2023",
        "ElsevierWebPDFSpecifications": "7.0",
        "robots": "noindex",
        "doi": "10.1016/j.bspc.2023.105158",
        "CrossMarkDomains[2]": "sciencedirect.com",
        "CrossmarkDomainExclusive": "true"
      },
      "ModDate": "D:20230905121037Z",
      "Author": "Yuhan Xie",
      "Title": "Detect, Grow, Seg: A weakly supervision method for medical image segmentation based on bounding box",
      "Keywords": "Weakly supervised medical image segmentation,GradCAM,Adaptive regional growth,UNet,YoloV5",
      "CreationDate": "D:20230905115403Z",
      "Producer": "Acrobat Distiller 8.1.0 (Windows)",
      "Subject": "Biomedical Signal Processing and Control, 86 (2023) 105158. doi:10.1016/j.bspc.2023.105158"
    },
    "metadata": {
      "crossmark:crossmarkdomains": "elsevier.comsciencedirect.com",
      "crossmark:crossmarkdomainexclusive": "true",
      "crossmark:doi": "10.1016/j.bspc.2023.105158",
      "crossmark:majorversiondate": "2010-04-23",
      "dc:format": "application/pdf",
      "dc:identifier": "10.1016/j.bspc.2023.105158",
      "dc:publisher": "Elsevier Ltd",
      "dc:description": "Biomedical Signal Processing and Control, 86 (2023) 105158. doi:10.1016/j.bspc.2023.105158",
      "dc:subject": [
        "Weakly supervised medical image segmentation",
        "GradCAM",
        "Adaptive regional growth",
        "UNet",
        "YoloV5"
      ],
      "dc:title": "Detect, Grow, Seg: A weakly supervision method for medical image segmentation based on bounding box",
      "dc:creator": [
        "Yuhan Xie",
        "Zhiyong Zhang",
        "Shaolong Chen",
        "Changzhen Qiu"
      ],
      "jav:journal_article_version": "VoR",
      "pdf:creationdate--text": "5th September 2023",
      "pdf:producer": "Acrobat Distiller 8.1.0 (Windows)",
      "pdf:keywords": "Weakly supervised medical image segmentation,GradCAM,Adaptive regional growth,UNet,YoloV5",
      "pdfx:creationdate--text": "5th September 2023",
      "pdfx:crossmarkdomains": "sciencedirect.comelsevier.com",
      "pdfx:crossmarkdomainexclusive": "true",
      "pdfx:crossmarkmajorversiondate": "2010-04-23",
      "hionimm39otn_zdygnwv.mcnnzgmqlwj-mgegndf8y.ylmdv.ztamo9eqn9iknm-sn9itma": "",
      "pdfx:doi": "10.1016/j.bspc.2023.105158",
      "pdfx:robots": "noindex",
      "prism:aggregationtype": "journal",
      "prism:copyright": "Â© 2023 Elsevier Ltd. All rights reserved.",
      "prism:coverdate": "2023-09-01",
      "prism:coverdisplaydate": "1 September 2023",
      "prism:doi": "10.1016/j.bspc.2023.105158",
      "prism:issn": "1746-8094",
      "prism:number": "PA",
      "prism:pagerange": "105158",
      "prism:publicationname": "Biomedical Signal Processing and Control",
      "prism:startingpage": "105158",
      "prism:url": "https://doi.org/10.1016/j.bspc.2023.105158",
      "prism:volume": "86",
      "xmp:createdate": "2023-09-05T11:54:03",
      "xmp:creatortool": "Elsevier",
      "xmp:metadatadate": "2023-09-05T12:10:37",
      "xmp:modifydate": "2023-09-05T12:10:37",
      "xmpmm:documentid": "uuid:1b499bed-4ce8-4c0c-b682-0d58baae1cbe",
      "xmpmm:instanceid": "uuid:b6ed5266-03cb-4855-90c0-636283357f42",
      "xmprights:marked": "True"
    },
    "text": "Biomedical Signal Processing and Control 86 (2023) 105158\nAvailable online 20 June 2023\n1746-8094/Â© 2023 Elsevier Ltd. All rights reserved.\nContents lists available at ScienceDirect\nBiomedical Signal Processing and Control\njournal homepage: www.elsevier.com/locate/bspc\nDetect, Grow, Seg: A weakly supervision method for medical image\nsegmentation based on bounding box\nYuhan Xie, Zhiyong Zhang, Shaolong Chen, Changzhen Qiu \nâˆ—\nSchool of Electronics and Communication Engineering, Sun Yat-sen University, Guangzhou, China\nA R T I C L E I N F O\nKeywords:\nWeakly supervised medical image\nsegmentation\nGradCAM\nAdaptive regional growth\nUNet\nYoloV5\nA B S T R A C T\nWeakly supervised semantic segmentation based on bounding box has been fully developed in natural scenes,\nbut in medical image scenes it often faces the difficulties of blurred edges, imaging noise, and mutual\ninterference between multiple organs, which makes the methods suitable in natural scenes tend to show\npoor performance in medical images. In this paper, we propose a weakly supervised segmentation method\nfor medical images based on Detection, Growth, and Segmentation (DGS). For weak labels in the form of\nbounding boxes, the detector YOLOV5 and GradCAM are introduced to obtain the heatmap representation,\nwhile the additional unlabeled data can be also utilized through the detector. The thresholded heatmap is\nused to generate seed points for adaptive regional growth so that better pseudo labels are obtained. Finally,\nthe pseudo labels are used for supervised training on UNet to obtain end to end segmentation. Sufficient\nexperiments show that, compared with other methods, the pseudo label generation and segmentation model\nformed by our method achieve the best performance, while the use of unlabeled data by the detector brings\nsignificant improvement for segmentation in low-label scenarios, which greatly reduce the time and demand\nof labeling.\n1. Introduction\nWith the development of deep learning, medical image segmenta-\ntion algorithms based on fully supervision [1â€“4] have been adequately\nresearched. However, insufficient datasets and the lack of high-quality\nlabels are two major difficulties in building fully supervised models\nin the medical domain. Now some methods contribute to generate\nmedical images for augmenting datasets through generative methods,\nsuch as [5,6]. However, high quality segmentation annotation in the\nmedical field often requires experienced doctors to spend a lot of time\nto produce, which is still really time-consuming. How to free doctors\nfrom tedious labeling tasks and obtain a segmentation model with good\nperformance by only relying on few concise annotation is an urgent\ntask.\nThe weakly supervised semantic segmentation adopts different type\nof weak labels to solve this problem: such as image-level annota-\ntion [7,8], point [9], scribble [10â€“13], bounding box [14â€“17], etc.\nAlthough image-level annotation is simple enough, the information it\ncan provide is too limited, so in practical application the accuracy\nof the model is not satisfactory. As a type of weak label with clear\nboundary information but simple enough to draw, the bounding box\nhas great advantages in practical applications. In natural scenes, clear\nboundaries and color information enables some traditional algorithms\nâˆ— \nCorresponding author.\nE-mail addresses: xieyh23@mail2.sysu.edu.cn (Y. Xie), qiuchzh@mail.sysu.edu.cn (C. Qiu).\nsuch as GrabCut [18] or DenseCRF [19] to be used to obtain good\npseudo labels based on bounding box. However, in medical images, it\nis often necessary to faced with blurred edges, complex imaging noise,\nand mutual interference between multiple organs. In DenseCRF, its\nunary potential energy and binary potential energy are highly related\nto the texture of the image, while the texture of the medical images\nis usually blurred. There may also be texture inside a certain organ,\nwhich is different from natural images. Therefore, we see that the\nsegmentation results of DenseCRF often appear discontinuous. GrabCut\nrelies heavily on color information, and medical images only contain\ngrayscale information, which makes the color space crowded after\nconversion to BGR, so it is less effective. All these difficulties make it\ndifficult to directly obtain the region of the target organs even though\nwe have the accurate annotation of the bounding box.\nThere are two key points in weakly supervised semantic segmenta-\ntion: how to obtain sufficiently realistic pseudo labels based on weak\nlabels, and how to train a sufficiently good segmentation model based\non noisy supervision information. We believe that these two points\nare equally important. In the tasks based on the bounding box, most\nwork focuses on the optimization of the second point, while the first\npoint faces many difficulties in the scene of medical images, which has\nnot been well studied and solved. We believe that in medical image,\nhttps://doi.org/10.1016/j.bspc.2023.105158\nReceived 21 October 2022; Received in revised form 14 May 2023; Accepted 8 June 2023\n\nBiomedical Signal Processing and Control 86 (2023) 105158\n2\nY. Xie et al.\nthe main reliable information during pseudo label generation is the\ngrayscale value rather than texture or color. Regional growth is a good\nmethod for segmentation based on grayscale value, but it is sensitive\nto seed points and threshold. In this background, it motivates us to\nuse GradCAM and adaptive threshold to get reliable seed points and\nsuitable threshold.\nTherefore, we propose a weakly supervised method combining de-\ntection, growth, and segmentation. It trains a weak label detector\nfor bounding box, and obtains the heatmap representation of the tar-\nget through GradCAM to maximize the semantic information brought\nby the bounding box. The weak label detector can detect the extra\nunlabeled data at the same time and includes it into pseudo-label gen-\neration, which can make up for the shortcomings of insufficient labeled\ndata. Based on the thresholded heatmap, we perform adaptive regional\ngrowth to make up for the incomplete positioning of the heatmap,\nso as to obtain better quality pseudo labels. Finally, the segmentation\nmodel is obtained by supervised training based on pseudo-labels so that\nan end to end segmentation solution is provided . Overall, our main\ncontributions are as follows:\nContributions:\n1. We point out the difference between medical images and natural\nimages in weakly supervised segmentation. Based on this discovery, we\nintroduce the detector YOLOV5 into weakly supervised segmentation\nand obtain the semantic representation of the target area through a\nproposed multiscale fusion method of GradCAM. The detector can also\nbe used to utilize additional unlabeled data.\n2. Based on the GradCAM heatmap and grayscale information,\nan adaptive regional growth is proposed to obtain more complete\npseudo labels. Supervised training based on pseudo-labels is then per-\nformed to obtain segmentation models, which provides an end to end\nsegmentation.\n3. With sufficient experiments, our method achieves top-performing\npseudo labels on ACDC and Synapse datasets. The performance of its\nsegmentation model is better than other SOTA methods. In the case of\nless labeled data, the introduction of the detector allows the unlabeled\ndata to be fully utilized and a significant performance improvement is\nachieved.\n2. Related work\n2.1. Traditional methods in semantic segmentation\nBefore deep learning, there are many traditional algorithms for\nimage segmentation. Although they are not as robust as deep learning-\nbased methods, they also have good results in restricted scenarios, and\nthese methods may be an important dependency for weakly supervised\nsemantic segmentation. These methods can be summarized as segmen-\ntation methods based on threshold, edge [20], region [21,22], graph\ntheory [18,23,24] and energy functional [25,26]. Among them, the\nostu and canny operators are the classic methods for extracting the\nthreshold and the edge respectively; and the regional-based methods\nmainly include the regional growth and the watershed. The regional\ngrowth regards segmentation as a process of gradually spreading from\nseed points, which is sensitive to seed point selection and boundary\nconditions. The representative methods of graph theory mainly include\nGrabCut [18], Random walk [24], etc. GrabCut needs to select the\ntarget area and then complete the segmentation through the mixed\nGaussian modeling(GMM). The energy functional method is derived\nfrom the development of the active contour model, in which the\nSnake [26] model and the level set are the representative work. The\nlevel set regards the image segmentation as the motion process of\nthe geometric curve, and its advantage is that it can deal with the\ntopological change of the curve.\n2.2. Weakly Supervised Semantic Segmentation (WSSS)\nThe weak label forms used for semantic segmentation mainly in-\nclude: image-level labels [7,8,27], points [9], scribbles [10â€“13], and\nbounding boxes [14â€“17]. Most of the methods based on image-level\nlabels generate class activation maps (CAM) [28] according to the\nclassification model, and then generate regions of interest for subse-\nquent segmentation. Although this method requires least annotation\ntime, it provides the weakest information, so its performance is not\nsatisfactory. The methods based on points or scribbles can describe the\nshape of target but are lack of strict boundary information, so that the\nperformance is generally slightly worse than the method based on the\nbounding boxes. For methods with bounding boxes supervision, there\nare mainly two type of pipelines: one is the pesudo label based method\nand the other is pesudo label free method. Pesudo label based methods\nuse traditional algorithms such as GrabCut [18], DenseCRF [19] or\nMCG [29] to directly obtain pseudo-labels, and then design the training\nof segmentation models based on pseudo-labels and bounding boxes.\nRepresentative works include BCM [14], Box2Seg [17], etc. Pesudo\nlabel free methods simply use bounding box as supervision without\ngenerating pesudo mask, which simplify the pipeline and try to guide\nthe model training by designing losses. Representative works include\nBoxInst [30], Box2Mask [31], etc.\n3. Methods\n3.1. Overall structure\nAs shown in Fig. 1, our proposed DGS method mainly consists of\nthree parts: detection, growth, and segmentation. In the detection part,\na weak label detector is introduced for the bounding boxes; and then\nwe conduct GradCAM on detector to generate a heatmap representing\nthe rough shape of target organ. In the growth part, we uses the\nhighly thresholded heatmap as seed points. According to the grayscale\ninformation in the bounding box, adaptive regional growth is carried\nout to obtain pseudo labels. In the segmentation part, pseudo labels\nare used for supervised training on UNet to obtain an end to end\nsegmentation solution.\n3.2. Weak label based detector\nAs shown in Fig. 2, by introducing a weak label detector, the seman-\ntic information provided by weak labels can be mined to the maximum\nextent through the GradCAM heatmap. The additional unlabeled data\ncan also be fully utilized to provide positioning information, which\ncan then be incorporated into the subsequent pseudo label generation\nprocess to improve the data utilization and reduce the number of\nannotations required. Here we use the concise and efficient end-to-\nend detection model YoloV5 for detection. Considering that there are\nmultiple detector heads for different target sizes, we upsample the\nGradCAM heatmaps of multiple heads to the original image size and\naverage to get the final heatmap. In this way, we can avoid the miss\ndetection of the heatmap due to the change of target size. Its formula\ncan be expressed as Formula (1):\nâ„ğ‘’ğ‘ğ‘¡ğ‘šğ‘ğ‘ =\nâˆ‘\nğ‘\nğ‘–=1 \nğ‘ˆ ğ‘(ğºğ‘Ÿğ‘ğ‘‘ğ¶ğ´ğ‘€(ğ‘™ğ‘ğ‘¦ğ‘’ğ‘Ÿ\nğ‘–\n))\nğ‘ \n(1)\nwhere ğ‘ is 3 in YoloV5 for there are three detector heads for different\nscale. After that, we crop the heatmap with the bounding box, then\nnormalize and threshold it to obtain the initial seed points with high\nconfidence. Since the seed points need to be accurate enough, we\nuniformly use a high confidence threshold of 0.8. Considering that\nthere may be positioning errors in the heat map, we use grayscale\ninformation to remove some seed points: we firstly obtain the OSTU\nthreshold for the image in the bounding box, then the seed points below\nthe threshold are eliminated to ensure the accuracy of seed points. Since\n\nBiomedical Signal Processing and Control 86 (2023) 105158\n3\nY. Xie et al.\nFig. 1. Overall structure of our DGS method. The preprocess includes cropping, normalization, thresholdization. The postprocess includes choosing maximum connected domain.\nFig. 2. Pesudo label generation. When cropping heatmap for existing weakly annotated data, we use the bounding boxes provided by weak labels. When cropping heatmap for\nextra unlabeled data, we use the bounding boxes generated by detector.\nOSTU pursues the minimization of inter-class variance, this threshold\nis a severe condition for target organ. The formulas can be described\nas Formula (2), (3):\nğ‘Ÿğ‘’ğ‘ğ‘¡â„ğ‘’ğ‘ğ‘¡ğ‘šğ‘ğ‘ = ğ‘ğ‘œğ‘Ÿğ‘šğ‘ğ‘™ğ‘–ğ‘§ğ‘’(ğ‘…ğ‘’ğ‘ğ‘¡(â„ğ‘’ğ‘ğ‘¡ğ‘šğ‘ğ‘)) (2)\nğ‘Ÿğ‘’ğ‘ğ‘¡â„ğ‘’ğ‘ğ‘¡ğ‘šğ‘ğ‘\nâ€²\n(ğ‘¥, ğ‘¦) =\n{\n1, ğ‘Ÿğ‘’ğ‘ğ‘¡â„ğ‘’ğ‘ğ‘¡ğ‘šğ‘ğ‘(ğ‘¥, ğ‘¦) â‰¥ ğœƒ\n0, ğ‘Ÿğ‘’ğ‘ğ‘¡â„ğ‘’ğ‘ğ‘¡ğ‘šğ‘ğ‘(ğ‘¥, ğ‘¦) < ğœƒ \n(3)\nwhere Rect denotes cropping with bounding box and ğœƒ is set to be 0.8.\nFrom the perspective of using unlabeled data, there are two meth-\nods: one is to use the detector to detect the unlabeled data, and the\nother is to use the segmenter to segment the unlabeled data. The\ndifference is shown in Fig. 3. We have conducted experiments about\nthis two methods in scenarios that extra unlabeled data can be utilized.\nResults are shown in next section, which proves detector is a better\nchoice. Segmenter based method is a one stage method that has already\nintroduced label annotation errors in the training process, so the noise\ncan affect the whole process in inference. Detector based method is a\ntwo stage method that keeps detector trained without label annotation\nerror, so the error influence range of ARG is reduced. Besides, in scenar-\nios with only a few annotated labels, the localization(get bounding box)\nand segmentation capabilities (get mask) of the segmenter are greatly\nreduced, while detector keeps good localization performance and the\nsegmentation capabilities is not affected for ARG is not a data-demand\napproach but a traditional method.\nSolutions for missing or wrongly-detected bounding box: for unla-\nbeled data, the quality of the bounding box is mainly reflected by the\nconfidence of the detector. In order to ensure that the bounding boxes\nof the newly added unlabeled data is accurate enough, we discard the\ndetection results with low confidence (This helps removing mis-located\nbounding boxes). For the missing bounding box, we cannot determine\nwhether it is missing or has no target. Since there are a large number\n\nBiomedical Signal Processing and Control 86 (2023) 105158\n4\nY. Xie et al.\nFig. 3. The difference between two methods that utilizing extra unlabeled data.\nFig. 4. Overall structure of UNet used in our method.\nof non-targeted slices in the dataset, simply discarding all the slices\nwithout detection targets will lose rich information. So we uniformly\ntreat it as untargeted, and the gain brought by this process will be\ngreater than the error caused by the detector losing bounding box(see\nFigs. 2, 10 and 11).\n3.3. Adaptive regional growth (ARG)\nAfter the initial seed points are obtained from the detector heat map,\nbased on the grayscale information of the image in the bounding box,\nwe use the bounding box as the boundary to perform adaptive regional\ngrowth to obtain more accurate pseudo-labels. Although we understand\nthat using the error correction ability of the model can gradually obtain\na more accurate model through iterative training in some cases, it is\ncumbersome and time-consuming. Therefore, we seek to be accurate\nenough in the pseudo-label generation process, and try to simplify the\noverall complexity for weak supervision method.\nBefore regional growth, we minimize the inter-class variance\nthrough OSTU to obtain the average gray level of foreground and back-\nground. According to the average gray level of foreground, background\nand the seed points, we set the stopping condition of adaptive region\n\nBiomedical Signal Processing and Control 86 (2023) 105158\n5\nY. Xie et al.\ngrowth as:\n|\n|\nğ‘ƒ\nğ‘›ğ‘’ğ‘¤ \nâˆ’ ğ‘ƒ\nÌ„ğ‘  \n|\n| \nâ‰¤ ğœ€ (4)\nğ‘ƒ\nğ‘›ğ‘’ğ‘¤ \nâ‰¥ \nğ‘ƒ\nÌ„\nğ‘“ \n+ ğ‘ƒ\nÌ„\nğ‘\n2 \n(5)\nğœ€ = (ğ‘ƒ\nÌ„\n \nğ‘“ \nâˆ’ ğ‘ƒ\nÌ„\n \nğ‘\n) Ã— ğ‘Ÿğ‘ğ‘¡ğ‘–ğ‘œ (6)\nwhere ğ‘ƒ\nğ‘›ğ‘’ğ‘¤ \ndenotes the grayscale of new point in growing, ğ‘ƒ\nÌ„ğ‘  \ndenotes\nthe average grayscale of seeds, ğ‘ƒ\nÌ„\n \nğ‘“ \nand ğ‘ƒ\nÌ„\nğ‘ \ndenote the average grayscale\nof foreground and background. The union of Formula (4) and For-\nmula (5) constitute the boundary conditions for growth. We notice that\nmost medical images show transition states at the boundary without\nclear edges, and the transition state pixels between the foreground av-\nerage grayscale and the background average grayscale are highly likely\nto belong to the foreground, which is due to medical image imaging.\nThis mechanism results in different characteristics from natural images,\nso the ratio should be set large enough within the range from 0 to 0.5\n(when ratio is larger than 0.5, the pixels of background will be included\ninto growth). In both ACDC and Synapse datasets, we set the ratio to\nbe 0.4. More analysis on ratio will be shown in ablation experiments.\nUsing the mean value of the seed points as the reference value can avoid\nthe inaccurate judgment of the grayscale mutation due to the absence of\nobvious edges and ensure the robustness of the algorithm. The absolute\nvalue limit and the conditional superposition of threshold judgment can\nbe combined with the advantages of the thresholding algorithm and\nthe growth algorithm, making the growth more accurate. For ACDC\ndataset which have many holes within organs, we conduct continuous\nexpansion and erosion to remove voids in post-processing. The pseudo\ncode of the pseudo label generation algorithm is shown in Algorithm\n1.\nConsidering that there are multiple targets in the same slice, after\ncompleting the adaptive regional growth, we deal with the possible\noverlapping between pesudo labels of different targets as follows: We\ncalculate the grayscale distance between the overlapping pixels and\ndifferent central region ğ‘ƒ\nÌ„ğ‘  \nand the pixels are assigned to the closer\nclasses. Since the localization of GradCAM in the target edge area is\nnot ideal, we do not use the class probability information provided by\nGradCAM heatmap when class conflicts.\n3.4. Segmentation\nAfter obtaining good enough pseudo-labels, we use pseudo-labels\nfor supervised training of the segmentation model. This is because al-\nthough the pseudo-label generation algorithm can obtain segmentation\nresults that are close enough to real labels, it needs to rely on manually\nannotated boxes to obtain boundary constraints, and the entire process\nis not an end-to-end solution. The training of the segmentation model\ncan get rid of the dependence on manually annotated bounding boxes\nand achieve an end-to-end, streamlined segmentation scheme.\nAs for the segmentation model, we adopt the classic UNet structure.\nThe overall structure of UNet can be shown in Fig. 4. The input\nand output of the UNet we use are both of 224*224 size, where the\nencoder part contains 5 double convolution blocks (similar to VGG)\nand 4 downsampling; the decoder part contains 4 double deconvolution\nblocks and 4 upsampling. Each double convolutional block contains\ntwo convolutional layers, and the convolutional layer consists of a 3*3\nconv, a batch normalization, and a activation function ReLU. we adopt\nDice and BCE loss for co-supervision. The k represents any point in\nthe image, and ğ‘¦\nğ‘˜\n,Ì‚ ğ‘¦ \nğ‘˜ \nrepresent the groundtruth and prediction result\nrespectively. The loss function can be shown in Formula (7).\nğ‘™ğ‘œğ‘ ğ‘  = \nâˆ‘\nğ‘˜\n(\nâˆ’ \n1\n2 \nğ‘¦\nğ‘˜\nğ‘™ğ‘œğ‘”Ì‚ğ‘¦ \nğ‘˜ \n+ 1 âˆ’ \n2 Ã— ğ‘¦\nğ‘˜ \nÃ—Ì‚ğ‘¦ \nğ‘˜\nğ‘¦\nğ‘˜\n+Ì‚ğ‘¦ \nğ‘˜\n)\n(7)\nAlgorithm 1: Pesudo label generation algorithm\nInput: Bounding box: ğ‘†\nğ‘Ÿğ‘’ğ‘ ğ‘¡ğ‘Ÿğ‘–ğ‘ğ‘¡ğ‘’ğ‘‘\nOutput: Pesudo label:ğ‘ƒ\nğ‘”ğ‘Ÿğ‘œğ‘¤ğ‘›\n1 GradCAM mapping: detector backpropagation as shown in\nFormula (1).\n2 Pre-process: cropping, normalization, thresholdization of\nheatmap ar conducted as shown in Formula (2), (3) to get\nseed points ğ‘ƒ\nğ‘ ğ‘’ğ‘’ğ‘‘ğ‘ \n.\n3 Initialization: grown points set ğ‘ƒ\nğ‘”ğ‘Ÿğ‘œğ‘¤ğ‘› \n= ğ‘ƒ\nğ‘ ğ‘’ğ‘’ğ‘‘ğ‘ \n; directions set\nğ‘‘ğ‘–ğ‘Ÿğ‘’ğ‘ğ‘¡ğ‘–ğ‘œğ‘›ğ‘  =\n(âˆ’1, âˆ’1), (0, âˆ’1), (1, âˆ’1), (1, 0), (1, 1), (0, 1), (âˆ’1, 1), (âˆ’1, 0);\n4 Regional growth:\n5 while ğ‘ƒ\nğ‘ ğ‘’ğ‘’ğ‘‘ğ‘  \nâ‰  âˆ… do\n6 ğ‘ƒ\nğ‘ ğ‘’ğ‘’ğ‘‘ \n= ğ‘ƒ\nğ‘ ğ‘’ğ‘’ğ‘‘ğ‘ \n[0];\n7 for direction in directions do\n8 ğ‘ƒ\nğ‘›ğ‘’ğ‘¤ \n= ğ‘ƒ\nğ‘ ğ‘’ğ‘’ğ‘‘ \n+ ğ‘‘ğ‘–ğ‘Ÿğ‘’ğ‘ğ‘¡ğ‘–ğ‘œğ‘›;\n9 if (|ğ‘ƒ \nğ‘›ğ‘’ğ‘¤ \nâˆ’ ğ‘ƒ\nÌ„ğ‘  \n| â‰¤ ğœ€ ğ‘œğ‘Ÿğ‘ƒ\nğ‘›ğ‘’ğ‘¤ \nâ‰¥ \nğ‘ƒ\nÌ„\nğ‘“ \n+ğ‘ƒ\nÌ„\n \nğ‘\n2 \n)ğ‘ğ‘›ğ‘‘ ğ‘ƒ\nğ‘›ğ‘’ğ‘¤ \nâˆˆ\nğ‘†\nğ‘Ÿğ‘’ğ‘ ğ‘¡ğ‘Ÿğ‘–ğ‘ğ‘¡ğ‘’ğ‘‘ \nğ‘ğ‘›ğ‘‘ ğ‘ƒ\nğ‘›ğ‘’ğ‘¤ \nâˆ‰ ğ‘ƒ\nğ‘”ğ‘Ÿğ‘œğ‘¤ğ‘› \nthen\n10 ğ‘ƒ\nğ‘ ğ‘’ğ‘’ğ‘‘ğ‘  \n= ğ‘ƒ\nğ‘ ğ‘’ğ‘’ğ‘‘ğ‘  \nâˆª ğ‘ƒ\nğ‘›ğ‘’ğ‘¤\n;\n11 ğ‘ƒ\nğ‘”ğ‘Ÿğ‘œğ‘¤ğ‘› \n= ğ‘ƒ\nğ‘”ğ‘Ÿğ‘œğ‘¤ğ‘› \nâˆª ğ‘ƒ\nğ‘›ğ‘’ğ‘¤\n;\n12 end\n13 end\n14 end\n15 Post-process: Choosing maximum connected domain is\nconducted for regional growth result. For ACDC dataset that\nhas many holes inside organs, continuous expansion and\nerosion are conducted to remove voids. The choosing of\nmaximum connected domain is also conducted for all datasets.\n16 Return: ğ‘ƒ\nğ‘”ğ‘Ÿğ‘œğ‘¤ğ‘›\n.\nFig. 5. Pipeline difference compared with other bounding box based WSSS methods.\n3.5. Pipeline difference compared with other bounding box based WSSS\nmethods\nAs shown in Fig. 5, the bounding box based WSSS methods can\nbe mainly divided into two types: pseudo label based methods and\npseudo free methods. The pseudo free methods generate model simply\nby bounding box based loss guidance. The typical method like BoxInst\nuses color based affinity loss and Box2Mask uses edge based energy\nloss. However in medical images, the lack of color attribute and the\nblurred edges may limit these methods that are useful in natural\ncolorful images. The pseudo based methods provide appropriate prior\n\nBiomedical Signal Processing and Control 86 (2023) 105158\n6\nY. Xie et al.\ninformation for segmentation training by pseudo image generation.\nMost methods like BCM and Box2Seg, simply use the traditional method\nfor the stage 1 and these methods work on natural images are not robust\nenough in medical images as well. Our method use the pseudo label\nbased pipeline and focus on the stage 1, in order to provide a concise\nand robust pipeline in medical field. The multiscale GardCAM provided\nby detector can show the semantic representation of the target area,\nand the adaptive regional growth can generate pseudo labels under the\nguidance of the semantic representation and grayscale information.\n4. Experiments\n4.1. Dataset\nACDC [32]: This is a dataset for the segmentation task of left atrium\nand right atrium. It contains 3D MRI images of 100 scanned cases and\ntheir human-annotated segmentation labels. The slice thickness varies\nfrom 5 to 8 mm, while the short-axis in-plane spatial resolution goes\nfrom 0.83 to 1.75 mm\n2\n. The width of each slice varies from 154 to 428\npixel and the length of each slice varies from 154 to 512 pixel. Each\nMRI volume consists of 8 to 12 slices.\nSynapse: This is a CT scan dataset for multi-organ segmentation\ntasks, which includes 3D CT images of 30 scan cases and human-\nannotated segmentation labels. The spatial resolution is 0.54 and 0.98\nmm\n2 \nin ğ‘¥ and ğ‘¦ dimension, while the vertical resolution varies from\n2.5 to 5.0 mm\n2\n. The width and length of each slice is 512 pixel and\n512 pixel. Each CT volume consists of 85 to 198 slices.\n4.2. Implementation details and evaluation metrics\nOur approach is implemented in Python with PyTorch and run\non four RTX 3070 card. During training of detection model YoloV5,\nthe batch size is set to be 8, the optimizer is SGD. The input image\nresolution is 256*256 pixel for ACDC and 224*224 pixel for Synapse.\nFor ACDC, each input image is padded into 256*256, while each input\nimage is resized to be 224*224 for Synapse.\nDuring training of segmentation model UNet, We choose ACDC [32]\nand Synapse dataset for experiment. We divide ACDC into training and\ntesting sets by 3:1 and divide Synapse into training and testing sets by\n3:2. In training process, the batch size is set to 8, and the optimizer is\nadam. On the ACDC dataset, the learning rate is set to 1eâˆ’4, and the\ntraining lasts for 100 epochs. On the Synapse dataset, the learning rate\nis set to 1eâˆ’4, and the training lasts for 50 epochs. In order to ensure\nthe reliability of the experimental results, we repeated the experiments\nfor each category 5 times and obtained the average of the experimental\nresults.\nFor the training of both detector and segmentor,we split the dataset\ninto training and testing set as the same ratio. (That is 75 training\nsamples, 25 testing samples in ACDC, while 18 training samples, 12\ntesting samples in Synapse. When in few label condition, only 9 training\nsamples, 3 testing samples of ACDC are provided, while 6 training\nsamples, 4 testing samples of Synapse are provided.)\nFor the preprocessing of ACDC and Synapse datasets, ACDC is\npadded to 256*256, and Synapse is resized to 224*224. We perform\nmaximum and minimum normalization on ACDC before entering the\nnetwork, and performs windowing processing on Synapse. The formulas\nare as follows:\nğ¼(ğ‘¥, ğ‘¦) = \nğ¼(ğ‘¥, ğ‘¦) âˆ’ ğ¼\nğ‘šğ‘–ğ‘›\nğ¼\nğ‘šğ‘ğ‘¥ \nâˆ’ ğ¼\nğ‘šğ‘–ğ‘›\n(8)\nğ¼(ğ‘¥, ğ‘¦) =\nâ§\nâª\nâ¨\nâª\nâ©\nğ‘”\nmax\n, ğ¼(ğ‘¥, ğ‘¦) â‰¥ ğ‘”\nmax\nğ¼(ğ‘¥, ğ‘¦), ğ‘”\nmin \nâ‰¤ ğ¼(ğ‘¥, ğ‘¦) â‰¤ ğ‘”\nmax\nğ‘”\nmin\n, ğ¼(ğ‘¥, ğ‘¦) â‰¤ ğ‘”\nmin\n(9)\nwhere ğ¼(ğ‘¥, ğ‘¦) denotes the grayscale of point (x,y), Imax and Imin denote\nthe maximum value and minimum value of an image. The maximum\nTable 1\nDSC(%) results on pseudo label generation.MC represents choosing maximum connected\ndomain,Box denotes simply covering all the area within bounding box,RG denotes\nregional growth with center point as seed and ARG denotes adaptive regional growth\nwith center point as seed. For synapse dataset, we only count the slices that have target\nsince empty slices will narrow the gap between different methods.\nMethods \nACDC Synapse\nRV LV RK LK\nGrabcut [18] 22.57 31.12 39.19 31.74\nDenseCRF [19] 68.44 82.24 78.26 82.52\nOSTU+MC 72.37 80.80 82.15 81.18\nGradCAM [33] 75.61 85.62 75.38 76.26\nBox 73.37 85.56 76.92 76.19\nRG 75.91 87.48 76.02 73.24\nARG 76.96 88.61 77.56 77.83\nGradCAM+RG 79.43 88.03 82.67 82.91\nGradCAM+ARG 84.42 90.83 84.81 85.79\nof CT window ğ‘”\nmax \nis equal to 275, the minimum of CT window ğ‘”\nmin \nis\nequal to âˆ’75.\nIn the experiments, we mainly use DSC (%), Precision (%), and\nRecall (%) to evaluate the segmentation results, and use DSC evaluation\nfor pseudo-labels.\nğ‘…ğ‘’ğ‘ğ‘ğ‘™ğ‘™ = \nğ‘‡ ğ‘ƒ\nğ‘‡ ğ‘ƒ + ğ¹ ğ‘ \n(10)\nğ‘ƒ ğ‘Ÿğ‘’ğ‘ğ‘–ğ‘ ğ‘–ğ‘œğ‘› = \nğ‘‡ ğ‘ƒ\nğ‘‡ ğ‘ƒ + ğ¹ ğ‘ƒ \n(11)\nğ·ğ‘†ğ¶ = \n2|ğ‘‹ âˆ© ğ‘Œ |\n|ğ‘‹| + |ğ‘Œ | \n(12)\nwhere TP denotes the true positive prediction result, FN denotes the\nfalse negative prediction result, FP denotes the false positive prediction\nresult. X denotes the prediction while Y denotes the label.\n4.3. Results\n4.3.1. Pseudo label generation results\nAccording to the results in Table 1, it can be seen that our pseudo-\nlabel generation method achieved the highest DSC results: 84.42%,\n90.83%, 84.81%, and 85.79% correspond to the four different target\norgans of RV (right ventricle), LV (left ventricle), RK (right kidney),\nand LK (left kidney), respectively. Compared with the DSC results of\nsimply filling the whole bounding box, the combination of GradCAM\nand adaptive region growth improves the DSC of pseudo-labels by\n11.05%, 5.27%, 7.89%, and 9.6% for RV, LV, RK, and LK respectively.\nFrom the performance of other methods, it can be seen that the methods\nGrabcut or DenseCRF, which perform better in natural images, have\npoor DSC performance for generating pseudo-labels, which also shows\nthat the methods applicable in natural images are not necessarily\napplicable in medical images; The pseudo-label results obtained using\nonly region growth or just GradCAM are also given in Table 1, which\nfully demonstrates the effectiveness of our method.\n4.3.2. Comparison of SOTA WSSS segmentation methods\nTable 2 and Table 3 show the segmentation results compared with\nother WSSS methods. Considering that most other WSSS methods are\naimed at natural images and the structure of UNet is more suitable in\nmedical images, so we uniformly use the same UNet like backbone for\nother WSSS methods. It should be emphasized that under the premise\nof the same backbone, we retain the improvement of the loss function\nand decoder of other WSSS methods. The pseudo label generation of\nother WSSS methods keeps the same as mentioned in their papers.\nFig. 6 shows the box and whisker plot of four datasets. According to\nthe results in Tables 2 and 3, our method DGS outperforms other SOTA\nWSSS methods based on bounding boxes in multiple indicators, which\nalso proves that those WSSS methods applicable to natural images is not\n\nBiomedical Signal Processing and Control 86 (2023) 105158\n7\nY. Xie et al.\nFig. 6. The box and whisker plot on ACDC(LV,RV) and Synapse(LK,RK) dataset.\nTable 2\nSegmentation results on ACDC dataset. The mean and standard deviation of results are shown as percentages (%).\nMethods RV LV\nDSC Precision Recall DSC Precision Recall\nBCM [14] 68.75 Â± 0.77 68.33 Â± 0.70 69.48 Â± 78 82.89 Â± 0.86 84.20 Â± 0.81 82.70 Â± 0.83\nBox2Seg [17] 21.29 Â± 1.43 22.36 Â± 1.48 23.75 Â± 1.37 32.80 Â± 1.58 32.66 Â± 1.46 34.31 Â± 1.67\nBoxInst [30] 78.56 Â± 0.75 80.87 Â± 0.68 76.72 Â± 0.76 86.33 Â± 0.79 86.75 Â± 0.71 84.88 Â± 0.66\nBox2Mask [31] 83.59 Â± 0.49 84.71 Â± 0.55 81.56 Â± 0.41 90.41 Â± 0.50 91.67 Â± 0.54 89.12 Â± 0.56\nDGS 86.31 Â± 0.73 88.28 Â± 0.68 85.31 Â± 0.77 92.75 Â± 0.56 93.78 Â± 0.62 92.02 Â± 0.47\nTable 3\nSegmentation results on Synapse dataset. The mean and standard deviation of results are shown as percentages (%).\nMethods RK LK\nDSC Precision Recall DSC Precision Recall\nBCM [14] 76.24 Â± 0.87 77.51 Â± 0.82 78.72 Â± 0.85 77.27 Â± 0.66 80.24 Â± 0.73 78.78 Â± 0.69\nBox2Seg [17] 36.07 Â± 1.24 36.88 Â± 1.17 37.34 Â± 1.26 28.52 Â± 0.68 29.24 Â± 0.79 30.41 Â± 0.82\nBoxInst [30] 76.32 Â± 0.96 78.78 Â± 0.93 74.23 Â± 1.01 76.81 Â± 0.75 78.06 Â± 0.80 75.16 Â± 0.69\nBox2Mask [31] 77.84 Â± 1.01 81.29 Â± 1.14 76.12 Â± 1.06 78.92 Â± 1.07 79.57 Â± 0.95 77.43 Â± 0.98\nDGS 78.60 Â± 0.74 83.76 Â± 0.79 76.46 Â± 0.71 80.17 Â± 0.89 81.73 Â± 0.78 79.05 Â± 0.88\nnecessarily applicable to medical images. The methods used in pseudo\nlabel generation of BCM and Box2Seg show suboptimal performance,\nwhich lead to the error propagation in stage 2: segmentation training.\nBoxInst use color in formation for affinity loss, but in medical field only\ngrayscale information is provided. Therefore, simply using grayscale\nwith a fixed hyperparameter for affinity loss in medical images is not\nthat useful as it shows in natural images. Meanwhile, the similarity\nbetween different organs may lead to much noise in affinity loss\noptimization. The Box2Mask use edge energy loss for training, but the\nblurred or missing edges may increase the difficulty of loss function\noptimization.\n4.3.3. Results on utilizing unlabeled data\nTable 4 shows the results of weakly supervised segmentation before\nand after using the detector to incorporate unlabeled data. In the case\nTable 4\nDSC (%) results on utilizing unlabeled data. Weak (few) denotes only a few weak\nlabels are provided (10 cases for training in ACDC, 6 cases for training in Synapse),\nReal (few) denotes few real labels are provided. Weak (few expand) denotes only a few\nweak labels are provided, but we expand the dataset with detector detecting unlabeled\ndata.\nMethods \nACDC Synapse\nRV LV RK LK\nWeak (few) 42.72 79.72 61.36 63.45\nReal (few) 47.27 81.65 63.83 65.57\nWeak (few expand) 73.29 91.78 74.65 74.85\nWeak (all) 86.31 92.75 78.60 80.17\nReal (all) 89.38 95.62 81.12 83.15\n\nBiomedical Signal Processing and Control 86 (2023) 105158\n8\nY. Xie et al.\nFig. 7. Comparison on two different methods that utilize extra unlabeled data.\nFig. 8. Ablation on threshold of GradCAM.\nof few labeled data, the detector locates unlabeled data and generates\npseudo-labels, and the segmentation model obtains a DSC gain of\n30.57% on RV, 12.06% on LV, 13.09% on the RK and 11.4% on LK\nrespectively. Compared with the results of full supervision, under the\ncondition of different proportions of training data, the gap between full\nsupervision and weak supervision is maintained at about 3 percentage\npoints, which fully demonstrates the feasibility of our weak supervision\nmethod in practical application. Fig. 7 shows the results of two different\nmethods utilizing extra unlabeled data. IOU indicates the intersection\nover union of bounding box generated from the pseudo mask and\nreal mask. It can be seen that detector-based method outperforms\nsegmentor-based method in both the segmentation and localization\nabilities.\n4.3.4. Results on detector\nIn Table 5 and Table 6, the results of detector on different organs\nand different scenarios are shown. When only very few labeled data\nis given, the performance of detector has a slight decrease but still\nmaintains a decent level. The detector tends to have a high precision\nbut a lower recall. Overall, the Map0.5 is high enough when compared\nwith the detector for most natural scenes, for medical images have a\nfix structure that can be easily recognized.\n4.3.5. Ablation on threshold of GradCAM and ARG\nThe Fig. 8 shows the results of pseudo-labels generated directly\nby GradCAM and the results of pseudo-labels generated by GradCAM\nwith ARG under different thresholds of GradCAM. For the case where\nFig. 9. Ablation on the threshold ratio used in ARG.\nGradCAM directly generates pseudo-labels, due to the high proportion\nof targets in the bounding box, it tends to get the best pseudo-label with\na lower threshold. But from the overall result, it can be seen that the\nDSC of pseudo-label directly generated by GradCAM is not high. When\nusing GradCAM as the seed points for regional growth, since the seed\npoints require a high degree of confidence, a GradCAM with a high\n\nBiomedical Signal Processing and Control 86 (2023) 105158\n9\nY. Xie et al.\nFig. 10. Visualization of pesudo label on ACDC dataset.\nFig. 11. Visualization of pesudo label on Synapse dataset.\nTable 5\nResults of detector on Synapse test dataset.\nOrgans Precision Recall Map0.5 Map0.5-0.95\nFew\nAvg 0.935 0.796 0.893 0.741\nRK 0.951 0.77 0.891 0.722\nLK 0.918 0.821 0.894 0.76\nAll\nAvg 0.992 0.95 0.978 0.885\nRK 0.994 0.944 0.975 0.878\nLK 0.99 0.955 0.98 0.892\nTable 6\nResults of detector on ACDC test dataset.\nOrgans Precision Recall Map0.5 Map0.5-0.95\nFew\nAvg 0.979 0.872 0.93 0.674\nRV 0.972 0.824 0.888 0.582\nLV 0.986 0.92 0.972 0.766\nAll\nAvg 0.985 0.942 0.964 0.784\nRV 0.985 0.914 0.952 0.74\nLV 0.986 0.97 0.976 0.827\n\nBiomedical Signal Processing and Control 86 (2023) 105158\n10\nY. Xie et al.\nthreshold will bring better segmentation results (the optimal threshold\nis around 0.8 by curve analysis). With the help of ARG, the final result\nis actually not sensitive to the threshold of GradCAM and performance\nis satisfactory.\nThe Fig. 9 shows how the threshold ratio in ARG effects the final\nresult of pseudo label. It can be seen that the four organs reach best\nresult at 0.3 or 0.4 and sharply decrease after 0.5. It is because when\nratio reaches 0.5, the boundary condition is almost equivalent to di-\nviding most pixels between the foreground and background determined\nby OSTU into foreground. When ratio is larger than 0.5, the boundary\ncondition incorrectly divides background into foreground, which leads\nto sharply decrease in DSC of pseudo label. The ablation proves that\nthe center gray of foreground and background determined by OSTU\nis accurate enough so that a sharp decrease appear after 0.5. The high\nconsistency of the peaks in Fig. 8 also shows that this is not the result of\nour manual adjustment, which is a common feature unique to medical\nimages. The result of DSC keeps a high level when ratio varies from 0.2\nto 0.5, which proves that our method is not sensitive to the threshold\nratio.\n4.4. Visualization of pseudo label results\nFigs. 10,11 shows the pesudo label generation results by different\nmethods. Our DSG method has best morphological representation and\nboundaries compared with other methods.\n5. Conclusion and future work\nIn this paper, we propose a method for weakly supervised medical\nimages segmentation based on Detection, Growth, and Segmentation\n(DGS). By introducing detector for weak label and conduct GradCAM on\ndetector, we get the heatmap on target; then we used the thresholded\nheatmap as seeds and conduct adaptive regional growth; finally, we\nconduct supervised training for segmentation based on pseudo label.\nExperiments have proved that the DGS method we proposed is better\nthan other methods on many indicators, which greatly reduces the\ntime and demand for marking. However, although the detector-based\nGradCAM result can describe the semantic information of the target,\nits localization accuracy is still not accurate enough, which is why we\nuse adaptive region growth to optimize it. If the context information\nbetween different slices and iterative correction strategy can be utilized\nto help optimize GradCAM result, the DGS performance can be better.\nTherefore, in the future work, we will try to optimize the results of\nGradCAM by using context information and iterative strategy.\nCRediT authorship contribution statement\nYuhan Xie: Conceptualization, Methodology, Software, Writing â€“\noriginal draft, Data curation. Zhiyong Zhang: Visualization, Investiga-\ntion. Shaolong Chen: Software, Validation. Changzhen Qiu: Supervi-\nsion, Funding acquisition, Writing â€“ review & editing.\nDeclaration of competing interest\nThe authors declare that they have no known competing finan-\ncial interests or personal relationships that could have appeared to\ninfluence the work reported in this paper.\nData availability\nData will be made available on request.\nAcknowledgments\nThe author thanks the whole authors in the referred articles. This\nwork was supported in part by the Science and Technology Plan-\nning Project of Guangdong Science and Technology Department under\nGrant Guangdong Key Laboratory of Advanced IntelliSense Technology\n(2019B121203006).\nReferences\n[1] O. Ronneberger, P. Fischer, T. Brox, U-net: Convolutional networks for\nbiomedical image segmentation, in: International Conference on Medical Image\nComputing and Computer-Assisted Intervention, Springer, 2015, pp. 234â€“241.\n[2] Z. Zhou, M.M.R. Siddiquee, N. Tajbakhsh, J. Liang, Unet++: Redesigning skip\nconnections to exploit multiscale features in image segmentation, IEEE Trans.\nMed. Imaging 39 (6) (2019) 1856â€“1867.\n[3] H. Huang, L. Lin, R. Tong, H. Hu, Q. Zhang, Y. Iwamoto, X. Han, Y.W. Chen,\nJ. Wu, Unet 3+: A full-scale connected unet for medical image segmentation,\nin: ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and\nSignal Processing, ICASSP, IEEE, 2020, pp. 1055â€“1059.\n[4] O. Oktay, J. Schlemper, L.L. Folgoc, M. Lee, M. Heinrich, K. Misawa, K. Mori,\nS. McDonagh, N.Y. Hammerla, B. Kainz, et al., Attention u-net: Learning where\nto look for the pancreas, 2018, arXiv preprint arXiv:1804.03999.\n[5] Z. Ren, Y. Zhang, S. Wang, LCDAE: Data augmented ensemble frame-\nwork for lung cancer classification, Technol. Cancer Res. Treat. 21 (2022)\n15330338221124372, PMID: 36148908.\n[6] Z. Ren, Y. Zhang, S. Wang, A hybrid framework for lung cancer classification,\nElectronics 11 (10) (2022).\n[7] Z. Huang, X. Wang, J. Wang, W. Liu, J. Wang, Weakly-supervised semantic\nsegmentation network with deep seeded region growing, in: Proceedings of\nthe IEEE Conference on Computer Vision and Pattern Recognition, 2018, pp.\n7014â€“7023.\n[8] X. Wang, S. You, X. Li, H. Ma, Weakly-supervised semantic segmentation\nby iteratively mining common object features, in: Proceedings of the IEEE\nConference on Computer Vision and Pattern Recognition, 2018, pp. 1354â€“1362.\n[9] A. Bearman, O. Russakovsky, V. Ferrari, L. Fei-Fei, Whatâ€™s the point: Semantic\nsegmentation with point supervision, in: European Conference on Computer\nVision, Springer, 2016, pp. 549â€“565.\n[10] D. Lin, J. Dai, J. Jia, K. He, J. Sun, Scribblesup: Scribble-supervised convolutional\nnetworks for semantic segmentation, in: Proceedings of the IEEE Conference on\nComputer Vision and Pattern Recognition, 2016, pp. 3159â€“3167.\n[11] M. Tang, A. Djelouah, F. Perazzi, Y. Boykov, C. Schroers, Normalized cut loss\nfor weakly-supervised cnn segmentation, in: Proceedings of the IEEE Conference\non Computer Vision and Pattern Recognition, 2018, pp. 1818â€“1827.\n[12] M. Tang, F. Perazzi, A. Djelouah, I. Ben Ayed, C. Schroers, Y. Boykov, On\nregularized losses for weakly-supervised cnn segmentation, in: Proceedings of\nthe European Conference on Computer Vision, ECCV, 2018, pp. 507â€“522.\n[13] B. Wang, G. Qi, S. Tang, T. Zhang, Y. Wei, L. Li, Y. Zhang, Boundary\nperception guidance: A scribble-supervised semantic segmentation approach, in:\nIJCAI International Joint Conference on Artificial Intelligence, 2019.\n[14] C. Song, Y. Huang, W. Ouyang, L. Wang, Box-driven class-wise region masking\nand filling rate guided loss for weakly supervised semantic segmentation, in:\nProceedings of the IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, 2019, pp. 3136â€“3145.\n[15] J. Dai, K. He, J. Sun, Boxsup: Exploiting bounding boxes to supervise con-\nvolutional networks for semantic segmentation, in: Proceedings of the IEEE\nInternational Conference on Computer Vision, 2015, pp. 1635â€“1643.\n[16] M.S. Ibrahim, A. Vahdat, M. Ranjbar, W.G. Macready, Semi-supervised seman-\ntic image segmentation with self-correcting networks, in: Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern Recognition, 2020, pp.\n12715â€“12725.\n[17] V. Kulharia, S. Chandra, A. Agrawal, P. Torr, A. Tyagi, Box2seg: Attention\nweighted loss and discriminative feature learning for weakly supervised seg-\nmentation, in: European Conference on Computer Vision, Springer, 2020, pp.\n290â€“308.\n[18] C. Rother, V. Kolmogorov, A. Blake, â€˜â€˜GrabCutâ€™â€™ interactive foreground extraction\nusing iterated graph cuts, ACM Trans. Graph. 23 (3) (2004) 309â€“314.\n[19] P. KrÃ¤henbÃ¼hl, V. Koltun, Efficient inference in fully connected crfs with gaussian\nedge potentials, Adv. Neural Inf. Process. Syst. 24 (2011).\n[20] L. Ding, A. Goshtasby, On the canny edge detector, Pattern Recognit. 34 (3)\n(2001) 721â€“725.\n[21] R. Harris, Models of regional growth: past, present and future, J. Econ. Surv. 25\n(5) (2011) 913â€“951.\n[22] V.P. Singh, D.K. Frevert, Watershed modeling, in: World Water & Environmental\nResources Congress 2003, 2003, pp. 1â€“37.\n\nBiomedical Signal Processing and Control 86 (2023) 105158\n11\nY. Xie et al.\n[23] S. Vicente, V. Kolmogorov, C. Rother, Graph cut based image segmentation with\nconnectivity priors, in: 2008 IEEE Conference on Computer Vision and Pattern\nRecognition, IEEE, 2008, pp. 1â€“8.\n[24] E.A. Codling, M.J. Plank, S. Benhamou, Random walk models in biology, J. R.\nSoc. Interface 5 (25) (2008) 813â€“834.\n[25] S. Osher, R.P. Fedkiw, Level set methods: an overview and some recent results,\nJ. Comput. Phys. 169 (2) (2001) 463â€“502.\n[26] M. Kass, A. Witkin, D. Terzopoulos, Snakes: Active contour models, Int. J.\nComput. Vis. 1 (4) (1988) 321â€“331.\n[27] Z. Chen, Z. Tian, J. Zhu, C. Li, S. Du, C-CAM: Causal CAM for weakly\nsupervised semantic segmentation on medical image, in: Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern Recognition, 2022, pp.\n11676â€“11685.\n[28] B. Zhou, A. Khosla, A. Lapedriza, A. Oliva, A. Torralba, Learning deep features for\ndiscriminative localization, in: Proceedings of the IEEE Conference on Computer\nVision and Pattern Recognition, 2016, pp. 2921â€“2929.\n[29] P. ArbelÃ¡ez, J. Pont-Tuset, J.T. Barron, F. Marques, J. Malik, Multiscale combi-\nnatorial grouping, in: Proceedings of the IEEE Conference on Computer Vision\nand Pattern Recognition, 2014, pp. 328â€“335.\n[30] Z. Tian, C. Shen, X. Wang, H. Chen, Boxinst: High-performance instance\nsegmentation with box annotations, in: Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, 2021, pp. 5443â€“5452.\n[31] W. Li, W. Liu, J. Zhu, M. Cui, R. Yu, X. Hua, L. Zhang, Box2Mask: Box-\nsupervised instance segmentation via level-set evolution, 2022, arXiv preprint\narXiv:2212.01579.\n[32] O. Bernard, A. Lalande, C. Zotti, F. Cervenansky, X. Yang, P.-A. Heng, I.\nCetin, K. Lekadir, O. Camara, M.A.G. Ballester, et al., Deep learning techniques\nfor automatic MRI cardiac multi-structures segmentation and diagnosis: is the\nproblem solved? IEEE Trans. Med. Imaging 37 (11) (2018) 2514â€“2525.\n[33] R.R. Selvaraju, A. Das, R. Vedantam, M. Cogswell, D. Parikh, D. Batra, Grad-CAM:\nWhy did you say that? 2016, arXiv preprint arXiv:1611.07450.",
    "version": "5.3.31"
  },
  {
    "numpages": 11,
    "numrender": 11,
    "info": {
      "PDFFormatVersion": "1.7",
      "Language": "en-US",
      "EncryptFilterName": null,
      "IsLinearized": true,
      "IsAcroFormPresent": false,
      "IsXFAPresent": false,
      "IsCollectionPresent": false,
      "IsSignaturesPresent": false,
      "Creator": "Elsevier",
      "Custom": {
        "CrossMarkDomains[1]": "elsevier.com",
        "CrossmarkMajorVersionDate": "2010-04-23",
        "CreationDate--Text": "14th May 2024",
        "ElsevierWebPDFSpecifications": "7.0",
        "robots": "noindex",
        "doi": "10.1016/j.bspc.2024.106164",
        "CrossMarkDomains[2]": "sciencedirect.com",
        "CrossmarkDomainExclusive": "true"
      },
      "ModDate": "D:20240514042422Z",
      "Author": "Xinyu Hao",
      "Title": "Predicting pathological complete response based on weakly and semi-supervised joint learning in breast cancer multi-parametric MRI",
      "Keywords": "Weakly-supervised learning,Semi-supervised learning,Attention mechanism,Pathological complete response,Breast cancer",
      "CreationDate": "D:20240514042039Z",
      "Producer": "Acrobat Distiller 8.1.0 (Windows)",
      "Subject": "Biomedical Signal Processing and Control, 93 (2024) 106164. doi:10.1016/j.bspc.2024.106164"
    },
    "metadata": {
      "crossmark:crossmarkdomains": "elsevier.comsciencedirect.com",
      "crossmark:crossmarkdomainexclusive": "true",
      "crossmark:doi": "10.1016/j.bspc.2024.106164",
      "crossmark:majorversiondate": "2010-04-23",
      "dc:format": "application/pdf",
      "dc:identifier": "10.1016/j.bspc.2024.106164",
      "dc:publisher": "Elsevier Ltd",
      "dc:description": "Biomedical Signal Processing and Control, 93 (2024) 106164. doi:10.1016/j.bspc.2024.106164",
      "dc:subject": [
        "Weakly-supervised learning",
        "Semi-supervised learning",
        "Attention mechanism",
        "Pathological complete response",
        "Breast cancer"
      ],
      "dc:title": "Predicting pathological complete response based on weakly and semi-supervised joint learning in breast cancer multi-parametric MRI",
      "dc:creator": [
        "Xinyu Hao",
        "Hongming Xu",
        "Nannan Zhao",
        "Tao Yu",
        "Timo Hamalainen",
        "Fengyu Cong"
      ],
      "jav:journal_article_version": "VoR",
      "pdf:creationdate--text": "14th May 2024",
      "pdf:producer": "Acrobat Distiller 8.1.0 (Windows)",
      "pdf:keywords": "Weakly-supervised learning,Semi-supervised learning,Attention mechanism,Pathological complete response,Breast cancer",
      "pdfx:creationdate--text": "14th May 2024",
      "pdfx:crossmarkdomains": "sciencedirect.comelsevier.com",
      "pdfx:crossmarkdomainexclusive": "true",
      "pdfx:crossmarkmajorversiondate": "2010-04-23",
      "zifnom.n8n.z-z9ignt_8y8nnnwyolt77mmmgmm37m9emzt38mtypo9eqn9iknm-sntetma": "",
      "pdfx:doi": "10.1016/j.bspc.2024.106164",
      "pdfx:robots": "noindex",
      "prism:aggregationtype": "journal",
      "prism:copyright": "Â© 2024 Elsevier Ltd. All rights reserved.",
      "prism:coverdate": "2024-07-01",
      "prism:coverdisplaydate": "1 July 2024",
      "prism:doi": "10.1016/j.bspc.2024.106164",
      "prism:issn": "1746-8094",
      "prism:pagerange": "106164",
      "prism:publicationname": "Biomedical Signal Processing and Control",
      "prism:startingpage": "106164",
      "prism:url": "https://doi.org/10.1016/j.bspc.2024.106164",
      "prism:volume": "93",
      "tdm:policy": "https://www.elsevier.com/tdm/tdmrep-policy.json",
      "tdm:reservation": "1",
      "xmp:createdate": "2024-05-14T04:20:39",
      "xmp:creatortool": "Elsevier",
      "xmp:metadatadate": "2024-05-14T04:24:22",
      "xmp:modifydate": "2024-05-14T04:24:22",
      "xmpmm:documentid": "uuid:1b499bed-4ce8-4c0c-b682-0d58baae1cbe",
      "xmpmm:instanceid": "uuid:b6ed5266-03cb-4855-90c0-636283357f42",
      "xmprights:marked": "True"
    },
    "text": "Biomedical Signal Processing and Control 93 (2024) 106164\nAvailable online 28 February 2024\n1746-8094/Â© 2024 Elsevier Ltd. All rights reserved.\nContents lists available at ScienceDirect\nBiomedical Signal Processing and Control\njournal homepage: www.elsevier.com/locate/bspc\nPredicting pathological complete response based on weakly and\nsemi-supervised joint learning in breast cancer multi-parametric MRI\nXinyu Hao \na,b,c\n, Hongming Xu \na,b,d,\nâˆ—\n, Nannan Zhao \ne\n, Tao Yu \ne\n, Timo Hamalainen \nc\n,\nFengyu Cong \na,b,c,d\na \nAffiliated Cancer Hospital, Dalian University of Technology, Dalian 116024, China\nb \nSchool of Biomedical Engineering, Faculty of Medicine, Dalian University of Technology, Dalian 116024, China\nc \nFaculty of Information Technology, University of JyvÃ¤skylÃ¤, 40014 JyvÃ¤skylÃ¤, Finland\nd \nKey Laboratory of Integrated Circuit and Biomedical Electronic System, Liaoning Province, Dalian University of Technology, Dalian 116024, China\ne \nCancer Hospital of Dalian University of Technology (Liaoning Cancer Hospital & Institute), China\nA R T I C L E I N F O\nKeywords:\nWeakly-supervised learning\nSemi-supervised learning\nAttention mechanism\nPathological complete response\nBreast cancer\nA B S T R A C T\nNeoadjuvant chemotherapy (NAC) is the primary treatment used to reduce the tumor size in early breast\ncancer. Patients who achieve a pathological complete response (pCR) after NAC treatment have a significantly\nhigher five-year survival rate. However, accurately predicting whether patients could achieve pCR remains\nchallenging due to the limited availability of manually annotated MRI data. This study develops a weakly\nand semi-supervised joint learning model that integrates multi-parametric MR images to predict pCR to NAC\nin breast cancer patients. First, the attention-based multi-instance learning model is designed to characterize\nthe representation of multi-parametric MR images in a weakly supervised learning setting. The Mean-Teacher\nlearning framework is then developed to locate tumor regions for extracting radiochemical parameters in\na semi-supervised learning setting. Finally, all extracted MR imaging features are fused to predict pCR to\nNAC. Our experiments were conducted on a cohort of 442 patients with multi-parametric MR images and\nNAC outcomes. The results demonstrate that our proposed model, which leverages multi-parametric MRI data,\nprovides the AUC value of over 0.85 in predicting pCR to NAC, outperforming other comparative methods.\n1. Introduction\nBreast cancer is the most common malignancy among women and\nthe second most lethal cancer after lung cancer [1]. It exhibits the\nfastest growth rate in developed countries and has a young age of onset,\nposing significant risks to womenâ€™s physical and emotional health.\nNeoadjuvant chemotherapy (NAC) is considered as a standard treat-\nment option for stage II-III breast cancer patients [2], and those who\nachieve a pathological complete response (pCR) or show no residual\nlesions after NAC treatment have demonstrated longer disease-free\nsurvival [3]. However, the significant heterogeneity of breast cancer\nresults in varied responses to the same chemotherapy regimen, render-\ning NAC ineffective in approximately 20% of cases [4]. This lack of\nsensitivity can lead to delays in local treatment and prolonged overall\ntreatment duration. Therefore, it is imperative to predict the treat-\nment response of breast cancer patients prior to NAC for determining\npersonalized treatment plans [5].\nThe effectiveness of NAC is currently assessed primarily through\ninvasive and delayed pathological examinations of postoperative spec-\nimens. However, this approach lacks the ability to predict the response\nâˆ— \nCorresponding author at: School of Biomedical Engineering, Faculty of Medicine, Dalian University of Technology, Dalian 116024, China.\nE-mail address: mxu@dlut.edu.cn (H. Xu).\nto NAC at early stages and is limited by its invasiveness and time delay,\nprimarily related to tissue specimen collection and slide examination.\nAs a result, non-invasive breast imaging examinations are of great value\nin evaluating and predicting the NAC efficacy. Commonly-used imaging\ntests include mammography, ultrasonography, and MR imaging, with\nthe latter being preferred due to its high sensitivity and ability to\nprovide morphological information about breast lesions [6â€“8].\nSeveral current studies have demonstrated that dynamic contrast-\nenhanced magnetic resonance imaging (DCE-MRI) has great potential\nin evaluating the therapeutic effect of breast cancer because of its high\nimaging resolution and dynamic information, which are not readily\navailable through other imaging methods [9â€“11]. Despite its ability\nto reflect hemodynamic changes in tumors, the prediction of pCR\nusing DCE-MRI alone remains challenging. To address this limitation,\ndiffusion-weighted imaging (DWI) has emerged as a promising tech-\nnique. DWI enables the quantitative measurement of water molecule\ndiffusion outside tumor cells, facilitating the monitoring of dynamic\nhttps://doi.org/10.1016/j.bspc.2024.106164\nReceived 27 July 2023; Received in revised form 6 February 2024; Accepted 24 February 2024\n\nBiomedical Signal Processing and Control 93 (2024) 106164\n2\nX. Hao et al.\nchanges in tumor cell density. Notably, in the early stages after neoad-\njuvant chemotherapy (NAC), the apparent diffusion coefficient (ADC)\nvalue of the lesion may increase. Consequently, several studies have\nsuggested that DWI imaging can identify potential alternative biomark-\ners for diagnosis and evaluation of treatment response [12â€“14]. In\nrecent years, more and more evidence shows that the combination\nof DCE-MR imaging and DWI seems to perform better in treatment\noutcome prediction than using either of them alone [15,16]. Conse-\nquently, our study aims to assess the effectiveness of multi-parametric\nMR images (a.k.a., multi-contrast MRI) in accurately predicting the\nresponse to NAC with improved reliability.\nExisting studies for predicting NAC response in breast cancer mainly\nrely on radiomic features, which extract a multitude of quantitative fea-\ntures from regions of interest (ROIs) in multi-modal medical images to\nestablish the relationship between features and potential pathological\noutcomes [17]. However, the use of different ROI delineation meth-\nods and measurements by various doctors may result in inconsistent\nanalysis results, which introduce significant human intervention and\nlack repeatability [18,19]. To address these challenges, researchers are\nincreasingly exploring the application of deep learning methods for\nanalyzing medical images. These studies include intelligent screening\nfor breast abnormalities in digital mammography [20,21], breast cancer\nclassification [22], and predicting the pathological response of breast\ncancer patients to neoadjuvant therapy [23â€“26]. Recent studies have\ndemonstrated that the performance of deep learning models in predict-\ning pCR can be enhanced by incorporating and fusing multi-parametric\nMR image features [27].\nPrevious studies [23,24] require radiologists to annotate a large\namount of data to train tumor segmentation models for extracting\ntumorous features and making classifications. However, manual anno-\ntation is a labor-intensive and time-consuming task, particularly in the\nmedical image analysis domain, where obtaining high-quality pixel-\nlevel annotations is extremely challenging. While some studies [25,26]\nhave explored approaches that do not rely on manual or automatic\ntumor segmentation, they have not taken into account the specific\ncharacteristics of tumor areas. Nevertheless, some studies [28,29] have\nreported that combining deep learning and radiomic features can lead\nto improved performance in predicting treatment response for cancer\npatients.\nMotivated by previous studies, in this work, we design a weakly\nand semi-supervised joint learning model that combines attentional MR\nimaging features and radiomic features of tumor regions for predict-\ning pCR of breast cancer patients. First, weakly supervised learning\nbased on only patient-level treatment information is employed, which\ndistinguished NAC responders from non-responders by using multi-\nparametric MRI and also determined key MRI slices for subsequent\ntumor segmentation. Here, the attention-based multi-instance learn-\ning is designed to train several parallel models that do not share\nweighting parameters, so as to obtain attentional features from multi-\nparametric MRI. The parallel network separately processes each of the\nmulti-parametric MRI inputs, and features are then fused through a con-\ncatenation module before inputting into the final classifier. Secondly,\na semi-supervised tumor segmentation model using the Mean-Teacher\nframework is developed, which can detect tumor regions in key MRI\nslices and then extract radiological characteristics of the tumor. To\nthe best of our knowledge, this is the first study to predict NAC\nresponse in breast cancer multi-parametric MRI based on weakly and\nsemi-supervised learning.\nThe following sections are organized as follows. Section 2 presents\ndetails of the proposed method, which is followed by evaluations in\nSection 3. The conclusion is finally provided in Section 4.\n2. Proposed technique\nIn this paper, we propose a deep learning framework that lever-\nages weakly and semi-supervised joint learning to predict pCR to\nNAC in breast cancer patients. The multi-parametric MRIs, includ-\ning T1-weighted image (T1WI), T2-weighted image (T2WI), dynamic\ncontrast-enhanced MRI (DCE-MRI), and diffusion-weighted MRI (DWI-\nMRI), are used as inputs for our proposed method. Fig. 1 shows the\nflowchart of our proposed method. As observed in Fig. 1, we design\nan attention-based multiple instance learning (MIL) module that effec-\ntively learns patient-level imaging features from multi-parametric MRI,\nenabling learning from weakly labeled data. The attention-based MIL\nmodule also generates attention scores for all MRI slices. It is assumed\nthat MRI slices with the highest attention scores across MRI volumes\ncontain valuable information for pCR prediction. To further exploit this\nattentive information, we introduce a semi-supervised tumor segmen-\ntation module for extracting radiomic features from informative MRI\nslices (i.e., with the highest attention scores) to enhance the specific\ncharacterization of the tumor. Note that since MRI slices are unlabeled,\nthe Mean-Teacher model is developed to generate soft pseudo-labels in\ndetecting tumor regions in MRI slices. The details of our method are\npresented as follows.\n2.1. Problem formulation\nLet ğ·\nğ¹ \n, ğ·\nğ‘Š \nand ğ·\nğ‘ˆ \ndenote the fully-labeled data (i.e., pixel-level\ntumor region annotations and patient-level labels), weakly-labeled data\n(i.e., only patient-level labels) and unlabeled data (i.e., label-free train-\ning data used in the semi-supervised learning block), respectively. As\ndemonstrated in Fig. 1, our joint learning framework comprises three\nprimary components: a weakly supervised learning and key instance\nmining block to learn from ğ·\nğ‘Š \n, a semi-supervised learning block to\nlearn from ğ·\nğ¹ \nand ğ·\nğ‘ˆ \n, and a joint learning block that could integrate\nclinical information as well.\n2.2. Attention-MIL based weakly-supervised learning\nTo learn from ğ·\nğ‘Š \n, we developed an attention-based MIL network\nto generate attentional MRI features and identify the key slice that\ncontains informative tumorous features within the breast MR imaging\nvolumes. In the MIL context, we consider MRI volumes as distinct â€˜bagsâ€™\nand slices within each MRI volume as its â€˜instancesâ€™. pCR or non-pCR\nlabels are only assigned to MRI volumes, while labels for individual\nMRI slices are unknown. Fig. 2 shows our designed multi-modal fusion\nmethod which integrates multi-parametric MRI data. In Fig. 2, our\nattention-based MIL network architecture is highlighted by the blue\nrectangle. Table 1 lists architecture details of our designed attention-\nbased MIL model. The model is used to generate bag-level feature\nrepresentations and select key MRI slices simultaneously.\nAssume that the MRI volume has a dimension of ğ¶ Ã— ğ» Ã— ğ‘Š , where\nğ¶, ğ» and ğ‘Š correspond to image channel number (i.e., the number of\ninstances), height and width, respectively. Given an input MR imaging\nvolume, a backbone CNN block ğ¹\nğœƒ \n(e.g., ResNet-like architecture) with\npretrained weights from ImageNet is adopted as a feature extractor.\nThe ğ¹\nğœƒ \nconverts every 2D MRI slice (i.e., instance) to a scale-specific\nfeature representation ğ‘’\nğ‘–ğ‘— \n(e.g., 512 Ã— 8 Ã— 8), where ğ‘– is the instance\nindex of the MRI bag and ğ‘— is the bag index of the dataset. Every\nğ‘’\nğ‘–ğ‘— \nin the ğ‘—ğ‘¡â„ bag is then fed to the spatial attention module ğ¹\nğœƒ,ğ‘† \nto\nlearn spatial feature representations, which outputs a spatial attention\nmap ğ‘ \nğ‘–ğ‘— \nof size 1 Ã— ğ»\nâ‹† \nÃ— ğ‘Š \nâ‹† \n(i.e., 1 Ã— 8 Ã— 8) for every instance.\nWe improved the spatial attention module based on the Ref. [30].\nSpecifically, given an input ğ‘’\nğ‘–ğ‘— \n, we concurrently perform the average\npooling and max pooling in the channel dimension of ğ‘’\nğ‘–ğ‘— \nto obtain two\nchannel descriptions, and then concatenate two descriptions together\nalong the channel direction. Two 7 Ã— 7 convolutional layers with tanh\nand sigmoid activation function, respectively, are then utilized, which\ngenerates a single channel output (1 Ã— 8 Ã— 8) reflecting the presence of\ntumor regions. The spatial attention map ğ‘ \nğ‘–ğ‘— \nand instance representa-\ntion ğ‘’\nğ‘–ğ‘— \nare performed element-wise multiplication to generate instance\nrepresentation ğ‘“\nğ‘–ğ‘— \noutput from spatial attention block.\n\nBiomedical Signal Processing and Control 93 (2024) 106164\n3\nX. Hao et al.\nFig. 1. Overview of our proposed method. Using weakly-supervised learning, we develop an attention-based MIL approach towards achieving accurate and explainable prediction\nof pCR to NAC from multi-parametric MRI. Using semi-supervised learning, we employ the Mean-Teacher model to guide the U-Net for tumor segmentation in key MRI slices. The\nbag-level MRI features and radiomic features are integrated for joint learning to make final pCR predictions. Note that clinical variables in the shaded block, as evaluated in our\nablation study, could also be integrated in the framework.\nFig. 2. Overview of our weakly-supervised learning module. (a) Illustration of learning multi-parametric MRI features by using our attention-based MIL model which is highlighted\nby the blue rectangle. (b) Spatial attention and instance attention blocks; (c) Network components. Note that this figure illustrates detail implementation of weakly supervised\nlearning and key instance mining shown in Fig. 1 (see the red dashed box).\nTable 1\nNetwork architecture of our designed attention-based MIL model.\nBlock name Numbers Layers Parameter setting Output\nConv1 1 {conv} [7 Ã— 7, 64] 64 Ã— 128 Ã— 128\nMax pooling 1 {Max-pooling} [3 Ã— 3] 64 Ã— 64 Ã— 64\nResidual Block1 2 {conv, batchnorm, ReLU} [3 Ã— 3, 64] 64 Ã— 64 Ã— 64\nResidual Block2 2 {conv, batchnorm, ReLU} [3 Ã— 3, 128] 128 Ã— 32 Ã— 32\nResidual Block3 2 {conv, batchnorm, ReLU} [3 Ã— 3, 256] 256 Ã— 16 Ã— 16\nResidual Block4 2 {conv, batchnorm, ReLU} [3 Ã— 3, 521] 512 Ã— 8 Ã— 8\nSpatial Attention Block 1 {conv, Max-pooling, Avg-pooling, Tanh, Sigmoid} [7 Ã— 7, 1] 512 Ã— 8 Ã— 8\nInstance Attention Block 1 {fc, Tanh, Sigmoid} [500, 1] 1 Ã— 500\na \n, 1 Ã— ğ¶\nb\nClassification Block 1 {fc, Avg-pooling, Softmax} [500 Ã— 1, 1] 1 Ã— 1\na \nRepresents output dimension of the instance attention module.\nb \nRepresents attention weights of the instance attention module, where ğ¶ represents the number of instances.\nSince the instance representation ğ‘“\nğ‘–ğ‘— \nhas a relatively high dimension\n(e.g., 512 Ã— 8 Ã— 8) that is inconvenient to perform subsequent instance\nattention calculation, we add a fully connected layer to map it to a\nnew representation â„\nğ‘–ğ‘— \n, with the dimension of 1 Ã— 500. For simplicity,\nlet us denote all instance feature representations of the ğ‘—ğ‘¡â„ MRI bag as\nğ»\nğ‘— \n= {â„\n1\n; â„\n2\n; â€¦ , â„\nğ¶ \n}. In order to aggregate all instance feature maps\nğ»\nğ‘— \n, a second module ğ¹\nğœƒ,ğ¼ \n, named the instance attention module, is\nimplemented to obtain a single bag-level representation ğ‘§ = ğ¹\nğœƒ,ğ¼ \n(ğ»\nğ‘— \n),\n\nBiomedical Signal Processing and Control 93 (2024) 106164\n4\nX. Hao et al.\nFig. 3. Visualizing examples of spatial attention block. Note that spatial attention\nalways focuses on or near breast regions, as tumors are mostly distributed within these\nregions. (Yellow indicates a higher attention value.).\nwhere ğ‘§ has the dimension of 1 Ã— 500. The instance attention module\nğ¹\nğœƒ,ğ¼ \nshares the same design architecture as the spatial attention module\nğ¹\nğœƒ,ğ‘† \n, with the exception that all convolutional layers are replaced by\nfully connected layers due to applying attention to instance embed-\ndings. Then, we employ attention based pooling proposed by [31] to\ncompute a weighted bag-level representation by using the following\nformula:\nğ‘§ =\nğ¶\nâˆ‘\nğ‘›=1\nğ‘\nğ‘›\nâ„\nğ‘› \n(1)\nwhere,\nğ‘\nğ‘› \n= \nexp \n{\nğ‘¤\nğ‘‡ \n(\ntanh \n(\nğ•â„\nğ‘‡\nğ‘›\n) \nâŠ™ sigm \n(\nğ”â„\nğ‘‡\nğ‘›\n))}\nâˆ‘\nğ¶\nğ‘=1 \nexp\n{\nğ‘¤\nğ‘‡\n(\ntanh\n(\nğ•â„\nğ‘‡\nğ‘\n)\nâŠ™ sigm\n(\nğ”â„\nğ‘‡\nğ‘\n))} \n(2)\nwhere ğ‘¤ âˆˆ R\nğ¶Ã—1\n, ğ‘‰ âˆˆ R\nğ¶Ã—ğ· \nand ğ‘ˆ âˆˆ R\nğ¶Ã—ğ· \nare trainable parameters.\nğ‘¡ğ‘ğ‘›â„() and ğ‘ ğ‘–ğ‘”ğ‘š() are non-linear activation functions, and âŠ™ represents\nelement-wise multiplication. Notably, ğ‘\nğ‘› \nis regarded as the attention\nscore for each instance, showing how much each instance contributes\nto the bag-level representation. Note that the maximum attention score\n(i.e., ğ‘šğ‘ğ‘¥{ğ‘\nğ‘›\n}) will be used as the second output of this block, and\nits corresponding MRI slice will be used in subsequent semi-supervised\nlearning for tumor segmentation and radiomic feature extraction. We\nassume that slices with the highest attention scores may encompass\nlarger tumor regions, potentially providing more valuable information\nfor predicting patient treatment responses. From a technical perspec-\ntive, attention-based pooling automatically assesses the contribution of\nevery single instance to the entire bag, avoiding the subjectivity in\nhuman selection. Thus, the pooled bag-level representation gains the\nability to focus on key MRI slices (e.g., the most informative tumor\ninstances). The synergistic combination of spatial and instance atten-\ntions helps the model to learn more robust features and deliver reliable\ninterpretation. Fig. 3 shows visualizing examples of spatial attention\nblock. It is clearly observed from Fig. 3 that spatial attention always\nfocuses on or near breast regions, as tumors are mostly distributed\nwithin these regions.\nAs shown in Fig. 2, since the multi-parametric MRI is fully explored\nfor pCR prediction, the attention-based MIL models are trained in a\nparallel manner with multi-parametric MRI as inputs. Each independent\nbranch has the same network structure, but they do not share weighting\nparameters. The aggregated features (i.e., ğ‘ğ‘œğ‘›ğ‘ğ‘ğ‘¡(ğ‘§\nğ‘š\n)) from all branches\nare fused in a concatenated manner and then input into the final clas-\nsifier ğ¹\nğ‘ \nto make the patient-level prediction, e.g.,Ì‚ ğ‘¦ \nğ‘– \n= ğ¹\nğ‘ \n(ğ‘ğ‘œğ‘›ğ‘ğ‘ğ‘¡(ğ‘§\nğ‘š\n)),\nwhere ğ‘š is the index of multi-parametric MRI volumes for the ğ‘–ğ‘¡â„\npatient, andÌ‚ ğ‘¦ is the probability of achieving full physicochemical re-\nsponse (pCR) or not (non-pCR) for the corresponding patient. Formally,\nwe employ the cross-entropy loss îˆ¸\nğ‘¤\n(Ì‚ğ‘¦ \nğ‘–\n, ğ‘¦\nğ‘–\n) to train the classifier ğ¹\nğ‘ \n, that\nis:\nîˆ¸\nğ‘¤ \n= âˆ’\nğ‘\nâˆ‘\nğ‘–=1\nğ‘¦\nğ‘– \nlog \n(\nÌ‚ğ‘¦ \nğ‘–\n) \n(3)\nwhere ğ‘ indicates the number of patients in training dataset.\n2.3. Mean-teacher based semi-supervised learning\nMedical image segmentation based on fully-supervised learning re-\nquires strongly labeled training data. However, generating strongly\nlabeled data via meticulously annotating all ROIs typically involves sig-\nnificant labor costs and subjective judgments from humans. Therefore,\nthere is usually a limited amount of labeled data in clinical settings,\nbut it has a large volume of unlabeled data. In light of the small\nnumber of patients with tumor annotations in our dataset, we design a\nsemi-supervised tumor segmentation model that is capable of utilizing\nunlabeled data effectively.\nNote that the slice with the maximum attention score (i.e., ğ‘šğ‘ğ‘¥{ğ‘\nğ‘›\n})\nestimated by the attention-based MIL block is used as the unlabeled\ndata to be learned. To learn from unlabeled data, we employ the\nprinciple of Mean-Teacher model [32] to train a semi-supervised tumor\nsegmentation network. Fig. 4 illustrates our developed semi-supervised\ntumor segmentation framework. As observed in Fig. 4(a), the Mean-\nTeacher model consists of two sub-models: the student and teacher\nmodels, both of which have the same architecture (i.e., Res-UNet\narchitecture [33]), but they do not share weighting parameters. The\nğœ‚ in Fig. 4 indicates the data augmentation applied to the training MR\nimage ğ‘¥\nğ‘–\n, including random horizontal flip, vertical flip, scaling, and\nrotation.\nIn our semi-supervised learning, both the labeled data (i.e., MRI\nslices with tumor annotations) and unlabeled data (i.e., only MRI slices)\nare used, which are formulated as:\nğ·\nL \n= \n{\nğ‘¥\nL\nğ‘– \n, ğ‘¦\nL\nğ‘–\n}\nğ‘\nL\nğ‘–=1 \n(4)\nğ·\nU \n= \n{\nğ‘¥\nU\nğ‘–\n}\nğ‘\nğ‘ˆ\nğ‘–=1 \n(5)\nwhere ğ‘\nL \nand ğ‘\nU \ndenote the number of labeled and unlabeled samples,\nrespectively. The labeled data trains the student model through the\nsupervised loss function, meanwhile both labeled data and unlabeled\ndata train the student model through the unsupervised consistency\nloss function. Let us denote the parameters of the student and teacher\nmodels as ğœƒ and\nÌ„\n ğœƒ, respectively. Formally, we use the following loss\nfunction to optimize the student modelâ€™s parameters ğœƒ:\nğœƒ = arg min\nğœƒ\n[ \nğ‘\nL\nâˆ‘\nğ‘–=1\nîˆ¸\nğ‘ \n(\nğ‘\nğ‘–\n, ğ‘¦\nğ‘–\n) \n+\nğ‘\nL\n+ğ‘\nU\nâˆ‘\nğ‘–=1\nîˆ¸\nğ‘¢\n(\nğ‘\nğ‘–\n,Ìƒğ‘ \nğ‘–\n)\n]\n(6)\nwhere îˆ¸\nğ‘  \nis the supervised loss and îˆ¸\nğ‘¢ \nis the unsupervised consistency\nloss encouraging the student and teacher models to give the same\npredictions for the same inputs. ğ‘\nğ‘– \nis the student modelâ€™s output with\nthe input ğ‘¥\nğ‘–\n, whileÌƒğ‘ \nğ‘– \nis the teacher modelâ€™s output with the augmented\nğ‘¥\nğ‘– \nas the input.\nIn this study, the supervised loss îˆ¸\nğ‘  \nis computed by combining\nthe binary cross entropy îˆ¸\nBCE \nand Dice-coefficient loss îˆ¸\nğ·ğ‘–ğ‘ğ‘’ \ntogether,\nwhich is defined as:\nîˆ¸\nBCE \n= âˆ’ \n1\nğ‘\nL\nâˆ‘\nğ‘–âˆˆğ‘\nL\n(\nğ‘¦\nğ‘– \nlog \n(\nğ‘\nğ‘–\n) \n+ \n(\n1 âˆ’ ğ‘¦\nğ‘–\n) \nlog \n(\n1 âˆ’ ğ‘\nğ‘–\n)) \n(7)\nîˆ¸\nğ·ğ‘–ğ‘ğ‘’ \n= 1 âˆ’ \n2 \nâˆ‘\nğ‘\nL\nğ‘–=1 \nğ‘¦\nğ‘–\nğ‘\nğ‘–\nâˆ‘\nğ‘\nL\nğ‘–=1 \nğ‘¦\nğ‘– \n+ \nâˆ‘\nğ‘\nL\nğ‘–=1 \nğ‘\nğ‘–\n(8)\nîˆ¸\nğ‘  \n= \n1\n2\n(\nîˆ¸\nBCE \n+ îˆ¸\nğ·ğ‘–ğ‘ğ‘’\n) \n(9)\nThe unsupervised consistency loss îˆ¸\nğ‘¢ \nmeasures the difference be-\ntween ğ‘\nğ‘– \nandÌƒ ğ‘ \nğ‘– \nusing the mean squared error that is computed as:\nîˆ¸\nğ‘¢ \n= \n1\nğ‘\nL \n+ ğ‘\nU\nâˆ‘\nğ‘–âˆˆğ‘\nL\n+ğ‘\nU\n(\nğ‘\nğ‘– \nâˆ’Ìƒğ‘ \nğ‘–\n)\n2 \n(10)\nThus, the total loss used to train the student model by using labeled\nand unlabeled data can be formulated as:\nîˆ¸\nğ‘¡ğ‘œğ‘¡ğ‘ğ‘™ \n= îˆ¸\nğ‘  \n+ îˆ¸\nğ‘¢ \n(11)\n\nBiomedical Signal Processing and Control 93 (2024) 106164\n5\nX. Hao et al.\nFig. 4. Overview of our developed Mean-Teacher based semi-supervised learning for tumor segmentation. (a) Illustration of Mean-Teacher framework for tumor segmentation in\nbreast cancer MRI. The green paths indicate training on both labeled and unlabeled data, whereas the orange paths indicate training on only labeled data. The loss function used\nto train the labeled data is defined as the supervised loss, whereas both the labeled data and unlabeled data are trained with the unsupervised consistency loss. Both teacher and\nstudent models are trained using the ResNet-style U-Net model with the attention mechanism. Weighting parameters of the teacher model are updated as per the changes of the\nstudent modelâ€™s parameters by using the SWA method. (b) The attention-based Res-UNet architecture used for student and teacher models.\nThe teacher model does not directly participate in the back prop-\nagation, and its parameters are updated to be the average values of\nthe student model. In traditional Mean-Teacher framework, weighting\nparameters of the teacher model are updated by using the Exponen-\ntial Moving Average (EMA) algorithm, which updates teacher modelâ€™s\nweighting parameters to the average weight values of the student\nmodel through a simple weighted average formula. Although simple\nand easy to implement, this algorithm may not always provide optimal\nprediction performance for the trained model. Therefore, we use the\nStochastic Weight Averaging (SWA) algorithm to update the teacher\nmodelâ€™s weighting parameters as per the Ref. [34]. The SWA algorithm\noptimizes the teacher model with an improved generalization by aver-\naging student modelâ€™s weights in different training stages. Assume that\neach cycle has ğ‘ iterations. The teacher model is updated once in every\ncycle. The formula for updating the weighting parameters in the SWA\nalgorithm is as follows:\nÌ„\nğœƒ =\n(\n1 âˆ’ \n1\nğ›½\n)\nÌ„\nğœƒ + \n1\nğ›½ \nğœƒ (12)\nwhere ğ›½ is a hyper-parameter (initialized at 1, the value is incremented\nby 1 with each iteration.) that balances contributions of weight values\nbetween student and teacher models. As illustrated in Eq. (12), the\nstudent modelâ€™s weight values ğœƒ in the ğ‘ğ‘¡â„ iteration of each cycle is\nweighted and fused with the previous teacher modelâ€™s weight values\nÌ„\n ğœƒ,\nin order to update teacher modelâ€™s weighting parameters. This updating\nprocedure is performed until ğ‘› iterations are completed. To adaptively\ndetermine the optimal value for the parameter ğ‘›, we initially set a large\nnumber of training iterations (i.e., 80) as a starting point and employ\nthe early stopping strategy to halt model training if the validation\nperformance does not improve. In the prediction stage, the teacher\nmodel is used to segment tumor regions in multi-parametric breast\ncancer MRI, as it usually makes more accurate predictions.\nFig. 5 depicts segmentation results by using our semi-supervised tu-\nmor segmentation model on several MR images randomly selected from\nour test dataset. As observed in Fig. 5, although our model is trained\nwith a very small number of patients (i.e., only 10 patients) with tumor\nannotations, it still can provide visually promising tumor segmentation\nresults (see Fig. 5(c)) by comparing with manually labeled ground\ntruths (see Fig. 5(b)). We also provide quantitative metrics for the\nexamples shown in Fig. 5, namely Intersection over Union (IoU) and\nDice coefficient. It is noted from Fig. 5(c) that segmentation results in\nthe first and third rows have high Dice coefficient values, specifically\n0.88 and 0.98, respectively. This indicates the efficacy of our devel-\noped Mean-Teacher based semi-supervised learning for breast cancer\nsegmentation in multi-parametric MRI. In contrast, the segmentation\nresult in the second row is relatively poorer, with IoU and Dice values\nof 0.68 and 0.74, respectively. In particular, it reveals differences in\nthe upper-left region compared to the manually annotated ground\ntruth. We attribute this to the inherent complexity of its anatomical\nFig. 5. Visual examples of semi-supervised tumor segmentation. (a) Randomly selected\nMRI from test data; (b) Ground truths of tumor regions; (c) Automatic segmentation\nof tumor regions (e.g., white foregrounds).\nstructure, where the irregularity in the shape of the lesion poses a\nchallenge for precise segmentation. Nevertheless, the overall promising\nperformance in tumor segmentation indicates the robustness of our\nmodel in handling variations in tumor shapes and sizes across patients.\n2.4. Multi-modal features based joint learning\nBased on tumor segmentation in key MRI slices determined by the\nattention-based MIL model, we extract widely-used radiomic features\nfrom segmented tumor regions for contributing to pCR predictions. As\nshown in Fig. 5, a slight discrepancy exists between the segmentation\noutcome of our proposed semi-supervised approach and the manually\nannotated ground truth. To mitigate this discrepancyâ€™s impact, we\nincorporate the concepts of intratumoral and peritumoral regions when\nextracting radiomic features. Leveraging a semi-supervised learning\nmodel for segmenting the regions of interest, we automatically extend\nthem by 5 mm [35] outward, enabling the extraction of radiomic\nfeatures from the comprehensive intratumoral and peritumoral areas.\nThat is, 104 radiomic features are automatically extracted from each\n\nBiomedical Signal Processing and Control 93 (2024) 106164\n6\nX. Hao et al.\nFig. 6. Examples of multi-parametric MRI used in this study, where superimposed red arrows indicate the existence of breast cancer. (a) DWI-MRI; (b) T2W-MRI; (c) T1W-MRI;\n(d) DCE-MRI.\ntumor region by using PyRadiomics package [36]. The extracted Ra-\ndiomic features can be broadly divided into 7 categories: first order\nstatistics (19 features), shape-based descriptors (10 features), gray-level\ncooccurrence matrix (24 features), gray-level run length matrix (16\nfeatures), gray-level size zone matrix (16 features), neighboring gray\ntone difference matrix (5 features) and gray-level dependence matrix\n(14 features). These radiomic features are to be fused with other multi-\nparametric MRI features for learning pCR predictions.We have included\nall the names of computed radiomic features in Supplementary Table 1.\nWe obtained bag-level MRI features (e.g., 500 dimension features)\nusing attention-based MIL model and radiomic features of segmented\ntumor regions (e.g., 104 dimension features) using PyRadiomics pack-\nage for every single MRI modality. Since multi-parametric MRI is\nexplored, we then concatenate all multi-parametric MRI features. Fi-\nnally, the concatenated multi-parametric MRI features are input into\na fully connected layer to make pCR predictions for breast cancer\npatients.\n3. Evaluations\nIn this section, we first provide data description and experimental\nsettings. The evaluations in terms of ablation study, different feature\nfusion methods and comparisons with existing related studies are then\nprovided.\n3.1. Data description\n3.1.1. Study population\nThis study complied with the Declaration of Helsinki and was\napproved by the Institutional Review Board/Ethics Committee of the\nCancer Hospital of Dalian University of Technology, China. All patients\nsigned an informed consent before enrollment. This work retrospec-\ntively studied 442 breast cancer patients at the hospital from January\n1, 2019 to December 31, 2021, including 95 pCR and 347 non-pCR\npatients, respectively. We provide an overview of the general charac-\nteristics of the patients enrolled in this study in Supplementary Table 2.\nOur dataset is utilized as follows:\n(i) In weakly supervised learning, the entire dataset was divided into\nthree sets: training, validation and testing. All patient-specific images\nwere assigned into the same set, in case different slices of the same\npatient would be used in both training and testing. To evaluate the\npCR prediction efficacy, 20% of cases (all with T1WI, T2WI, DCE-MRI,\nand DWI-MRI) were randomly selected as an independent testing set\nwhich includes 19 pCR and 68 non-pCR cases. The remaining 80% of\ncases in the dataset were then arbitrarily divided into five groups on\naverage, creating training and validation sets in a ratio of 4:1 for 5-\nfold cross-validation. The model with the highest performance in 5-fold\ncross validation was selected for subsequent multi-modal feature fusion\nand independent testing.\n(ii) In semi-supervised learning, only 10 patients in our dataset had\nannotated tumor regions, and only three central-most slices of each\nmodal MRI were annotated. These annotated MR slices were utilized\nas labeled data to train the semi-supervised tumor segmentation model.\nIn total, we collected 150 annotated MR slices, with 100 allocated for\nmodel training and the remaining used for testing. Our semi-supervised\ntumor segmentation model achieved a mean IoU value of 0.68 and\na Dice coefficient of 0.72 on the test set. Notably, this performance\nclosely aligns with a recent study by Oh et al. [37], even though we\nused fewer training samples.\n3.1.2. Pathological evaluation\nPostoperative pathological sections were examined, and the\nchemotherapeutic effect was graded as 1â€“5 by following the Miller\nand Payne classification criteria [38,39]. Pathological reaction grade\n5 was regarded as pathological complete response (pCR), while patho-\nlogical reaction grades 1â€“4 were regarded as not pathological complete\nresponse (non-pCR).\n3.1.3. MRI preprocessing\nIn this study, DWI, T1WI, T2WI and DCE MR images were used.\nFig. 6 displays examples of multi-parametric MRI for two patients,\nwhere superimposed red arrows indicate the existence of breast cancer.\nWhile down-sampling may reduce data resolution and result in a loss\nof detailed information, it is necessary to handle the distinct three-\ndimensional volumes present in our multi-parametric MRI data. These\nvariations stem from differences in imaging equipment and patient\nscan results. To standardize the diverse image scales and meet memory\nlimitation, the MR image is first down-sampled to the dimension of\n256 Ã— 256 Ã— ğ·, where ğ· is the volume depth that is 48, 18, 48, 48\nfor DWI-MRI, T2WI, T1WI, DCE-MRI, respectively. To decrease imag-\ning bias, intensity normalization for every MRI is then performed by\nsubtracting its mean intensity value and dividing by standard deviation\n\nBiomedical Signal Processing and Control 93 (2024) 106164\n7\nX. Hao et al.\nFig. 7. ROC and PR curves obtained in testing different combinations of multi-parametric MRI for predicting response to NAC in our testing dataset. (a) Receiver Operating\nCharacteristic (ROC) curves. (b) Precision-Recall (PR) curves.\nof its intensity values. Image augmentations including horizontal flip,\nvertical flip, scaling, rotation, shear and elastic transformations are\nadded during training. Additionally, Gaussian noise, introduced as a\nregularization technique during the preprocessing stage, is added to all\nMRI input modalities. This serves to prevent overfitting by introducing\ncontrolled randomness during training. It also helps reduce the modelâ€™s\nsensitivity to minor variations or noise in the data, contributing to the\nextraction of more robust features [40,41]. These pre-processing steps\nsignificantly improve modelâ€™s capacity to generalize and effectively\neliminate over-fitting issue during model training.\n3.2. Experimental settings\nWe use the PyTorch (version 1.1.0) open-source deep learning\npackage as the main tool for implementing our proposed framework.\nThe training was conducted on a DGX Station with four NVIDIA Tesla\nP100 GPUs. The model was trained with the Adam optimizer by using\na learning rate of 1ğ‘’ âˆ’ 5, a weight decay of 0.00001, a batch size\nof 1 for weakly-supervised learning, and the early stopping to avoid\nexcessive training. To train the semi-supervised tumor segmentation\nmodel, we use the batch size of 8, a learning rate of 0.001, and all other\nparameters kept the same as those of weakly-supervised learning.\nThe performance of automatic pCR prediction is evaluated using the\nfollowing metrics: accuracy (ACC), sensitivity (SEN), specificity (SPE),\narea under the receiver operating characteristic curve (AUC) and area\nunder the precisionâ€“recall curve (AUPRC). The applicable formulas are\ndefined as:\nACC = \nTP + TN\nTP + FP + FN + TN \n(13)\nSEN = \nTP\nTP + FN \n(14)\nSPE = \nTN\nTN + FP \n(15)\nwhere TP, FP, TN, and FN represent the True Positive, False Positive,\nTrue Negative and False Negative, respectively.\n3.3. Ablation study\nSince the multi-parametric MRI were explored in the pCR predic-\ntion, we performed an ablation study to evaluate the performance by\ncombining different modal MR images according to Refs. [15,16]. In\naddition, as illustrated in Fig. 1, we tried to integrate clinicopathologic\ncharacteristics of patients with MR imaging features for evaluation.\nThe integrated clinicopathologic variables include age, estrogen recep-\ntor (ER), progesterone receptor (PR), human epidermal growth factor\nTable 2\nComparison of different multi-parametric MRI combinations.\nMethod ACC SEN SPE AUC AUPRC\nDWI-MRI 0.74 0.76 0.67 0.72 0.74\nDCE-MRI 0.76 0.89 0.58 0.73 0.79\nT2W image 0.71 0.79 0.62 0.70 0.85\nT1W image flip angle 1\na \n0.76 0.72 0.59 0.80 0.82\nT1W image flip angle 1+clinical 0.76 0.77 0.63 0.75 0.74\nT1W image flip angle 2\na \n0.73 0.68 0.77 0.76 0.76\nDCE&DWI-MRI 0.61 0.74 0.48 0.74 0.74\nDCE&DWI-MRI+radiomic 0.69 0.79 0.59 0.75 0.77\nDCE&DWI-MRI+radiomic+clinical 0.79 0.87 0.68 0.81 0.80\na \nRepresents different angles of image scanning.\nreceptor 2 (HER2), Ki-67, histological subtypes, ğ‘‡ stage, ğ‘ stage,\nADC value, dynamic enhancement curve type (TIC), and NAC therapy\nstrategy. To encode these clinicopathologic variables, we utilized minâ€“\nmax values to normalize numeric features, including age, ER, PR, Ki-67,\nand ADC value, to the range of 0 to 1. We transformed categorical\ncharacteristics such as HER2, histologic subtype, ğ‘‡ stage, ğ‘ stage,\nTIC type and NAC therapy strategy into one-hot encoding vectors.\nNote that since only 147 patients in our dataset had clinicopathologic\ninformation, the remaining patients were padded with 0 when we train\nand test the model in the ablation study.\nTable 2 lists the designed studies along with corresponding results.\nFig. 7(a)(b) shows the corresponding ROC and PR curves. As observed\nin Table 2 and Fig. 7, evaluations using different MRI modalities yield\nthe AUC values ranging between 0.70 and 0.81. Particularly, it is found\nthat the T1-weighted images at a specific angle (i.e., flip angle 1) yield\na high AUC value of 0.80, while the T2-weighted images achieve the\nhighest AUPRC value of 0.85. This phenomenon may be attributed to\nthe enhanced suitability of T1- and T2-weighted images for capturing\nanatomical details and lesion locations, which are crucial for predicting\npatient treatment responses [42]. Furthermore, compared to other MRI\nmodalities, T1- and T2-weighted images exhibit superior image quality,\npotentially providing more distinctive information for pCR prediction.\nThe overall best performance (e.g., AUC=0.81) was obtained by making\nuse of DCE-MRI, DWI-MRI, radiomic features and clinical information\ntogether. This indicates the advantages of our framework by integrating\nmulti-modal features for pCR prediction.\nWe further conducted ablation studies to evaluate the effectiveness\nof different components in the proposed framework. Experiments were\nconducted with the following configurations:\n1. Semi-supervised learning (SL): We used radiomics features de-\nrived from semi-supervised learning based tumor segmentation\nto predict the pCR status.\n\nBiomedical Signal Processing and Control 93 (2024) 106164\n8\nX. Hao et al.\nFig. 8. ROC and PR curves obtained in ablation study for predicting response to NAC in our testing dataset. (a) Receiver Operating Characteristic (ROC) curves. (b) Precision-Recall\n(PR) curves.\nTable 3\nAblation study results about different modules.\nMethod ACC SEN SPE AUC AUPRC\nSL\na \n0.78 0.79 0.71 0.73 0.74\nSL+clinical variables\na \n0.73 0.71 0.73 0.67 0.69\nWSL\na \n0.75 0.79 0.67 0.82 0.85\nWSSL 0.81 0.91 0.66 0.85 0.89\nWSSL+clinical variables\na \n0.77 0.86 0.63 0.84 0.87\na \nRepresents p < 0.05 (Mannâ€“Whitney U test).\n2. SL+clinical variables: Clinical variables were concatenated with\nradiomics features to predict the pCR status.\n3. Weakly supervised learning (WSL): We employed the attention-\nbased MIL model that learned attentional features from multi-\nparametric MRI to predict the pCR status.\n4. Weakly & semi-supervised learning (WSSL): We fused attentional\nand radiomic features that were learned based on weakly and\nsemi-supervised learning from multi-parametric MRI to predict\nthe pCR status.\n5. WSSL+clinical variables: Besides the fused features learned\nbased on weakly & semi-supervised learning, we included clin-\nical variables encoded by one-hot coding to predict the pCR\nstatus.\nTable 3 lists ablation study results in terms of SL, SL with clinical\nvariables, WSL, WSSL, and WSSL with clinical variables on our inde-\npendent testing set. Supplementary Table 3 lists means and standard\ndeviations of 5-fold cross-validation on training set. Fig. 8(a)(b) sep-\narately shows the ROC and precisionâ€“recall (PR) curves for ablation\nstudy results. As observed in Table 3 and Fig. 8, when relying solely on\nradiomics features, the SL method provides the AUC and AUPRC values\nof 0.73 and 0.74, respectively. The integration of clinical variables\nwith radiomics features results in decreased AUC and AUPRC values,\nunderscoring the challenge of training models with missing data. Using\nonly weakly-supervised learning, the obtained AUC and AUPRC values\nfor pCR prediction reach up to 0.82 and 0.85, respectively, which\nindicates the effectiveness of our developed attention-based MIL model\nthat learns features from multi-parametric MRI. By integrating weakly\nsupervised learning and semi-supervised learning, WSSL enhances the\nAUC and AUPRC values to 0.85 and 0.89, respectively. This improve-\nment aligns with the conclusions drawn by Bizzego et al. [29] and\nLi et al. [28], highlighting the necessity of combining deep learning\nfeatures with radiomics features to enhance the predictive performance\nof cancer patient treatment response. In addition, it is found that\nclinical variables do not add values in pCR prediction when they are\nintegrated with all multi-parametric MRI together, although the WSSL\nwith clinical variables also provides reasonable good performances\n(e.g., AUC=0.84 and AUPRC=0.87). We mainly attribute this to the\nsmall proportion of patients with clinical variables in our dataset, and\nthe majority of patients without clinical variables were simply padded\nwith zero to ensure the same length of feature representation. Never-\ntheless, further experimental verification is highly required in order to\ntest clinical variables and their contributions to pCR prediction. Since\nthe WSSL provides the best performance in our evaluation, it is referred\nas the proposed model in subsequent sections. To further assess the sig-\nnificance of differences in predictions, the Mannâ€“Whitney U tests were\nused between our proposed WSSL model and each ablation method.\nA significant difference in predicted outcomes is considered when the\nğ‘-value is less than 0.05. As illustrated in Table 3, our experimental\nresults show that all ablation methods significantly differ from our\nproposed model, highlighting notable improvements achieved through\nthe integration of weakly-supervised and semi-supervised learning.\n3.4. Comparison of different feature fusion methods\nFig. 9 illustrates commonly-used multi-modal feature fusion strate-\ngies that fall into three categories: early fusion, late fusion, and model-\nlevel fusion. As shown in Fig. 9(a), early fusion aggregates multi-modal\nfeatures directly before training the classification head. Late fusion\n(see Fig. 9(b)) is often used when there exists significant correlation\nbetween data from different modalities. Specifically, each single-modal\ndata is trained independently to make a prediction, and then predic-\ntions from multi-modal data are fused via the rules such as maximum\nor average fusion. Model-level fusion, unlike the previous two methods,\ndoes not describe a deterministic fusion operation such as feature\nconcatenation, but instead uses a neural network to determine how to\nmerge multi-modal data more effectively. As depicted in Fig. 9(c), we\nreproduce an autoencoder-based technique for evaluating model-level\nfusion strategy [43]. This method is an adaptive fusion methodology\nthat learns to condense information from multiple modalities.\nIn order to compare different feature fusion strategies on the influ-\nence of pCR predictions, we conducted tests with the aforementioned\nthree feature fusion methods. Note that these feature fusion strategies\nare applied with our WSSL method that utilizes bag-level features\nfrom multi-parametric MRI and radiomics features from key MRI slices.\nTable 4 lists the corresponding comparative results. Early fusion and\nlate fusion yield comparable results, with the AUC values of 0.85\nand 0.86, respectively. By contrast, model-level fusion provides much\n\nBiomedical Signal Processing and Control 93 (2024) 106164\n9\nX. Hao et al.\nFig. 9. Three typical multi-modal feature fusion strategies. (a) Early fusion, (b) Late fusion, (c) Model-level fusion.\nTable 4\nComparisons of different feature fusion strategies.\nMethod ACC SEN SPE AUC\nEarly fusion 0.81 0.91 0.66 0.85\nLate fusion 0.75 0.84 0.66 0.86\nModel-level fusion 0.71 0.48 0.78 0.60\nTable 5\nComparisons with existing studies.\nMethod Patient No. ACC SEN SPE AUC\nEl Adoui et al. [44] 42 0.88 0.922 0.791 0.91\nComes et al. [45] 108 0.923 0.857 0.947 â€“\nLiu et al. [46] 586 â€“ â€“ â€“ 0.79\nTahmassebi et al. [47] 38 â€“ â€“ â€“ 0.857\n3D CNN+Clinical [27] 442 0.74 0.63 0.76 0.80\nProposed Model 442 0.81 0.91 0.66 0.85\npoorer performance (e.g., AUC=0.60), which is probably due to the\nfact that we only replicated the autoencoder-based method without\nadjusting its hyper-parameters. Overall, our evaluations show that\nit is important to apply appropriate feature fusion strategies when\nprocessing multi-modal data.\n3.5. Comparison with existing methods\nWe conducted a thorough comparison of our proposed method\nwith existing studies including ROI-based and ROI free methods. The\nstudies [44â€“47] extracted different type of features from manually\ndelimited ROIs for pCR prediction. In particular, Refs. [44,45] extracted\ndeep learning features, whereas Refs. [46,47] extracted radiomic fea-\ntures. Note that these existing studies fully rely on manual ROI an-\nnotations in pCR prediction. To learn from weakly-annotated data,\nJoo et al. [27] built a 3D CNN model to combine high-dimensional\nimage features from multi-parametric MRI and clinical data for pCR\nprediction. Although the method in [27] also functioned with weak\nannotations, they were incapable of exploiting radiological character-\nistics of malignancies. By contrast, our proposed method works in a\nweak annotation setting where only patient-level labels (e.g., pCR or\nnon-pCR) are available. We introduce a semi-supervised tumor seg-\nmentation model that can efficiently exploit a vast quantity of MRI\nwithout pixel-level ROI annotations, so that radiological characteristics\nof tumors are incorporated to improve the pCR prediction performance.\nSince these existing studies [44â€“47] performed pCR prediction\nbased on manually annotated ROIs (e.g., tumor regions) in MR images,\nwe did not directly implement and evaluate them on our dataset due\nto the lack of manual tumor annotations. We cited original results\nfrom Refs. [44â€“47] for intuitive comparisons. Additionally, to make\na further direct comparison with the 3D CNN, we reproduced the\nmethod described in Ref. [27] using MONAI\n1 \nand tested it on our\ndataset. The comparative performances are shown in Table 5. Although\n1 \nhttps://monai.io/\nthe Refs. [44,45] reported good performances (e.g., AUC=0.91 [44])\nin pCR prediction, they were tested on rather small datasets with\nonly 14 patients and 26 patients, respectively. In addition, both of\nthem replied on manual selection of ROIs in MR images, which is\npractically challenging due to the large volume of 3D MR imaging.\nThe Ref. [46] studied a large data cohort including a total of 586\npotentially eligible patients, which first computed radiomic features\nsuch as shape- and size-based features and textural features from\nmanually labeled tumor regions and then predicted pCR status based\non feature selection and support vector machine. However, this study\nonly reported the AUC value of 0.79, which is much lower than that\n(e.g., 0.85) of our proposed method. The Ref. [47] evaluated the impact\nof traditional machine learning with multi-parametric MRI for early\nprediction of response to NAC. This study evaluated only 38 breast\ncancer patients, and reported the mean AUC value of 0.8577 by using\nthe best XGBoost classifier. The Ref. [27] explored the potential of a 3D\nCNN model combined with clinical data to predict the effectiveness of\nneoadjuvant therapy. When reproduced and tested on our dataset, their\nmodel achieved an AUC value of 0.80. In comparison, our proposed\nmodel demonstrates significant improvements on AUC (0.85), ACC\n(0.81) and SEN (0.91) values. Our WSSL method with clinical variables\n(i.e., WSSL+clinical variables in Table 3) also delivers superior perfor-\nmance over the 3D CNN based method [27]. Thus, this improvement is\nprimarily attributed to the incorporation of radiomic features computed\nbased on semi-supervised tumor segmentation. Overall, it could be\nconcluded that our proposed method can provide more accurate pCR\npredictions from multi-parametric MRI than existing methods.\n4. Conclusion\nUtilizing the maximum potential of MR images, we propose a\nweakly and semi-supervised joint learning model that can predict\nwhether breast cancer patients achieve a pCR with neoadjuvant therapy\nor not. Our experimental results suggest that the performance of pCR\nprediction can be enhanced by incorporating multi-modal imaging, as\nopposed to using only single-parameter MRI. Our multi-modal fusion\nmethod provides the best prediction performance with the AUC of 0.85.\nIn comparison to other deep learning studies in the field, our\nresearch offers several significant advantages. First, we incorporate\nboth bilateral and unilateral MR images of the lesion, enabling us\nto leverage a broader range of information about the tumor. Unlike\nprevious studies [44] that trained the model with cropped MR images,\nour model takes entire MR images as the inputs. We hypothesize that\nthe model trained with entire images performs better than that of\ncropped images. This may be attributed to the AI modelâ€™s ability to\nlearn features that are not visible to the naked eye or due to the\nadditional information provided by the identification of abnormalities\nin other organs within the MRI. Second, it is not necessary to manu-\nally annotate a large number of tumor regions in order to train our\nmodel. This advantage brought by the semi-supervised learning greatly\nreduces labor intensity. Finally, we included 442 breast cancer patients\nwith multi-parametric MRIs for evaluations. This is a relatively larger\nstudy cohort for neoadjuvant efficacy prediction in comparison with\n\nBiomedical Signal Processing and Control 93 (2024) 106164\n10\nX. Hao et al.\nexisting relevant studies, which ensures the reliability of our findings.\nBy incorporating these advancements, our research contributes novel\ninsights to the field of neoadjuvant efficacy prediction. The utilization\nof entire MR images, the application of semi-supervised learning, and\nthe inclusion of a substantial patient cohort collectively reinforce the\nscientific rigor and applicability of our findings.\nNevertheless, our study has certain limitations. First, this is a ret-\nrospective, single-center investigation, and the data were exclusively\ncollected from stationary hospital scanners. Future studies incorporat-\ning MR images from multiple institutions are warranted to assess the\nmodelâ€™s generalizability and enhance its clinical applicability. Further-\nmore, although our experimental results demonstrate excellent predic-\ntion performance in terms of pathological complete response (pCR)\nthrough the integration of multi-parametric MRI and clinical informa-\ntion, it is crucial to acknowledge that the clinical information obtained\nin our study was insufficient. It is advisable for future research to\nincorporate more specialized variables to enable a more comprehensive\nevaluation of the modelâ€™s performance. Additionally, future endeavors\nare anticipated to explore representation fusion methodologies, such\nas attention-based feature fusion and cross-modal feature fusion. These\napproaches could play a crucial role in enhancing classification accu-\nracy. Investigating diverse data modalities such as ultrasound, digital\nmammography, and multi-parametric MRI, for predicting pCR status in\nbreast cancer patients is also an attractive research direction.\nIn conclusion, we propose a method of combining weakly and\nsemi-supervised learning to maximize our dataset utilization, where\nthe multi-parametric MRI sequences are used to enhance the pCR\nprediction performance. Based on our experimental findings, we be-\nlieve that the multi-modal data fusion based deep learning model\ncan provide valuable predictive information in estimating neoadjuvant\ntreatment outcomes, which can assist physicians in making better clini-\ncal decisions. Our study would make a significant contribution towards\nadvancing precision medicine for breast cancer patients.\nCRediT authorship contribution statement\nXinyu Hao: Investigation, Methodology, Software, Writing â€“ origi-\nnal draft. Hongming Xu: Funding acquisition, Project administration,\nWriting â€“ review & editing. Nannan Zhao: Data curation. Tao Yu:\nData curation. Timo Hamalainen: Writing â€“ review & editing. Fengyu\nCong: Writing â€“ review & editing.\nDeclaration of competing interest\nThe authors declare that they have no known competing finan-\ncial interests or personal relationships that could have appeared to\ninfluence the work reported in this paper.\nData availability\nThe authors do not have permission to share data.\nAcknowledgments\nThis work was supported by National Natural Science Foundation of\nChina (Grant No. 82102135), the Fundamental Research Funds for the\nCentral Universities (Grant No. DUT22YG114, Grant No. DUT23YG130),\nthe Natural Science Foundation of Liaoning Province (Grant No. 2022-\nYGJC-36) and the scholarship from the China Scholarship Council (No.\n202006060060).\nAppendix A. Supplementary data\nSupplementary material related to this article can be found online\nat https://doi.org/10.1016/j.bspc.2024.106164.\nReferences\n[1] Eun Ha Jeong, et al., Prediction of axillary lymph node metastasis in early\nbreast cancer using dynamic contrast-enhanced magnetic resonance imaging and\ndiffusion-weighted imaging, Investig. Magn. Reson. Imag. 23 (2) (2019) 125â€“135.\n[2] Tara Hyder, et al., Approaching neoadjuvant therapy in the management of\nearly-stage breast cancer, Breast Cancer Targets Ther. 13 (2021) 199.\n[3] Patricia Cortazar, et al., Pathological complete response and long-term clinical\nbenefit in breast cancer: the ctneobc pooled analysis, Lancet 384 (9938) (2014)\n164â€“172.\n[4] Eleftherios P. Mamounas, Neoadjuvant chemotherapy for operable breast cancer:\nis this the future? Clin Breast Cancer 4 (2003) S10â€“S19.\n[5] Gunter Von Minckwitz, et al., Definition and impact of pathologic complete\nresponse on prognosis after neoadjuvant chemotherapy in various intrinsic breast\ncancer subtypes, J. Clin. Oncol. 30 (15) (2012) 1796â€“1804.\n[6] Jieun Kim, et al., Prediction of pathologic complete response on mri in patients\nwith breast cancer receiving neoadjuvant chemotherapy according to molecular\nsubtypes, Eur. Radiol. 32 (6) (2022) 4056â€“4066.\n[7] Soo-Yeon Kim, et al., Factors affecting pathologic complete response following\nneoadjuvant chemotherapy in breast cancer: development and validation of a\npredictive nomogram, Radiology 299 (2) (2021) 290â€“300.\n[8] Filippo Pesapane, et al., Radiomics of mri for the prediction of the pathological\nresponse to neoadjuvant chemotherapy in breast cancer patients: a single referral\ncentre analysis, Cancers 13 (17) (2021) 4271.\n[9] Beatrice Cavallo Marincola, et al., Can unenhanced mri of the breast replace\ncontrast-enhanced mri in assessing response to neoadjuvant chemotherapy? Acta\nRadiol. 60 (1) (2019) 35â€“44.\n[10] Yanbo Li, et al., Development and validation of a nomogram based on pretreat-\nment dynamic contrast-enhanced mri for the prediction of pathologic response\nafter neoadjuvant chemotherapy for triple-negative breast cancer, Eur. Radiol.\n32 (3) (2022) 1676â€“1687.\n[11] Zhifan Li, et al., The diagnostic performance of diffusion-weighted imaging and\ndynamic contrast-enhanced magnetic resonance imaging in evaluating the patho-\nlogical response of breast cancer to neoadjuvant chemotherapy: A meta-analysis,\nEur. J. Radiol. 143 (2021) 109931.\n[12] Kay J.J. van der Hoogt, et al., Factors affecting the value of diffusion-weighted\nimaging for identifying breast cancer patients with pathological complete re-\nsponse on neoadjuvant systemic therapy: a systematic review, Insights Imag. 12\n(1) (2021) 1â€“22.\n[13] Nu N. Le, et al., Effect of inter-reader variability on diffusion-weighted mri ap-\nparent diffusion coefficient measurements and prediction of pathologic complete\nresponse for breast cancer, Tomography 8 (3) (2022) 1208â€“1220.\n[14] Katerina Dodelzon, Increasing imaging value to breast cancer care through\nprognostic modeling of multiparametric mri features in patients undergoing\nneoadjuvant chemotherapy, Academic Radiol. 29 (2022) S164â€“S165.\n[15] Yan-Lin Gu, et al., Role of magnetic resonance imaging in detection of patho-\nlogic complete remission in breast cancer patients treated with neoadjuvant\nchemotherapy: a meta-analysis, Clin. Breast Cancer 17 (4) (2017) 245â€“255.\n[16] Melanie A. Lindenberg, et al., Imaging performance in guiding response to\nneoadjuvant therapy according to breast cancer subtypes: a systematic literature\nreview, Crit. Rev. Oncol./Hematol. 112 (2017) 198â€“207.\n[17] Zhenyu Liu, et al., The applications of radiomics in precision diagnosis and\ntreatment of oncology: opportunities and challenges, Theranostics 9 (5) (2019)\n1303.\n[18] Jarkko Johansson, Kati Alakurtti, Juho Joutsa, Jussi Tohka, Ulla Ruotsalainen,\nJuha O. Rinne, Comparison of manual and automatic techniques for substriatal\nsegmentation in 11c-raclopride high-resolution pet studies, Nucl. Med. Commun.\n37 (10) (2016) 1074â€“1087.\n[19] Per Stenkrona, et al., [11C] sch23390 binding to the d1-dopamine receptor in\nthe human brainâ€”a comparison of manual and automated methods for image\nanalysis, EJNMMI Res. 8 (1) (2018) 1â€“12.\n[20] Yu-Dong Zhang, et al., Computer-aided diagnosis of abnormal breasts in mam-\nmogram images by weighted-type fractional fourier transform, Adv. Mech. Eng.\n8 (2) (2016) 1687814016634243.\n[21] Yudong Zhang, et al., Smart detection on abnormal breasts in digital mammog-\nraphy based on contrast-limited adaptive histogram equalization and chaotic\nadaptive real-coded biogeography-based optimization, Simulation 92 (9) (2016)\n873â€“885.\n[22] Yu-Dong Zhang, et al., Improved breast cancer classification through combining\ngraph convolutional network and convolutional neural network, Inf. Process.\nManage. 58 (2) (2021) 102439.\n[23] Mingming Ma, et al., Radiomics features based on automatic segmented mri\nimages: Prognostic biomarkers for triple-negative breast cancer treated with\nneoadjuvant chemotherapy, Eur. J. Radiol. 146 (2022) 110095.\n[24] Mohammed El Adoui, et al., Predict breast tumor response to chemotherapy\nusing a 3d deep learning architecture applied to dce-mri data, in: International\nWork-Conference on Bioinformatics and Biomedical Engineering, Springer, 2019,\npp. 33â€“40.\n[25] Richard Ha, et al., Prior to initiation of chemotherapy, can we predict breast\ntumor response? deep learning convolutional neural networks approach using a\nbreast mri tumor dataset, J. Digit. Imag. 32 (5) (2019) 693â€“701.\n\nBiomedical Signal Processing and Control 93 (2024) 106164\n11\nX. Hao et al.\n[26] Kavya Ravichandran, et al., A deep learning classifier for prediction of patho-\nlogical complete response to neoadjuvant chemotherapy from baseline breast\ndce-mri, in: Medical Imaging 2018: Computer-Aided Diagnosis, vol. 10575, SPIE,\n2018, pp. 79â€“88.\n[27] Sunghoon Joo, et al., Multimodal deep learning models for the prediction of\npathologic response to neoadjuvant chemotherapy in breast cancer, Sci. Rep. 11\n(1) (2021) 1â€“8.\n[28] Zeju Li, et al., Deep learning based radiomics (dlr) and its usage in noninvasive\nidh1 prediction for low grade glioma, Sci. Rep. 7 (1) (2017) 1â€“11.\n[29] Andrea Bizzego, et al., Integrating deep and radiomics features in can-\ncer bioimaging, in: 2019 IEEE Conference on Computational Intelligence in\nBioinformatics and Computational Biology, CIBCB, IEEE, 2019, pp. 1â€“8.\n[30] Sanghyun Woo, et al., Cbam: Convolutional block attention module, in: Pro-\nceedings of the European Conference on Computer Vision, ECCV, 2018, pp.\n3â€“19.\n[31] Maximilian Ilse, et al., Attention-based deep multiple instance learning, in:\nInternational Conference on Machine Learning, PMLR, 2018, pp. 2127â€“2136.\n[32] Antti Tarvainen, et al., Mean teachers are better role models: Weight-averaged\nconsistency targets improve semi-supervised deep learning results, Adv. Neural\nInf. Process. Syst. 30 (2017).\n[33] Olaf Ronneberger, et al., U-net: Convolutional networks for biomedical image\nsegmentation, in: International Conference on Medical Image Computing and\nComputer-Assisted Intervention, Springer, 2015, pp. 234â€“241.\n[34] Ben Athiwaratkun, et al., There are many consistent explanations of unlabeled\ndata: Why you should average, in: ICLR, 2019.\n[35] Nathaniel M. Braman, et al., Intratumoral and peritumoral radiomics for\nthe pretreatment prediction of pathological complete response to neoadjuvant\nchemotherapy based on breast dce-mri, Breast Cancer Res. 19 (2017) 1â€“14.\n[36] Joost J.M. Van Griethuysen, et al., Computational radiomics system to decode\nthe radiographic phenotype, Cancer Res. 77 (21) (2017) e104â€“e107.\n[37] YoungTack Oh, et al., Semi-supervised breast lesion segmentation using local\ncross triplet loss for ultrafast dynamic contrast-enhanced mri, in: Proceedings of\nthe Asian Conference on Computer Vision, 2022, pp. 2713â€“2728.\n[38] Misun Choi, et al., Assessment of pathologic response and long-term outcome\nin locally advanced breast cancers after neoadjuvant chemotherapy: comparison\nof pathologic classification systems, Breast Cancer Res. Treat. 160 (3) (2016)\n475â€“489.\n[39] A. Romero, et al., Correlation between response to neoadjuvant chemotherapy\nand survival in locally advanced breast cancer patients, Ann. Oncol. 24 (3)\n(2013) 655â€“661.\n[40] Alexander Camuto, et al., Towards a theoretical understanding of the robustness\nof variational autoencoders, in: International Conference on Artificial Intelligence\nand Statistics, PMLR, 2021, pp. 3565â€“3573.\n[41] Martin Weber Kusk, S. Lysdahlgaard, The effect of gaussian noise on pneu-\nmonia detection on chest radiographs, using convolutional neural networks,\nRadiography 29 (1) (2023) 38â€“43.\n[42] Richard Bitar, et al., Mr pulse sequences: what every radiologist wants to know\nbut is afraid to ask, Radiographics 26 (2) (2006) 513â€“537.\n[43] Gaurav Sahu, Olga Vechtomova, Adaptive fusion techniques for multimodal data,\n2019, arXiv preprint arXiv:1911.03821.\n[44] Mohammed El Adoui, et al., Multi-input deep learning architecture for predicting\nbreast tumor response to chemotherapy using quantitative mr images, Int. J.\nComput. Assist. Radiol. Surg. 15 (9) (2020) 1491â€“1500.\n[45] Maria Colomba Comes, et al., Early prediction of neoadjuvant chemotherapy\nresponse by exploiting a transfer learning approach on breast dce-mris, Sci. Rep.\n11 (1) (2021) 1â€“12.\n[46] Zhenyu Liu, et al., Radiomics of multiparametric mri for pretreatment prediction\nof pathologic complete response to neoadjuvant chemotherapy in breast cancer:\na multicenter study, Clin. Cancer Res. 25 (12) (2019) 3538â€“3547.\n[47] Amirhessam Tahmassebi, et al., Impact of machine learning with multiparametric\nmagnetic resonance imaging of the breast for early prediction of response to\nneoadjuvant chemotherapy and survival outcomes in breast cancer patients,\nInvest. Radiol. 54 (2) (2019) 110.",
    "version": "5.3.31"
  },
  {
    "numpages": 10,
    "numrender": 10,
    "info": {
      "PDFFormatVersion": "1.7",
      "Language": "en-US",
      "EncryptFilterName": null,
      "IsLinearized": true,
      "IsAcroFormPresent": false,
      "IsXFAPresent": false,
      "IsCollectionPresent": false,
      "IsSignaturesPresent": false,
      "Creator": "Elsevier",
      "Custom": {
        "CrossMarkDomains[1]": "elsevier.com",
        "CrossmarkMajorVersionDate": "2010-04-23",
        "CreationDate--Text": "12th August 2024",
        "ElsevierWebPDFSpecifications": "7.0",
        "robots": "noindex",
        "doi": "10.1016/j.bspc.2024.106584",
        "CrossMarkDomains[2]": "sciencedirect.com",
        "CrossmarkDomainExclusive": "true"
      },
      "ModDate": "D:20240812061104Z",
      "Author": "Yesid GutiÃ©rrez",
      "Title": "A contrastive weakly supervised learning to characterize malignant prostate lesions in BP-MRI",
      "Keywords": "Prostate cancer diagnosis,Weakly supervised learning,BP-MRI,Contrastive learning",
      "CreationDate": "D:20240812055743Z",
      "Producer": "Acrobat Distiller 8.1.0 (Windows)",
      "Subject": "Biomedical Signal Processing and Control, 96 (2024) 106584. doi:10.1016/j.bspc.2024.106584"
    },
    "metadata": {
      "crossmark:crossmarkdomains": "elsevier.comsciencedirect.com",
      "crossmark:crossmarkdomainexclusive": "true",
      "crossmark:doi": "10.1016/j.bspc.2024.106584",
      "crossmark:majorversiondate": "2010-04-23",
      "dc:format": "application/pdf",
      "dc:identifier": "10.1016/j.bspc.2024.106584",
      "dc:publisher": "Elsevier Ltd",
      "dc:description": "Biomedical Signal Processing and Control, 96 (2024) 106584. doi:10.1016/j.bspc.2024.106584",
      "dc:subject": [
        "Prostate cancer diagnosis",
        "Weakly supervised learning",
        "BP-MRI",
        "Contrastive learning"
      ],
      "dc:title": "A contrastive weakly supervised learning to characterize malignant prostate lesions in BP-MRI",
      "dc:creator": ["Yesid GutiÃ©rrez", "John Arevalo", "Fabio MartÃ­nez"],
      "jav:journal_article_version": "VoR",
      "pdf:creationdate--text": "12th August 2024",
      "pdf:producer": "Acrobat Distiller 8.1.0 (Windows)",
      "pdf:keywords": "Prostate cancer diagnosis,Weakly supervised learning,BP-MRI,Contrastive learning",
      "pdfx:creationdate--text": "12th August 2024",
      "pdfx:crossmarkdomains": "sciencedirect.comelsevier.com",
      "pdfx:crossmarkdomainexclusive": "true",
      "pdfx:crossmarkmajorversiondate": "2010-04-23",
      "fci27y939mdunmdagzd_-nsnnm.emlt-pym6gypisotijngmnntb.o9eqn9iknm-snt-tma": "",
      "pdfx:doi": "10.1016/j.bspc.2024.106584",
      "pdfx:robots": "noindex",
      "prism:aggregationtype": "journal",
      "prism:copyright": "Â© 2024 Elsevier Ltd. All rights are reserved, including those for text and data mining, AI training, and similar technologies.",
      "prism:coverdate": "2024-10-01",
      "prism:coverdisplaydate": "1 October 2024",
      "prism:doi": "10.1016/j.bspc.2024.106584",
      "prism:issn": "1746-8094",
      "prism:number": "PA",
      "prism:pagerange": "106584",
      "prism:publicationname": "Biomedical Signal Processing and Control",
      "prism:startingpage": "106584",
      "prism:url": "https://doi.org/10.1016/j.bspc.2024.106584",
      "prism:volume": "96",
      "tdm:policy": "https://www.elsevier.com/tdm/tdmrep-policy.json",
      "tdm:reservation": "1",
      "xmp:createdate": "2024-08-12T05:57:43",
      "xmp:creatortool": "Elsevier",
      "xmp:metadatadate": "2024-08-12T06:11:04",
      "xmp:modifydate": "2024-08-12T06:11:04",
      "xmpmm:documentid": "uuid:1b499bed-4ce8-4c0c-b682-0d58baae1cbe",
      "xmpmm:instanceid": "uuid:b6ed5266-03cb-4855-90c0-636283357f42",
      "xmprights:marked": "True"
    },
    "text": "Biomedical Signal Processing and Control 96 (2024) 106584\nAvailable online 10 July 2024\n1746-8094/Â© 2024 Elsevier Ltd. All rights are reserved, including those for text and data mining, AI training, and similar technologies.\nContents lists available at ScienceDirect\nBiomedical Signal Processing and Control\njournal homepage: www.elsevier.com/locate/bspc\nA contrastive weakly supervised learning to characterize malignant prostate\nlesions in BP-MRI\nYesid GutiÃ©rrez \na\n, John Arevalo, Fabio MartÃ­nez \na,\nâˆ—\na \nBiomedical Imaging Vision and Learning Laboratory (BIV L\n2 \nab), Universidad Industrial de Santander, Carrera 27 #9, Bucaramanga, Colombia\nA R T I C L E I N F O\nKeywords:\nProstate cancer diagnosis\nWeakly supervised learning\nBP-MRI\nContrastive learning\nA B S T R A C T\nBackground: Early prostate cancer diagnosis from BP-MRI studies constitutes the new guidelines in PI-RADS-\n2 protocol. From such sequences, malignant lesions are characterized by morphological and cellular density\nproperties. Nonetheless, such characterization is sensible to high variability among different MRI sequences\nand prostate zones, which often results in misdiagnosis. Current supervised deep learning representations have\nshown promising results to support the diagnosis. Nevertheless, such strategies require a huge amount of\nannotated MRI lesions.\nMethods: This work introduces a weakly supervised learning approach from a deep BP-MRI representation\nto classify malignant lesions, overcoming supervised deep learning approaches that use MP-MRI. Redundant\nand rich tissue patches are taken from the prostate gland, allowing to adjust a representation to discriminate\nbetween lesions and healthy tissue. This pretext task is performed under a contrastive learning scheme, learning\nan embedding projection that groups similar patches while maximizing the distance among different classes.\nThen, from such representation, it is carried out a fine-tuning process to discriminate between benign and\nmalignant lesions related to prostate cancer lesions.\nResults: The proposed approach outperformed baseline studies in a public dataset, achieving a ROC-AUC of\n0.85 using the 80% of the available annotated lesions. Also, using 20% of the lesions, the proposed strategy\nachieved a ROC-AUC of 0.80, being a promising result to transfer models to the clinical routine.\nConclusions: The projected contrastive embedded space better differentiates malignant lesion regions. Also,\nthis representation could be transferred in scenarios with scarce labeled data, approaching self-supervised\nlearning of raw original data.\n1. Introduction\nThe World Health Organization reported around five million\nprostate cancer cases worldwide, and more than 375.000 deaths dur-\ning 2020, being the most prevalent cancer [1]. Typically, in clinical\nroutine, prostate cancer screening includes a Prostatic Specific Antigen\n(PSA), Digital Rectal Examination (DRE), and Trans-Rectal Ultrasound-\nguided Biopsy (TRUS). Nevertheless, these current screening tests usu-\nally lead to over-diagnosis, and over-treatment, which results in a\nsilent spread of malignant tumors [2]. For instance, the PSA reports a\nsignificantly low specificity (around 25%) related to misdiagnosis asso-\nciated with other pathologies such as Prostatitis, and Benign Prostate\nHyperplasia (BPH), while some positive cancer cases report low PSA\nvalues [3â€“5]. To overcome such limitations, the PSA is carried out with\nDRE to confirm the disease [3,6]. Nonetheless, the DRE is invasive and\nhighly subjective (only 66% of physicians can palpate correctly the\nprostate gland), reporting a low agreement (kappa = 0.25) [7]. Besides,\nâˆ— \nCorresponding author.\nE-mail address: famarcar@saber.uis.edu.co (F. MartÃ­nez).\nthe TRUS guided biopsy is performed to analyze from a microscopical\nperspective the presence of carcinoma regions [8] more than 30%\nof clinically significant cancer regions are lost [9]. Moreover, there\nare associated risks during the procedure, such as rectal bleeding and\nsepsis [10].\nThe development of a non-invasive mechanism for early diagno-\nsis is key to effective treatments, which may impact the mortality\nindex [1]. Nowadays, the multi-parametric MRI sequences (MP-MRI)\nare a promising alternative to enhance the diagnosis, treatment, and\nsurveillance of prostate cancer, evidencing a detection enhancement\nof cancer lesions, effective tumorâ€™s volume characterization [11â€“13],\nand even an in-vivo estimation of tumors progression [11]. The T2\nweighted imaging (T2WI) is used to depict the morphological features\nof the prostatic gland, allowing the identification of abnormal masses,\nor potential lesions. Complementary, diffusion-weighted imaging (DWI)\nallows to quantify the cellular density of the tissues as a measure\nhttps://doi.org/10.1016/j.bspc.2024.106584\nReceived 22 May 2023; Received in revised form 1 May 2024; Accepted 24 June 2024\n\nBiomedical Signal Processing and Control 96 (2024) 106584\n2\nY. GutiÃ©rrez et al.\nof impedance in the signal, being the Apparent Diffusion Coefficient\n(ADC), and the Maximum B value (B-VAL), two important maps es-\ntimated from such sequences. Complementary, the ğ¾\nğ‘¡ğ‘Ÿğ‘ğ‘›ğ‘  \nmaps from\ndynamic contrast-enhanced (DCE) sequences allow characterization\nof potentially malignant lesions from micro-circulation properties, re-\nflecting vascular patterns as a response of gadolinium influx [11,14].\nThe PI-RADS (Prostate Imaging Reporting and Data System) is today\nthe standard system to quantify, stratify and diagnose cancer lesions,\ninvolving MP-MRI sequences. This system standardizes the interpreta-\ntion, reporting, and scoring about particular suspicious lesions [12,15].\nTypically, the classical PI-RADS involves the integrated analysis of\nT2- Weighted Imaging (T2WI), Diffusion Weighted Imaging (DWI),\nand Dynamic Contrast Enhanced (DCE) sequences. Nevertheless, in the\ncurrent system, PI-RADS-v2 is only taken into account a bi-parametric\nperspective that includes T2WI and DWI sequences, avoiding the use of\ncontrast agents, minimizing time of acquisition, associated costs, and\nachieving a relatively similar diagnosis [15,16]. Despite the reported\nadvances in MRI characterization, the analysis of these sequences is\nstill fully dependent on the radiologistsâ€™ experience, introducing an\ninter-reader variability. In fact, some studies report a very moderate\nagreement to characterize lesion malignancy levels over the prostatic\ngland (ğ‘˜ = 0.419), and a low agreement among radiologists to perform\nsuch interpretation over the transitional zone (TZ) (ğ‘˜ = 0.250) [17].\nDeep learning strategies have recently emerged as a support di-\nagnosis tool to characterize malignant lesions, observed from MRI\nsequences [18â€“20]. These strategies typically involve convolutional\narchitectures, integrate different MRI sequences, and also include other\nclinical variables to classify localized lesions in the assessment of\nprostate cancer [20]. Nonetheless, these approaches are trained from\nfully supervised schemes using typically the cross entropy loss function,\nwhile requiring a huge amount of labeled and stratified data. Addition-\nally, training schemes based on the cross-entropy loss function follow\nan inter-class minimization without considering the high variability\namong samples of the same class. Besides, such annotations are biased\nfrom radiologist annotations or may be labeled from biopsy output re-\nsults, which results expensive, limiting the collection of representative\nsamples to adjust deep representations. We hypothesize that alternative\nlearning schemes can positively impact the characterization of prostate\nMRI lesion observations, exploiting redundant information of prostatic\ntissue without any lesion annotations. In such cases, the strategy may\nlearn lesion findings variability because of biopsy annotations but\nalso weight and contrast samples concerning neighborhood patches\nthat correspond to control samples. Also, the weakly supervision al-\nlows to capture of complex multimodal relationships without a strict\ndependency on expert annotations that may bias the computational\napproximation.\nThis work introduces a Weakly Supervised Contrastive Learning\n(WSCL) to discriminate clinically significant regions in the prostate\ngland, simulating clinical scenarios, where labeled MRI regions are\ntypically scarce. This strategy learns a low-dimensional embedding\nspace, measuring textural mutual information of classes to close ob-\nservation of the same class while maximizing distance among different\nclasses. The embedding space is refined in two stages to achieve the\ndiscrimination of patches associated with prostate cancer. The achieved\nresults show that the proposed WSCL strategy (ROC-AUC = 0.84)\noutperformed baseline approaches based on supervised learning (ROC-\nAUC = 0.75) and supervised contrastive learning (ROC-AUC = 0.77)\nschemes, using only the 40% of the available labeled MRI regions. The\nmain contributions of this work are:\nâ€¢ A weakly supervised approach that overcomes scarce biopsy-level\nannotations by including a first training phase that considers\npseudo-random-selected benign prostate gland tissue.\nâ€¢ A refined low-dimensional embedding space, learned from con-\ntrastive learning mechanisms, that first adjusts a deep repre-\nsentation to separate benign vs. malign patches, approaching\nnon-informed prostate patches. Then, a refined separation with\nlabeled information allows to adjustment of embedding space to\nseparate cancer lesions from the rest of the lesion observations.\nâ€¢ The proposed approach supports the characterization and dis-\ncrimination of prostate cancer lesions using BP-MRI sequences\nwithout additional requirements of contrast agent observations,\nas suggested by the current PI-RADS protocol.\n2. Current work\nDeep learning representations, to characterize lesions from MP-\nMRI prostate sequences, usually involve convolutional architectures\nwith early or late fusions, or also include clinical variables [18,20].\nSuch deep learning approaches deal with large observational vari-\nability by adjusting multi-level node representations in hierarchical\nconfigurations. Nonetheless, adjusting these node parameters requires a\nhuge amount of observations to discover non-linear boundaries among\nclasses. In fact, such labeled data should follow other conditions such as\nbeing well-balanced among class samples and confidence with respect\nto a proper ground truth that avoids biasing the model to wrong label\nsamples. This last issue is a critical point in medical applications with\nlabels that have a strong dependency on radiologist annotations with\nvariability in the observations.\nFrom a discriminative point of view, such models are typically\ntrained using the cross-entropy loss function, which guides training\nfrom an inter-class minimization but without additional considerations.\nFor instance, Mehrtash et al. [18] proposed a 3D convolutional ar-\nchitecture that integrated different regions of ADC, B-VAL, and ğ¾\nğ‘¡ğ‘Ÿğ‘ğ‘›ğ‘ \nimages together with zonal information of the prostate gland. How-\never, the proposed strategy deals with the lack of labeled lesions by\nusing an artificial data augmentation strategy, that includes translations\nand flipping. Such transformations could introduce noise in the final\nrepresentation related to textural information of neighboring organs\nsuch as the bladder or the rectum. Afterward, Liu et al. proposed\nXMASNET, a 3D deep convolutional model inspired by the classical\nVGG16 [21]. In this approach, Liu early integrated DWI, ADC, and\nğ¾\nğ‘¡ğ‘Ÿğ‘ğ‘›ğ‘  \nsequences through stacking of the sequences very similar to RGB\ncolor codification. Although the author performed an ablation study\namong the available sequences, the best architecture lost morphological\npatterns available in T2WI sequences. In addition, the proposed ar-\nchitecture does not discriminate lesions across prostate regions, which\ncould introduce variability and sensitivity lost on the prediction outputs\nto support the diagnosis.\nSubsequently, Chen et al. [19] proposed a transfer-based com-\nputational strategy from a pre-trained deep learning model to inte-\ngrate T2WI sequences, ADC, and ğ¾\nğ‘¡ğ‘Ÿğ‘ğ‘›ğ‘  \nimages. Although this strategy\navoided training a relatively deep architecture, textural differences\nbetween medical and natural images limit the deep representation.\nThen Hung Le et al. [22] integrated T2WI and ADC sequences using\ntwo deep learning models responsible for representing prostate lesions\nin a low-dimensional space. As a result, a vector encoding was obtained\nto represent the textural similarity between morphological and cellular\ndensity patterns for the same tissue. Then, Tsehay et al. [23] proposed\na three-channel convolution scheme to integrate T2WI, ADC, and B-\nValue = 2000 images, that follows deep supervision to characterize\nprostate lesions at different scales. On the other hand, Yang et al. [24]\nproposed a strategy that first identify regions from T2WI images, and\nthen used a co-trained deep learning model from patterns related to\ncellular density (ADC), and morphology (T2WI). This approach esti-\nmated a probability map allowing to localize tumor malignancy but\nfrom a co-learning perspective, the representation may be sensitive to\nfalse-positive tumors.\nMore recently, Wang et al. [25] proposed a weakly supervised\nstrategy to localize prostate lesions from an end-to-end learning scheme\nof two staked nets: (1) one dedicated to the detection and multi-\nmodal co-registration and the other (2) dedicated to the detection\n\nBiomedical Signal Processing and Control 96 (2024) 106584\n3\nY. GutiÃ©rrez et al.\nof prostate lesions. Both nets are mutually trained from a weakly\nsupervised scheme with the pretext task to identify slices with potential\nlesions, guiding the extraction of relevant features. The model included\noverlapping, consistency, and classification loss functions to find poten-\ntial prostate lesion localization. This approach nonetheless may have\nstrong dependencies on co-registration tasks with may propagate error\nlocalization to the lesioned network. Also, this model is limited to\ninclude intensity variability that represents prostate tissues at different\nzones. Subsequently, Bleker et al. [20] extracted radiomic features,\nwhich were classified with a Random Forest (RF) algorithm and the\nExtreme Gradient Boosting (XGB) algorithm. This work was validated\nonly in the peripheral zone of the prostate gland. Additionally, Aldoj\net al. [26], proposed a multimodal 3D convolutional architecture using\ndifferent combinations of T2WI, DWI, ADC sequences, and ğ¾\nğ‘¡ğ‘Ÿğ‘ğ‘›ğ‘  \nim-\nages. More recently, Sunoqrot et al. [27] trained a logistic regression\nclassifier computing special radiomics, that achieved a homogeneous\nT2WI tissue representation considering edge references from regions of\nthe prostate gland related to fat (brighter tissues) and muscles (darker\ntissues). Also, a 3D multimodal late fusion learning strategy using\na LeNet-based architecture integrated ğ¾\nğ‘¡ğ‘Ÿğ‘ğ‘›ğ‘  \nimages and T2WI MRI\nsequences to characterize malignant lesions [28]. Nevertheless, such\na late fusion strategy lacks deep end-to-end learning, since the late\nfusion depends on a voting scheme with fixed ğ›¼ values. Additionally,\nan Inception-based 3D architecture was proposed to integrate ADC and\nğ¾\nğ‘¡ğ‘Ÿğ‘ğ‘›ğ‘  \nmaps through 1 Ã— 1 convolutional modules cross-correlating\nthe textural patterns of these MRI maps [29]. Such a scheme is not\naligned which may inside in the observation of same patterns across\ndifferent MRI sequences. Moreover, the difference in resolution and\ncoordinates among sequences may affect the proposed architecture,\nincluding surrounding lesion regions that could introduce noise in the\nfinal representation. Another study considered a 3D CNN architecture\nthat learned prostate tumor patterns under a multimodal contrastive\nlearning strategy to support prostate cancer diagnosis [30]. In this\nstudy, malignant lesions were characterized in a fully supervised con-\ntrastive learning space by measuring the textural similarities among\nBP-MRI cancer lesions, while globally contrasting each positive region\nagainst batches of benign lesions. Despite this study reporting promis-\ning results, the performance dropped in critical scenarios probably due\nto the lack of an unsupervised learning strategy to coarsely adjust the\ndeep backbone for medical imaging domain-related tasks.\nAlso, Duran et al. [31] proposed a BP-MRI deep architecture in-\nspired by the U-Net model to characterize and localize malignant\nprostate regions from scribble annotations. The major contribution\nof this study was the penalty term introduced in the loss function\nto minimize segmentations that are outside of the real annotation\nboundaries. However, such a study requires scribble annotations during\nthe training stage to learn the localizations of suspicious prostate\nlesions. More importantly, since these lesions are usually small-sized,\nsome important shapes of the lesions could be misclassified as back-\nground. Likewise, Pellicer-Valero et al. [32] proposed a BP-MRI Retina\nU-net architecture empowered by feature pyramid networks to deal\nwith small-sized lesions and support prostate cancer diagnosis. Also,\nKaragoz et al. [33] proposed a 3D BP-MRI self-adapting deep network\ninspired by the 3D nn-U-Net architecture to support prostate cancer\ndiagnosis. Despite these studies allowing the automatic localization and\ndepiction of suspicious lesions related to prostate cancer disease, these\napproaches also require the radiologist to do an intense pixel-wise level\nannotation, which is scarce in real clinical domains. More importantly,\nto tackle this limitation, these studies simulated lesion masks from a\ngrowing seed algorithm, using the centroid of the lesions as a seed. As\na result, the resultant learned lesion segmentations could be biased due\nto tissue intensity level homogeneities between different prostate zones\nand tumors.\n3. Proposed approach\nThis work introduces a weakly supervised contrastive learning\nframework that allows the discrimination of malignant lesions related\nto prostate cancer disease over BP-MRI sequences. To overcome the\nrequirement of a huge amount of malignancy annotations, the proposed\nstrategy follows contrastive learning mechanisms that update a deep\nrepresentation in two stages. Firstly, the deep representation generates\nan embedding space to separate prostate lesions from non-labeled\ncontrol regions. This training level adjusts the backbone to receive and\nmodel textural patterns related to multimodal MRI information. Then,\nfine-grained training is achieved with a limited set of annotated BP-\nMRI lesions correlated with biopsy studies, following also contrastive\nembedding tasks. Hence, the embedding space allows a prostate cancer\nclassification following a simple linear separation. Fig. 1 illustrates the\npipeline of the proposed approach.\n3.1. Selection of candidate BP-MRI regions\nProstate lesions have a remarked variability in size and texture,\neven reporting lesions with diameters less than 5 mm with associated\nPIRADS degrees three and four [34]. In consequence, typical supervised\nschemes underestimated many prostatic regions without lesions but\nwith close similarity to regions of interest. These schemes may be over-\nfitting particular lesion representations but are impractical in clinical\nscenarios by mapping suspicious regions, that do not exhibit learned\ntextural patterns. To supply lesion label scarcity, a pretext task is firstly\ndefined as the discrimination between lesion and control (non-lesion)\nprostate tissue, allowing to recovery of redundant textural information,\nalong prostate tissue, to adjust the deep convolutional scheme ğ‘“\nğœƒ \n(.).\nIn such a particular case, the main objective is to consider control\ntissue samples that are close to lesions and correspond to the prostatic\ngland. This initial adjusting of the convolutional scheme ğ‘“\nğœƒ \n(.) allows\nto learn the variability of prostate gland texture. Fig. 2 summarizes\nthe workflow to select challenging control patches that close textural\ninformation w.r.t a lesion reference. The proposed strategy extracts\nğ‘˜ multimodal MRI regions from radiological findings in the prostate\ngland at the lesion-affected and control regions. As observed in Fig. 2,\na set of samples are extracted to form training tuples, taking as refer-\nence the observed T2WI prostate tissue (ğ‘¥\nğ‘˜\nğ‘‡ 2ğ‘Š ğ¼ \n). Control samples are\nbounded from a segmentation mask, ğ‘ˆ\nğœƒ \n(ğ‘¥\nğ‘˜\nğ‘‡ 2ğ‘Š ğ¼ \n) [35], while lesions are\ntaken around the expert annotation.\nSuspicious Lesion location samples {ğ‘Ÿ\nğ‘˜\n} are taken from radiologists\nat image-level annotations but without considering any label associ-\nated to cancer disease. Afterward, over each lesion location ğ‘Ÿ\nğ‘˜\n, it is\nprojected a 2D Gaussian mask ğº(ğ‘¥\nğ‘˜\nğ‘‡ 2ğ‘Š ğ¼ \n) = îˆº\n2\n(ğœ‡, ğœ\n2\n) that simulates\nthe boundaries of a lesion. Such mask is centered on the suspicious\nlocation ğ‘Ÿ\nğ‘˜\n, as (ğœ‡\nğ‘¥ \n= ğ‘Ÿ\nğ‘˜\nğ‘¥\n, ğœ‡\nğ‘¦ \n= ğ‘Ÿ\nğ‘˜\nğ‘¦ \n), and the spread of the lesion is modeled\nby the co-variances (ğœ\n2\nğ‘¥ \n= ğœ\n2\nğ‘¦ \n= ğ‘ ğ‘), where ğ‘ ğ‘ âˆˆ (0.5, 1) is a random\nnumber that simulates different propagations of lesions over tissues.\nComplementary, the control region of the prostate gland is estimated\nfrom the Gaussian lesion-mask as ğ»(ğ‘¥\nğ‘˜\n) = ğ‘ˆ\nğœƒ \n(ğ‘¥\nğ‘˜\nğ‘‡ 2ğ‘Š ğ¼ \n) âˆ’ ğº(ğ‘¥\nğ‘˜\nğ‘‡ 2ğ‘Š ğ¼ \n). In\nthis case, ğ‘ˆ\nğœƒ \n(ğ‘¥\nğ‘˜\nğ‘‡ 2ğ‘Š ğ¼ \n) is a prostate mask that defines the boundary of\nthe gland tissue. From this control region, MRI candidate regions are\nrandomly selected preserving a minimum Euclidean distance ğ among\nall the selected regions. As it can be noted at the first training stage,\nwe use only the spatial reference of lesions, i.e., the centroids to bound\na Gaussian 2D mask. Considering such spatial-Gaussian reference, we\nextract control regions outside of the probable pixels that belong to\nlesions. Hence, we considered a weak supervision stage because we\nonly consider spatial references of suspicious regions but not their\nmalignancy types (cancer or other type of lesions). As an extra criterion,\nthe minimum distance ğ‘‘ was defined among sampled patches to ensure\nvariability and avoid region overlapping on the training batch.\nFig. 2-A illustrates the selection of prostate patches. These selected\npoints {ğ‘Ÿ\nğ‘–\n, ğ‘Ÿ\nğ‘— \n, â€¦ ğ‘Ÿ\nğ‘˜\n} are then projected to each MRI sequence to carry\n\nBiomedical Signal Processing and Control 96 (2024) 106584\n4\nY. GutiÃ©rrez et al.\nFig. 1. Pipeline of the proposed approach. A first contrastive learning stage was carried out to generate an embedding space to separate control prostate regions from lesion\nregions. In this stage, a weakly adjusting is achieved by training tuples, composed by a query patch ğ±\nğ¢ \n(prostate/lesion), a positive sample ğ±\nğ£ \n, and a set of 2N ğ±\nğ£ \nnegative samples.\nThen, in the second training stage is learned a contrastive embedding space to discriminate between benign and malignant lesions. In this case, training tuples are coded from\nannotated lesions.\nFig. 2. Extraction of candidate regions from MRI sequences. First in (A), using a state-\nof-the-art architecture ğ‘ˆ\nğœƒ \nthe prostate gland is delineated and divided into anatomical\nzones. Then, using random 2D Gaussian maps, the prostate gland is divided into control\nand lesion-affected regions. Afterward, in section (B) these findings are projected to\nADC, BVAL, and T2WI sequences in order to obtain a multimodal representation of the\nsame regions. Finally, in section (C) multimodal volumetric patches are extracted from\nthe projected findings.\nout a bi-parametric extraction of candidate regions. In Fig. 2-C is shown\na set of volumetric regions ğ‘¥\nğ‘— \n, ğ‘¥\nğ‘˜ \nthat was extracted from the weakly\nlabeled findings ğ‘Ÿ\nğ‘— \n, ğ‘Ÿ\nğ‘˜ \nfor each of the BP-MRI sequences, obtaining, as a\nresult, a set of weakly selected MRI observations related to control and\nlesion tissue samples.\n3.2. Prostate lesions deep visual representation\nThe proposed approach is flexible to adopt any multimodal deep ar-\nchitecture adjusting the deep representation from a weakly supervised\nstrategy. The main objective of this work is to validate an alternative\ntraining scheme, following two complementary stages. At each stage,\nthe training samples are contrasted with positive and negative samples\nto build a discriminative embedding space. In such a case, any con-\nvolutional scheme could be adapted in the recovery of a contrastive\nembedding space. So, we implemented a state-of-the-art multimodal\nconvolutional architecture that encodes suspicious prostate lesions into\nembedded vectors, integrating zonal information, and MRI sequences\nfrom independent branches [18]. This architecture has evidenced a\nrobust coding of each input modality, following independent branches\nthat output a bank of features with the most relevant texture patterns.\nIn this context, this architecture is a suitable candidate architecture\nto follow the contrastive hypothesis and generate embeddings with\nsufficient capability to discriminate cancer lesions. In the literature,\nsuch representation was originally evaluated with input branches re-\nlated to ADC, B-VAL, and ğ¾\nğ‘¡ğ‘Ÿğ‘ğ‘›ğ‘  \nmaps, incorporating also one-hot\nencoding vectors with anatomical zone information. Most of the studies\nin computer-aided diagnosis of prostate cancer in MRI propose to\narbitrarily use different MRI sequences, ignoring the PI-RADS proto-\ncol guidelines. In this work, this deep representation was adjusted\nto receive BP-MRI inputs (ADC, BVAL, T2WI), as suggested in the\nlast PI-RADS v2 protocol [11,12]. Additionally, the representation was\nadjusted following a typical supervised learning scheme using the cross-\nentropy loss function. Contrary, we adapted the architecture to encode\nmultimodal MRI sequences as an embedding vector following a con-\ntrastive learning scheme. This scheme has also the capability to include\nzonal information on the prostate gland to deal with such variability in\nthe interpretation.\nConsequently, each BP-MRI region ğ±\nğ¢ \nis represented as a set of bi-\nparametric parameters ğ±\nğ¢ \nâˆ¶ {ğ‘¥\nğ‘–\nğ´ğ·ğ¶ \n, ğ‘¥\nğ‘–\nğµğ‘‰ ğ´ğ¿\n, ğ‘¥\nğ‘–\nğ‘‡ 2ğ‘Š ğ¼ \n}, where ğ±\nğ¢ \nâˆˆ ğ‘\nğ‘¤Ã—â„Ã—ğ‘ Ã—ğ‘\nis a volumetric region of ğ‘¤ Ã— â„ pixels with ğ‘  slices. In such case, ğ‘\nis a specific MRI sequence from ğ±\nğ¢\n. Afterward, each of these volu-\nmetric parameters is mapped to independent convolutional branches\nğ‘“\nğœƒ,ğ‘\n(ğ±\nğ¢\nğ©\n) â†’ ğ³\nğ¢\nğ© \nthat represents each sequence as a low dimensional\nembedding vector (ğ³\nğ¢\nğ© \nâˆˆ ğ‘\nğ§\n). So, we encoded each lesion ğ±\nğ¢ \nwith\nthree independent convolutional branches {ğ‘“ \nğ‘–\nğœƒ,ğ‘\n1 \n, ğ‘“ \nğ‘–\nğœƒ,ğ‘\n2 \n, ğ‘“ \nğ‘–\nğœƒ,ğ‘\n3 \n}, obtaining\nas a response a set of embedding vectors {ğ³\nğ¢\nğŸ\n, ğ³\nğ¢\nğŸ\n, ğ³\nğ¢\nğŸ‘\n}. From Mp-mri\nobservations, the lesions exhibit different textural and color features\ndepending on the prostate zone [36]. For instance, there exist current\nchallenges to differentiate prostatitis in the peripheral zone and hyper-\nplasia in the transitional zone, regarding malignant lesions associated\nwith cancer [37]. In such a sense, prostate observations may include a\nbroad variability depending on the zone, being a key component in the\ndifferentiation of cancer-associated regions, even for experts. In conse-\nquence, zonal information ğ³\nğ¢\nğ³ğ¨ğ§ğ \nis also integrated as one hot encoding\nvector that represents the prostate region anatomy. To do so, a simple\none-hot-encoding projection was carried out for each zone (a total of\nthree zones were considered). Each resultant zone vector ğ‘§\nğ‘§ğ‘œğ‘›ğ‘’ \nhas a\ndimension of three (ğ‘§\nğ‘§ğ‘œğ‘›ğ‘’ \nâˆˆ ğ‘\n3\n), which is then concatenated with each\nimage modality projection. The overall representation can be expressed\nas ğ‘§\nğœƒ \n(ğ±\nğ¢\n) =\n[\nğ‘“ \nğ‘–\nğœƒ,ğ´ğ·ğ¶ \n; ğ‘“ \nğ‘–\nğœƒ,ğµğ‘‰ ğ´ğ¿\n; ğ‘“ \nğ‘–\nğœƒ,ğ‘‡ 2ğ‘Š ğ¼ \n; ğ‘§\nğ‘§ğ‘œğ‘›ğ‘’\n]\n, obtaining concatenated\nembedding branches ğ‘§\nğ‘–\nğœƒ \n, where ğ‘§\nğ‘–\nğœƒ \nâˆ¶\n[\nğ‘§\nğ‘–\nğœƒ,ğ´ğ·ğ¶ \n; ğ‘§\nğ‘–\nğœƒ,ğµğ‘‰ ğ´ğ¿\n, ğ‘§\nğ‘–\nğœƒ,ğ‘‡ 2ğ‘Š ğ¼ \n; ğ‘§\nğ‘–\nğ‘§ğ‘œğ‘›ğ‘’\n]\nis\nan embedding projection that encodes and represents BP-MRI observa-\ntions.\n\nBiomedical Signal Processing and Control 96 (2024) 106584\n5\nY. GutiÃ©rrez et al.\n3.3. Contrastive learning for weakly-supervised regions\nLesion characterization is here performed through a contrastive\nlearning strategy, discriminating samples projected into a geometrical\nspace. A key initial step during learning is the selection of contrastive\nbatches, that allow the representation of low-dimensional samples, with\nsufficient discriminative character to be separated from a linear hypoth-\nesis. From a contrastive perspective, we require tuples constituted by\na query-positive pair (ğ±\nğ¢\n, ğ±\nğ£\n) and a set of adversarial (negative) samples\n{ğ±\nğ¤\n}. Each sample can adopt the role of query, so exhaustively the total\nof samples (|{ğ‘¥\nğ‘— \n}|) can be exploited during training. Nevertheless, using\nall these triplets during the learning stage is computationally expensive\n(ğ‘‚(ğ‘›\n3\n)), and due to sample redundancy, it could overfit the mutual\ninformation among MRI regions at the final representation.\nTo effectively codify training batches, we select predominant sam-\nples maximizing the intra-class mutual textural information among\nquery-positive MRI regions while maximizing the inter-class variabil-\nity between query-negative regions. For so doing, we opted for a\nlesion mining strategy that selects the most representative positive\nğ±\nğ£\n, and negative ğ±\nğ¤ \nsamples concerning a reference a query region\nğ±\nğ¢\n. To obtain the closest positive neighboring region ğ±\nğ£\n, we used\nthe easy-positive mining criteria, which is formally defined as ğ±\nğ£ \n=\narg min\nğ±\nğ£ \nğ‘‘(ğ‘“ (ğ±\nğ¢\n), ğ‘“ (ğ±\nğ£\n)) âˆ¶ ğ¶(ğ±\nğ£\n) = ğ¶(ğ±\nğ¢\n), where ğ‘‘(, ) is the euclidean\ndistance among the encoded representation of two MRI regions, and\nğ¶() represents the class or category of an MRI region. Regarding\nnegative MRI region ğ±\nğ¤\n, we selected the semi-hard negative mining\ncriteria, which is formally defined as ğ±\nğ¤ \n= arg min\nğ±\nğ¤ \nğ‘‘(ğ‘“ (ğ±\nğ¢\n), ğ‘“ (ğ±\nğ¤\n)) âˆ¶\nğ¶(ğ±\nğ¤\n) â‰  ğ¶(ğ±\nğ¢\n), and it is constrained by ğ‘‘(ğ‘“ (ğ±\nğ¢\n), ğ‘“ (ğ±\nğ¤\n)) > ğ‘‘(ğ‘“ (ğ±\nğ¢\n), ğ‘“ (ğ±\nğ£\n)).\nThese mining criteria allowed us to filter the available triplets of\nlabeled MRI regions from |{ğ‘¥\nğ‘–\n}| âˆ— |{ğ‘¥\nğ‘— \n}| âˆ— |{ğ‘¥\nğ‘˜\n}| possible triplets\nto |{ğ‘¥\nğ‘–\n}| representative triplets that maximize the similarities among\npositive regions, preserving also some textural variability for negative\nregions [38].\nAfterward, these MRI region tuples are used to train a deep archi-\ntecture ğ‘“\nÌ‚\nğœƒ \n, which projects MRI tissue patches as embedding vectors\n(ğ‘“\nÌ‚\nğœƒ \n(ğ±\nğ¢\n) âˆˆ ğ‘\nğ‘›\n). In classical deep supervised training, labels are used to\ncreate a discriminative space, considering each sample prediction, in-\ndependently. Contrarily, the use of positive and negative tuples allows\nthe generating of a geometrical embedding space, forcing samples of\nthe same class to remain as close as possible while maximizing distance\namong samples of different classes (see Fig. 3). To do so, the deep\narchitecture ğ‘“\nÌ‚\nğœƒ \nis adjusted following the Normalized Temperature (NT-\nXent) contrastive loss function over the embedding representation of\nthe regions ğ‘“\nÌ‚\nğœƒ \n(ğ±\nğ¢\n). This objective loss measures the quality of the repre-\nsentations as an error function that depends on the mutual information\namong MRI regions. Such loss is formally defined as follows:\nğ¿\nğ‘–,ğ‘— \n= âˆ’ğ‘™ğ‘œğ‘” \nğ‘’ğ‘¥ğ‘(\n[\nğ‘“\nÌ‚\n \nğœƒ \n(ğ±\nğ¢\n)\nâŠ¤ \nâ‹… ğ‘“\nÌ‚\nğœƒ \n(ğ±\nğ£\n)\n] \nâˆ•ğœ)\nâˆ‘\n2ğ‘\nğ‘˜=1 \n1\n[ğ‘˜â‰ ğ‘–]\nğ‘’ğ‘¥ğ‘(\n[\nğ‘“\nÌ‚\nğœƒ \n(ğ±\nğ¢\n)\nâŠ¤ \nâ‹… ğ‘“\nÌ‚\nğœƒ \n(ğ±\nğ¤\n)\n] \nâˆ•ğœ) \n(1)\nA close distance of samples with the same class ((ğ±\nğ¢\n, ğ±\nğ£\n)) is regu-\nlarized by the distance of ğ±\nğ¢ \nregarding a batch of adversarial samples\nğ±\nğ¤\n. In such cases, the best query samples are close to positive samples\nbut also demonstrate a remarkable distance from negatives. The batch\nof negative samples plays a key role in generating a discriminative\nspace, and also giving importance to query samples during the training\nscheme. Also, the ğœ is a temperature hyper-parameter that scales the\nmutual information among samples. For instance, when 0 < ğœ < 1\nthe resultant error from the loss is magnified according to the textural\nvariability among MRI regions. This behavior is desirable to allow\ntextural information to have a major influence on the error. Besides,\nthe ğœ hyperparameter may help for possible numerical instability,\nduring the learning phase. Hence, the NT-Xent loss adjusts the model\nweights\nÌ‚\n ğœƒ for our model ğ‘“ that increases the textural similarity w.r.t\nthe query-anchor, and positive MRI regions ğ‘’ğ‘¥ğ‘(\n[\nğ‘“\nÌ‚\nğœƒ \n(ğ±\nğ¢\n)\nâŠ¤ \nâ‹… ğ‘“\nÌ‚\nğœƒ \n(ğ±\nğ£\n)\n] \nâˆ•ğœ),\nwhile at the same time increasing the textural variability between the\nquery-anchor and all the available negative MRI regions in the batch\nFig. 3. Weakly contrastive learning of regions. (A) a set of pseudo-labeled prostate\nregions are projected into an embedding representation using the adjusted deep network\nğ‘“\nğœƒ \n(.). A tuple example {ğ‘¥\nğ‘– \n, ğ‘¥\nğ‘— \n, {ğ‘¥\nğ‘˜ \n}\n2ğ‘ \n} is constituted by a query control prostate region\nğ‘¥\nğ‘– \n, a positive prostate region ğ‘¥\nğ‘— \n(separated by a minimum distance ğ‘‘, and a set of 2ğ‘\nnegative lesion samples ğ‘¥\nğ‘˜ \n). Tuples also can be constructed from query lesions. (B) the\ncontrastive embedding space that preserves a small distance among MRI regions of the\nsame class. Additionally, such embedding space can linearly separate the classes of the\nMRI regions.\nâˆ‘\n2ğ‘\nğ‘˜=1 \n1\n[ğ‘˜â‰ ğ‘–]\nğ‘’ğ‘¥ğ‘(\n[\nğ‘“\nÌ‚\nğœƒ \n(ğ±\nğ¢\n)\nâŠ¤ \nâ‹… ğ‘“\nÌ‚\n \nğœƒ \n(ğ±\nğ¤\n)\n] \nâˆ•ğœ). Also, a major contribution in this\nwork is the capability of the resultant embedding representation not\nonly from the biopsy annotations indicating the presence or absence of\ncancer disease, but also from the mutual information among textural\nMRI patterns, which is a main advantage w.r.t traditional supervised\nlearning schemes based on the Cross-entropy loss function.\nTwo progressive contrastive learning stages were implemented to\nobtain a deep representation with sufficient sensitivity to separate\ncancer-related regions from other prostatic lesions. The two phases are\ndescribed as follows:\nâ€¢ first training stage (control vs. lesion). In the first stage, the\ndeep representation is updated from control and lesion regions\nextracted from the MRI sequences under a weekly supervised\nscheme. From these regions, the deep network ğ‘“\nÌ‚\nğœƒ \n(.) is adjusted\nto discriminate the MRI regions from an embedding projection.\nA tuple example {ğ‘¥\nğ‘–\n, ğ‘¥\nğ‘— \n, {ğ‘¥\nğ‘˜\n}\n2ğ‘ \n} is constituted by a query control\nprostate region ğ‘¥\nğ‘–\n, a positive prostate region ğ‘¥\nğ‘— \n(separated by a\nminimum distance ğ‘‘), and a set of 2ğ‘ negative lesion samples\nğ‘¥\nğ‘˜\n. In case tuples from query lesions ğ‘¥\nğ‘–\n, the positive sample ğ‘¥\nğ‘—\nis a lesion sample taken from a typical transformation, and the\nnegative samples are prostate samples.\nâ€¢ second training stage (Bening vs. malignant). In this stage,\nthe deep representation is transferred from the previous stage\nand refined (ğ‘“\nÌ‚\nğœƒ \n(.) â†’ ğ‘“\nğœƒ \n), using the radiologists and biopsy\nannotations. During this stage, the deep model is also updated\nfollowing contrastive learning mechanisms but preserving the\ntextural similarity among lesions, and the variability concerning\nmalignant regions related to prostate cancer. Thus, the proposed\napproach learns an optimal projection of prostate lesions into\na geometrical representation. This contrastive embedding space\nis linearly separable. Hence, MRI samples are projected to this\nembedding space, retrieving the corresponding embedding repre-\nsentation, which thereafter is automatically classified. To carry\nout such training, tuples are formed from strict labeled lesion\nsamples. Two tuple options are possible in such case, considering\nas reference being lesions or considering as query reference the\nmalignant lesions. For instance, a tuple from a malignant lesion\nğ‘¥\nğ‘–\n, has as positive sample ğ‘¥\nğ‘— \nother malignant sample (taken from a\ntransformation), and the negative samples {ğ‘¥\nğ‘˜\n}\n2ğ‘ \nare taken from\nsamples of being lesions.\n\nBiomedical Signal Processing and Control 96 (2024) 106584\n6\nY. GutiÃ©rrez et al.\nFig. 4. A malignant prostate lesion located in the peripheral zone observed from the\nmulti-parametric perspective of MRI sequences. From left to right: ADC, Maximum B\nValue, T2WI, and ğ¾\nğ‘¡ğ‘Ÿğ‘ğ‘›ğ‘  \nsequences.\nTo discriminate malignant lesions related to prostate cancer dis-\nease, a logistic regression classifier was trained using tuples:\n{(ğ‘“\nğœƒ \n(ğ‘¥\n1\n), ğ‘¦\n1\n), â€¦ , (ğ‘“\nğœƒ \n(ğ‘¥\nğ‘–\n), ğ‘¦\nğ‘–\n), â€¦ (ğ‘“\nğœƒ \n(ğ‘¥\nğ‘›\n), ğ‘¦\nğ‘›\n)}. The descriptor used for\ntraining is composed of the embedded MRI region represen-\ntation ğ‘“\nğœƒ \n(ğ‘¥\nğ‘–\n), and the corresponding annotations about biopsy\nhistopathology examination ğ‘¦\nğ‘–\n. These supervised annotations con-\nfirm the presence of malignant tumors in the prostatic gland. After\nthe training stage, the resultant classifier obtains a hyperplane\nthat defines the boundary of separation between benign and\nmalignant lesions. Then, a projection over embedding space and\na simple linear decision allows to associate the malignancy level\nof each sample.\n4. Materials\n4.1. Dataset\nThis work was trained and validated with data provided by the\npublic SPIE-AAPM-NCI Prostate MR Classification (PROSTATEx) Chal-\nlenge\n1\n[39]. This dataset includes a retrospective study of 204 patients for the\ntraining cohort, that corresponds to MP-MRI studies with a total of 320\nlabeled lesions. For the testing cohort, the dataset is compounded by\n120 studies that correspond to 210 lesions. For each of these lesions, the\ndataset provides radiological label findings related to the localization\nof the lesions and the clinical significance of the lesions supported by\nthe biopsy test. In this study for training, we use a total of 80 samples\nassociated with prostate cancer lesions, and 240 samples related with\nother lesions.\nEach study counts with four sets of MRI sequences: two sets of\nT2-weighted images, ADC and B-VAL images computed from DWI,\nand ğ¾\nğ‘¡ğ‘Ÿğ‘ğ‘›ğ‘  \nimages (computed from dynamic contrast-enhanced (DCE)\nimages) [39]. In clinical routine such modalities complement radi-\nologist analysis and observations, integrating information related to\nmorphology patterns, zonal information, and abnormal activation re-\nlated to malignant tumors. For instance, the T2WI sequences offer\nanatomical lesion information. Then, the information available from\nDWI sequences (ADC and B-value) related to the restriction of water\ndiffusion, may complement lesion discrimination with respect to high\ncellular density regions. In fact, some state-of-the-art studies [40â€“\n42] have reported a correlation between the quantitative ADC maps\nand, the Gleason Score (a measure of prostate cancer aggressiveness\nused mainly in biopsies). For such a reason, the DWI is considered a\ncomplement for T2WI to discriminate potential carcinoma lesions in\nthe prostate gland and contribute to the assessment of the disease. in\nFig. 4, it can be observed a multi-parametric analysis of a malignant\nlesion.\nThese MRI sequences were acquired using two 3T magnetic reso-\nnance scanners, the Siemens MAGNETOM Trio and Skyra under the\nfollowing configurations: T2WI images were acquired using a turbo\nspin echo sequence configuration with a resolution around 0.5 mm\nin-plane, and a slice thickness of 3.6 mm. The DWI sequences were\nobtained with a single-shot echo-planar imaging configuration with\n1 \nhttps://prostatex.grand-challenge.org/.\na resolution of 2 mm in-plane and 3.6 mm slice thickness and with\ndiffusion-encoding gradients in three directions using three b-values\nintensities (50, 400, and 800). Subsequently, the ADC maps were\nautomatically estimated by the scanners software [39].\n4.2. Experimental setup\nA crucial point in a weakly supervised scheme is to define input\ntuples to perform contrastive learning. In this work, we center a 2D\nGaussian (ğœ‡ = 0) around each radiologist annotation with variance\nğœ = [0.0625, 0.25) and over T2WI-MRI sequences. Patches center in each\ndistribution were taken as malignant samples. The control patches were\nthen taken outside from this Gaussian region but inside of prostate\ngland segmentation. These samples were randomly collected with the\nrule that among patches should have a minimum euclidean distance\nğ‘‘ = 40 pixels. Each patch was cropped as volumetric information of\n(12 Ã— 32 Ã— 32).\nRegarding the deep architecture, we designed a 3D convolutional\nneural network ğ‘“\nğœƒ \ninspired by Mehrtashâ€™s architecture [18]. This deep\nmodel characterizes MRI regions as three independent convolutional\nbranches, compounded by nine 3D convolutional layers using the\nLeakyReLu (ğ‘ = 0.3) activation function. The architecture integrated\nthe zonal information using a one-hot encoded embedding vector, fused\nat the end of the convolutional branches.\nDuring the training, each volumetric MRI region was represented\nwith a fused embedding feature vector ğ‘“\nğœƒ \n(ğ‘¥\nğ‘–\n) âˆˆ ğ‘\n128\n. The optimization\nfollows an RMSprop algorithm using a momentum of 0.6 and a learning\nrate of 1 Ã— 10\nâˆ’6\n. The parameters of architecture were empirically\nselected following standard parameters in medical image architectures.\nAdditionally, to measure the textural similarities among MRI re-\ngions, the proposed approach underlies on a contrastive learning\nscheme that was validated using the Triplet Loss (TL), and the NT-Xent\nloss function ğ¿\nğ‘–,ğ‘— \n. From a validation cohort (sub-set of data extracted\nfrom the training cohort) the ğœ parameter was fixed in ğœ = 0.07.\nAlso, to increase the amount of MRI regions, we artificially augmented\nthe samples through image transformations such as random rotations,\nflipping with respect to the horizontal plane, and horizontal and ver-\ntical translations. Considering the importance of textural information\nand the description of lesions in different MP-MRI observations, we\nrestricted the augmentation of data to geometrical transformations.\nIn such cases, the rotation and translation of the patches preserved\nthe textural pattern information, together with the alignment among\ndifferent modalities. Then, the training data was augmented to a total\nof 640.000 samples (originally a total of 320 lesions).\nA main limitation to transfer technologies is the scarce availability\nof redundant, and balanced data in clinical centers. For such a rea-\nson in this work, we validated the proposed approach with different\nchallenging scenarios that only take a reduced amount of training\ndata. In this case, we sub-sampled the original dataset from 20â€“100%,\nkeeping the same proportion of lesions per class as the original dataset.\nAdditionally, all the reported experiments were obtained from the test\nset defined and evaluated by the authors of the PROSTATEx challenge.\nThe metric performance selected by the challenge is the Area Under the\nReceiver Operating Characteristic Curve (ROC-AUC).\n5. Evaluation and results\nA first experiment was carried out to evaluate the performance of\nthe proposed contrastive learning scheme, with respect to different\nstrategies to select candidate patches. Four different strategies were\nconsidered to select relevant MRI patches during the weakly supervised\nstage. The first strategy corresponds to the selection of eight candidate\nregions from neighboring regions that surround the image-level radi-\nologist annotations (neighboring grid). The second strategy (Control\ntissue-A) consists of the selection of key control tissue patches ğ»(ğ‘¥\nğ‘˜\n) es-\ntimated from the prostate segmentation tissue ğ‘ˆ\nğœƒ \n(ğ‘¥\nğ‘˜\nğ‘‡ 2ğ‘Š ğ¼ \n), but excluding\n\nBiomedical Signal Processing and Control 96 (2024) 106584\n7\nY. GutiÃ©rrez et al.\nFig. 5. ROC-AUC performance achieved by different selection strategies to extract relevant candidate MRI regions.\nTable 1\nComparison of different candidate selection strategies using the NT-Xent and the Triplet\nLoss objective functions over all the labeled MRI data.\nLoss function Neighboring Control tissue\ngrid A M B\nNT Xent 0.82 0.81 0.84 0.83\nTriplet loss 0.81 0.81 0.82 0.79\nTable 2\nComparison of different candidate selection strategies using the NT-Xent loss objective\nfunctions and different data configurations.\nDataset Neighboring Control tissue\n% grid A M B\n20 0.69 0.78 0.8 0.65\n40 0.8 0.82 0.84 0.8\n60 0.8 0.81 0.84 0.8\n80 0.83 0.82 0.85 0.82\n100 0.82 0.81 0.84 0.83\nthe lesion affected region maps ğº(ğ‘¥\nğ‘˜\nğ‘‡ 2ğ‘Š ğ¼ \n) centered in the radiologist an-\nnotations. Additionally, for strategies three (Control tissue-M) and four\n(Control tissue-B), we performed a similar strategy but included only\nthe benignant (B) and malignant (M) confirmed lesions respectively.\nThese configurations were run following the NT-Xent loss function and\nthe triplet loss.\nTable 1 summarizes the achieved results for the different strategies\nto select candidate patches regarding two contrastive loss functions.\nThe patches selected from malignant lesions as reference achieved the\nbest performance (0.84 from NT Xent, and 0.82 from triple loss). Such\nselection has a gain of about 2% with respect to the neighboring grid,\nbeing interesting to have a reference with respect to some lesions to\nrecover the other control patches. It should be noted, that a careful\ndelineation of lesion regions allows to better characterization and\ndiscrimination of patches, which in consequence determines a proper\ndeep representation of lesions. Also, the NT-Xent was superior in whole\nexperiments regarding the triplet loss. This could be attributed to\nthe nature of the NT-Xent loss, which globally estimates the textural\nvariability among an anchor region and ğ‘ negative regions.\nIn a second experiment was evaluated the capability of a weakly\ncontrastive approach to capture complex textural lesion patterns from\na reduced set of samples. To do so, we run several experiments using\nrandom subsets taking incremental parts of training data (each 20% of\navailable data). Fig. 5 summarizes the achieved results in this experi-\nment, finding consistent results from the Control tissue-M strategy that\nachieved the best results in all training subsets. It should be noted that\nthe proposed approach achieves a competitive ROC-AUC of 0.84 using\nonly 40% of the total training set, which suggests a potential use on\nTable 3\nComparison of different candidate selection strategies using the Triplet loss objective\nfunction and different data configurations.\nDataset Neighboring Control tissue\n% grid A M B\n20 0.71 0.76 0.79 0.61\n40 0.79 0.81 0.83 0.72\n60 0.82 0.82 0.82 0.76\n80 0.82 0.81 0.82 0.79\n100 0.81 0.81 0.82 0.79\nFig. 6. Results obtained by our proposed WSCL of prostate regions in MRI, and baseline\nsupervised learning schemes under different data configurations. In red, we have the\nwork proposed by Mehrtash using a BCE loss [18], in green, we have a multimodal\nsupervised contrastive learning approach previously proposed [30], and finally, in\npurple, we have our WSCL scheme.\nreal scenarios with scarce label sets. Also, the best result was achieved\nwith 80% of total data in training, achieving a remarkable ROC-AUC\nof 0.85. In such case, the strategy can achieve the best ROC-AUC score\n(0.85) without the total set of available annotations, the score gain that\nmay be associated with the weakly supervised character. Contrary, in\nFig. 5, the supervised contrastive scheme achieves remarkable results\nbut the dependency of annotations impacts the performance, being\nalways inferior to the weakly supervised approximation, during the\nincremental training experimentation. Interestingly, the strategy shows\na robust characterization of malignant lesions, with the capability to\ndeal with complex textural observations of the lesions. This fact may\nbe associated with the weak strategy that approaches other regions of\nthe images to adjust deep representations.\nTo complement such analysis, Table 2 (using NT-Xent loss) and\nTable 3 (from triplet loss) summarize the performance achieved by the\nintroduced strategy, following an incremental training set of data, but\n\nBiomedical Signal Processing and Control 96 (2024) 106584\n8\nY. GutiÃ©rrez et al.\nFig. 7. GradCAM attention maps obtained by the deep visual representation over a malignant lesion; from top to bottom we have ADC, BVAL and T2WI sequences. From left to\nright we have the progression over the attention maps for the weakly pre-trained representation fine-tuned over 0, 20, 40, 60, 80 and 100% of the labeled dataset respectively.\nalso considering different strategies to select patches in the contrastive\nframework. Interestingly, the strategy can capture sufficient variability\nto discriminate patches, using only the 40% of available training data\nbut being careful to select only malignant annotations. In such cases,\nthe malignant annotations may exhibit textural radiological patterns\nthat help during the generation of geometrical embedding space. Also,\nit should be noted that the selection of patches from session references\nplays a key role in scenarios with little data. For instance, the worst\nperformance of the neighboring grid is with less than 60% of training\ndata. Contrary, in scenarios with redundant data, the selection of\npatches has less importance. The validation of such parameters may\nimpact in the generalization performance of the proposed strategy.\nParticularly, to guarantee an appropriate adjustment of a deep strategy\nwith little data, it should be careful with the selection of reference\npatches, especially considering potential malignant regions.\nAs a baseline, we implemented a standard architecture validated in\nthis challenge [18]. Hence, we trained and adjusted this representation\nfollowing a classical binary cross-entropy (BCE) loss function [18],\nfollowing a supervised contrastive learning (SCL) scheme [30] and\nalso with the here introduced weakly supervised strategy (WSCL). To\nevidence the performance of the learning approaches with scarce train-\ning scenarios, we run several experiments with incremental subsets of\nlabeled MRI lesions to measure the capability of these approaches under\nclinical scenarios with the scarcity of labeled MRI data. As observed\nin Fig. 6, the proposed WSCL scheme (Control tissue-M + XT-Xent)\nobtained the best results for all of the available data configurations.\nRemarkably, using only 20% of the labeled lesions, our WSCL achieved\na ROC-AUC of 0.80, obtaining an improvement of 10, and 24% with\nrespect to the reported baseline strategies. It should be also noted, that\ncontrastive schemes result in the best alternative to train schemes in\nscenarios with scarce data, while classical cross-entropy rules remain\nlimited to learn lesion variability.\nA qualitative analysis was here carried out by recovering salience\nmaps that stand out regions, at each MRI parameter, that major con-\ntributes to the final prediction. To recover such attention maps, we\nrun the GradCAM strategy over the selected backbone after the trained\nrepresentation [43]. This strategy back propagated the output predic-\ntion into convolutional branches, allowing evidence of localized regions\nwith major association with respect to the estimations. Fig. 7 shows\nthe retrieved maps for BVAL and ADC maps at different labeled data\nconfigurations. As expected, there is a general major activation of\nthese maps in malignant regions. Interestingly enough, this is coherent\nwith respect to previous studies that support a negative correlation\nbetween the Gleason Grade of Gliomas and the DWI sequence. On the\nother hand, as observed in Fig. 7, the T2WI sequence complements the\ncharacterization depicting textural properties related to the anatomy of\nthe prostatic tissue of study. Additionally, in the second column of the\nfigure, it can be observed how most of the attention maps are already\nlearned from the pre-trained representation that discriminates lesion\nand control regions. Then from left to right, it is possible to evidence\nhow for each of the MRI sequences, the attention maps are fine-tuned\nby progressively including more data to adjust and learn the resultant\ndeep representation.\n6. Discussion\nThis work presented a BP-MRI Weakly Supervised Contrastive\nLearning (WSCL) strategy to characterize and discriminate malignant\nlesions related to prostate cancer disease from suspicious image-level\nannotations. The proposed strategy was adjusted, measuring the mutual\ninformation among MRI samples of prostate tissue, and dealing with\nthe scarcity of biopsy-labels, as typically reported in clinical scenarios.\nTo tackle this problem, this work designed a multi-level training strat-\negy that firstly approaches non-labeled data to coarsely discriminate\nbetween being and malign regions. In such cases, observations are\ncontrasted from mutual information rules, regarding lesion-centered\nregions. Afterward, lesions are classified as malignant or benign, fol-\nlowing a fine-tuning stage concerning the malignancy biopsy-level\nannotations. Classically, the approaches use artificial augmentation\ndata strategies by applying several transformations to data, to cover the\nrequirements of the amount of data to train deep learning representa-\ntions. These strategies are however limited in the variability of domain\ndata, and restricted to learning generalizable representations. Contrary,\nin this work we use random patches from the same image studies,\ntaking advantage of whole prostate volume information. This particular\napproach has a major chance to learn prostate observations, having\nthe potential to achieve appropriate results from unseen observations,\nespecially from other machines and acquisition protocols. Hence, rep-\nresentation is initialized from a learned distribution of control prostate\nregions. As a result, The proposed approach achieves similar results\nw.r.t supervised learning approaches (ROC-AUC of 80%) using only the\n20% of the available training data, and outperforms supervised schemes\nachieving a ROC-AUC of 85%, using only the 80% of total data.\nThe proposed approach significantly increases the amount of train-\ning data by following the weakly supervised strategy, allowing a first\ncoarse classification between benign and malignant tissue. As supported\nin Fig. 7, this first training stage is sufficient to recover abnormal\ntissue patterns, from Bi-MRI studies. Then, in the second training\n\nBiomedical Signal Processing and Control 96 (2024) 106584\n9\nY. GutiÃ©rrez et al.\nstage, the fine-tuning allows more sensible discrimination among ab-\nnormal tissues related to cancer, regarding other prostate affectations.\nAs expected, the maps encoded from DWI such as BVAL and the\nADC seem to be the most important MRI sequences to characterize\nmalignant lesions, this is an interesting fact that has also been reported\nin the literature since DWI maps have shown a correlation with the\nGleason grade of gliomas [40â€“42]. Consequently, we decided to princi-\npally adjust the representation to encode patterns from a bi-parametric\n(bp-MRI) perspective (integrating only T2WI and DWI), according to\nrecently suggested in PI-RADS v2 protocol, avoiding the contrast agent\ndependency [11,12].\nIn the literature, exist different solutions to support malignant lesion\nclassification from MP-MRI studies. For instance, Mehrtashâ€™s work [18],\nproposed a 3d convolutional backbone that learns independent paths\nfor each parameter, which is further integrated into an embedding\nrepresentation. Also, early fusion strategies from inception modules\nhave been implemented to classify prostate cancer lesions [30]. These\napproaches nonetheless lie on classical cross-entropy rule with strictly\nsupervised schemes, that may collapse representation, overfitting spe-\ncific patterns and limiting deep representation to support classification.\nContrary, the proposed approach using a restricted set of parameters\n(Bi-modal representation) overcame the ROC-AUC diagnosis of these\nstrategies by 10 and 20% respectively using only the 20% of the\nlabeled MRI data (see in Fig. 6). Since these three strategies share a\nsimilar pipeline in terms of deep architecture, image pre-processing,\nand cropping of MRI regions, we hypothesize that the obtained results\nare due to our proposed WSCL scheme that not only takes into account\nthe regions labeled by expert radiologists but also the pseudo-labeled\ncontrol MRI regions that were estimated from the prostate gland.\nTo support class imbalance, Chen et al. [19] proposed a Transfer\nLearning scheme from the open ImageNet dataset to adjust the deep\nrepresentation into the clinical domain. As a result, the author obtained\na ROC-AUC of 0.82 using 100% of the annotated data. Additionally,\nother studies in the state-of-the-art such as Liu et al. [21], and Aldoj\net al. [26] were also proposed to characterize malignant regions in\nthe prostate gland using MRI sequences, achieving a ROC-AUC of 0.84\nand 0.89 respectively. Nevertheless, these authors included different\ndata architectures, image pre-processing pipelines, and cropping region\nschemes, which difficult a proper comparison against our proposed\nstrategy. Moreover, these authors did not report more data configura-\ntions to evaluate their performance in clinical scenarios with scarcity\nof data.\nThe proposed strategy achieves competitive results, reaching re-\nmarkable results to face challenging scenarios with a scarcity of biopsy-\nlevel annotations of BP-MRI regions. We hypothesize that weakly super-\nvised learning strategies could empower deep learning applications in\nclinical scenarios by adjusting deep architectures directly with avail-\nable observations, facilitating the technological transfer to the clinical\nroutine. Besides, the contrastive scheme and the design of alterna-\ntive learning tasks may impact the domain distribution modeling,\nand in consequence, a remarkable capability to achieve remarkable\ndiscrimination among lesion classes. Despite important advances in\nsuch representations, the principal limitation of the proposed approach\nis the current dependency of image-level annotations to localize the\nconfirmed lesions. These annotations may induce an expert bias, even\nfor the selection of non-lesion regions. It is expected to design schemes\nthat learn from a huge amount of random observations, which supply\nthe marked dependency of such annotations. Also to consider schemes\nthat compute and include bias of experts into the learning schemes to\nbetter approximate the generalization capabilities of the representation.\nEven further clinical studies should be carried out to measure the im-\npact of support lesion characterization of BP-MRI samples, considering\nmultiple observers, biopsy results, and further evolution of patient from\ndiagnosis.\n7. Conclusions\nThis work introduced a weakly supervised contrastive learning strat-\negy to characterize malignant MRI regions related to prostate cancer\ndisease. The proposed strategy encodes multimodal morphological and\ncellular density patterns available in T2WI and DWI (specifically from\nADC and B-VAL maps) to represent control and lesion regions in a\ncontrastive embedded space. A fine-tuning stage allowed cancer lesion\ndiscrimination from a linear hyperplane. The achieved results suggest\nthat contrasting prostate lesions with control regions, under a weakly\nsupervised learning scheme improved the characterization and discrim-\nination of malignant lesions, especially in clinical scenarios with limited\namount of labeled lesions. Moreover, our experimental setup shows\nthat the deep representation obtained a decent ROC-AUC performance\nof 0.8 in the diagnosis using only the 20% of annotated lesions. As\na result, the deep representation better differentiates between healthy\nand lesion regions in a projected contrastive embedded space. This\nWSCL strategy could be potentially used in clinical scenarios with\nscarce annotated MRI data to empower deep learning applications\nin the clinical stages and support the diagnosis of the disease. Fu-\nture works include the evaluation of extensive datasets, from different\nscanners, allowing the establishment of a generalization level of the\nweakly supervised strategy. Also, there will be carried out different\nefforts to avoid human labeled annotations during training, through the\ndevelopment of self-supervised mechanisms.\nCRediT authorship contribution statement\nYesid GutiÃ©rrez: Writing â€“ review & editing, Writing â€“ origi-\nnal draft, Software, Methodology, Investigation, Data curation. John\nArevalo: Writing â€“ original draft, Validation, Supervision, Method-\nology, Formal analysis, Conceptualization. Fabio MartÃ­nez: Writing\nâ€“ review & editing, Writing â€“ original draft, Validation, Supervision,\nResources, Project administration, Methodology, Investigation, Funding\nacquisition, Formal analysis, Conceptualization.\nDeclaration of competing interest\nThe authors declare the following financial interests/personal rela-\ntionships which may be considered as potential competing interests:\nFabio Martinez reports financial support was provided by Ministry of\nScience, Technology and Innovation of Colombia (MINCIENCIAS).\nData availability\nData will be made available on request.\nAcknowledgments\nThe authors thank Ministry of Science, Technology and Innovation\nof Colombia (MINCIENCIAS) for supporting this research work by the\nproject â€˜â€˜Mecanismos computacionales de aprendizaje profundo para\nsoportar tareas de localizaciÃ³n, segmentaciÃ³n y pronÃ³stico de lesiones\nasociadas con accidentes cerebrovasculares isquÃ©micosâ€™â€™, with code\n91934.\nCompliance with ethical standards\nThis research study was conducted retrospectively using human\nsubject data made available in open access by the 2017 SPIE Medical\nImaging Symposium on the SPIE-AAPM-NCI Prostate MR Classification\nChallenge [39]. Ethical approval was not required as confirmed by the\nlicense attached with the open-access data.\n\nBiomedical Signal Processing and Control 96 (2024) 106584\n10\nY. GutiÃ©rrez et al.\nReferences\n[1] R.L. Siegel, et al., Univ. West. Ont. Med. J. 82 (1) (2013) 10â€“11.\n[2] S. Loeb, M.A. Bjurlin, J. Nicholson, T.L. Tammela, D.F. Penson, H.B. Carter,\nP. Carroll, R. Etzioni, Overdiagnosis and overtreatment of prostate cancer, Eur.\nUrol. 65 (6) (2014) 1046â€“1055.\n[3] M.J. Barry, Prostate-specificâ€“antigen testing for early diagnosis of prostate\ncancer, N. Engl. J. Med. 344 (18) (2001) 1373â€“1377.\n[4] B. HolmstrÃ¶m, M. Johansson, A. Bergh, U.-H. Stenman, G. Hallmans, P. Stattin,\nProstate specific antigen for early detection of prostate cancer: longitudinal study,\nBMJ 339 (2009).\n[5] J. Gudmundsson, J.K. Sigurdsson, L. Stefansdottir, B.A. Agnarsson, H.J. Isaksson,\nO.A. Stefansson, S.A. Gudjonsson, D.F. Gudbjartsson, G. Masson, M.L. Frigge, et\nal., Genome-wide associations for benign prostatic hyperplasia reveal a genetic\ncorrelation with serum levels of PSA, Nat. Commun. 9 (1) (2018) 1â€“8.\n[6] E.D. Crawford, E.P. DeAntoni, R. Etzioni, V.C. Schaefer, R.M. Olson, C.A. Ross,\nT.P.C.E. Council, Serum prostate-specific antigen and digital rectal examination\nfor early detection of prostate cancer in a national community-based program,\nUrology 47 (6) (1996) 863â€“869.\n[7] A. Zhang, T. Fear, H. Ahmed, Digital rectal examination in prostate cancer\nscreening, Univ. West. Ont. Med. J. 82 (1) (2013) 10â€“11.\n[8] H.U. Ahmed, A. El-Shater Bosaily, L.C. Brown, R.S. Kaplan, Y. Colaco-Moraes, K.\nWard, R.G. Hindley, A. Freeman, A.K. Kirkham, R. Oldroyd, et al., The PROMIS\nstudy: A paired-cohort, blinded confirmatory study evaluating the accuracy of\nmulti-parametric MRI and TRUS biopsy in men with an elevated PSA, 2016.\n[9] E.C. Serefoglu, S. Altinova, N.S. Ugras, E. Akincioglu, E. Asil, M.D. Balbay,\nHow reliable is 12-core prostate biopsy procedure in the detection of prostate\ncancer? Can. Urol. Assoc. J. 7 (5â€“6) (2013) E293.\n[10] M.R. Quinlan, D. Bolton, R.G. Casey, The management of rectal bleeding\nfollowing transrectal prostate biopsy: A review of the current literature, Can.\nUrol. Assoc. J. 12 (3) (2018) E146.\n[11] G. Murphy, et al., The expanding role of MRI in prostate cancer, Am. J.\nRoentgenol. 201 (6) (2013) 1229â€“1238.\n[12] J.O. Barentsz, et al., ESUR prostate MR guidelines 2012, Eur. Radiol. 22 (4)\n(2012) 746â€“757.\n[13] M. de Rooij, E.H. Hamoen, J.J. FÃ¼tterer, J.O. Barentsz, M.M. Rovers, Accuracy\nof multiparametric MRI for prostate cancer detection: a meta-analysis, Am. J.\nRoentgenol. 202 (2) (2014) 343â€“351.\n[14] C. Cuenod, D. Balvay, Perfusion and vascular permeability: basic concepts and\nmeasurement in DCE-CT and DCE-MRI, Diagn. Interv. Imaging 94 (12) (2013)\n1187â€“1204.\n[15] H. Vargas, A. HÃ¶tker, D. Goldman, C. Moskowitz, T. Gondo, K. Matsumoto, B.\nEhdaie, S. Woo, S. Fine, V. Reuter, et al., Updated prostate imaging reporting\nand data system (PIRADS v2) recommendations for the detection of clinically\nsignificant prostate cancer using multiparametric MRI: critical evaluation using\nwhole-mount pathology as standard of reference, Eur. Radiol. 26 (6) (2016)\n1606â€“1612.\n[16] K.K. Porter, A. King, S.J. Galgano, R.L. Sherrer, J.B. Gordetsky, S. Rais-Bahrami,\nFinancial implications of biparametric prostate MRI, Prostate Cancer Prostatic\nDis. 23 (1) (2020) 88â€“93.\n[17] M.D. Greer, J.H. Shih, N. Lay, T. Barrett, L. Bittencourt, S. Borofsky, I. Kabakus,\nY.M. Law, J. Marko, H. Shebel, et al., Interreader variability of prostate imaging\nreporting and data system version 2 in detecting and assessing prostate cancer\nlesions at prostate MRI, Am. J. Roentgenol. (2019) 1.\n[18] A. Mehrtash, et al., Classification of clinical significance of MRI prostate\nfindings using 3D convolutional neural networks, in: Medical Imaging 2017:\nComputer-Aided Diagnosis, Vol. 10134, SPIE, 2017, 101342A.\n[19] Q. Chen, et al., A transfer learning approach for classification of clinical\nsignificant prostate cancers from mpMRI scans, in: Medical Imaging 2017:\nComputer-Aided Diagnosis, Vol. 10134, SPIE, 2017, 101344F.\n[20] J. Bleker, et al., Multiparametric MRI and auto-fixed volume of interest-based\nradiomics signature for clinically significant peripheral zone prostate cancer, Eur.\nRadiol. 30 (3) (2020) 1313â€“1324.\n[21] S. Liu, et al., Prostate cancer diagnosis using deep learning with 3D multipara-\nmetric MRI, in: Medical Imaging 2017: Computer-Aided Diagnosis, Vol. 10134,\nSPIE, 2017, 1013428.\n[22] M.H. Le, J. Chen, L. Wang, Z. Wang, W. Liu, K.-T.T. Cheng, X. Yang, Automated\ndiagnosis of prostate cancer in multi-parametric MRI based on multimodal\nconvolutional neural networks, Phys. Med. Biol. 62 (16) (2017) 6497.\n[23] Y.K. Tsehay, N.S. Lay, H.R. Roth, X. Wang, J.T. Kwak, B.I. Turkbey, P.A. Pinto,\nB.J. Wood, R.M. Summers, Convolutional neural network based deep-learning\narchitecture for prostate cancer detection on multiparametric magnetic resonance\nimages, in: Medical Imaging 2017: Computer-Aided Diagnosis, Vol. 10134, SPIE,\n2017, pp. 20â€“30.\n[24] X. Yang, C. Liu, Z. Wang, J. Yang, H. Le Min, L. Wang, K.-T.T. Cheng, Co-\ntrained convolutional neural networks for automated detection of prostate cancer\nin multi-parametric MRI, Med. Image Anal. 42 (2017) 212â€“227.\n[25] Z. Wang, C. Liu, D. Cheng, L. Wang, X. Yang, K.-T. Cheng, Automated detection\nof clinically significant prostate cancer in mp-MRI images based on an end-to-end\ndeep neural network, IEEE Trans. Med. Imaging 37 (5) (2018) 1127â€“1139.\n[26] N. Aldoj, S. Lukas, M. Dewey, T. Penzkofer, Semi-automatic classification of\nprostate cancer on multi-parametric MR imaging using a multi-channel 3D\nconvolutional neural network, Eur. Radiol. 30 (2) (2020) 1243â€“1253.\n[27] M.R. Sunoqrot, G.A. Nketiah, K.M. SelnÃ¦s, T.F. Bathen, M. Elschot, Automated\nreference tissue normalization of T2-weighted MR images of the prostate using\nobject recognition, Magn. Reson. Mater. Phys. Biol. Med. 34 (2021) 309â€“321.\n[28] Y. GutiÃ©rrez, J. Arevalo, F. MartÃ­nez, A ktrans deep characterization to measure\nclinical significance regions on prostate cancer, in: 15th International Symposium\non Medical Information Processing and Analysis, Vol. 11330, SPIE, 2020, pp.\n80â€“88.\n[29] Y. GutiÃ©rrez, J. Arevalo, F. MartÃ­nez, An inception-based deep multiparametric\nnet to classify clinical significance MRI regions of prostate cancer, Phys. Med.\nBiol. 67 (22) (2022) 225004.\n[30] Y. GutiÃ©rrez, J. Arevalo, F. MartÃ¡nez, Multimodal contrastive supervised learning\nto classify clinical significance MRI regions on prostate cancer, in: 2022 44th\nEMBC, IEEE, 2022, pp. 1682â€“1685.\n[31] A. Duran, G. Dussert, C. Lartizien, Learning to segment prostate cancer by\naggressiveness from scribbles in bi-parametric MRI, in: Medical Imaging 2022:\nImage Processing, Vol. 12032, SPIE, 2022, pp. 178â€“184.\n[32] O.J. Pellicer-Valero, J.L. Marenco Jimenez, V. Gonzalez-Perez, J.L. Casanova\nRamon-Borja, I. Martin Garcia, M. Barrios Benito, P. Pelechano Gomez, J. Rubio-\nBriones, M.J. RupÃ©rez, J.D. MartÃ­n-Guerrero, Deep learning for fully automatic\ndetection, segmentation, and gleason grade estimation of prostate cancer in\nmultiparametric magnetic resonance images, Sci. Rep. 12 (1) (2022) 2975.\n[33] A. Karagoz, D. Alis, M.E. Seker, G. Zeybel, M. Yergin, I. Oksuz, E. Karaarslan,\nAnatomically guided self-adapting deep neural network for clinically significant\nprostate cancer detection on bi-parametric MRI: a multi-center study, Insights\nImaging 14 (1) (2023) 1â€“11.\n[34] E. Ã–zden, Ã‡. AkpÄ±nar, A. Ä°biÅŸ, E. Kubilay, A. Erden, Ã–. Yaman, Effect of lesion\ndiameter and prostate volume on prostate cancer detection rate of magnetic\nresonance imaging: Transrectal-ultrasonography-guided fusion biopsies using\ncognitive targeting, Turk. J. Urol. 47 (1) (2021) 22.\n[35] A. Meyer, M. Rakr, D. Schindele, S. Blaschke, M. Schostak, A. Fedorov, C.\nHansen, Towards patient-individual PI-rads v2 sector map: CNN for automatic\nsegmentation of prostatic zones from T2-weighted MRI, in: 2019 16th ISBI, IEEE,\n2019, pp. 696â€“700.\n[36] J. Bao, X. Wang, C. Hu, J. Hou, F. Dong, L. Guo, Differentiation of prostate\ncancer lesions in the transition zone by diffusion-weighted MRI, Eur. J. Radiol.\nOpen 4 (2017) 123â€“128.\n[37] C.E. Lovegrove, M. Matanhelia, J. Randeva, D. Eldred-Evans, H. Tam, S. Miah, M.\nWinkler, H.U. Ahmed, T.T. Shah, Prostate imaging features that indicate benign\nor malignant pathology on biopsy, Transl. Androl. Urol. 7 (Suppl 4) (2018) S420.\n[38] H. Xuan, et al., Improved embeddings with easy positive triplet mining, in:\nProceedings of the IEEE/CVF Winter Conference on Applications of Computer\nVision, 2020, pp. 2474â€“2482.\n[39] G. Litjens, et al., Computer-aided detection of prostate cancer in MRI, IEEE Trans.\nMed. Imaging 33 (5) (2014) 1083â€“1092.\n[40] L. Zhang, Z. Min, M. Tang, S. Chen, X. Lei, X. Zhang, The utility of diffusion\nMRI with quantitative ADC measurements for differentiating high-grade from\nlow-grade cerebral gliomas: evidence from a meta-analysis, J. Neurol. Sci. 373\n(2017) 9â€“15.\n[41] Y.-C. Hu, L.-F. Yan, Q. Sun, Z.-C. Liu, S.-M. Wang, Y. Han, Q. Tian, Y.-Z. Sun,\nD.-D. Zheng, W. Wang, et al., Comparison between ultra-high and conventional\nmono b-value DWI for preoperative glioma grading, Oncotarget 8 (23) (2017)\n37884.\n[42] S. Chen, P. Hou, L. Lou, X. Jin, T. Wang, J. Xu, The correlation between MR\ndiffusion-weighted imaging and pathological grades on glioma, Eur. Rev. Med.\nPharmacol. Sci. 18 (13) (2014) 1904â€“1909.\n[43] R.R. Selvaraju, M. Cogswell, A. Das, R. Vedantam, D. Parikh, D. Batra, Grad-\ncam: Visual explanations from deep networks via gradient-based localization, in:\nProceedings of the IEEE International Conference on Computer Vision, 2017, pp.\n618â€“626.",
    "version": "5.3.31"
  },
  {
    "numpages": 11,
    "numrender": 11,
    "info": {
      "PDFFormatVersion": "1.7",
      "Language": "en-US",
      "EncryptFilterName": null,
      "IsLinearized": true,
      "IsAcroFormPresent": false,
      "IsXFAPresent": false,
      "IsCollectionPresent": false,
      "IsSignaturesPresent": false,
      "Creator": "Elsevier",
      "Custom": {
        "CrossMarkDomains[1]": "elsevier.com",
        "CrossmarkMajorVersionDate": "2010-04-23",
        "CreationDate--Text": "22nd April 2025",
        "ElsevierWebPDFSpecifications": "7.0.1",
        "robots": "noindex",
        "doi": "10.1016/j.bspc.2025.107656",
        "CrossMarkDomains[2]": "sciencedirect.com",
        "CrossmarkDomainExclusive": "true"
      },
      "ModDate": "D:20250422022255Z",
      "Author": "Yuming Zhong",
      "Title": "Bounding boxes for weakly-supervised breast cancer segmentation in DCE-MRI",
      "Keywords": "Breast cancer,DCE-MRI,Weakly-supervised segmentation,Bounding box annotation,Knowledge distillation",
      "CreationDate": "D:20250422022249Z",
      "Producer": "Acrobat Distiller 8.1.0 (Windows)",
      "Subject": "Biomedical Signal Processing and Control, 105 (2025) 107656. doi:10.1016/j.bspc.2025.107656"
    },
    "metadata": {
      "crossmark:crossmarkdomains": "elsevier.comsciencedirect.com",
      "crossmark:crossmarkdomainexclusive": "true",
      "crossmark:doi": "10.1016/j.bspc.2025.107656",
      "crossmark:majorversiondate": "2010-04-23",
      "dc:format": "application/pdf",
      "dc:identifier": "10.1016/j.bspc.2025.107656",
      "dc:publisher": "Elsevier Ltd",
      "dc:description": "Biomedical Signal Processing and Control, 105 (2025) 107656. doi:10.1016/j.bspc.2025.107656",
      "dc:subject": [
        "Breast cancer",
        "DCE-MRI",
        "Weakly-supervised segmentation",
        "Bounding box annotation",
        "Knowledge distillation"
      ],
      "dc:title": "Bounding boxes for weakly-supervised breast cancer segmentation in DCE-MRI",
      "dc:creator": [
        "Yuming Zhong",
        "Zeyan Xu",
        "Chu Han",
        "Zaiyi Liu",
        "Yi Wang"
      ],
      "jav:journal_article_version": "VoR",
      "pdf:creationdate--text": "22nd April 2025",
      "pdf:producer": "Acrobat Distiller 8.1.0 (Windows)",
      "pdf:keywords": "Breast cancer,DCE-MRI,Weakly-supervised segmentation,Bounding box annotation,Knowledge distillation",
      "pdfx:creationdate--text": "22nd April 2025",
      "pdfx:crossmarkdomains": "sciencedirect.comelsevier.com",
      "pdfx:crossmarkdomainexclusive": "true",
      "pdfx:crossmarkmajorversiondate": "2010-04-23",
      "tgji2ogykytupmtugogelmcnnmd6llwinnd6gzgn.y.mpy9qjn92ro9eqnmakmmmsot-tma": "",
      "pdfx:doi": "10.1016/j.bspc.2025.107656",
      "pdfx:robots": "noindex",
      "prism:aggregationtype": "journal",
      "prism:copyright": "Â© 2025 Elsevier Ltd. All rights are reserved, including those for text and data mining, AI training, and similar technologies.",
      "prism:coverdate": "2025-07-01",
      "prism:coverdisplaydate": "1 July 2025",
      "prism:doi": "10.1016/j.bspc.2025.107656",
      "prism:issn": "1746-8094",
      "prism:pagerange": "107656",
      "prism:publicationname": "Biomedical Signal Processing and Control",
      "prism:startingpage": "107656",
      "prism:url": "https://doi.org/10.1016/j.bspc.2025.107656",
      "prism:volume": "105",
      "tdm:policy": "https://www.elsevier.com/tdm/tdmrep-policy.json",
      "tdm:reservation": "1",
      "xmp:createdate": "2025-04-22T02:22:49",
      "xmp:creatortool": "Elsevier",
      "xmp:metadatadate": "2025-04-22T02:22:55",
      "xmp:modifydate": "2025-04-22T02:22:55",
      "xmpmm:documentid": "uuid:1b499bed-4ce8-4c0c-b682-0d58baae1cbe",
      "xmpmm:instanceid": "uuid:b6ed5266-03cb-4855-90c0-636283357f42",
      "xmprights:marked": "True"
    },
    "text": "Contents lists available at ScienceDirect\nBiomedical Signal Processing and Control\njournal homepage: www.elsevier.com/locate/bspc\nBounding boxes for weakly-supervised breast cancer segmentation in\nDCE-MRI\nYuming Zhong \na\n, Zeyan Xu \nb\n, Chu Han \nc,d\n, Zaiyi Liu \nc,d,\nâˆ—\n, Yi Wang \na ,\nâˆ—âˆ—\na \nSmart Medical Imaging, Learning and Engineering (SMILE) Lab, Medical UltraSound Image Computing (MUSIC) Lab, School of Biomedical Engineering, Shenzhen\nUniversity Medical School, Shenzhen University, Shenzhen, 518060, China\nb \nDepartment of Radiology, The Third Affiliated Hospital of Kunming Medical University, Yunnan Cancer Hospital, Yunnan Cancer\nCenter, Kunming, 650000, China\nc \nDepartment of Radiology, Guangdong Provincial Peopleâ€™s Hospital (Guangdong Academy of Medical Sciences), Southern Medical\nUniversity, Guangzhou, 510080, China\nd \nGuangdong Provincial Key Laboratory of Artificial Intelligence in Medical Image Analysis and Application, Guangzhou, 510080, China\nA R T I C L E I N F O\nKeywords:\nBreast cancer\nDCE-MRI\nWeakly-supervised segmentation\nBounding box annotation\nKnowledge distillation\nA B S T R A C T\nAccurate segmentation of cancerous regions in breast dynamic contrast-enhanced magnetic resonance imaging\n(DCE-MRI) is crucial for the diagnosis and prognosis assessment of high-risk breast cancer. Deep learning\nmethods have achieved success in this task. However, their performance heavily relies on large-scale fully\nannotated training data, which are time-consuming and labor-intensive to acquire. To alleviate the annotation\neffort, we propose a simple yet effective bounding box supervised segmentation framework, which consists\nof a primary network and an ancillary network. To fully exploit the bounding box annotations, we initially\ntrain the ancillary network. Specifically, we integrate a bounding box encoder into the ancillary network to\nserve as a naive spatial attention mechanism, thereby enhancing feature distinction between voxels inside\nand outside the bounding box. Additionally, we convert uncertain voxel-wise labels inside bounding box into\naccurate projection labels, ensuring a noise-free initial training process. Subsequently, we adopt an alternating\noptimization scheme where self-training is performed to generate voxel-wise pseudo labels, and a regularized\nloss is optimized to correct potential prediction error. Finally, we employ knowledge distillation to guide\nthe training of the primary network with the pseudo labels generated by the ancillary network. We evaluate\nour method on an in-house DCE-MRI dataset containing 461 patients with 561 biopsy-proven breast cancers\n(mass/non-mass: 319/242). Our method attains a mean Dice value of 81.42%, outcompeting other weakly-\nsupervised methods in our experiments. Notably, for the non-mass-like lesions with irregular shapes, our\nmethod can still generate favorable segmentation with an average Dice of 79.31%. The code is publicly available\nat https://github.com/Abner228/weakly_box_breast_cancer_seg.\n1. Introduction\nBreast cancer is the most common malignant tumor and the second\nleading cause of cancer-related deaths among Chinese females [1,2].\nThe incidence and mortality rates of breast cancer continue to increase\nin recent years [3]. Early diagnosis and prompt treatment are therefore\nurgently needed in China to improve the survival rate and prognosis\nof breast cancer patients. Several imaging modalities can be applied\nfor the breast examination, including mammography, ultrasonography,\nâˆ— \nCorrespondence to: Department of Radiology, Guangdong Provincial Peopleâ€™s Hospital (Guangdong Academy of Medical Sciences), Southern Medical\nUniversity.\nâˆ—âˆ— \nCorrespondence to: Smart Medical Imaging, Learning and Engineering (SMILE) Lab, Medical UltraSound Image Computing (MUSIC) Lab, School of Biomedical\nEngineering, Shenzhen University Medical School, Shenzhen University.\nE-mail addresses: 2017222028@email.szu.edu.cn (Y. Zhong), zeyx0708@163.com (Z. Xu), hanchu@gdph.org.cn (C. Han), zyliu@163.com (Z. Liu),\nonewang@szu.edu.cn (Y. Wang).\nmagnetic resonance imaging (MRI), and histopathological images [4,5].\nAmong these imaging methods, the MRI has proven as an indispens-\nable modality for the tumor diagnosis, especially beneficial in Chinese\nfemales who often have dense breasts [2,6]. The dynamic contrast-\nenhanced (DCE)-MRI has the capability to reflect tumor features such\nas morphology, texture, and kinetic heterogeneity [7,8], and is with the\nsuperior sensitivity for breast cancer screening [9]. However, contrast\nenhancement may lack disease specificity in many non-tumor regions\nhttps://doi.org/10.1016/j.bspc.2025.107656\nReceived 18 July 2024; Received in revised form 22 January 2025; Accepted 31 January 2025\nBiomedical Signal Processing and Control 105 (2025) 107656\nAvailable online 11 February 2025\n1746-8094/Â© 2025 Elsevier Ltd. All rights are reserved, including those for text and data mining, AI training, and similar technologies.\n\nY. Zhong et al.\nFig. 1. The red annotations in the breast DCE-MRI slices and 3D visualization denote cancerous regions. Breast cancers vary in shape, size, and location. In addition, the\nnon-mass-like cancerous lesion has irregular boundary. All these factors result in difficulties of voxel-wise annotations and training of weakly-supervised segmentation networks.\nsuch as nodules, vessels and internal organs, named as background\nparenchymal enhancement (BPE). Moreover, irregular shapes, various\nsizes and uncertain locations of the cancerous regions all generate\ncomplexity for the cancer diagnosis. Therefore, computer-aided fea-\nture quantification and diagnosis algorithms are expected to facilitate\nradiologists analyze breast DCE-MRI [10,11], in which automatic can-\ncer segmentation is a prior and important step.\nTo assist the radiologists in breast cancer diagnosis, various segmen-\ntation algorithms have been developed for many years [12]. Early stud-\nies applied image processing techniques by utilizing graph-cut segmen-\ntation [13] or analyzing low-level hand-crafted features [14â€“16]. Most\nof these methods require manual interactions, and are lack of efficiency\nfor the computation of volumetric data. Recently, deep-learning-based\nmethods have been successfully applied to segment breast tumor [17â€“\n19]. Compared with traditional methods, deep networks automatically\nextract hierarchical features with powerful representation and per-\nform segmentation in an end-to-end manner, which contribute to the\nimprovement in the segmentation accuracy and efficiency.\nWhile deep segmentation networks have achieved impressive per-\nformance, their efficacy heavily relies on the sufficient data with\nvoxel-wise annotations. But acquisition of such data remains expen-\nsive and time-consuming, as it requires medical expertise. To reduce\nthe huge annotation burden, weakly-supervised strategies have been\nproposed to develop high-performance segmentation models with less\nannotations e.g., image-level annotations [20], scribbles [21,22], ex-\ntreme points [23,24], bounding boxes [25,26]. However, most of\nthese weakly-supervised methods are designed for the task of organ\nsegmentation, might not suitable for the segmentation of lesions with\nvarious shapes and uncertain locations (see Fig. 1). Moreover, non-\nmass-like malignant lesions always have irregular boundaries, as shown\nin Fig. 1. The networks trained using only weak annotations might\nnot powerful enough to delineate such complicated boundaries. Ker-\nvadec et al. [26], and Wang and Xia [27] utilized tightness prior\nfor regularization and trained tumor segmentation models based on\nbounding box annotations. However, it is insufficient for the models\nto accurately segment the foreground within the bounding boxes, as\nthere are various solutions that can satisfy the tightness prior. Dorent\net al. [24] proposed a weakly-supervised segmentation method based\non extreme points annotation. They generated foreground scribbles and\npropagated label by minimizing a regularized loss. However, the initial\nforeground scribbles could be inaccurate. Meng et al. [28] proposed a\nmethod based on orthogonal slices annotation. They estimated tumor\nvolume by orthogonal slices and constrained the model outputs. It is\ninsufficient due to the variability in breast tumor volume.\nBased on these observations, we develop an effective weakly-\nsupervised breast cancer segmentation framework by using only the\nbounding box annotations. The proposed framework consists of a\nprimary network and an ancillary network. To effectively leverage the\nlimited annotation information of the bounding box, we initially train\nthe ancillary network by using bounding box encoder and projection\nconstraint. In particular, the bounding box encoder is devised to al-\nleviate the class imbalance issue, while the projection constraint is\nemployed to avoid introducing noise during the initial training phase.\nIn addition, an alternating optimization scheme is introduced to refine\nthe ancillary network, enabling it to generate high-quality pseudo\nlabels. Finally, we fix the ancillary network and employ knowledge\ndistillation to guide the training of the primary network with the\npseudo labels generated by the ancillary network. The advantage\nof the proposed framework is evaluated on an in-house DCE-MRI\ndataset containing 461 patients with 561 biopsy-proven breast can-\ncers. Experimental results demonstrate our method outperforms several\nweakly-supervised segmentation methods.\nIn summary, we list our contributions as follows:\nâ€¢ We integrate a bounding box encoder into the ancillary network\nto serve as a naive spatial attention mechanism, thereby enhanc-\ning feature distinction between voxels inside and outside the\nbounding box. The performance gain can be transferred to the\nprimary network through knowledge distillation.\nâ€¢ We employ projection constraint to convert uncertain voxel-wise\nlabels inside bounding box into accurate projection labels, ensur-\ning a noise-free initial training process.\nâ€¢ We propose an alternating optimization scheme where self-\ntraining is performed to generate voxel-wise pseudo labels, and a\nregularized loss is optimized to correct potential prediction error.\nâ€¢ For the non-mass-like malignant lesions with irregular shapes,\nour method consistently generates favorable segmentation results,\noutcompeting other weakly-supervised methods in our experi-\nments.\n2. Related work\n2.1. Breast tumor segmentation in MRI\nComputer-assisted breast tumor segmentation in MRI has been a\nlong-standing research topic. Early studies mainly focused on semi-\nautomated algorithms such as graph-cut segmentation [13], Markov\nrandom field [14], random forest classification [15], and fuzzy C-means\nclustering [16]. All these methods require pre-defined slices or regions\ncontaining tumors, which means the radiologists still have to provide\nsome form of annotations in the inference phase.\nRecently, many deep-learning-based methods have achieved domi-\nnant performance in the field of breast tumor segmentation. Benjelloun\net al. [29] employed U-net [30] to conduct tumor segmentation on 2D\nMRI slices. On a dataset containing 43 patients with breast cancer,\nthis method [29] achieved a mean intersection over union (IoU) of\n76%. Wang et al. [31] proposed a tumor sensitive synthesis module\nto suppress false-positive regions with similar contrast enhancement\ncharacteristics to breast tumors, and obtained an average Dice of\n79% on 422 patients. To boost the segmentation performance, several\nstudies designed hierarchical strategies [17,32â€“34]. Zhang et al. [17]\nproposed a mask-guided hierarchical learning framework for breast\nBiomedical Signal Processing and Control 105 (2025) 107656\n2\n\nY. Zhong et al.\ntumor segmentation. In order to address the class-imbalance issue\nin DCE-MRI, cascaded structure was utilized to firstly extract breast\nregion-of-interest (ROI) then segment tumors from the extracted breast\nROI. By using 224 DCE-MRI scans for training, this framework had\na mean Dice of 72% on 48 testing data. Zhou et al. [\n32] combined\ntwo sub-networks, and further designed a 3D affinity learning module\nto refine the segmentation of non-mass enhancement and small-sized\ntumors. The network achieved 78% Dice on 90 breast cancer patients.\nPeng et al. [\n34] also proposed a hierarchical model, which divided\nthe whole segmentation procedure into two steps of coarse tumor\nlocalization and fine tumor segmentation. This model [\n34] got a mean\nDice of 90% on 118 patients with breast tumors. In addition to hi-\nerarchical strategies, leveraging the attention mechanisms is another\neffective solution to improve the segmentation accuracy. Gao et al. [\n35]\nintegrated residual attention structures into the encoder and decoder\nparts of the 2D segmentation network, and attained an average Dice\nof 81% on a test set containing 87 samples. Li et al. [\n36] proposed a\n2D attention network for breast mass segmentation, which generated a\nmean Dice value of 77% on 313 MRI slices. Qiao et al. [\n37] designed\na self-attention module to aggregate tumor information, and achieved\n86% Dice on 59 patients. Huang et al. [\n18] presented a joint-phase\nattention network, which attained Dice values of 88% and 83% on\ninternal and external datasets, respectively.\nAlthough aforementioned methods have achieved promising perfor-\nmance on breast tumor segmentation, they require abundant data with\nvoxel-wise annotations, which can be laborious and time-consuming for\npreparing such annotated dataset.\n2.2. Weakly-supervised segmentation\nTo alleviate the annotation burden, weakly-supervised semantic\nsegmentation has emerged as an active research topic in computer\nvision. Various weak annotations including image-level annotations,\nscribbles, extreme points, and bounding boxes, have been utilized to\nguide the training of segmentation networks [\n38]. Image-level annota-\ntions refer to assigning categories to an entire image, without specifying\nthe location or boundaries of objects. The mainstream method based\non image-level annotations is to first use the classification network to\ngenerate class activation mapping (CAM) [39], as the initial coarse\nlabels for segmentation, and then train the segmentation network.\nScribble annotations refer to drawing a scribble on the main body\nof each object to represent the location of objects. The mainstream\nsolution is to propagate sparse pixel annotations through the similarity\nbetween pixels to generate pseudo labels [40]. Papadopoulos et al. [41]\nfirst proposed extreme points, defined as the left/right/top/bottom-\nmost points of the object, and incorporated extreme points into Grab-\nCut [\n42], leading to a satisfactory initial segmentation for further\ntraining. Bounding box annotations are usually used as the coarse label\nto train segmentation network. Song et al. [43] proposed a box-driven\nclass-wise region masking module to learn class-aware attention maps,\nand trained segmentation network by the fill rate within bounding box.\nAlthough above algorithms have shown efficacy on natural images,\nadditional improvements are required for adaptation on 3D medical\nimages. Wu et al. [\n20] proposed an attentional CAM technique to\nlocalize the lesion regions via image-level annotations, and exploited\nrepresentation learning to further improve lesion segmentation. How-\never, the CAM localization step could easily be inaccurate in DCE-MRI\ndue to the issue of confounding background. The scribble annotation\nis typically not utilized in 3D semantic segmentation. Because scribble\nannotations need to be provided for several slices, otherwise the seg-\nmentation network would over-fit the tiny number of annotated voxels.\nIn contrast, the extreme points of the 3D target region are cheaper\nthan scribbles, and can provide more accurate six sides boundary\ninformation of the foreground. Roth et al. [23] first developed extreme\npoints annotation technique for 3D organ segmentation. Specifically,\nscribbles were generated by searching the shortest path between each\npair extreme points on image gradient magnitude. Then to increase the\nforeground voxels, initial pseudo-masks were made by random walker\nalgorithm [44], using only image gradient information. The key of [23]\nis to accurately generate scribbles and increase foreground voxels, if\nnot, noisy learning would be a knotty task. To constrain the scribbles\npass through the inside of the tumor, Dorent et al. [\n24] proposed to\npenalize the scribbles not only by accumulating the image gradient\nmagnitudes but also Euclidean distance and the network background\nprobabilities. Moreover, a conditional random field (CRF) regularized\nloss was proposed to propagate label and encourage the prediction\nconsistency over homogeneous regions. However, the amount of fore-\nground voxels is far less than background voxels, leading to a severe\nclass imbalance. Similar to extreme points, bounding box also reveals\nsix sides boundary, but without the tangent points of foreground and\nbounding box. Du et al. [\n25] proposed a bounding box supervised\nframework that integrates geometric prior and contrastive similarity,\nwhich heavily relies on geometric prior and is primarily designed for\norgan segmentation rather than lesions. Kervadec et al. [\n26] utilized\ntightness prior and size constraint for regularization. Such regulariza-\ntion strategies contribute to performance enhancement, but still subject\nto the assumption that targets should have a roughly similar volume\nsize.\nExisting weakly-supervised medical image segmentation methods\nhave achieved satisfactory results on their respective datasets, but they\nmay not be effective solutions to perform breast tumor segmentation\ndue to the irregular shapes and varied sizes of breast tumors, espe-\ncially for non-mass-like malignant lesions. To our knowledge, there\nhave been two recent studies [\n28,45] that focus on weakly-supervised\nbreast mass segmentation. Meng et al. [28] employed orthogonal-slice\nto alleviate the annotation cost, then constrained segmentation by\nestimated volume using the partial annotation. The method achieved\na mean Dice of 83% on 28 testing scans. Zhong and Wang [\n45] utilized\nextreme points and designed a similarity-aware propagation learning\nstrategy to train segmentation network. The network achieved 81%\nDice on 185 DCE-MRI scans. In this study, we propose a weakly-\nsupervised framework based on bounding box annotations to improve\nbreast cancer segmentation, especially for non-mass-like lesions.\n3. Method\nOur weakly-supervised segmentation framework includes a primary\nnetwork ğ‘ƒ and an ancillary network ğ‘„, as shown in \nFig. 2. The primary\nnetwork is the main network used in the inference phase, while the\nancillary network is designed to assist the training of the primary\nnetwork. Let ğ‘‹ âˆ¶ ğ›º âŠ‚ R\n3 \nâ†’ R denotes a training volume and ğµ âˆ¶\nğ›º âŠ‚ R\n3 \nâ†’ {0, 1} denotes its corresponding bounding box annotation,\nwhere ğ›º is the spatial domain. ğ›º\nğ‘‚ \nâŠ‚ R\n3 \nâ†’ {0} and ğ›º\nğ¼ \nâŠ‚ R\n3 \nâ†’ {1}\ndefine the space outside and inside the bounding box, respectively, with\nğ›º\nğ‘‚ \nâˆªğ›º\nğ¼ \n= ğ›º. Each bounding box is associated with a cancerous region.\nThe primary network forms a voxel-wise label distribution ğ‘ƒ (ğ‘‹; ğœƒ)\ngiven a training volume ğ‘‹. The ancillary network forms distribution\nğ‘„(ğ‘‹ , ğµ; ğœ™) given a volume ğ‘‹ and its bounding box ğµ. ğœƒ and ğœ™ are the\nrespective parameters of each network.\nTo fully exploit the limited annotation information of the bounding\nbox, we initially train the ancillary network by using the designed\nbounding box encoder (Section \n3.1) and the projection constraint (Sec-\ntion 3.2). Then an alternating optimization scheme (Section 3.3) is em-\nployed to refine the ancillary network. Finally, by leveraging the high-\nquality pseudo labels generated from the ancillary network, the primary\nnetwork is optimized using knowledge distillation (Section \n3.4).\n3.1. Bounding box encoder\nTo directly make use of the bounding box information to enhance\nthe feature learning procedure, we integrate a bounding box encoder\ninto the ancillary network. This design is to embed the bounding\nBiomedical Signal Processing and Control 105 (2025) 107656\n3\n\nY. Zhong et al.\nFig. 2. An overview of the proposed weakly-supervised breast cancer segmentation framework, which consists of a primary network and an ancillary network. During the training\nphase, we first train the ancillary network by minimizing the projection loss. Subsequently, self-training and the conditional random field (CRF) regularized loss are incorporated to\nrefine the ancillary network. Finally, we fix the ancillary network and guide the training of the primary network by knowledge distillation. The projection loss and CRF regularized\nloss are utilized to optimize the primary network as well. Note that the primary network is the main network used in the inference phase, while the ancillary network is designed\nto assist the training of the primary network.\nFig. 3. Two examples to illustrate the conversion from the bounding box to projection\nlabels. The bounding box (green) is projected onto the three edges of a training patch\n(blue). Although the bounding box cannot provide accurate voxel-wise labels, we can\nstill obtain the same information as cancer (red) through projection.\nbox information into the feature encoding. As shown in Fig. 2, the\nbounding box binary tensor ğµ is resized to the target size then pass\nthrough a same 3 Ã— 3 Ã— 3 convolution layer with sigmoid activa-\ntion. Feature maps generated by the first two convolution block of\nthe ancillary network are fused with the bounding box embedding\ninformation through element-wise multiplication, and then fed to the\ndecoder. Such operation can be considered as a naive spatial attention\nmechanism. The features corresponding to voxels outside the bounding\nbox would be discarded during this process. It prevents the network\nfrom struggling to identify BPE regions and cancerous regions, thereby\navoiding interference in the classification of cancer voxels inside the\nbounding box. Moreover, feature distinction between voxels inside and\noutside the bounding box is enhanced, leading to a trivial solution, that\nis to predict the voxels as background while its features be filtered out.\nThen the ancillary network would focus on classifying the voxels inside\nthe bounding box, which alleviates the issue of class imbalance.\n3.2. Projection constraint\nIt is certain that all voxels outside the bounding box should repre-\nsent the background. Hence, the partial cross entropy loss is expected\nto be minimum on those voxels outside the bounding box:\nîˆ¸\nğ‘ğ‘” \n= âˆ’ \n1\n|ğ›º\nğ‘‚ \n|\nâˆ‘\nğ’—âˆˆğ›º\nğ‘‚\nğ‘™ğ‘œğ‘”(1 âˆ’ ğ‘„(ğ‘‹ , ğµ; ğœ™)(ğ’—)), (1)\nwhere ğ’— denotes voxel. By minimizing this loss function, we can achieve\na set of parameters that make the model predict a foreground proba-\nbility of 0 for the voxels outside the bounding box.\nHowever, unlike other weak annotations such as extreme points\nand scribbles, none of the voxels can be identified as foreground\naccording to the bounding box annotation. That is to say for voxels\ninside the bounding box, their labels are uncertain. To tackle this issue,\ninspired by the tightness prior [46], we employ projection constraint\nto convert uncertain voxel-wise labels inside the bounding box into\naccurate projection labels (as illustrated in Fig. 3), which avoids in-\ntroducing noise during the initial training phase. As shown in Fig. 3,\nsuch constraint remains valid when dealing with multiple bounding\nboxes, even in the presence of the overlap among boxes. Based on the\nbounding box annotation, we can establish three inequality constraints\nthat correspond to the three dimensions:\nâˆ‘\nğ‘ âˆˆğ‘º\nğ‘¡\nâˆ‘\nğ’—âˆˆğ‘ \nğ‘„(ğ‘‹ , ğµ; ğœ™)(ğ’—) â‰¥ |ğ‘º\nğ‘¡\n|, (2)\nâˆ‘\nğ‘ âˆˆğ‘º\nğ‘\nâˆ‘\nğ’—âˆˆğ‘ \nğ‘„(ğ‘‹ , ğµ; ğœ™)(ğ’—) â‰¥ |ğ‘º\nğ‘ \n|, (3)\nâˆ‘\nğ‘ âˆˆğ‘º\nğ‘ \nâˆ‘\nğ’—âˆˆğ‘ \nğ‘„(ğ‘‹ , ğµ; ğœ™)(ğ’—) â‰¥ |ğ‘º\nğ‘ \n|, (4)\nwhere ğ‘º\nğ‘¡\n, ğ‘º\nğ‘ \n, ğ‘º\nğ‘  \nrepresent the set of all transverse planes, coronal\nplanes, and sagittal planes inside the bounding box, respectively. The\nelement sizes of these sets correspond to the lengths of the three edges\nof the bounding box.\nHence, the ancillary network is optimized by minimizing îˆ¸\nğ‘ğ‘” \nwith\nthe projection constraints defined in Eqs. (2)â€“(4). We use the log-\nbarrier extensions [26,47] to approximate this constrained optimization\nproblem, which stabilizes the training of the ancillary network. For an\ninequality constraint ğ‘§ â‰¤ 0, the log-barrier extension is defined as:\nÌƒğœ“ \nğ‘¡\n(ğ‘§) =\n{\nâˆ’ \n1\nğ‘¡ \nlog(âˆ’ğ‘§) if ğ‘§ â‰¤ âˆ’ \n1\nğ‘¡\n2\nğ‘¡ğ‘§ âˆ’ \n1\nğ‘¡ \nlog( \n1\nğ‘¡\n2 \n) + \n1\nğ‘¡ \notherwise, \n(5)\nwhere ğ‘¡ is a parameter related to the barrier. By using the log-barrier\nextension, we define our projection loss as:\nBiomedical Signal Processing and Control 105 (2025) 107656\n4\n\nY. Zhong et al.\nîˆ¸\nğ‘ğ‘Ÿğ‘œğ‘— \n= îˆ¸\nğ‘ğ‘” \n+ ğœ†\nğ‘ğ‘Ÿğ‘œğ‘—\n[\nÌƒğœ“ \nğ‘¡\n(\n|ğ‘º\nğ‘¡\n| âˆ’ \nâˆ‘\nğ‘ âˆˆğ‘º\nğ‘¡\nâˆ‘\nğ‘£âˆˆğ‘ \nğ‘„(ğ‘‹ , ğµ; ğœ™)(ğ’—)\n)\n+Ìƒğœ“ \nğ‘¡\n(\n|ğ‘º\nğ‘ \n| âˆ’ \nâˆ‘\nğ‘ âˆˆğ‘º\nğ‘\nâˆ‘\nğ‘£âˆˆğ‘ \nğ‘„(ğ‘‹ , ğµ; ğœ™)(ğ’—)\n)\n+Ìƒğœ“ \nğ‘¡\n(\n|ğ‘º\nğ‘ \n| âˆ’ \nâˆ‘\nğ‘ âˆˆğ‘º\nğ‘ \nâˆ‘\nğ‘£âˆˆğ‘ \nğ‘„(ğ‘‹ , ğµ; ğœ™)(ğ’—)\n)\n]\n, (6)\nwhere ğœ†\nğ‘ğ‘Ÿğ‘œğ‘— \nis the parameter balancing the partial cross entropy loss îˆ¸\nğ‘ğ‘”\nand the projection constraint. All log-barrier extensionsÌƒ ğœ“ \nğ‘¡ \nuse a same\nğ‘¡. Based on the bounding box annotations, we identify voxels outside\nbounding box as background voxels, thus îˆ¸\nğ‘ğ‘” \nshould be minimized.\nWhile the classification labels of the voxels inside the bounding box\nremain uncertain, the log-barrier extension can still provide weak\nsupervision signals, ensuring that the networkâ€™s predicted foreground\nfollow the projection constraint.\n3.3. Alternating optimization\nTo facilitate the training of the ancillary network, we further adopt\nan alternating optimization scheme that iteratively updates the self-\ntraining and the CRF regularization. First of all, the self-training is\nemployed to increase confidence of network predictions. We extract\nthe voxels with high confidence from voxels inside bounding box, and\nset voxel ğ’— as foreground if ğ‘„(ğ‘‹ , ğµ; ğœ™)(ğ’—) > ğ‘¡\nğ‘“ ğ‘” \n, while background if\nğ‘„(ğ‘‹ , ğµ; ğœ™)(ğ’—) < ğ‘¡\nğ‘ğ‘” \n, where ğ‘¡\nğ‘“ ğ‘” \nand ğ‘¡\nğ‘ğ‘” \nare the threshold parameters. We\nfurther randomly sample ğ‘› percent voxels from these extracted voxels\nrespectively, and train the ancillary network using cross entropy loss:\nîˆ¸\nğ‘ğ‘ ğ‘’ğ‘¢ğ‘‘ ğ‘œ \n= âˆ’ \n1\n|ğ›º\nğ‘† \n|\nâˆ‘\nÌ‚ğ‘¦ (ğ’—)=0,ğ’—âˆˆğ›º\nğ‘†\nlog(1 âˆ’ ğ‘„(ğ‘‹ , ğµ; ğœ™)(ğ’—))\nâˆ’ \n1\n|ğ›º\nğ‘† \n|\nâˆ‘\nÌ‚ğ‘¦ (ğ’—)=1,ğ’—âˆˆğ›º\nğ‘†\nlog(ğ‘„(ğ‘‹ , ğµ; ğœ™)(ğ’—)),\n(7)\nwhereÌ‚ ğ‘¦ denotes the pseudo labels and ğ›º\nğ‘† \ndenotes voxels that are\nrandomly sampled.\nAfter back-propagation, a CRF regularized loss [21] is used to\nfine-tune the ancillary network. This regularization term encourages\nspatial and intensity consistency over homogeneous regions. The CRF\nregularized loss is defined as:\nîˆ¸\nğ‘ ğ‘Ÿğ‘“ \n= \nğœ†\nğ‘ ğ‘Ÿğ‘“\n|ğ›º|\nâˆ‘\nğ’Œ,ğ’âˆˆğ›º\n[\nğ‘„(ğ‘‹ , ğµ; ğœ™)(ğ’Œ) â‹… (1 âˆ’ ğ‘„(ğ‘‹ , ğµ; ğœ™)(ğ’))\nâ‹… exp\n(\nâˆ’ \nğ‘‘(ğ’Œ, ğ’)\n2\n2ğœ\n2\nğ›¼\nâˆ’ \n(ğ‘‹(ğ’Œ) âˆ’ ğ‘‹(ğ’))\n2\n2ğœ\n2\nğ›½\n) ]\n,\n(8)\nwhere the function ğ‘‘(ğ’Œ, ğ’) calculates the Euclidean distance between the\nvoxels ğ’Œ and ğ’. ğœ\nğ›¼ \nand ğœ\nğ›½ \nrespectively control the spatial and intensity\nconsistency, and ğœ†\nğ‘ ğ‘Ÿğ‘“ \nis a weight coefficient. îˆ¸\nğ‘ ğ‘Ÿğ‘“ \nis minimum when\nvoxel ğ’Œ and ğ’ belong to the same class, are spatially adjacent, and have\nsimilar grayscale intensity. In our implementation, the Permutohedral\nLattice approximation algorithm [\n48] is utilized to reduce the time\ncomplexity of the 4D Gaussian filtering.\nThrough training with the pseudo labels using îˆ¸\nğ‘ğ‘ ğ‘’ğ‘¢ğ‘‘ ğ‘œ\n, the ancillary\nnetwork can generate better probability map for subsequent îˆ¸\nğ‘ ğ‘Ÿğ‘“ \nop-\ntimization. By optimizing îˆ¸\nğ‘ ğ‘Ÿğ‘“ \n, the obtained pseudo labels would be\nmore accurate, thus facilitating self-training optimization.\n3.4. Knowledge distillation\nAfter the ancillary network training, we train the primary network\nwith the soft pseudo labels generated by the ancillary network inside\nbounding box. Similar to the knowledge distillation [49], we simply fix\nthe ancillary network and minimize a weighted Kullback Leibler (KL)\ndivergence between two distributions:\nîˆ¸\nğ‘˜ğ‘‘ \n= \nğœ†\nğ‘˜ğ‘‘\n|ğ›º\nğ¼ \n|\nâˆ‘\nğ’—âˆˆğ›º\nğ¼\nğ‘„(ğ‘‹ , ğµ; ğœ™)(ğ’—)ğ‘™ğ‘œğ‘”( \nğ‘„(ğ‘‹ , ğµ; ğœ™)(ğ’—)\nğ‘ƒ (ğ‘‹; ğœƒ)(ğ’—) \n), (9)\nAlgorithm 1: The Proposed Algorithm for Weakly-Supervised\nBreast Cancer Segmentation in DCE-MRI\nInput: Training volume ğ‘‹ and bounding box ğµ.\nOutput: The primary network ğ‘ƒ (ğ‘‹; ğœƒ).\nâˆ–âˆ– Train the ancillary network ğ‘„(ğ‘‹ , ğµ; ğœ™)\nwhile ğ‘’ğ‘ğ‘œğ‘ â„ < 20 do\nCompute îˆ¸\nğ‘ğ‘Ÿğ‘œğ‘— \n. Update ğœ™.\nend\nwhile ğ‘’ğ‘ğ‘œğ‘ â„ >= 20 and ğ‘’ğ‘ğ‘œğ‘ â„ < 200 do\nCompute îˆ¸\nğ‘ğ‘Ÿğ‘œğ‘— \n, îˆ¸\nğ‘ğ‘ ğ‘’ğ‘¢ğ‘‘ ğ‘œ\n. Update ğœ™.\nCompute îˆ¸\nğ‘ğ‘Ÿğ‘œğ‘— \n, îˆ¸\nğ‘ ğ‘Ÿğ‘“ \n. Update ğœ™.\nend\nâˆ–âˆ– Train the primary network ğ‘ƒ (ğ‘‹; ğœƒ)\nwhile ğ‘’ğ‘ğ‘œğ‘ â„ < 20 do\nCompute îˆ¸\nğ‘ğ‘Ÿğ‘œğ‘— \n. Update ğœƒ.\nend\nwhile ğ‘’ğ‘ğ‘œğ‘ â„ >= 20 and ğ‘’ğ‘ğ‘œğ‘ â„ < 200 do\nCompute îˆ¸\nğ‘ğ‘Ÿğ‘œğ‘— \n, îˆ¸\nğ‘˜ğ‘‘ \n. Update ğœƒ.\nCompute îˆ¸\nğ‘ğ‘Ÿğ‘œğ‘— \n, îˆ¸\nğ‘ ğ‘Ÿğ‘“ \n. Update ğœƒ.\nend\nreturn ğœƒ\nwhere ğœ†\nğ‘˜ğ‘‘ \nis a weight coefficient. Note that îˆ¸\nğ‘˜ğ‘‘ \nis minimized to update\nthe primary network only, and the gradient is stop for the ancillary\nnetwork. In addition, the projection loss and the CRF regularized loss\nare utilized to train the primary network as well.\n3.5. Implementation details\nThe proposed framework is implemented in PyTorch, using a\nNVIDIA GeForce RTX 2080 Ti with 11 GB of memory. 3D U-net [50]\nis employed as our network backbone. In the first stage, we train the\nancillary network by minimizing projection loss îˆ¸\nğ‘ğ‘Ÿğ‘œğ‘— \nfor 20 epochs.\nThen self-training and the CRF regularized loss are incorporated to\nenhance its performance for 200 epochs. Parameters of the ancillary\nnetwork are optimized by SGD optimizer with an initial learning rate\nof 0.0001. The ploy learning policy is used to adjust the learning rate,\n(1 âˆ’ epochâˆ•200)\n0.9\n. In the second stage, the ancillary network is fixed\nto guide the primary network by knowledge distillation. We train the\nprimary network for 200 epochs, with a same optimization strategy as\nthat in the first stage. The training batch size is set to 4, consisting of\n2 random foreground patches and 2 random background patches. The\npatch size is set to 128 Ã— 128 Ã— 96. For the projection loss, we set ğœ†\nğ‘ğ‘Ÿğ‘œğ‘—\nto 0.01. All log-barrier extensionsÌƒ ğœ“ \nğ‘¡ \nuse a same ğ‘¡ of 5. Regarding the\nself-training, we set the threshold ğ‘¡\nğ‘“ ğ‘” \nto 0.95 for foreground class and\nğ‘¡\nğ‘ğ‘” \nto 0.05 for background class. We further sample 5 percent voxels\nfrom the extracted voxels as pseudo labels to preform self-training. For\nthe CRF regularized loss, ğœ\nğ›¼ \n, ğœ\nğ›½ \nand ğœ†\nğ‘ ğ‘Ÿğ‘“ \nare respectively set to 15, 0.05\nand 0.001. For the knowledge distillation, ğœ†\nğ‘˜ğ‘‘ \nis adjusted based on a\nGaussian decay:\nğœ†\nğ‘˜ğ‘‘ \n= ğ‘šğ‘–ğ‘›\n(\n0.05, 0.5 Ã— ğ‘’ğ‘¥ğ‘\n(\nâˆ’ \n(ğ‘’ğ‘ğ‘œğ‘ â„âˆ•200)\n2\n2 Ã— (0.3)\n2\n))\n. (10)\nIt manages how much the primary network learns from the ancillary\nnetwork. Considering that the ancillary networkâ€™s segmentation results\ncontain noises, we let the primary network learn aggressively from\nancillary networkâ€™s segmentation results at the early stage of train-\ning. In the later phase, the optimization of the network is mainly\ndriven by minimizing the CRF regularization loss. The Algorithm 1 pro-\nvides the pseudo code of the proposed weakly-supervised segmentation\nmethod.\nIn the inference phase, volumes are predicted with a sliding window\napproach. The window size equals to the patch size used during training\nand stride is set as half of a patch.\nBiomedical Signal Processing and Control 105 (2025) 107656\n5\n\nY. Zhong et al.\nFig. 4. One example to show (a) the original breast DCE-MRI, (b) the annotated tumor mask indicated in red region, and (c) the annotated bounding box indicated in blue box.\nNote that for the ease of illustration, the images and annotations in this figure are shown in 2D format, actually we process 3D DCE-MRI volume and the annotations are conducted\nin 3D.\n4. Experiments\n4.1. Datasets\nExperiments were carried on breast DCE-MRI scans obtained from\n461 patients (with 561 biopsy-proven breast cancers) at the Guangdong\nProvincial Peopleâ€™s Hospital, Guangzhou, Guangdong, China. This ret-\nrospective study was conducted with the approval of the Institutional\nReview Board of Guangdong Provincial Peopleâ€™s Hospital, and the ne-\ncessity for obtaining written informed consent was waived. Among all\nthe 561 malignant lesions, there are 319 masses and 242 non-mass-like\nlesions,\n1 \nrespectively. All DCE-MRI scans were acquired on the axial\nplane using a 1.5 T Avanto MRI scanner (Siemens, Erlangen, Germany)\nwith a four-channel dedicated breast coil, in the prone position with\nTR = 4.43 ms, TE = 1.50 ms and flip angle = 10\nâ—¦\n. The gadolinium-\nbased contrast agent (Omniscan, GE Healthcare) was injected at a\nrate of 2 ml/s with a dose of 0.2 ml/kg followed by 20 ml saline.\nThe fat-suppressed T1-weighted gradient-echo sequence was employed.\nThe DCE-MRI volumes have two kinds of resolution, 0.379 Ã— 0.379 Ã—\n1.700 mm\n3 \nand 0.511 Ã— 0.511 Ã— 1.000 mm\n3\n.\nAll cancer masks and bounding boxes were manually annotated by\nan experienced radiologist and further confirmed by another radiol-\nogist. Fig. 4 visualizes one example containing the breast DCE-MRI,\nthe corresponding tumor mask and bounding box annotations. In cases\ninvolving multiple tumors, a unique bounding box was assigned to\neach tumor, which corresponded to each connected region in cancer\nmask (see Fig. 3). We resampled all volumes into the same target\nspacing 0.600 Ã— 0.600 Ã— 1.000 mm\n3 \nand normalized them as zero mean\nand unit variance. We randomly employed 260 volumes for training\n(mass/non-mass: 183/130) and the remaining 201 volumes for testing\n(mass/non-mass: 136/112). The size distribution of all cancers is shown\nin Fig. 5.\n4.2. Evaluation metrics\nWe employed five typical metrics to assess the segmentation ac-\ncuracy, including Dice similarity coefficient (DSC, 0%â€“100%), Recall\n(0%â€“100%), Precision (0%â€“100%), average symmetric surface distance\n(ASSD, mm), and 95th percentile Hausdorff Distance (95HD, mm).\nThe region-based DSC was used to evaluate the volumetric overlap\nbetween the ground truth and the prediction. The ASSD and 95HD\n1 \nThe lesions in our dataset can be categorized into mass and non-mass\nenhancement. Mass enhancement refers to space-occupying lesions. Non-mass\nenhancement refers to areas of enhancement without a clear space-occupying\nlesions present. According to Mann et al. [6], masses are generally described\nin terms of their shape and margin. Based on the shape characteristics, masses\ncan be categorized as spherical, ellipsoidal and irregular types. In terms of\nmargin characteristics, masses can be categorized as circumscribed, irregular\nand spiculated types. Non-mass enhancements are generally described by their\ndistribution patterns, typically classified as segmental, multiple regions, and\ndiffuse types.\nwere employed to assess the surface similarity. The ASSD measured the\naverage surface distance between the ground truth and the prediction,\nwhile the 95HD calculated the surface disagreement. The voxel-wise\nclassification metrics Recall and Precision were used to quantify the\ntrue-positive prediction. Recall evaluated the under-segmentation ratio\nand Precision assessed the over-segmentation ratio. A better segmen-\ntation shall have larger DSC, Recall, Precision, and smaller ASSD and\n95HD.\n4.3. Comparison methods\nWe compared our method with several state-of-the-art weakly-\nsupervised segmentation methods:\nâ€¢ DeepCut [51]: this method is based on bounding box annotations.\nIt formulates an iterative energy minimization problem defined\nover a densely-connected CRF and uses this to optimize the\nsegmentation model.\nâ€¢ BBConst [26]: this method is based on bounding box annotations.\nIt integrates several global constraints, including the tightness\nprior, a global background emptiness constraint, and a region-size\nconstraint.\nâ€¢ MIL [27]: this method is based on bounding box annotations. It\nproposes generalized multiple instance learning (MIL) and smooth\nmaximum approximation to integrate the tightness prior into the\nnetwork.\nâ€¢ InExtremIS [24]: this method is based on extreme points. It uses\nextreme points to generate foreground scribbles to supervise the\nnetwork training. A CRF loss is employed to regularize the label\npropagation.\nâ€¢ OrthoSlice [28]: this method is based on orthogonal slices (i.e., 3\nslices) and originally designed for breast cancer segmentation. It\nconstrains segmentation by estimated volume using the orthog-\nonal slices. An outlier-suppression loss is proposed to reduce the\nfalse positives. Note that such annotation approach provides more\nvoxel-wise labels compared to others.\nAll comparison methods were implemented in 3D. We also evalu-\nated the upper bound of our method by training the framework using\nground-truth cancer masks.\n4.4. Results and discussion\n4.4.1. Comparison results\nTable 1 reports the numerical results from different methods on the\ntesting dataset. Note that ASSD and 95HD may not include all testing\nsamples if there was any prediction failure, where the entire image\nwas predicted as background. It can be observed from Table 1 that our\nmethod achieved a mean DSC value of 81.42%, and outperformed other\nweakly-supervised methods in terms of Recall and ASSD. The BBConst\nmethod not only had the lowest performance on all evaluation metrics,\nbut also predicted the most failures. This may be attributed to the\nfact that it solely relied on inequality constraints to regularize network\nBiomedical Signal Processing and Control 105 (2025) 107656\n6\n\nY. Zhong et al.\nFig. 5. The size (volume, in cm\n3 \n) distribution of all cancers in our dataset. The volume of all cancers are across different ranges, from <1 cm\n3 \nto >10 cm\n3 \n. The size distributions\nof the training dataset (left) and testing dataset (right) are similar.\nTable 1\nQuantitative segmentation results on the testing dataset (Mean Â± SD, best weakly-supervised results are highlighted in\nbold).\nMethods DSC [%] Recall [%] Precision [%] ASSD [mm] 95HD [mm] Failures\nFully supervision 85.56 Â± 14.78 83.95 Â± 16.11 91.23 Â± 13.48 2.84 Â± 6.99 13.92 Â± 36.20 0\nDeepCut [51] 73.42 Â± 20.74 72.94 Â± 23.62 81.62 Â± 18.95 5.21 Â± 11.48 22.67 Â± 48.39 2\nBBConst [26] 33.82 Â± 30.76 27.47 Â± 27.52 64.02 Â± 41.70 15.15 Â± 24.39 43.68 Â± 57.71 39\nMIL [27] 58.99 Â± 14.98 78.03 Â± 24.98 53.40 Â± 14.64 8.17 Â± 9.66 46.83 Â± 61.02 0\nInExtremIS [24] 50.58 Â± 26.90 39.86 Â± 24.88 89.21 Â± 22.70 7.80 Â± 16.00 23.66 Â± 40.69 2\nOrthoSlice [28] 81.29 Â± 16.53 83.41 Â± 17.77 83.90 Â± 15.01 5.98 Â± 8.90 31.69 Â± 55.09 0\nOurs 81.42 Â± 14.02 86.83 Â± 14.65 80.67 Â± 15.46 4.63 Â± 8.73 25.71 Â± 48.52 0\nTable 2\nQuantitative segmentation results for mass-like breast malignant lesions (Mean Â± SD, best weakly-supervised results are\nhighlighted in bold).\nMethods DSC [%] Recall [%] Precision [%] ASSD [mm] 95HD [mm] Failures\nFully supervision 87.48 Â± 14.17 88.51 Â± 11.77 89.61 Â± 16.14 3.36 Â± 8.44 16.07 Â± 40.77 0\nDeepCut [51] 75.98 Â± 19.58 77.17 Â± 21.72 80.47 Â± 20.43 6.11 Â± 13.62 25.87 Â± 54.30 1\nBBConst [26] 39.59 Â± 31.94 33.15 Â± 29.23 67.24 Â± 39.85 15.19 Â± 26.19 45.08 Â± 63.79 20\nMIL [27] 60.19 Â± 14.92 82.48 Â± 23.11 51.90 Â± 14.79 9.46 Â± 11.21 52.88 Â± 66.23 0\nInExtremIS [24] 56.51 Â± 26.52 46.10 Â± 25.03 89.04 Â± 22.16 7.96 Â± 17.07 26.21 Â± 47.33 0\nOrthoSlice [28] 83.55 Â± 15.36 88.26 Â± 12.63 82.60 Â± 17.37 6.74 Â± 10.19 34.23 Â± 58.23 0\nOurs 82.68 Â± 13.94 89.99 Â± 10.18 79.61 Â± 17.24 5.31 Â± 9.91 29.41 Â± 53.19 0\noutputs, lacking voxel-level supervision. Moreover, the BBConst strug-\ngled to optimize the region-size constraint loss for non-mass-like lesions\nwith diverse morphologies. The InExtremIS exhibited poor performance\nin terms of DSC and Recall. Although the InExtremIS employed the\ngenerated foreground scribbles to supervise the network training, the\nclass imbalance issue persisted that there was a big disparity in the\nnumber of voxels between the foreground scribbles and the intricate\nbackground. The MIL showed a tendency towards over-segmentation,\nwhich may be attributed to the fact that its regularization didnâ€™t utilize\nthe image gradient information. The DeepCut had a large improve-\nment in terms of DSC, but it still fell short of being sufficient for\ncancer segmentation. With more voxel-wise annotations, the OrthoSlice\nachieved good results of DSC, Recall and Precision, but had a poor\nperformance on 95HD due to the absence of boundary constraint of\nlesions in three-dimensional space (i.e., bounding box).\nTo show the efficacy of the proposed method, we further separately\nevaluated the quantitative results for mass-like (Table 2) and non-\nmass-like (Table 3) malignant lesions. It can be observed that all\nweakly-supervised methods appeared varying degrees of performance\ndegradation in non-mass segmentation compared to mass segmentation,\nwhich indicates the difficulty in the accurate segmentation of the non-\nmass-like lesions with irregular shapes. Among all comparison methods,\nour method exhibited relatively less performance degradation in non-\nmass segmentation, with an average DSC of 82.68% and 79.31% for\nmass and non-mass segmentation, respectively. It is worth noting that\nfor non-mass segmentation, our method not only achieved better DSC\nresults than bounding box based DeepCut, BBConst and MIL (without\nany certain foreground voxels), but also outcompeted extreme points\nbased InExtremIS (with six certain foreground voxels) and OrthoSlice\n(with lots of certain foreground voxels). This result highlights the\nefficacy of our method.\nWe also analyzed the segmentation performance for tumors strati-\nfied by sizes on testing dataset, as shown in Fig. 6. It can be observed\nthat our method achieved close DSC results across various size ranges\ncompared to the fully-supervised method, with the exception of the\n9âˆ¼10cm\n3 \nrange due to a limited sample size of 2 (see Fig. 5). Both the\nfully-supervised method and our method exhibited a larger standard\ndeviation of DSC values on non-mass-like lesions compared to mass-like\nlesions. While our method generated close segmentation performance\nto the fully-supervised results, the challenge in accurately segmenting\nsmall tumors persisted.\nFig. 7 shows the 2D visualization of our segmentation results on\nfour testing DCE-MRI scans. Despite the output of the ancillary network\ncontained noises, the prediction of the primary network achieved a\nsatisfactory agreement with the manually annotated ground truth. This\ndemonstrates that our training strategy enabled the primary network\nto learn from noisy labels. Fig. 8 further visualizes the 3D compar-\nison of the segmentation results from different methods. It can be\nBiomedical Signal Processing and Control 105 (2025) 107656\n7\n\nY. Zhong et al.\nTable 3\nQuantitative segmentation results for non-mass-like breast malignant lesions (Mean Â± SD, best weakly-supervised results are\nhighlighted in bold).\nMethods DSC [%] Recall [%] Precision [%] ASSD [mm] 95HD [mm] Failures\nFully supervision 82.32 Â± 15.23 76.27 Â± 19.20 93.96 Â± 6.10 1.97 Â± 3.19 10.31 Â± 26.44 0\nDeepCut [51] 69.11 Â± 21.91 65.82 Â± 24.95 83.56 Â± 15.97 3.70 Â± 6.14 17.27 Â± 35.65 1\nBBConst [26] 24.14 Â± 25.93 17.92 Â± 21.20 58.61 Â± 44.10 15.09 Â± 20.56 41.03 Â± 43.82 19\nMIL [27] 56.96 Â± 14.87 70.55 Â± 26.19 55.92 Â± 14.02 6.01 Â± 5.62 36.66 Â± 49.43 0\nInExtremIS [24] 40.63 Â± 24.49 29.37 Â± 20.77 89.50 Â± 23.57 7.51 Â± 13.96 19.26 Â± 24.83 2\nOrthoSlice [28] 77.48 Â± 17.70 75.28 Â± 21.73 86.08 Â± 9.47 4.69 Â± 5.93 27.42 Â± 49.05 0\nOurs 79.31 Â± 13.90 81.52 Â± 18.88 82.45 Â± 11.68 3.49 Â± 6.09 19.50 Â± 38.65 0\nFig. 6. The DSC results for different ranges of tumor sizes, generated by fully-supervised method (left) and our method (right). Our method achieves close DSC results across\nvarious size ranges compared to the fully-supervised method.\nFig. 7. 2D visualization of the segmentation results on four testing DCE-MRI scans, with transverse (top row) and sagittal (bottom row) views.\nobserved that both BBConst and InExtremIS struggled to capture the\nentire extent of the cancerous regions. In contrast, the MIL exhibited\na tendency towards over-segmentation. The DeepCut, on the other\nhand, outperformed these methods. However, it still fell short in ac-\ncurately delineating the edges of cancerous regions. Our method and\nOrthoSlice achieved segmentation results that were the closest to the\nfully-supervised results. It shows that the segmentation results pre-\ndicted by OrthoSlice had smoother surfaces, which can be attributed to\nan ample number of voxel annotations near the boundaries provided\nby the three orthogonal slices.\n4.4.2. Ablation analysis on the ancillary network\nThe primary network was trained by leveraging the pseudo labels\ngenerated from the ancillary network. Therefore, the performance of\nthe primary network heavily depended on the performance of the\nancillary network. To quantify the contribution of each component of\nthe ancillary network, we conducted ablation experiments by altering\nkey designs of the ancillary network\n2\n: bounding box encoder, îˆ¸\nğ‘ğ‘ ğ‘’ğ‘¢ğ‘‘ ğ‘œ\nand îˆ¸\nğ‘ ğ‘Ÿğ‘“ \n. The ablation results are shown in Table 4.\n3\nIt can be seen that by integrating the bounding box encoder (BBE),\nthe ancillary network could generate better pseudo labels for the train-\ning of the primary network. As regard to the alternating optimization\nprocedure, the self-training with îˆ¸\nğ‘ğ‘ ğ‘’ğ‘¢ğ‘‘ ğ‘œ \nand the CRF regularization\nîˆ¸\nğ‘ ğ‘Ÿğ‘“ \nshould be mutually reinforcing to progressively enhance the seg-\nmentation accuracy. Without the îˆ¸\nğ‘ğ‘ ğ‘’ğ‘¢ğ‘‘ ğ‘œ\n, the ancillary network lacked\n2 \nThe projection constraint is employed to convert uncertain voxel-wise\nlabels inside the bounding box into accurate projection label, and therefore\nfacilitating the initial training. Thus it is indispensable for the network\ntraining, and we didnâ€™t test the network without the projection constraint.\n3 \nAs shown in Eq. (9), the primary network was trained on pseudo labels\nwithin the bounding box. As a result, the evaluation metrics in Table 4 were\ncomputed based on the ancillary networkâ€™s prediction within the bounding\nbox.\nBiomedical Signal Processing and Control 105 (2025) 107656\n8\n\nY. Zhong et al.\nFig. 8. 3D visualization of the segmentation results on seven testing scans. The numerical values indicate the DSC (%) results.\nTable 4\nQuantitative comparison of different components of ancillary network on the testing dataset (Mean Â± SD, best results are\nhighlighted in bold).\nBBE îˆ¸\nğ‘ğ‘ ğ‘’ğ‘¢ğ‘‘ ğ‘œ \nîˆ¸\nğ‘ ğ‘Ÿğ‘“ \nDSC [%] Recall [%] Precision [%] ASSD [mm] 95HD [mm] Failures\nâœ“ âœ“ 80.42 Â± 17.83 74.81 Â± 22.55 93.86 Â± 4.87 0.85 Â± 0.94 3.32 Â± 3.63 0\nâœ“ âœ“ 82.24 Â± 9.38 73.28 Â± 13.11 95.81 Â± 4.70 0.68 Â± 0.40 2.36 Â± 1.34 0\nâœ“ âœ“ 62.98 Â± 10.28 79.16 Â± 5.61 54.01 Â± 13.47 2.01 Â± 0.88 5.80 Â± 3.23 0\nâœ“ âœ“ âœ“ 86.17 Â± 6.09 94.19 Â± 3.90 80.08 Â± 9.65 0.69 Â± 0.42 3.33 Â± 2.55 0\nTable 5\nQuantitative comparison of knowledge distillation with different weight on the testing dataset (Mean Â± SD, best results are\nhighlighted in bold).\nğœ†\nğ‘˜ğ‘‘ \nDSC [%] Recall [%] Precision[%] ASSD [mm] 95HD [mm] Failures\n0.5 75.41 Â± 15.62 91.65 Â± 9.38 67.70 Â± 19.73 12.22 Â± 13.56 73.72 Â± 71.10 0\n0.1 78.89 Â± 14.01 89.10 Â± 11.74 74.49 Â± 18.08 9.10 Â± 11.74 57.39 Â± 68.71 0\n0.05 75.81 Â± 16.24 81.56 Â± 16.77 76.40 Â± 19.99 11.08 Â± 12.22 64.27 Â± 70.36 0\nOurs 81.42 Â± 14.02 86.83 Â± 14.65 80.67 Â± 15.46 4.63 Â± 8.73 25.71 Â± 48.52 0\nthe impetus to propagate foreground labels, leading to a relatively\nlow Recall. Without the îˆ¸\nğ‘ ğ‘Ÿğ‘“ \n, the pseudo labels were prone to errors,\nleading to a severe performance degradation.\n4.4.3. Knowledge distillation for the primary network\nThe primary network was optimized by knowledge distillation loss\nîˆ¸\nğ‘˜ğ‘‘ \nand CRF loss îˆ¸\nğ‘ ğ‘Ÿğ‘“ \nalternately. The predefined parameter ğœ†\nğ‘˜ğ‘‘ \nwas\nemployed to control the impact of the soft pseudo labels. Here, we\ntested ğœ†\nğ‘˜ğ‘‘ \nas 0.5, 0.1 and 0.05 to train the primary network. The\ncomparison results are listed in Table 5. It can be observed that a\nhigher weight of knowledge distillation would result in an increase in\nRecall and a decrease in Precision, while a lower weight value would\nhave the opposite effect. This indicates that soft pseudo labels still\nTable 6\nComparison of annotation time for different methods.\nAnnotation methods Full mask Orthogonal slices Extreme points Bounding box\nAverage time (s) 242 83 81 53\ncontained noises. Therefore, we dynamically adjusted ğœ†\nğ‘˜ğ‘‘ \nbased on a\nGaussian decay defined in Eq. (10), which enabled the primary network\nto aggressively learn from pseudo labels in the early stage of training\nand subsequently refined through CRF loss.\n4.4.4. Comparison of annotation time\nTable 6 reports the annotation time for different methods, includ-\ning full segmentation mask, orthogonal slices, extreme points, and\nBiomedical Signal Processing and Control 105 (2025) 107656\n9\n\nY. Zhong et al.\nbounding box. The average time taken to fully annotate one DCE-MRI\nscan using voxel-wise annotation was 242 s. Annotating orthogonal\nslices took an average of 83 s, while annotating extreme points spent\n81 s. When utilizing bounding box, the annotation time was reduced\nto 53 s. In general, the time cost required for annotating full mask\nand orthogonal slices was approximately 5 Ã— and 1.5 Ã— longer than\nbounding box, respectively.\n5. Conclusion and future work\nBreast cancer segmentation in DCE-MRI is crucial for the accu-\nrate diagnosis, treatment planning and prognosis assessment. Deep-\nlearning-based breast cancer segmentation models still rely heavily\non manually annotated voxel-wise ground truth, which is expensive\nand time-consuming. In order to reduce the annotation burden, we\npresent a bounding box based weakly-supervised learning method. To\neffectively leverage the limited annotation information of bounding\nbox, we initially employ bounding box encoder and projection con-\nstraint to train the ancillary network. To further refine the segmentation\nresults generated from the ancillary network, we design an alternat-\ning optimization scheme by iteratively updating the self-training and\nCRF regularization. Ultimately, the performance gain brought by the\nbounding box encoder is transferred to the primary network through\nknowledge distillation. By leveraging the bounding boxes, our method\nnot only economizes the annotation efforts, but also consistently gen-\nerates favorable segmentation results, especially for the non-mass-like\nmalignant lesions.\nIt is worth noting that the bounding box annotations are rela-\ntively weak supervision signals for training the segmentation networks,\ntherefore there is still a certain gap in accuracy between our weakly-\nsupervised method and the fully-supervised method. Especially for\nsome cases, our method may generate segmentation predictions with\ntopological errors such as several adjacent isolated areas. In our future\nstudy, we attempt to constrain the topology of the target area by using\nsuch as the Betti number [\n52].\nCRediT authorship contribution statement\nYuming Zhong: Writing â€“ original draft, Visualization, Valida-\ntion, Methodology, Conceptualization. Zeyan Xu: Writing â€“ original\ndraft, Validation, Investigation, Data curation. Chu Han: Visualiza-\ntion, Validation. Zaiyi Liu: Writing â€“ review & editing, Supervision,\nFunding acquisition, Conceptualization. Yi Wang: Writing â€“ review\n& editing, Validation, Supervision, Resources, Project administration,\nInvestigation, Funding acquisition, Conceptualization.\nDeclaration of competing interest\nThe authors declare that they have no known competing finan-\ncial interests or personal relationships that could have appeared to\ninfluence the work reported in this paper.\nAcknowledgments\nThis work was supported in part by the Key-Area Research and De-\nvelopment Program of Guangdong Province, China (No.\n2021B0101420006), in part by the Guangdong Provincial Key Lab-\noratory of Artificial Intelligence in Medical Image Analysis and Ap-\nplication, China (No. 2022B1212010011), in part by the High-level\nHospital Construction Project (No. DFJHBF202105), in part by the\nNational Natural Science Foundation of China under Grants 62471306\nand 62071305, in part by the Guangdong-Hong Kong Joint Fund-\ning for Technology and Innovation, China (2023A0505010021), in\npart by the Shenzhen Medical Research Fund, China under Grant\nD2402010, in part by the Guangdong Basic and Applied Basic Re-\nsearch Foundation, China under Grant 2022A1515011241, and in part\nby the Shenzhen Major Scientific and Technological, China Project\n(KJZD20230923114615031).\nData availability\nData will be made available on request.\nReferences\n[1] W. Cao, H. Chen, Y. Yu, N. Li, W. Chen, Changing profiles of cancer burden\nworldwide and in China: a secondary analysis of the global cancer statistics\n2020, Chin. Med. J. (Engl) 134 (07) (2021) 783â€“791.\n[2] R. Ding, Y. Xiao, M. Mo, Y. Zheng, Y. Jiang, Z. Shao, Breast cancer screening\nand early diagnosis in Chinese women, Cancer Biol. Med. 19 (4) (2022) 450.\n[3] S. Lei, R. Zheng, S. Zhang, R. Chen, S. Wang, K. Sun, H. Zeng, W. Wei, J. He,\nBreast cancer incidence and mortality in women in China: temporal trends and\nprojections to 2030, Cancer Biol. Med. 18 (3) (2021) 900.\n[4] C.H. Lee, D.D. Dershaw, D. Kopans, P. Evans, B. Monsees, D. Monticciolo, R.J.\nBrenner, L. Bassett, W. Berg, S. Feig, et al., Breast cancer screening with imaging:\nrecommendations from the society of breast imaging and the ACR on the use\nof mammography, breast MRI, breast ultrasound, and other technologies for the\ndetection of clinically occult breast cancer, J. Am. Coll. Radiol. 7 (1) (2010)\n18â€“27.\n[5] M. Hayat, N. Ahmad, A. Nasir, Z.A. Tariq, Hybrid deep learning EfficientNetV2\nand vision transformer (EffNetV2-ViT) model for breast cancer histopathological\nimage classification, IEEE Access 12 (2024) 184119â€“184131.\n[6] R.M. Mann, N. Cho, L. Moy, Breast MRI: state of the art, Radiology 292 (3)\n(2019) 520â€“536.\n[7] J.Y. Kim, J.J. Kim, L. Hwangbo, H.B. Suh, S. Kim, K.S. Choo, K.J. Nam,\nT. Kang, Kinetic heterogeneity of breast cancer determined using computer-\naided diagnosis of preoperative MRI scans: relationship to distant metastasis-free\nsurvival, Radiology 295 (3) (2020) 517â€“526.\n[8] Z. Li, Y. Zhong, Y. Wang, Exploring kinetic curves features for the classification\nof benign and malignant breast lesions in DCE-MRI, in: 2024 IEEE 37th\nInternational Symposium on Computer-Based Medical Systems, CBMS, IEEE,\n2024, pp. 496â€“501.\n[9] E. Verburg, C.H. Van Gils, B.H. Van Der Velden, M.F. Bakker, R.M. Pijnappel,\nW.B. Veldhuis, K.G. Gilhuijs, Deep learning for automated triaging of 4581 breast\nMRI examinations from the DENSE trial, Radiology 302 (1) (2022) 29â€“36.\n[10] Y. Jiang, A.V. Edwards, G.M. Newstead, Artificial intelligence applied to breast\nMRI for improved diagnosis, Radiology 298 (1) (2021) 38â€“46.\n[11] D. Sheth, M.L. Giger, Artificial intelligence in the interpretation of breast cancer\non MRI, J. Magn. Reson. Imaging 51 (5) (2020) 1310â€“1324.\n[12] Z. Rezaei, A review on image-based approaches for breast cancer detection,\nsegmentation, and classification, Expert Syst. Appl. 182 (2021) 115204.\n[13] Y. Zheng, S. Baloch, S. Englander, M.D. Schnall, D. Shen, Segmentation and\nclassification of breast tumor using dynamic contrast-enhanced MR images, in:\nMICCAI, Springer, 2007, pp. 393â€“401.\n[14] A.B. Ashraf, S.C. Gavenonis, D. Daye, C. Mies, M.A. Rosen, D. Kontos, A\nmultichannel Markov random field framework for tumor segmentation with an\napplication to classification of gene expression-based breast cancer recurrence\nrisk, IEEE Trans. Med. Imaging 32 (4) (2012) 637â€“648.\n[15] A. Gubern-MÃ©rida, R. MartÃ­, J. Melendez, J.L. Hauth, R.M. Mann, N. Karssemei-\njer, B. Platel, Automated localization of breast cancer in DCE-MRI, Med. Image\nAnal. 20 (1) (2015) 265â€“274.\n[16] C. Militello, L. Rundo, M. Dimarco, A. Orlando, V. Conti, R. Woitek, I. Dâ€™Angelo,\nT.V. Bartolotta, G. Russo, Semi-automated and interactive segmentation of\ncontrast-enhancing masses on breast DCE-MRI using spatial fuzzy clustering,\nBiomed. Signal Process. Control. 71 (2022) 103113.\n[17] J. Zhang, A. Saha, Z. Zhu, M.A. Mazurowski, Hierarchical convolutional neu-\nral networks for segmentation of breast tumors in MRI with application to\nradiogenomics, IEEE Trans. Med. Imaging 38 (2) (2018) 435â€“447.\n[18] R. Huang, Z. Xu, Y. Xie, H. Wu, Z. Li, Y. Cui, Y. Huo, C. Han, X. Yang, Z.\nLiu, Y. Wang, Joint-phase attention network for breast cancer segmentation in\nDCE-MRI, Expert Syst. Appl. 224 (2023) 119962.\n[19] J. Zhang, Z. Cui, Z. Shi, Y. Jiang, Z. Zhang, X. Dai, Z. Yang, Y. Gu, L. Zhou,\nC. Han, et al., A robust and efficient AI assistant for breast tumor segmentation\nfrom DCE-MRI via a spatial-temporal framework, Patterns 4 (9) (2023).\n[20] K. Wu, B. Du, M. Luo, H. Wen, Y. Shen, J. Feng, Weakly supervised brain lesion\nsegmentation via attentional representation learning, in: MICCAI, Springer, 2019,\npp. 211â€“219.\n[21] R. Dorent, S. Joutard, J. Shapey, S. Bisdas, N. Kitchen, R. Bradford, S. Saeed,\nM. Modat, S. Ourselin, T. Vercauteren, Scribble-based domain adaptation via\nco-segmentation, in: MICCAI, Springer, 2020, pp. 479â€“489.\n[22] Z. Yang, D. Lin, D. Ni, Y. Wang, Non-iterative scribble-supervised learning with\npacing pseudo-masks for medical image segmentation, Expert Syst. Appl. 238\n(2024) 122024.\n[23] H.R. Roth, D. Yang, Z. Xu, X. Wang, D. Xu, Going to extremes: weakly supervised\nmedical image segmentation, Mach. Learn. Knowl. Extr. 3 (2) (2021) 507â€“524.\n[24] R. Dorent, S. Joutard, J. Shapey, A. Kujawa, M. Modat, S. Ourselin, T. Ver-\ncauteren, Inter extreme points geodesics for end-to-end weakly supervised image\nsegmentation, in: MICCAI, Springer, 2021, pp. 615â€“624.\nBiomedical Signal Processing and Control 105 (2025) 107656\n10\n\nY. Zhong et al.\n[25] H. Du, Q. Dong, Y. Xu, J. Liao, Weakly-supervised 3D medical image segmenta-\ntion using geometric prior and contrastive similarity, IEEE Trans. Med. Imaging\n42 (10) (2023) 2936â€“2947.\n[26] H. Kervadec, J. Dolz, S. Wang, E. Granger, I.B. Ayed, Bounding boxes for weakly\nsupervised segmentation: Global constraints get close to full supervision, in:\nMedical Imaging with Deep Learning, PMLR, 2020, pp. 365â€“381.\n[27] J. Wang, B. Xia, Bounding box tightness prior for weakly supervised image\nsegmentation, in: MICCAI, Springer, 2021, pp. 526â€“536.\n[28] X. Meng, J. Fan, H. Yu, J. Mu, Z. Li, A. Yang, B. Liu, K. Lv, D. Ai, Y. Lin, et\nal., Volume-awareness and outlier-suppression co-training for weakly-supervised\nMRI breast mass segmentation with partial annotations, Knowl.-Based Syst. 258\n(2022) 109988.\n[29] M. Benjelloun, M. El Adoui, M.A. Larhmam, S.A. Mahmoudi, Automated breast\ntumor segmentation in DCE-MRI using deep learning, in: International Con-\nference on Cloud Computing Technologies and Applications, IEEE, 2018, pp.\n1â€“6.\n[30] O. Ronneberger, P. Fischer, T. Brox, U-net: Convolutional networks for\nbiomedical image segmentation, in: MICCAI, Springer, 2015, pp. 234â€“241.\n[31] S. Wang, K. Sun, L. Wang, L. Qu, F. Yan, Q. Wang, D. Shen, Breast tumor\nsegmentation in DCE-MRI with tumor sensitive synthesis, IEEE Trans. Neural\nNetw. Learn. Syst. 34 (8) (2023) 4990â€“5001.\n[32] L. Zhou, S. Wang, K. Sun, T. Zhou, F. Yan, D. Shen, Three-dimensional affinity\nlearning based multi-branch ensemble network for breast tumor segmentation in\nMRI, Pattern Recognit. 129 (2022) 108723.\n[33] H. Wu, Y. Huo, Y. Pan, Z. Xu, R. Huang, Y. Xie, C. Han, Z. Liu, Y. Wang,\nLearning pre-and post-contrast representation for breast cancer segmentation\nin DCE-MRI, in: 2022 IEEE 35th International Symposium on Computer-Based\nMedical Systems, CBMS, IEEE, 2022, pp. 355â€“359.\n[34] C. Peng, Y. Zhang, J. Zheng, B. Li, J. Shen, M. Li, L. Liu, B. Qiu, D.Z. Chen,\nIMIIN: An inter-modality information interaction network for 3D multi-modal\nbreast tumor segmentation, Comput. Med. Imaging Graph. 95 (2022) 102021.\n[35] Y. Gao, Y. Zhao, X. Luo, X. Hu, C. Liang, Dense encoder-decoder network based\non two-level context enhanced residual attention mechanism for segmentation\nof breast tumors in magnetic resonance imaging, in: BIBM, IEEE, 2019, pp.\n1123â€“1129.\n[36] C. Li, H. Sun, Z. Liu, M. Wang, H. Zheng, S. Wang, Learning cross-modal deep\nrepresentations for multi-modal MR image segmentation, in: MICCAI, Springer,\n2019, pp. 57â€“65.\n[37] M. Qiao, S. Suo, F. Cheng, J. Hua, D. Xue, Y. Guo, J. Xu, Y. Wang,\nThree-dimensional breast tumor segmentation on DCE-MRI with a multilabel\nattention-guided joint-phase-learning network, Comput. Med. Imaging Graph. 90\n(2021) 101909.\n[38] M. Zhang, Y. Zhou, J. Zhao, Y. Man, B. Liu, R. Yao, A survey of semi-and\nweakly supervised semantic segmentation of images, Artif. Intell. Rev. 53 (2020)\n4259â€“4288.\n[39] B. Zhou, A. Khosla, A. Lapedriza, A. Oliva, A. Torralba, Learning deep features\nfor discriminative localization, in: CVPR, 2016, pp. 2921â€“2929.\n[40] D. Lin, J. Dai, J. Jia, K. He, J. Sun, Scribblesup: Scribble-supervised convolutional\nnetworks for semantic segmentation, in: CVPR, 2016, pp. 3159â€“3167.\n[41] D.P. Papadopoulos, J.R. Uijlings, F. Keller, V. Ferrari, Extreme clicking for\nefficient object annotation, in: ICCV, 2017, pp. 4930â€“4939.\n[42] C. Rother, V. Kolmogorov, A. Blake, GrabCut: interactive foreground extraction\nusing iterated graph cuts, ACM Trans. Graph. 23 (3) (2004) 309â€“314.\n[43] C. Song, Y. Huang, W. Ouyang, L. Wang, Box-driven class-wise region masking\nand filling rate guided loss for weakly supervised semantic segmentation, in:\nCVPR, 2019, pp. 3136â€“3145.\n[44] L. Grady, Random walks for image segmentation, IEEE Trans. Pattern Anal. Mach.\nIntell. 28 (11) (2006) 1768â€“1783.\n[45] Y. Zhong, Y. Wang, SimPLe: Similarity-aware propagation learning for weakly-\nsupervised breast cancer segmentation in DCE-MRI, in: MICCAI, Springer, 2023,\npp. 567â€“577.\n[46] V. Lempitsky, P. Kohli, C. Rother, T. Sharp, Image segmentation with a bounding\nbox prior, in: ICCV, IEEE, 2009, pp. 277â€“284.\n[47] S.P. Boyd, L. Vandenberghe, Convex Optimization, Cambridge University Press,\n2004.\n[48] A. Adams, J. Baek, M.A. Davis, Fast high-dimensional filtering using the\npermutohedral lattice, in: Computer Graphics Forum, Vol. 29, Wiley Online\nLibrary, 2010, pp. 753â€“762.\n[49] G. Hinton, O. Vinyals, J. Dean, Distilling the knowledge in a neural network,\n2015, arXiv preprint arXiv:1503.02531.\n[50] Ã–. Ã‡iÃ§ek, A. Abdulkadir, S.S. Lienkamp, T. Brox, O. Ronneberger, 3D U-Net:\nlearning dense volumetric segmentation from sparse annotation, in: MICCAI,\nSpringer, 2016, pp. 424â€“432.\n[51] M. Rajchl, M.C. Lee, O. Oktay, K. Kamnitsas, J. Passerat-Palmbach, W. Bai, M.\nDamodaram, M.A. Rutherford, J.V. Hajnal, B. Kainz, et al., Deepcut: Object seg-\nmentation from bounding box annotations using convolutional neural networks,\nIEEE Trans. Med. Imaging 36 (2) (2016) 674â€“683.\n[52] J.R. Clough, N. Byrne, I. Oksuz, V.A. Zimmer, J.A. Schnabel, A.P. King, A\ntopological loss function for deep-learning based image segmentation using\npersistent homology, IEEE Trans. Pattern Anal. Mach. Intell. 44 (12) (2020)\n8766â€“8778.\nBiomedical Signal Processing and Control 105 (2025) 107656\n11",
    "version": "5.3.31"
  },
  {
    "numpages": 8,
    "numrender": 8,
    "info": {
      "PDFFormatVersion": "1.7",
      "Language": "English",
      "EncryptFilterName": null,
      "IsLinearized": true,
      "IsAcroFormPresent": false,
      "IsXFAPresent": false,
      "IsCollectionPresent": false,
      "IsSignaturesPresent": false,
      "Creator": "Elsevier",
      "Custom": {
        "CrossMarkDomains[1]": "elsevier.com",
        "CrossmarkMajorVersionDate": "2010-04-23",
        "CreationDate--Text": "13th October 2025",
        "ElsevierWebPDFSpecifications": "7.0.1",
        "robots": "noindex",
        "doi": "10.1016/j.bspc.2025.108907",
        "CrossMarkDomains[2]": "sciencedirect.com",
        "CrossmarkDomainExclusive": "true"
      },
      "ModDate": "D:20251013030234Z",
      "Author": "Dichao Hu",
      "Title": "Development of a new method M2LC for few-slice-annotated MRI image segmentation and validation of its performance on multiple datasets",
      "Keywords": "Medical image semantic segmentation,Few-slice-annotated,Entropy optimization,Cross supervision",
      "CreationDate": "D:20251013024841Z",
      "Producer": "Acrobat Distiller 8.1.0 (Windows)",
      "Subject": "Biomedical Signal Processing and Control, 113 (2026) 108907. doi:10.1016/j.bspc.2025.108907"
    },
    "metadata": {
      "ali:license_ref": "http://creativecommons.org/licenses/by/4.0/",
      "crossmark:crossmarkdomains": "elsevier.comsciencedirect.com",
      "crossmark:crossmarkdomainexclusive": "true",
      "crossmark:doi": "10.1016/j.bspc.2025.108907",
      "crossmark:majorversiondate": "2010-04-23",
      "dc:format": "application/pdf",
      "dc:identifier": "10.1016/j.bspc.2025.108907",
      "dc:publisher": "Elsevier Ltd",
      "dc:description": "Biomedical Signal Processing and Control, 113 (2026) 108907. doi:10.1016/j.bspc.2025.108907",
      "dc:subject": [
        "Medical image semantic segmentation",
        "Few-slice-annotated",
        "Entropy optimization",
        "Cross supervision"
      ],
      "dc:title": "Development of a new method M2LC for few-slice-annotated MRI image segmentation and validation of its performance on multiple datasets",
      "dc:creator": [
        "Dichao Hu",
        "He Liu",
        "Mingyang Li",
        "Zekun Jiang",
        "Wenbo Wu",
        "Minghui Sun",
        "Wei Shen",
        "Mingmin Liu",
        "Haitao Liu"
      ],
      "jav:journal_article_version": "VoR",
      "pdf:creationdate--text": "13th October 2025",
      "pdf:producer": "Acrobat Distiller 8.1.0 (Windows)",
      "pdf:keywords": "Medical image semantic segmentation,Few-slice-annotated,Entropy optimization,Cross supervision",
      "pdfx:creationdate--text": "13th October 2025",
      "pdfx:crossmarkdomains": "sciencedirect.comelsevier.com",
      "pdfx:crossmarkdomainexclusive": "true",
      "pdfx:crossmarkmajorversiondate": "2010-04-23",
      "d2uudz9-jy9annwigmdmny8nnywj9lweontagotj7ym7-nt2my.eso9eqn9iknm-smd2tma": "",
      "pdfx:doi": "10.1016/j.bspc.2025.108907",
      "pdfx:robots": "noindex",
      "prism:aggregationtype": "journal",
      "prism:copyright": "Â© 2025 The Authors. Published by Elsevier Ltd.",
      "prism:coverdate": "2026-03-01",
      "prism:coverdisplaydate": "1 March 2026",
      "prism:doi": "10.1016/j.bspc.2025.108907",
      "prism:issn": "1746-8094",
      "prism:number": "PA",
      "prism:pagerange": "108907",
      "prism:publicationname": "Biomedical Signal Processing and Control",
      "prism:startingpage": "108907",
      "prism:url": "https://doi.org/10.1016/j.bspc.2025.108907",
      "prism:volume": "113",
      "tdm:policy": "https://www.elsevier.com/tdm/tdmrep-policy.json",
      "tdm:reservation": "1",
      "xmp:createdate": "2025-10-13T02:48:41",
      "xmp:creatortool": "Elsevier",
      "xmp:metadatadate": "2025-10-13T03:02:34",
      "xmp:modifydate": "2025-10-13T03:02:34",
      "xmpmm:documentid": "uuid:1b499bed-4ce8-4c0c-b682-0d58baae1cbe",
      "xmpmm:instanceid": "uuid:b6ed5266-03cb-4855-90c0-636283357f42",
      "xmprights:marked": "True"
    },
    "text": "Development of a new method M2LC for few-slice-annotated MRI image\nsegmentation and validation of its performance on multiple datasets\nDichao Hu \na,1\n, He Liu \na,1\n, Mingyang Li \na,1\n, Zekun Jiang \nb\n, Wenbo Wu \na\n, Minghui Sun \na \n,\nWei Shen \nb,*\n, Mingmin Liu \nc,*\n, Haitao Liu \na,*\na \nDepartment of Urology, Shanghai General Hospital, Shanghai Jiao Tong University School of Medicine, No.100, Haining Road, Shanghai 200080, China\nb \nMinistry of Education (MoE) Key Lab of Artificial Intelligence, Artificial Intelligence (AI) Institute, Shanghai Jiao Tong University, Shanghai 200080, China\nc \nDepartment of Obstetrics and Gynecology, Shanghai General Hospital, Shanghai Jiao Tong University School of Medicine, No.100, Haining Road, Shanghai 200080,\nChina\nA R T I C L E I N F O\nKeywords:\nMedical image semantic segmentation\nFew-slice-annotated\nEntropy optimization\nCross supervision\nA B S T R A C T\nPurpose: We designed a semi-supervised method that effectively utilized few-slice-annotated data for efficient\nMRI segmentation.\nMethods: We investigate a weakly-supervised scenario for medical image segmentation named as few-slice-\nannotated segmentation, where only a limited number of slices within each 3D volume are annotated for\nmodel training. To address the significant label scarcity in this scenario, we present a method called Maximum\nMinimum Entropy Cross Supervision and Label Correction (M2LC). M2LC initially trains semi-supervised 2D\nsegmentation networks with a Siamese structure by transforming 3D volumes into 2D slices. During training, we\napply entropy minimum and maximum constraints to the two branches of the network, respectively, and employ\nmutual supervision. Subsequently, we use the minimum entropy network to generate 2D pseudo-labels, which\nare then assembled back into 3D volumes and refined via a 3D label correction process. We validated the M2LC\nmethod on several datasets, including BraTs, ADCD, and a bladder cancer dataset collected from the Shanghai\nGeneral Hospital.\nResults: Compared with fully-supervised methods, M2LC achieves 83 % labeling accuracy with 0.6 % annotated\nslices on the BraTs dataset, and 79 % labeling accuracy with 1.2 % annotated slices on the ADCD dataset. On the\nbladder cancer dataset from Shanghai General Hospital, M2LC achieves labeling accuracies of 95.79 % and\n99.13 % for the bladder wall and bladder tumor regions, respectively, with 23.17 % and 24.51 % annotated\nslices.\nConclusion: This study successfully developed a semi-supervised deep learning method for MRI image segmen-\ntation under partial slice annotation conditions, achieving state-of-the-art performance across multiple datasets.\n1. Introduction\nAccurate medical image segmentation is essential for precise diag-\nnosis and treatment of diseases [1,2]. However, the task of annotating\nsegmentation masks can be a daunting and time-consuming process for\nmedical experts who prioritize patient care. Classical image\nsegmentation networks, such as U-net, rely on fully supervised learning,\nwhich demands substantial human annotations for training [3]. Even\nworse, in the medical field, only domain experts can provide precise\nannotations. This problem is further exacerbated by the volumetric na-\nture of medical data [2], increasing the annotation burden. While\nvarious methods have been developed to address the issue of limited\nAbbreviations: M2LC, Maximum Minimum Entropy Cross Supervision and Label Correction; BC, bladder cancer; NMIBC, non-muscle invasive bladder cancer;\nMIBC, muscle-invasive bladder cancer; DSC, Dice Similarity Coefficient; ASD, Average Surface Distance; DWI, Diffusion Weighted Imaging; ET, Enhancing Tumor;\nED, Peri-tumoral Edema; NET, Non-Enhancing Tumor; LV, Left Ventricle; RV, Right Ventricle; MYO, Myocardium; 2D UNet, Direct 2D Fully-Supervised Algorithm;\nMT, Mean Teacher; UAMT, Uncertainty Aware Mean Teacher; CPS, Cross Pseudo Supervision; 3D UNet, Direct 3D Fully-Supervised Algorithm; Con2R, Contrastive\nConstrained Regularization.\n* Corresponding authors.\nE-mail addresses: wei.shen@sjtu.edu.cn (W. Shen), 18918251972@163.com (M. Liu), haitao.liu@shgh.cn (H. Liu).\n1 \nThese authors equally contributed to this work.\nContents lists available at ScienceDirect\nBiomedical Signal Processing and Control\njournal homepage: www.elsevier.com/locate/bspc\nhttps://doi.org/10.1016/j.bspc.2025.108907\nReceived 25 May 2025; Received in revised form 11 September 2025; Accepted 6 October 2025\nBiomedical Signal Processing and Control 113 (2026) 108907\nAvailable online 13 October 2025\n1746-8094/Â© 2025 The Authors. Published by Elsevier Ltd. This is an open access article under the CC BY license (\nhttp://creativecommons.org/licenses/by/4.0/).\n\ndata, most rely on semi-supervised learning [4â€“8] with limited anno-\ntated volumes and large unannotated volumes. In real-world annotation\nscenarios, medical professionals typically opt to annotate only a few 2D\nslices or even just one slice per patient, as it can be done in fragmented\nperiods. In contrast, annotating a full 3D volumetric image requires\nsubstantial time and effort. To alleviate this issue, we propose a new task\ncalled few-slice-annotated segmentation, where only a small number of\nslices from each 3D volumetric image are annotated (Fig. 1).\nExisting methods are inadequate in addressing these challenges: 1)\nTraining a 3D network directly on such data is intuitively unreasonable,\nas the low information density severely limits model performance; 2)\nThe small amount of weakly-annotated data prevents existing weak\nsupervision methods from extracting sufficient information; 3) Existing\nsemi-supervised approaches lack mechanisms to capture space conti-\nnuity as the annotated slices are often non-contiguous. Consequently,\nefficiently utilizing few-slice-annotated data remains an unsolved\nproblem.\nTo address this challenge, we propose the Maximum Minimum En-\ntropy Cross Supervision and Label Correction (M2LC) method. M2LC\ninvolves semi-supervised training of two 2D segmentation networks\nperturbed with different initialization by reorganizing 3D volumes into\n2D slices. During the training process, we apply entropy minimum and\nentropy maximum constraints separately to each of the two networks\nand allow them to mutually supervise each other. Maximizing entropy\nincreases the modelâ€™s uncertainty, allowing it to better explore the data\nspace and identify potential unannotated regions while minimizing en-\ntropy reduces the modelâ€™s uncertainty, enabling it to adapt better to the\nlabeled data distribution. The complementary and adversarial training\nof the two enhances the efficiency of information utilization under\nextremely limited labeled slices. After training the 2D networks, we\ngenerate 2D pseudo-labels and assemble them back into 3D volumes.\nThese pseudo-labels are considered as noisy labels and then refined\nthrough a 3D label correction process.\nThe problem of arduous and time-consuming manual annotation also\nexists in bladder cancer (BC) medical image segmentation, highlighting\nthe need to enhance information utilization efficiency with limited\nlabeled slices. Pelvic MRI has become the preferred modality for clinical\nstaging of BC. Among various MRI sequences, Diffusion Weighted Im-\naging (DWI) provides distinct signal contrast between BC and the\nbladder wall [9], effectively reflecting the biological characteristics of\ntumor [10], and thus playing a pivotal role in clinical decision-making.\nTherefore, we used the Shanghai General Hospital BC dataset to evaluate\nthe performance of the M2LC method in bladder cancer MRI image\nsegmentation.\nWe summarize our contributions: 1. We investigate the â€œfew-slice-\nannotatedâ€ segmentation setting and propose an effective framework to\naddress this scenario, providing a viable direction for tackling this\nproblem. 2. In our framework, we introduce the Maximum Entropy and\nMinimum Entropy Cross Supervision Network, which can produce\nhigher-quality pseudo-labels by imposing constraints on two parallel\nnetworks in a complementary manner (Fig. 2). 3. We validate our\nmethod on various datasets, including BraTs, ACDC and Shanghai\nGeneral Hospital BC datasets, achieving excellent results. Our approach\nachieves up to 83 % of the fully annotated segmentation performance\nusing less than 1 % of the annotated slices.\n2. Materials and methods\n2.1. Development of model\n2.1.1. Problem and framework\nConsider a set of N 3D volumetric images represented as\n{\n(I\n1\n, L\n1\n), â‹¯, (I\nN\n, L\nN\n)|I\nt \nâˆˆ R\ndÃ—DÃ—HÃ—W \n, L\nt \nâˆˆ R\nCÃ—DÃ—HÃ—W \n}\n, where d is the\nnumber of channels, D is the number of slices, W is the slice width, and H\nis the slice height. We define the â€œfew slices annotationâ€ problem as the\nscenario in which only a small number of slices in a volumetric image\nare annotated. Given an image I\nt \nand its label L\nt\n, we denote F (k) as the\nâ€œk slicesâ€ segmentation, indicating that k slices are annotated for a\nvolumetric image with \nâˆ‘\nD\nj=1\n1(L\nt \n[j] ) = k. The indicator function 1 de-\nnotes whether there is a non-background label annotation for a given\nslice.\n1(L\nt \n(j)) =\n{ \n0ifallelementsinL[j]are0\n1otherwise\nOur proposed framework provides a robust solution to the few-slices\nproblem with an effective algorithm. We assign the j-th layer slice of the\ndata pair (I\nt\n, L\nt\n) to the labeled set if 1 (L\nt \n[j]) = 1, and to the unlabeled set\notherwise. Our approach utilizes two U-nets with identical structures\nbut different initializations. The two networks are trained with entropy\nminimization and maximization constraints, as detailed in Section 2.1.2,\nand supervise each other during the learning process. The trained 2D U-\nnet generates pseudo-labels for the unlabeled data in D\nu\n, which are\ncombined with the ground truth labels of the labeled slices to create 3D\npseudo-labels for each 3D volume. This approach enables effective\ntraining of a 2D model on 3D volumes with few labeled slices, consid-\nering cross-sample information and learning generalizable image\nknowledge. However, the resulting segmentation may not be entirely\naccurate, as it lacks 3D contextual information. The mixed labels are\nmodel-generated and can be considered as noisy labels of the ground\ntruth. To address this issue, we propose a 3D label correction approach,\ndetailed in Section 2.1.3, which aims to recover the true labels from the\nmixed ones and address the lack of 3D contextual information.\n2.1.2. Maximum and minimum entropy cross supervision\nGiven a limited set of labeled slices D\nl \nand a larger set of unlabeled\nslices D\nu\n, our objective is to employ a semi-supervised strategy to train a\n2D model that effectively utilizes information, even with extremely\nlimited labeled data. To achieve this, we apply entropy minimum and\nentropy maximum constraints separately to each of the two networks\nand allow them to mutually supervise each other. Maximizing entropy\nincreases the modelâ€™s uncertainty [11,12], allowing it to better explore\nthe data space and identify potential unannotated regions. Conversely,\nminimizing entropy reduces uncertainty [13,14], enabling the model to\nbetter align with the labeled data distribution. The two networks learn\ncomplementary information under different entropy constraints and\nmutually supervise each other, resulting insignificant improvement in\nthe modelâ€™s information utilization and error correction capabilities. To\nFig. 1. Comparison of two segmentation settings.\nD. Hu et al. \nBiomedical Signal Processing and Control 113 (2026) 108907\n2\n\ngenerate more stable and cleaner pseudo-labels, we use the network that\nutilizes entropy minimization as the final output prediction network to\nproduce 2D pseudo-labels.\nDuring the training process, our method includes three types of\nlosses: the supervised loss L\ns \nthe cross-supervision loss L\nc \n, and the en-\ntropy losses L\nemi n \nand L\nemax \nfor f\nÎ¸1 \n(â‹…) and f\nÎ¸2 \n(â‹…). We define Î¸\n1 \nand Î¸\n2 \nas the\nparameters of the networks for performing minimum entropy and\nmaximum entropy, respectively. The supervision loss L\ns \nis formulated\nusing the standard pixel-wise cross-entropy loss L \nce \nand dice loss L \ndice\non the labeled slices over two segmentation networks:\nL \nc e \n= \n1\nâƒ’âƒ’\nD \nl\nâƒ’âƒ’\nâˆ‘\nXâˆˆD \nl\n1\nW Ã— H\nâˆ‘\nWÃ—H\ni=0\n(l \nce\n(p\n1i\n, y\ni\n) + l \nce\n(p\n2i\n, y\ni\n) ) (1)\nL \ndice\n\u0000 \np\ni,c\n) \n= \n1\nâƒ’âƒ’\nD \nl\nâƒ’âƒ’\nâˆ‘\nXâˆˆD \nl\nâˆ‘\nC\nc=0\n(\n1 \u0000 \n2\nâˆ‘\nWÃ—H\ni=0 \np\ni,c \nÃ— y\ni,c\nâˆ‘\nWÃ—H\ni=0 \np\n2\ni,c \n+ \nâˆ‘\nWÃ—H\ni=0 \ny\n2\ni,c\n)\n(2)\nL \ns \n= L \nc e \n+ L \ndice\n\u0000 \np\n1i,c\n) \n+ L \ndice\n\u0000 \np\n2i,c\n) \n(3)\nIn the equation, C represents the number of classes, yi represents the\nlabel at each position i, p\n1i \n(p\n2i\n) represents the output of network f\nÎ¸1 \n(â‹…)\n(f\nÎ¸2 \n(â‹…)) at i, p\ni,c \nrepresents the output value at i and class c, l \nce \nrepresents\nthe cross-entropy loss function. L\nc e \nrepresents the cross-entropy loss of\nthe network, while L \ndice \n(p\ni,c\n) represents the dice loss. Cross-supervision\nis the mutual supervision between the two networks, which is man-\nifested as:\nL \nc \n= \n1\nâƒ’âƒ’\nD \nu \nâˆª D \nl\nâƒ’âƒ’\nâˆ‘\nXâˆˆD \nu \nâˆªD \nl\n1\nW Ã— H\nâˆ‘\nWÃ—H\ni=0 \n(l \nce\n(p\n1i\n, p\n2i\n) + l \nce\n(p\n2i\n, p\n1i\n) )\n(4)\nFinally, we apply entropy constraints on f\nÎ¸1 \n(â‹…) and f\nÎ¸2 \n(â‹…). Specifically,\nwe impose an entropy minimization loss on f\nÎ¸1 \n(â‹…) and an entropy\nmaximization loss on f\nÎ¸2 \n(â‹…). The constraint loss for entropy minimiza-\ntion for f\nÎ¸1 \n(â‹…) can be defined as:\nL \nemi n \n= \n1\nâƒ’âƒ’\nD \nu \nâˆª D \nl\nâƒ’âƒ’\nâˆ‘\nXâˆˆD \nu \nâˆªD \nl\n\u0000 1\nlog(C)\nâˆ‘\nWÃ—H\ni=0 \np\n1i\n(log(p\n1i\n) )\nT \n(5)\nSimilarly, the entropy maximization loss for f\nÎ¸2 \n(â‹…) can be defined as:\nL \nemax \n= \n\u0000 1\nâƒ’âƒ’\nD \nu \nâˆª D \nl\nâƒ’âƒ’\nâˆ‘\nXâˆˆD \nu \nâˆªD \nl\n\u0000 1\nlog(C)\nâˆ‘\nWÃ—H\ni=0 \np\n2i\n(log(p\n2i\n) )\nT \n(6)\nThe whole training objective is written as:\nL = L \ns \n+ Î»(L \nc \n+ L \nemi n \n+ L \nemax \n) (7)\nwhere Î» is the trade-off weight.\n2.1.3. Label correction\nAlthough Maximum and Minimum Entropy Cross Supervision can\nimprove segmentation accuracy, it lacks 3D contextual information. Our\nmethod considers these 3D pseudo-labe\nls \nas noisy labels of the ground\ntruth, which are refined using a 3D label correction process. Research\nhas demonstrated that deep neural networks tend to learn true labels\nduring the early-learning phase, before eventually fitting to examples\nwith noisy labels [15â€“17]. Based on this observation, we adopt label\ncorrection based on early learning to refine our mixed labels. Specif-\nically, we train a 3D U-Net on the mixed labels and evaluate segmen-\ntation performance on the training set by measuring IoU between the\nmodel output and the noisy label (3D mixed label). We use the observed\ndeceleration to update the noisy annotations. We fit an exponential\nparametric model to the training IoU using least squares and compute\nthe derivative Iou\nÊ¹\n(t) at each iteration. The label for each class is cor-\nrected when \n|Iou\nÊ¹\n(1)\u0000 Iou\nÊ¹\n(t) |\n|Iou\nÊ¹\n(1) | \n> r, r = 0.7. The specific parameter settings are\nthe same as previous works [16]. Although label correction has been\nused in previous methods to improve segmentation accuracy, our\napproach differs in that we perform iterative label correction until the\nmodel determines that there are too few labels to be modified further.\nThis is necessary due to the complexity of 3D information and the need\nfor a deeper exploration of these features.\n2.2. Patients\nThis retrospective study was approved by the Shanghai General\nHospital Institutional Review Board, and the need to obtain informed\nFig. 2. Overview of our Maximum Minimum Entropy Cross Supervision and Label Correction (M2LC) architecture. M2LC utilizes semi-supervised training of two 2D\nsegmentation networks with entropy constraints and mutual supervision. The 2D pseudo-labels are generated using the entropy minimum network and assembled\ninto 3D volumes. These 3D pseudo-labels are then refined using a 3D label correction process.\nD. Hu et al. \nBiomedical Signal Processing and Control 113 (2026) 108907\n3\n\nconsent was waived. From January 2016 to December 2022, 156 pa-\ntients with BC who underwent surgical treatment were selected from the\nDepartment of Urology at Shanghai General Hospital. Among them,\nthere were 122 cases of non-muscle invasive bladder cancer (NMIBC)\npatients and 38 cases of muscle-invasive bladder cancer (MIBC) patients.\nInclusion criteria were as follows: (i) postoperative pathological\nconfirmation of uroepithelial carcinoma of the bladder without the\npresence of other malignant tumors; (ii) MRI imaging of the corre-\nsponding lesion and matched paracancerous tissues obtained within 30\ndays prior to surgery; (iii) availability of complete clinicopathological\ninformation; (iv) availability of complete follow-up prognostic infor-\nmation. Exclusion criteria were as follows: (i) Patients previously have\nreceived pelvic radiotherapy or surgical treatment; (ii) Poor quality of\nimaging data, such as severe motion artifacts. Additionally, to compre-\nhensively evaluate the performance of the developed architecture across\nvarious diseases and databases, an external BraTs dataset was intro-\nduced [2,18].\n2.3. Image equipment\nMRI images were acquiring using three devices: 1. Discovery MR750\n3.0 T, General Electric Medical Systems, Milwaukee, USA. 2. Discovery\nMR750w 3.0 T, General Electric Medical Systems, Milwaukee, USA. 3.\nSigna HDxt 1.5 T, General Electric Medical Systems, Milwaukee, USA.\nThe DWI sequence scanning parameters during MRI image acquisition\nwere set as follows: TR 4000 ms, TE 85.46 ms, b-value 1000 s/mm\n2\n,\nscanning bandwidth 1953.12 KHz, frequency 64, echo chain length 1,\nlayer thickness 6 mm, spacing 7 mm, and flip angle 90\nâ—¦\n.\n2.4. Statistical analysis\nStatistical analyses were conducted using Python (version 3.6.5, http\ns://www.python.org) with the Pyradiomics (version 2.2.0, https:\n//pyradiomics.readthedocs.io/en/latest/index.html) and Scikit-learn\n(version 0.24.2, https://scikit-learn.org/stable/index.html) packages\nfor automated feature extraction. To assess the accuracy of the 3D seg-\nmentation predictions, we employed two standard evaluation metrics:\nFig. 3. Example of manual annotation of images (green: tumor region, red: bladder wall region).\nD. Hu et al. \nBiomedical Signal Processing and Control 113 (2026) 108907\n4\n\nthe Dice Similarity Coefficient (DSC) and the Average Surface Distance\n(ASD).\n3. Results\n3.1. Setup\nWe evaluated our method on three datasets: ACDC [19], BraTs\n[2,18], and our own BC dataset. BraTs is a widely recognized brain\ntumor dataset, where we segmented three tumor regionsâ€”enhancing\ntumor (ET), peri-tumoral edema (ED), and non-enhancing tumor\n(NET)â€”using only FLAIR modality images for training. The ACDC\ndataset, which is a publicly available dataset for cardiac segmentation\nand functional analysis, consists of 200 annotated short-axis cardiac\ncine-MRI scans from 100 subjects, and includes segmentation labels for\nthe left ventricle (LV), right ventricle (RV), and myocardium (MYO).\nFollowing prior works [20â€“22], we utilized 250 samples from the official\nBraTs training set for training, 25 for validation, and 60 for testing.\nAdditionally, we employed 140 scans from the ACDC dataset for\ntraining, 20 for validation, and 40 for testing.\n3.2. Image preprocessing\nTo further train the model, this study requires precise manual\nannotation of cohort images from Shanghai General Hospital. This\nannotation task was conducted by a senior radiologist with more than 6\nyears of experience in reading urological images, focusing on the\nbladder region and its bladder wall region. Two reference examples are\nshown in Fig. 3.\nOnce the annotations were completed, the dataset was randomly\nsplit into training and test sets, maintaining the actual distribution ratio\nof NMIBC and MIBC patients (7:3), thus constructing a fully labeled\npatient dataset. Subsequently, based on the training set of this dataset, a\nspecific labeling strategy was applied: it retained the first and last layers\nof labeling in the 3D volume, along with one label that identified the\ntumor region, resulting in a partially labeled patient group containing\nonly three labeled images.\n3.3. Data augmentation and network configuration\nDuring 2D training, we used a 2D UNet [3], while for 3D label\ncorrection, we utilized a 3D UNet [23]. To preprocess the BraTs dataset,\nwe resized the input volumes to 155 Ã— 240 Ã— 240 and cropped them to\n240 Ã— 240 during 2D training, and cropped them to 128 Ã— 240 Ã— 240\nduring label correction. For the ACDC dataset, we used a 2D crop size of\n256 Ã— 256 and a 3D crop size of 16 Ã— 256 Ã— 256.\nWe trained our networks using the 3D UNet optimizer with a batch\nsize of 24 for 2D training and a batch size of 2 for 3D training, with half\nof the samples in each batch labeled for semi-supervised learning. The\npoly learning rate strategy was used to adjust the learning rate, starting\nwith an initial rate of 0.01. We implemented our method in PyTorch\n[24] and conducted all experiments on a Ubuntu desktop with one single\nRTX3090 GPU. To perform the Î», we follow the approach [25] by using a\ntime-dependent Gaussian warming up function, Î»(t\ni\n) =\n0.25â‹…e\n\u0000 5\n(\n1\u0000 \nt\ni\nt\nmax\n)\n2\n, where the current training iteration is denoted by t\ni\n(0 <= t\ni \n<=t\nmax\n), and t\nmax \nrepresents the maximum number of iterations.\n3.4. Comparison with other methods\nWe conduct a comprehensive evaluation of our method and compare\nit with other baselines and state-of-the-art methods in semi-supervised\nlearning, including: (1) Direct 2D fully-supervised algorithm (2D\nUNet) [3]; (2) Mean Teacher (MT) [6]; (3) Uncertainty Aware Mean\nTeacher (UAMT) [25]; (4) FixMatch [26]; (5) Cross Pseudo Supervision\n(CPS) [4]; (6) Direct 3D fully-supervised algorithm (3D UNet) [23]; (7)\nContrastive Constrained Regularization (Con2R) [27]. For all 2D\nmethods, we first transformed the 3D volumes into 2D slices, and then\nperformed semi-supervised learning using the 2D slices with the same\nsettings as the first part of our proposed method. Regarding the 3D fully\nsupervised and Con2R methods, we followed the setup outlined in the\nCon2R paper. As the authors did not provide complete code, we\nimplemented the training process as described in the paper. We assigned\nthe upper bound of using fully labeled data as the maximum value be-\ntween the 2D fully supervised and 3D fully supervised methods. Spe-\ncifically, for the BraTs dataset, we used 3D fully supervised training,\nwhile for the ACDC dataset, we used 2D fully supervised training.\nBraTs. In Table 1, we report benchmark performances on BraTs for\ndifferent amounts of labeled slices. Our method achieves up to 83 %\nperformance of the fully supervised approach using only 0.6 % of the\nslices and 90 % using 1.9 % of the slices. For dense 3D medical images,\nsuch as those in the BraTs dataset, our study reveals that baseline 3D\nmethods can achieve relatively good results, with Con2R surpassing\nmost 2D semi-supervised networks. This is due to the abundant\ncontextual information present in the data, which may be lost with a\npurely 2D approach. By contrast, our proposed M2LC method achieves\nsuperior segmentation results, surpassing both 2D and 3D approaches.\nThis is due to its ability to leverage the strengths of both paradigms,\nthereby enhancing the segmentation performance on challenging data-\nsets with limited annotated data.\nACDC. We present the results on the ACDC dataset in Table 2. Our\nproposed method maintains its superiority over all other compared\nmethods, achieving a remarkable performance of 95 % of full supervi-\nsion using only 6.4 % of the data, and a commendable performance of\n0.79 % of full supervision with a meager 1.2 % of the data. During our\nexperiments, we observed that using 2D methods significantly out-\nperformed 3D methods for data where the slices are sparsely distributed\nin the z-axis, such as in the ACDC dataset. This is because sparse 3D data\ninherently contains less contextual information in the z-axis, and 2D\nmethods can better explore the relationship between labeled and unla-\nbeled slices. Nevertheless, our proposed method achieved state-of-the-\nart performance for both sparse and dense 3D data, demonstrating the\nrobustness of our approach.\nShanghai General Hospital Cohort. To thoroughly evaluate the\nmodelâ€™s performance developed in this study using cohort data from\nShanghai General Hospital, we utilized partially labeled slice groups for\nmodel training. Meanwhile, the fully labeled group in the fully super-\nvised algorithm served as a benchmark for performance comparison.\nStatistical analysis revealed that each patient in the Shanghai General\nHospital cohort had an average of 19.6 DWI sequence image slices, with\n8.63 slices on average containing annotations and 4.08 slices with tumor\nregion annotations. During the screening process of the partially labeled\ngroup, 23.17 % of slices were utilized for the bladder wall region and\n24.51 % for the bladder tumor region. Notably, due to the bladder wallâ€™s\ncontinuous, curvilinear, and thin structure, ASD could introduce sig-\nnificant errors in segmentation assessment, leading this study to exclude\nthis metric from the evaluation process.\nFrom the results (Table 3), 23.17 % and 24.51 % of the labeled slices\nwere applied in the bladder wall and bladder tumor regions, respec-\ntively. The study achieved 95.79 % performance in the bladder wall\nregion and 99.13 % in the bladder tumor region with its partially labeled\ngroup method, compared to the fully supervised segmentation results of\nthe fully labeled group. These results highlight our methodâ€™s excellent\nperformance. Meanwhile, comparative analysis with mainstream in-\ndustry methods underscores its significant performance advantage,\nfurther validating its practical value.\n4. Discussion\nIn this study, an innovative image segmentation method is developed\nfor the task of segmenting medical images under partially labeled slices\nD. Hu et al. \nBiomedical Signal Processing and Control 113 (2026) 108907\n5\n\nwith high accuracy and efficiency. By integrating semi-supervised\nlearning and pseudo-labeling techniques, alongside two key compo-\nnents: maximum minimum entropy cross-supervision and label correc-\ntion, the challenge of limited labeled data in medical image\nsegmentation is effectively addressed. Experimental results on the BraTs\ndataset, ADCD dataset and the Shanghai General Hospital cohort dataset\ndemonstrate that our method achieves significant advancements in\npartially labeled scenarios compared to fully supervised method.\nNotably, on the BraTs dataset, with minimal labeled slices, our\nmethodâ€™s performance approaches that of fully supervised approaches,\nshowcasing its efficacy in processing dense 3D medical image data,\nespecially in contextually informative and structurally complex scenes.\nCompared to other semi-supervised learning methods, our approach\nachieves satisfactory segmentation performance in bladder wall and\nbladder tumor regions on the Shanghai General Hospital cohort dataset,\nachieving DSC values as high as 95.79 % and 99.13 %, respectively, even\nwith partially labeled slices. This fully demonstrates the versatility and\nrobustness of the method in this study in dealing with the task of seg-\nmenting different structures in medical images.\nFurthermore, in terms of performance, the method in this study has\nsignificant advantages over other semi-supervised learning methods. On\nthe BraTs dataset, this studyâ€™s method achieved the highest Dice simi-\nlarity coefficient on all categories, and performed well in terms of ASD.\nOn the Shanghai General Hospital cohort dataset, the method of this\nstudy also outperformed other methods in the segmentation results of\nbladder wall and bladder tumor regions.\nOverall, this study exhibits several significant strengths: 1. Dual\nApplication of Entropy Maximization and Minimization: The study\nemploys entropy maximization to facilitate comprehensive exploration\nof the entire data space, particularly in unlabeled regions. This approach\nenhances the modelâ€™s capability to identify latent and underutilized\nfeatures and patterns within the dataset, thereby improving its learning\nand generalization abilities. Concurrently, entropy minimization en-\nsures the model adapts effectively to the distribution of labeled data,\nTable 1\nComparison of segmentation performance (DSC [%] / ASD [mm]) on BraTs test set under two â€œfew-slice-annotatedâ€ segmentation setting (1 slice annotated per volume\nF (1) or 3 slices annotated per volume F (3)).\nLabeled Method ET TC WT Mean\nDSC â†‘ ASD â†“DSC â†‘ ASD â†“ DSC â†‘ ASD â†“ DSC â†‘ ASD â†“\nF (1)\nFull Supervised (2D) 20.63 50.12 25.00 42.49 74.56 12.14 40.06 34.92\nMT 26.63 27.29 27.55 47.78 80.05 8.25 44.74 27.77\nUAMT 15.92 28.49 27.98 15.42 78.87 9.76 40.92 17.89\nFixMatch 25.86 22.02 40.89 33.19 78.82 4.51 48.52 19.91\nCPS 21.82 33.46 30.98 44.44 80.98 8.29 44.59 28.73\nFull supervised (3D) 17.23 27.79 27.74 43.84 79.98 8.09 41.65 26.57\nCon2R 25.78 33.57 38.98 33.61 77.63 10.38 47.79 25.85\nM2LC (ours) 29.07 13.63 44.20 22.98 82.51 6.92 51.93 14.51\nF (3)\nFull Supervised (2D) 22.48 28.63 28.37 43.18 78.94 12.14 43.26 27.98\nMT 30.61 22.47 32.13 31.79 81.73 5.94 48.16 20.07\nUAMT 25.54 33.27 39.57 51.51 82.69 6.95 49.27 14.32\nFixMatch 28.68 22.38 46.72 31.03 79.87 6.65 51.76 19.35\nCPS 29.23 22.62 34.25 42.09 82.69 6.95 48.72 23.89\nFull Supervised (3D) 22.46 26.90 23.92 42.73 81.11 7.91 42.49 25.84\nCon2R 28.16 33.81 34.19 40.15 82.78 6.69 48.37 26.88\nM2LC (ours) 32.97 8.99 53.24 16.46 84.12 5.29 56.78 10.25\nAll Full Supervised 34.96 7.23 65.47 7.73 88.05 1.52 62.83 5.49\nTable 2\nComparison of segmentation performance (DSC [%] / ASD [mm]) on ACDC test set under three â€œfew-slice-annotatedâ€ segmentation setting (F (1/2) represents each\n3D sample body labeled trilaminar slice, F (1/10) denotes annotating one slice for every ten 3D anatomical samples).\nLabeled Method RV LYO MV Mean\nDSC â†‘ ASD â†“DSC â†‘ ASD â†“ DSC â†‘ ASD â†“ DSC â†‘ ASD â†“\nF (1/2)\nFull Supervised\n(2D)\nMT\nUAMT\nFixMatch\nCPS Con2R\nM2LC (ours)\n81.76 83.89\n84.11 65.14\n84.84 48.23\n85.58\n1.420.74\n0.69\n9.46 0.81 10.54\n0.77\n83.49 83.36\n83.67 77.45\n84.55 50.30\n85.27\n1.31 1.64\n1.55 4.61\n1.24 11.51\n0.85\n89.67 89.78\n89.91 84.23\n89.79 53.76\n90.51\n2.49 2.55\n2.61 6.79\n1.83 13.86\n1.57\n84.97 85.68\n85.89 75.61\n86.39 50.76\n87.12\n1.74 1.64\n1.62 6.95\n1.29 11.97\n1.07\nF (1/10)\nFull Supervised\n(2D)\nMT\nUAMT\nFixMatch\nCPS Con2R\nM2LC (ours)\n28.95 44.03\n49.94 49.61\n48.96 20.20\n61.27\n18.15 8.69 8.72\n9.44 4.57 54.07\n1.83\n32.62 49.92\n51.10 62.69\n53.09 25.45\n74.00\n10.90 5.36\n5.29 3.66\n3.53 37.93\n2.46\n45.58 47.32\n52.00 55.94\n60.27 31.34\n81.89\n8.55 8.79\n8.02 4.37\n4.24 28.07\n4.10\n35.72 47.09\n51.02 56.08\n54.11 25.66\n72.39\n12.53 7.61\n7.34 5.49\n4.11 40.02\n2.80\nAll Full Supervised 91.15 0.38 88.81 1.06 94.83 0.86 91.60 0.76\nTable 3\nComparison of segmentation results on the Shanghai General Hospital cohort\ntest set (DSC [%] / ASD [mm]) (F (3) represents each 3D sample body labeled\ntrilaminar slice).\nLabeled Method Bladder Wall Tumor\nDSCâ†‘ DSCâ†‘ ASDâ†“\nF (3) Full Supervised (2D) 53.79 53.48 13.14\nMT 53.42 63.19 9.23\nUAMT 60.12 68.96 6.79\nFixMatch 58.11 72.79 8.61\nCPS 55.034 65.24 10.75\nFull supervised (3D) 49.03 58.97 12.37\nCon2R 58.87 74.51 6.92\nM2LC 62.40 77.57 5.21\nAll Full Supervised 65.14 78.25 4.92\nD. Hu et al. \nBiomedical Signal Processing and Control 113 (2026) 108907\n6\n\nthereby closely aligning with reliable labeled information during the\nlearning process. 2. Complementary and Adversarial Network Training:\nThe training strategies of the two networks are designed to be comple-\nmentary and adversarial, which significantly improves the efficiency of\ninformation utilization under limited labeled data. By mutually super-\nvising each other, these networks progress together, leading to a more\ncomprehensive understanding of the data. 3. 2D Pseudo-label Genera-\ntion and 3D Label Correction: Upon completion of training, this study\nutilizes two 2D networks to generate high-quality pseudo-labels, which\nare subsequently assembled into 3D volumetric data. Given potential\nnoise in pseudo-labels, a meticulous 3D label correction process is\nimplemented to optimize them, thereby enhancing the accuracy and\nreliability of final segmentation results. This process not only improves\npseudo-label accuracy from a correction standpoint but also enriches the\nlearning of contextual continuity within 3D volumetric data. This is\ncritical for compensating missing information in 2D network training.\nHowever, this study also has several limitations. Firstly, the method\nin our study requires high labeling accuracy and quality. Secondly, it\nmay be impacted by uneven labeling distributions, potentially leading to\nless accurate segmentation results in specific regions. Moreover, there\nremains scope to optimize computational and memory resources to\naccommodate larger datasets and more complex tasks. Future research\nwill focus on fine-tuning network architectures for specific application\nscenarios, aiming to significantly enhance segmentation performance\nfor targeted organs, diseases, and image sequences.\n5. Conclusion\nIn this study, we addressed the challenging and novel problem of\nâ€œfew-slice annotatedâ€ medical image segmentation and proposed an\neffective solution: M2LC, which outperformed existing methods on two\nbenchmark datasets and Shanghai General Hospital cohort. Our\napproach utilizes entropy minimization and maximization constraints to\neffectively utilize both labeled and unlabeled data, and we demon-\nstrated its robustness on both dense and sparse 3D data. Our work has\nimportant implications for the field of medical image segmentation,\noffering a promising direction for improving segmentation accuracy\nwith limited annotated slice resources.\nEthics Statement\nThis retrospective study was approved by the Shanghai General\nHospital Institutional Review Board (IRB), and the need to obtain\ninformed consent was waived.\nCRediT authorship contribution statement\nDichao Hu: Writing â€“ original draft, Data curation. He Liu: Re-\nsources. Mingyang Li: Writing â€“ original draft, Data curation. Zekun\nJiang: Writing â€“ original draft, Data curation. Wenbo Wu: Resources.\nMinghui Sun: Writing â€“ review & editing. Wei Shen: Conceptualiza-\ntion. Mingmin Liu: Data curation. Haitao Liu: Conceptualization.\nFunding\nThis work was supported by a project of the National Facility for\nTranslational Medicine (Shanghai) (TMSK-2021-118) and the project of\nconstructing and validating a Therapeutic Efficacy Prediction System for\nBladder Cancer Based on Multi-modal Multi-omics Features (MicroPort).\nDeclaration of competing interest\nThe authors declare that they have no known competing financial\ninterests or personal relationships that could have appeared to influence\nthe work reported in this paper.\nData availability\nData will be made available on request.\nReferences\n[1] Y. Zhou, L. Xie, W. Shen, Y. Wang, E.K. Fishman, A.L. Yuille, A,, in: Fixed-Point\nModel for Pancreas Segmentation in Abdominal CT Scans, Springer International\nPublishing, Cham, 2017, pp. 693â€“701, https://doi.org/10.1007/978-3-319-66182-\n7_79.\n[2] M. Antonelli, A. Reinke, S. Bakas, K. Farahani, A. Kopp-Schneider, B.A. Landman,\nG. Litjens, B. Menze, O. Ronneberger, R.M. Summers, B. van Ginneken, M. Bilello,\nP. Bilic, P.F. Christ, R.K.G. Do, M.J. Gollub, S.H. Heckers, H. Huisman, W.\nR. Jarnagin, M.K. McHugo, S. Napel, J.S.G. Pernicka, K. Rhode, C. Tobon-Gomez,\nE. Vorontsov, J.A. Meakin, S. Ourselin, M. Wiesenfarth, P. Arbel\nÂ´\naez, B. Bae,\nS. Chen, L. Daza, J. Feng, B. He, F. Isensee, Y. Ji, F. Jia, I. Kim, K. Maier-Hein,\nD. Merhof, A. Pai, B. Park, M. Perslev, R. Rezaiifar, O. Rippel, I. Sarasua, W. Shen,\nJ. Son, C. Wachinger, L. Wang, Y. Wang, Y. Xia, D. Xu, Z. Xu, Y. Zheng, A.\nL. Simpson, L. Maier-Hein, M.J. Cardoso, The medical segmentation decathlon,\nNat. Commun. 13 (2022) 4128, https://doi.org/10.1038/s41467-022-30695-9.\n[3] O. Ronneberger, P. Fischer, T. Brox, U-Net: Convolutional Networks for Biomedical\nImage Segmentation, in: N. Navab, J. Hornegger, W.M. Wells, A.F. Frangi (Eds.),\nMedical Image Computing and Computer-Assisted Intervention â€“ MICCAI 2015,\nSpringer International Publishing, Cham, 2015: pp. 234â€“241. https://doi.org\n/10.1007/978-3-319-24574-4_28.\n[4] X. Chen, Y. Yuan, G. Zeng, J. Wang, in: Semi-Supervised Semantic Segmentation\nwith Cross Pseudo Supervision, IEEE, Nashville, TN, USA, 2021, pp. 2613â€“2622,\nhttps://doi.org/10.1109/CVPR46437.2021.00264.\n[5] D. Nie, Y. Gao, L. Wang, D. Shen, ASDNet attention based semi-supervised deep\nnetworks for medical image segmentation, in: A.F. Frangi, J.A. Schnabel,\nC. Davatzikos, C. Alberola-L\nÂ´\nopez, G. Fichtinger (Eds.), Medical Image Computing\nand Computer Assisted Intervention â€“, Springer International Publishing, Cham,\n2018, pp. 370â€“378, https://doi.org/10.1007/978-3-030-00937-3_43.\n[6] A. Tarvainen, H. Valpola, Mean teachers are better role models: Weight-averaged\nconsistency targets improve semi-supervised deep learning results, (2018). htt\np://arxiv.org/abs/1703.01780 (accessed August 7, 2024).\n[7] C. You, R. Zhao, L.H. Staib, J.S. Duncan, Momentum Contrastive Voxel-Wise\nRepresentation Learning for Semi-supervised Volumetric Medical Image\nSegmentation, in: L. Wang, Q. Dou, P.T. Fletcher, S. Speidel, S. Li (Eds.), Medical\nImage Computing and Computer Assisted Intervention â€“ MICCAI 2022, Springer\nNature Switzerland, Cham, 2022: pp. 639â€“652. https://doi.org/10.1007/9\n78-3-031-16440-8_61.\n[8] C. You, Y. Zhou, R. Zhao, L. Staib, J.S. Duncan, SimCVD: simple contrastive voxel-\nwise representation distillation for semi-supervised medical image segmentation,\nIEEE Trans. Med. Imaging 41 (2022) 2228â€“2237, https://doi.org/10.1109/\nTMI.2022.3161829.\n[9] I. Caglic, V. Panebianco, H.A. Vargas, V. Bura, S. Woo, M. Pecoraro, S. Cipollari,\nE. Sala, T. Barrett, MRI of bladder cancer: local and nodal staging, J. Magn. Reson.\nImaging 52 (2020) 649â€“667, https://doi.org/10.1002/jmri.27090.\n[10] S. Yoshida, T. Takahara, T.C. Kwee, Y. Waseda, S. Kobayashi, Y. Fujii, DWI as an\nimaging biomarker for bladder cancer, AJR Am. J. Roentgenol. 208 (2017)\n1218â€“1228, https://doi.org/10.2214/AJR.17.17798.\n[11] V. Sharma, R.N. Mir, Maximum entropy-based semi-supervised learning for\nautomatic detection and recognition of objects using deep ConvNets, IJCVR 11\n(2021) 328, https://doi.org/10.1504/IJCVR.2021.115165.\n[12] P. Kr\nÂ¨\nahenbÃ¼hl, V. Koltun, Efficient Inference in Fully Connected CRFs with\nGaussian Edge Potentials, (2012). http://arxiv.org/abs/1210.5644 (accessed\nAugust 9, 2024).\n[13] T.-H. Vu, H. Jain, M. Bucher, M. Cord, P. Perez, Advent,, in: Adversarial Entropy\nMinimization for Domain Adaptation in Semantic Segmentation, IEEE, Long Beach,\nCA, USA, 2019, pp. 2512â€“2521, https://doi.org/10.1109/CVPR.2019.00262.\n[14] M.C. Clark, L.O. Hall, D.B. Goldgof, R.P. Velthuizen, F.R. Murtaugh, M.L. Silbiger,\nAutomatic brain tumor segmentation, in: K.M. Hanson (Ed.), San Diego, CA, 1998:\npp. 533â€“544. https://doi.org/10.1117/12.310932.\n[15] D. Arpit, S. Jastrzebski, N. Ballas, D. Krueger, E. Bengio, M.S. Kanwal, T. Maharaj,\nA. Fischer, A. Courville, Y. Bengio, S. Lacoste-Julien, A Closer Look at\nMemorization in Deep Networks, (n.d.).\n[16] S. Liu, K. Liu, W. Zhu, Y. Shen, C. Fernandez-Granda, in: Adaptive Early-Learning\nCorrection for Segmentation from Noisy Annotations, IEEE, New Orleans, LA, USA,\n2022, pp. 2596â€“2606, https://doi.org/10.1109/CVPR52688.2022.00263.\n[17] S. Liu, J. Niles-Weed, N. Razavian, C. Fernandez-Granda, Early-Learning\nRegularization Prevents Memorization of Noisy Labels, (2020). http://arxiv.\norg/abs/2007.00151 (accessed August 9, 2024).\n[18] S. Bakas, H. Akbari, A. Sotiras, M. Bilello, M. Rozycki, J.S. Kirby, J.B. Freymann,\nK. Farahani, C. Davatzikos, Advancing the cancer genome atlas glioma MRI\ncollections with expert segmentation labels and radiomic features, Sci. Data 4\n(2017) 170117, https://doi.org/10.1038/sdata.2017.117.\n[19] O. Bernard, A. Lalande, C. Zotti, F. Cervenansky, X. Yang, P.-A. Heng, I. Cetin,\nK. Lekadir, O. Camara, M.A. Gonzalez Ballester, G. Sanroma, S. Napel, S. Petersen,\nG. Tziritas, E. Grinias, M. Khened, V.A. Kollerathu, G. Krishnamurthi, M.-M. Rohe,\nX. Pennec, M. Sermesant, F. Isensee, P. Jager, K.H. Maier-Hein, P.M. Full, I. Wolf,\nS. Engelhardt, C.F. Baumgartner, L.M. Koch, J.M. Wolterink, I. Isgum, Y. Jang,\nY. Hong, J. Patravali, S. Jain, O. Humbert, P.-M. Jodoin, Deep learning techniques\nfor automatic MRI cardiac multi-structures segmentation and diagnosis: is the\nD. Hu et al. \nBiomedical Signal Processing and Control 113 (2026) 108907\n7\n\nproblem solved? IEEE Trans. Med. Imaging 37 (2018) 2514â€“2525, https://doi.org/\n10.1109/TMI.2018.2837502.\n[20] X. Luo, J. Chen, T. Song, Y. Chen, G. Wang, S. Zhang, Semi-supervised Medical\nImage Segmentation through Dual-task Consistency, (2023). http://arxiv.\norg/abs/2009.04448 (accessed August 9, 2024).\n[21] J. Liu, C. Desrosiers, Y. Zhou, Semi-supervised Medical Image Segmentation Using\nCross-Model Pseudo-Supervision with Shape Awareness and Local Context\nConstraints, in: L. Wang, Q. Dou, P.T. Fletcher, S. Speidel, S. Li (Eds.), Medical\nImage Computing and Computer Assisted Intervention â€“ MICCAI 2022, Springer\nNature Switzerland, Cham, 2022: pp. 140â€“150. https://doi.org/10.1007/9\n78-3-031-16452-1_14.\n[22] Y. Wu, Z. Wu, Q. Wu, Z. Ge, J. Cai, Exploring Smoothness and Class-Separation for\nSemi-supervised Medical Image Segmentation, (2022). http://arxiv.org/abs/22\n03.01324 (accessed August 9, 2024).\n[23] \nÂ¨\nO. Ã‡iÃ§ek, A. Abdulkadir, S.S. Lienkamp, T. Brox, O. Ronneberger, 3D U-Net:\nLearning dense volumetric segmentation from sparse annotation, in: S. Ourselin,\nL. Joskowicz, M.R. Sabuncu, G. Unal, W. Wells (Eds.), Medical Image Computing\nand Computer-Assisted Intervention â€“, Springer International Publishing, Cham,\n2016, pp. 424â€“432, https://doi.org/10.1007/978-3-319-46723-8_49.\n[24] A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan, T. Killeen, Z. Lin, N.\nGimelshein, L. Antiga, A. Desmaison, A. K\nÂ¨\nopf, E. Yang, Z. DeVito, M. Raison, A.\nTejani, S. Chilamkurthy, B. Steiner, L. Fang, J. Bai, S. Chintala, PyTorch: An\nImperative Style, High-Performance Deep Learning Library, (2019). http://arxiv.\norg/abs/1912.01703 (accessed August 9, 2024).\n[25] L. Yu, S. Wang, X. Li, C.-W. Fu, P.-A. Heng, Uncertainty-aware self-ensembling\nmodel for semi-supervised 3D left atrium segmentation, in: D. Shen, T. Liu, T.\nM. Peters, L.H. Staib, C. Essert, S. Zhou, P.-.-T. Yap, A. Khan (Eds.), Medical Image\nComputing and Computer Assisted Intervention â€“, Springer International\nPublishing, Cham, 2019, pp. 605â€“613, https://doi.org/10.1007/978-3-030-32245-\n8_67.\n[26] K. Sohn, D. Berthelot, C.-L. Li, Z. Zhang, N. Carlini, E.D. Cubuk, A. Kurakin, H.\nZhang, C. Raffel, FixMatch: Simplifying Semi-Supervised Learning with\nConsistency and Confidence, (2020). http://arxiv.org/abs/2001.07685 (accessed\nAugust 9, 2024).\n[27] S. ReiÃŸ, C. Seibold, A. Freytag, E. Rodner, R. Stiefelhagen, Graph-Constrained\nContrastive Regularization for Semi-weakly Volumetric Segmentation, in: S.\nAvidan, G. Brostow, M. Ciss\nÂ´\ne, G.M. Farinella, T. Hassner (Eds.), Computer Vision â€“\nECCV 2022, Springer Nature Switzerland, Cham, 2022: pp. 401â€“419. https://doi.\norg/10.1007/978-3-031-19803-8_24.\nD. Hu et al. \nBiomedical Signal Processing and Control 113 (2026) 108907\n8",
    "version": "5.3.31"
  },
  {
    "numpages": 12,
    "numrender": 12,
    "info": {
      "PDFFormatVersion": "1.7",
      "Language": "en-US",
      "EncryptFilterName": null,
      "IsLinearized": true,
      "IsAcroFormPresent": false,
      "IsXFAPresent": false,
      "IsCollectionPresent": false,
      "IsSignaturesPresent": false,
      "Creator": "Elsevier",
      "Custom": {
        "CrossMarkDomains[1]": "elsevier.com",
        "CrossmarkMajorVersionDate": "2010-04-23",
        "CreationDate--Text": "27th October 2025",
        "ElsevierWebPDFSpecifications": "7.0.1",
        "robots": "noindex",
        "doi": "10.1016/j.bspc.2025.108948",
        "CrossMarkDomains[2]": "sciencedirect.com",
        "CrossmarkDomainExclusive": "true"
      },
      "ModDate": "D:20251027134641Z",
      "Author": "Guizeng Wang",
      "Title": "Weakly supervised brain image segmentation guided by texture knowledge",
      "Keywords": "Brain medical image segmentation,Texture knowledge,Weakly supervised,CAM",
      "CreationDate": "D:20251027131135Z",
      "Producer": "Acrobat Distiller 8.1.0 (Windows)",
      "Subject": "Biomedical Signal Processing and Control, 113 (2026) 108948. doi:10.1016/j.bspc.2025.108948"
    },
    "metadata": {
      "crossmark:crossmarkdomains": "elsevier.comsciencedirect.com",
      "crossmark:crossmarkdomainexclusive": "true",
      "crossmark:doi": "10.1016/j.bspc.2025.108948",
      "crossmark:majorversiondate": "2010-04-23",
      "dc:format": "application/pdf",
      "dc:identifier": "10.1016/j.bspc.2025.108948",
      "dc:publisher": "Elsevier Ltd",
      "dc:description": "Biomedical Signal Processing and Control, 113 (2026) 108948. doi:10.1016/j.bspc.2025.108948",
      "dc:subject": [
        "Brain medical image segmentation",
        "Texture knowledge",
        "Weakly supervised",
        "CAM"
      ],
      "dc:title": "Weakly supervised brain image segmentation guided by texture knowledge",
      "dc:creator": [
        "Guizeng Wang",
        "Huimin Lu",
        "Songzhe Ma",
        "Niya Li",
        "Pengcheng Sang",
        "Yilong Wang"
      ],
      "jav:journal_article_version": "VoR",
      "pdf:creationdate--text": "27th October 2025",
      "pdf:producer": "Acrobat Distiller 8.1.0 (Windows)",
      "pdf:keywords": "Brain medical image segmentation,Texture knowledge,Weakly supervised,CAM",
      "pdfx:creationdate--text": "27th October 2025",
      "pdfx:crossmarkdomains": "sciencedirect.comelsevier.com",
      "pdfx:crossmarkdomainexclusive": "true",
      "pdfx:crossmarkmajorversiondate": "2010-04-23",
      "ml8zfmmz.ymakmpegot6nm8nnogyolwekmd2gmdinnmb-zdf8m9yjo9eqn9iknm2lnt-tma": "",
      "pdfx:doi": "10.1016/j.bspc.2025.108948",
      "pdfx:robots": "noindex",
      "prism:aggregationtype": "journal",
      "prism:copyright": "Â© 2025 Elsevier Ltd. All rights are reserved, including those for text and data mining, AI training, and similar technologies.",
      "prism:coverdate": "2026-03-01",
      "prism:coverdisplaydate": "1 March 2026",
      "prism:doi": "10.1016/j.bspc.2025.108948",
      "prism:issn": "1746-8094",
      "prism:number": "PA",
      "prism:pagerange": "108948",
      "prism:publicationname": "Biomedical Signal Processing and Control",
      "prism:startingpage": "108948",
      "prism:url": "https://doi.org/10.1016/j.bspc.2025.108948",
      "prism:volume": "113",
      "tdm:policy": "https://www.elsevier.com/tdm/tdmrep-policy.json",
      "tdm:reservation": "1",
      "xmp:createdate": "2025-10-27T13:11:35",
      "xmp:creatortool": "Elsevier",
      "xmp:metadatadate": "2025-10-27T13:46:41",
      "xmp:modifydate": "2025-10-27T13:46:41",
      "xmpmm:documentid": "uuid:1b499bed-4ce8-4c0c-b682-0d58baae1cbe",
      "xmpmm:instanceid": "uuid:b6ed5266-03cb-4855-90c0-636283357f42",
      "xmprights:marked": "True"
    },
    "text": "Contents lists available at ScienceDirect\nBiomedical Signal Processing and Control\njournal homepage: www.elsevier.com/locate/bspc\nWeakly supervised brain image segmentation guided by texture knowledge\nGuizeng Wang \na,b,c\n, Huimin Lu \na,b,c ,\nâˆ—\n, Songzhe Ma \na,b,c\n, Niya Li \nc\n, Pengcheng Sang \na,b,c\n,\nYilong Wang \nd,e\na \nSchool of Computer Science and Engineering, Changchun University of Technology, Changchun, 130102, PR China\nb \nJilin Provincial Smart Health Joint Innovation Laboratory for the New Generation of AI, Changchun, 130102, PR China\nc \nKey Laboratory of Symbolic Computation and Knowledge Engineering of Ministry of Education, Jilin University, Changchun, 130012, PR China\nd \nSchool of Mathematics and Statistics, Changchun University of Technology, Changchun, 130012, PR China\ne \nThe First Hospital of Jilin University, Changchun 130061, China\nA R T I C L E I N F O\nKeywords:\nBrain medical image segmentation\nTexture knowledge\nWeakly supervised\nCAM\nA B S T R A C T\nIn weakly supervised learning-based medical image segmentation, the incompleteness of labels and the\ncomplexity of medical image structures often lead to under-segmentation and over-segmentation issues in\npractical predictions. Therefore, a weakly supervised brain image segmentation guided by texture knowledge\n(TKG-Net) is proposed in this paper. Its core idea is based on the interaction between the perceptual system\nand the rational system in the cognitive dual-path model to locate the fuzzy boundaries of the target. First, a\nsimple and effective perceptual system, called the Stepped Class Activation Map Fusion Algorithm, is designed,\nwhich utilizes the ability of deep class activation map (CAM) to capture the localization characteristics of the\ntarget position and the ability of shallow CAM to highlight the texture characteristics of the target details, and\nmaximally removes background noise to emphasize foreground targets. Second, a novel rational system, called\nthe Texture Knowledge Extraction Module, is introduced into the network, through the collaborative action\nof fuzzy K-Means and Prewitt operator to efficiently extract the texture features of the target. Subsequently,\nthese captured target texture structural features are effectively integrated into the texture consistency loss\nfunction. Finally, a novel Cross-guided module is constructed, which can fully exploit the correlation between\ntexture information and semantic information, achieving more effective fusion of input vectors and thus\nachieving more precise segmentation of the target region. Our method is evaluated on the BraTS2019 and\nINSTANCE2022 datasets. Experimental results demonstrate that the proposed TKG-Net exhibits excellent\nsegmentation performance compared to other state-of-the-art methods.\n1. Introduction\nAutomated and accurate methods for medical image segmentation\nare crucial for accurately extracting target structures or lesion regions\nto assist clinical decision-making and improve patient treatment out-\ncomes. However, medical images often exhibit complex structures,\nmultiscale information and different imaging modalities, which greatly\nincreases the challenges of medical image segmentation. Addition-\nally, traditional medical image segmentation methods typically rely on\nhigh-quality labeled data for supervised learning [1], which is often\nexpensive and time-consuming in the medical domain. Therefore, how\nto perform medical image segmentation in the absence of accurate\nlabels becomes a research hotspot and challenge.\nIn recent years, image segmentation methods based on weakly su-\npervised learning [2] have been widely recognized. Weakly supervised\nlearning utilizes training data with incomplete or inaccurate labels\nâˆ— \nCorresponding author at: School of Computer Science and Engineering, Changchun University of Technology, Changchun, 130102, PR China.\nE-mail address: luhuimin@ccut.edu.cn (H. Lu).\nfor model training, thereby overcoming the limitations of traditional\nsupervised learning in medical image segmentation. This approach can\nbe trained using rich but incomplete medical image data. For example,\nrelying on image-level labels, boundary labels, scribble labels or using\nexternal prior knowledge (such as shape, position, texture [3]) for\nconstraints. Weakly supervised learning methods offer a more econom-\nical, efficient, and practical solution for medical image segmentation.\nHowever, medical image segmentation based on weakly supervised\nlearning faces many challenges, such as the adaptability of models to\nlabel incompleteness and the complexity of medical image structures.\nIn this context, a novel weakly supervised brain image segmentation\nguided by texture knowledge is proposed, aiming to alleviate the issues\nof label incompleteness and complexity of medical image structures in\nmedical image segmentation. In this paper, abundant brain medical\nimage data are utilized for training, while external prior knowledge\nhttps://doi.org/10.1016/j.bspc.2025.108948\nReceived 28 June 2024; Received in revised form 11 June 2025; Accepted 12 October 2025\nBiomedical Signal Processing and Control 113 (2026) 108948\nAvailable online 27 October 2025\n1746-8094/Â© 2025 Elsevier Ltd. All rights are reserved, including those for text and data mining, AI training, and similar technologies.\n\nG. Wang et al.\n(image texture information) is introduced to enhance the accuracy of\nthe segmentation model.\nThe main contributions of this paper are as follows:\nA novel weakly supervised brain image segmentation guided by\ntexture knowledge method is proposed in this paper, comprising three\nkey components: the stepped CAM fusion algorithm, the texture knowl-\nedge extraction module, and the cross-guided module. Firstly, accurate\nlocalization of the target region is achieved by effectively integrating\nCAMs at different levels. Secondly, the texture knowledge extraction\nmodule is introduced to extract texture features related to the target\nshape from the original image, used for weakly supervised learning.\nFinally, the texture consistency loss and semantic consistency loss\nare employed to improve the prediction of mask edges by the cross-\nguided module, thereby obtaining more accurate pseudo-labels. These\nhighlights include:\nâˆ™ A simple and effective stepped class activation map fusion algo-\nrithm is proposed. This algorithm fully utilizes the characteristics of\ndeep CAM and shallow CAM to accurately locate the target region.\nâˆ™ A novel texture knowledge extraction module is proposed. This\nmodule efficiently captures the texture information of the target\nthrough the collaborative action of fuzzy K-Means and the Prewitt\noperator, and organically integrates it into the texture consistency loss\nfunction to guide the training of the network.\nâˆ™ A novel cross-guided module is introduced. It achieves the initial\nfusion of class activation maps and shallow CAMs through the utiliza-\ntion of semantic consistency loss and texture consistency loss, thereby\nachieving more precise segmentation of the target region.\nâˆ™ The performance of this method is validated on the BraTS2019\nand INSTANCE2022 datasets, demonstrating its superiority in weakly\nsupervised segmentation tasks, surpassing the current state-of-the-art\nmethods.\nThe remaining sections of this paper are organized as follows. The\nrelated work of this topic is comprehensively introduced in Section 2.\nThe proposed weakly supervised brain image segmentation guided by\ntexture knowledge is detailed in Section 3. Dataset introduction, ex-\nperimental details, comparative experiments, ablation studies, and the\nsetting of hyperparameters are presented in Section 4. The limitations\nof this approach and potential avenues for improvement are discussed\nin Section 5. Finally, conclusions are drawn in the last section.\n2. Related work\n2.1. Weakly supervised medical image semantic segmentation\nIn medical image segmentation, weakly supervised learning meth-\nods leverage limited supervision information, such as image-level la-\nbels [4,5], boundary labels [6], scribble labels [7] and others, to train\nmodels that can accurately perform image segmentation even with a\nsmall number of annotated images.\nValvano et al. [8] designed a multi-scale attention gating mech-\nanism to learn target features from scribble labels, improving object\nsegmentation. Lyu et al. [9] designed a novel CouinaudNet using\nCouinaud segment annotations as pixel-level supervision. Yu et al. [10]\nproposed a two-branch soft erasure segmentation network to expand\nthe unsegmented foreground region. Gao et al. [11] first proposed the\nuse of scribble labels for image segmentation and designed a novel\nconsistency loss to calculate the error between boundaries and outliers.\nBen Hamida et al. [12] presented a novel augmentation model for\nATTUNET using a multi-step training strategy to address Whole Slide\nImages with sparse annotations and class imbalance. Zhang et al. [13]\ndesigned a new hybrid enhancement and cycle consistency segmenta-\ntion framework, which achieved good segmentation results by using\nscribble labels. While boundary labels and scribble labels have demon-\nstrated good segmentation results, image-level labels are relatively easy\nto obtain compared to other types of labels. Therefore, some researchers\nonly use image-level labels for research. Ma et al. [14] proposed an eye\nlesion region segmentation combining multi-scale strategy and atten-\ntion mechanism to enrich the features of the lesion region. Chikontwe\net al. [15] jointly predicted image labels and improved initial CAM\nthrough a Pixel Correlation Module (PCM) coupled with self-supervised\nloss. Meng et al. [16] developed a method to detect a variety of\nretinal diseases based on complementary heat maps specifically based\non fundus images to achieve accurate labeling of the lesion area on\nthe retina. Xie et al. [17] designed an end-to-end regression activation\ngraph network that fully incorporates the size of the object, captur-\ning detailed information about the desired lesion structures. Zhong\net al. [18] proposed a two-stage segmentation framework combining\nCAM and cross-learning to effectively remove the background area in\nthe pseudo label and enhance the segmentation accuracy of the target\narea.\nHowever, these methods are not always ideal for segmentation in\ncertain cases, especially when precise segmentation of fine structures\nor local regions within the image is required. Therefore, completing\nmedical image segmentation tasks using only image-level labels is still\nconsidered challenging.\n2.2. Prior knowledge in deep learning for medical imaging\nCombining prior knowledge with deep learning can effectively en-\nhance the segmentation performance for both natural images [19â€“21]\nand medical images [22,23]. In the context of Weakly Supervised\nSemantic Segmentation (WSSS), prior knowledge can efficiently com-\npensate for the lack of information in pseudo-label [22]. Particularly,\nprior knowledge plays a more significant role in medical images com-\npared to natural images, as organs in medical images often have specific\nshapes, sizes and textures. This information is intrinsic, just like the\nknowledge of the anatomical locations within the body. For instance,\nZotti et al. [24] utilized shape priors to aid in the segmentation of\ncardiac MRI images.\nChen et al. [25] integrated anatomical prior knowledge into weakly\nsupervised segmentation networks by constructing a causal relationship\nmodule. Li et al. [26] used anatomical priors to constrain regions\nfor the fat layer and breast layer, significantly reducing the search\nspace for breast tumor localization. Dalca et al. [27] suggested using\na Variational Autoencoder (VAE) to learn shape priors and subse-\nquently sharing some VAE weights with the segmentation model. Wang\net al. [28] seamlessly integrated tight bounding box priors into the\nneural network. Meng et al. [29] leveraged domain-specific knowledge\nbetween boundaries and regions, namely the perimeter and area of the\nelliptical optic disc (OD) and optic cup (OC), to propose a differentiable\nVertical cup to disc ratio (vCDR) estimation module for end-to-end\ntraining. Furthermore, prior knowledge is often incorporated into aug-\nmented loss functions [30â€“32]. For instance, Kervadec et al. [30]\nproposed a loss function that takes size information as a limiting\ncondition, which greatly enhances the convergence speed of network\ntraining and the accuracy of segmentation. Therefore, it is crucial to\nintroduce medical knowledge features to solve the adaptability of the\nmodel to label incompleteness and structural complexity of medical\nimages.\nIn weakly supervised image segmentation tasks, algorithms are\noften challenged to accurately delineate object boundaries due to the\nlack of pixel-level annotation information. By introducing texture infor-\nmation containing details relevant to object boundaries into the model,\nthe model can better identify object boundaries, thereby improving\nsegmentation accuracy.\n3. Method\nImage-level labels lack specific object localization information com-\npared to pixel-level labels. In situations where only image-level labels\nare utilized, a method is proposed in this paper based on the interaction\nbetween the perceptual system and the rational system in the cognitive\nBiomedical Signal Processing and Control 113 (2026) 108948\n2\n\nG. Wang et al.\nAlgorithm 1: The algorithm flow proposed in this paper\ninput: Image ğ¼ and Image-level labels;\noutput: pseudo-label;\n1: Pre-train the classification backbone network ResNet50 and freeze the network;\n2: while(epoch<iteration):\n3: The original CAM obtained using the branches of the classification network;\n4: The target region image ğ‘€\nâ€²\nğ‘“ ğ‘¢ğ‘ ğ‘–ğ‘œğ‘› \nis located through Eq. (1) to (8);\n5: GLCM is used to obtain and filter the texture features with rich information;\n6: Fuzzy K-Means is used to calculate the texture features and obtain the mask image of the target region;\n7: The Prewitt operator is used to extract the edge texture of the image, and the global texture image is obtained;\n8: The mask image obtained in Eq. (17) is multiplied with the texture image to obtain the regional texture image;\n9: Feed ğ‘€\nâ€²\nğ‘“ ğ‘¢ğ‘ ğ‘–ğ‘œğ‘› \nand ğ‘€\nâ€²\nğ‘ â„ğ‘ğ‘™ğ‘™ğ‘œğ‘¤ \ninto Cross-guided module described by Eq. (18) to (23);\n10: Eq. (27) is used to train the cross-guided module;\n11: end while\nFig. 1. TKG-Net: Overview of the weakly supervised brain image segmentation\nguided by texture knowledge (Training Phase: Orange, blue, and black arrows;\nPrediction Phase: Only orange arrows).\ndual-path model. This method aims to reasonably capture and utilize\nthe position information and texture information of specific objects, as\nshown in Fig. 1.\n(1) Perceptual system: In order to rapidly and accurately locate\nthe entire target region, different layers of CAM are effectively fused to\nensure that the entire target region is covered.\n(2) Rational system: In order to obtain the texture information\nused to guide weakly supervised medical image segmentation, a new\ntexture knowledge extraction module is designed, and the extracted\ntexture features in the target region are encoded.\nDuring the training process, these encoded texture features are\ndesigned to be utilized as the true labels in the texture consistency loss\nto guide the correct classification of voxel. Finally, we encourage the\ncross-guided module to predict the mask edges accurately by adopt-\ning texture consistency loss and semantic consistency loss, thereby\nobtaining accurate pseudo-labels.\nFirstly, a pre-trained and frozen ResNet50 network is employed to\nextract the initial CAMs, which are then refined using a Stepped class\nactivation map fusion algorithm to accurately localize the target regions\nwithin the image. Subsequently, the Gray-Level Co-occurrence Matrix\n(GLCM) is utilized to extract informative texture features from the\nimage. These features are used in a fuzzy K-Means clustering algorithm\nto generate an initial mask of the target regions. In parallel, the Prewitt\noperator is applied to extract global edge texture information, which is\nthen multiplied with the initial mask to obtain a more target-specific\nregional texture map. Finally, both the localized target image and\nthe corresponding texture map are fed into a cross-guided network,\nwhere a dedicated loss function is used to iteratively optimize the\nprocess. This facilitates the fine-grained generation and enhancement of\npseudo-labels, thereby improving the accuracy and robustness of target\nregion recognition. The detailed algorithmic procedure is illustrated in\nAlgorithm 1.\n3.1. Stepped class activation map fusion algorithm\nMany WSSS methods [33â€“35] utilize class activation maps for se-\nmantic image segmentation. Inspired by the work of layerCAM [36],\na novel stepped CAM fusion algorithm (Step-CAM) is designed in this\npaper to effectively fuse CAMs from different network layers to achieve\nprecise target localization effects.\nLet the convolutional feature map at the ğ‘£ğ‘¡â„ layer be\nğ¹\nğ‘£ \n=\n{\nğ‘“\n1\n, ğ‘“\n2\n, â€¦ , ğ‘“\nğ¾\nğ‘£\n}\n, and the corresponding classification weights be\nğ‘Š\nğ‘£ \n=\n{\nğ‘¤\n1\n, ğ‘¤\n2\n, â€¦ , ğ‘¤\nğ¾\nğ‘£\n}\n. Then, the CAM is defined as:\nğ‘€\nğ‘£ \n=\nğ¾\nğ‘£\nâˆ‘\nğ‘˜=1\nğ‘¤\nğ¾\nğ‘£ \nâ‹… ğ‘“\nğ¾\nğ‘£ \n(1)\nAs network depth increases, there exists a phenomenon of position\noffset in class activation maps, which has a certain impact on the\nfusion of deep CAMs. To address this issue, the consistency information\nbetween multi-layer class activation maps is fully utilized. Specifically,\ndeep CAMs are adjusted based on the class activation maps of all\ncurrent layers to ensure their spatial alignment. Firstly, the centroid\ncoordinates ğ‘” are computed by summing up the class activation maps\nof all layers, as shown in Eq. (1). Secondly, the centroid coordinates\nof each layerâ€™s CAM are separately calculated. Then, the offset is\ncomputed using the centroid coordinates ğ‘” and the centroid position\nof each deep CAM. Finally, a translation transformation is applied\nto correct the deep CAMs. The specific calculations are described in\nEq. (2).\nğ‘” = ğº(\nğ‘†\nâˆ‘\nğ‘£=1\nğ‘€\nğ‘£\n) (2)\nğ‘€\nâ€²\nğ‘£ \n= ğ´(ğº(ğ‘”, ğº(ğ‘€\nğ‘£\n))) (3)\nBiomedical Signal Processing and Control 113 (2026) 108948\n3\n\nG. Wang et al.\nwhere, ğ‘” represents the centroid coordinates calculated by summing up\nall layer activations. ğ´(â‹…) represents the translation transformation. ğº(â‹…)\nrepresents the computation of CAM centroid coordinates. ğ‘€\nğ‘£ \nrepresents\nthe CAM of the ğ‘£ğ‘¡â„ layer, and ğ‘€\nâ€²\nğ‘£ \nrepresents the corrected CAM of the\nğ‘£ğ‘¡â„ layer. ğ‘£ âˆˆ [ğ‘ , ğ‘†] represents the depth range of the deep network\nlayers. ğ‘  indicates the boundary between deep and shallow layers and\nğ‘† denotes the total number of CAM.\nLet the normalized CAM at the ğ‘£th layer be\n Ìƒ\n ğ‘€\nğ‘£ \nâˆˆ [0, 1], and given\na threshold ğœ âˆˆ [0, ], the effective activation region is defined as:\nğ‘€\nâ€²\nğ‘£ \n= \n{\n(\nğ‘¥, ğ‘¦\n) \n|\nÌƒ\n ğ‘€\nğ‘£\n(ğ‘¥, ğ‘¦) â‰¥ ğœ\n} \n(4)\nSecondly, ğ‘€\nâ€² \n= ğ‘€\nâ€²\n1 \nâˆ© ğ‘€\nâ€²\n2 \nâ‹¯ ğ‘€\nâ€²\nğ‘ \nis defined as the intersection\nof effective regions for each activation layer. Next, the weights ğœ”\nğ‘£\nfor each layerâ€™s CAM are calculated based on the effective activation\nregions of ğ‘€\nâ€² \nand ğ‘€\nâ€²\nğ‘£\n, aiming to suppress background noise in each\nclass activation layer. Then, each layerâ€™s CAM is multiplied by ğœ”\nğ‘£ \nand\nsummed up to integrate more spatial or semantic information. Finally,\nposition-accurate effective features ğ‘€\nâ€²\nğ‘ â„ğ‘ğ‘™ğ‘™ğ‘œğ‘¤ \nand ğ‘€\nâ€²\nğ‘‘ğ‘’ğ‘’ğ‘ \nare obtained\nseparately, with specific calculations described in Eq. (5) to (7).\nğ‘¤\nğ‘£ \n=\nâˆ‘\nğ‘ƒ\nğ‘=1 \nğ‘€\nâ€²\nğ‘\nâˆ‘\nğ‘„\nğ‘=1 \nğ‘€\nâ€²\nğ‘£\nğ‘\n(5)\nğ‘€\nâ€²\nğ‘ â„ğ‘ğ‘™ğ‘™ğ‘œğ‘¤ \n=\nğ‘ \nâˆ‘\nğ‘£=1\nğ‘€\nâ€²\nğ‘£\nğœ”\nğ‘£ \n(6)\nğ‘€\nâ€²\nğ‘‘ğ‘’ğ‘’ğ‘ \n=\nğ‘†\nâˆ‘\nğ‘£=ğ‘ +1\nğ‘€\nâ€²\nğ‘£\nğœ”\nğ‘£ \n(7)\nwhere, ğ‘ƒ represents the total number of activation values within the\neffective activation region of ğ‘€\nâ€²\n, ğ‘„ represents the total number of\nactivation values within the effective activation region of ğ‘€\nâ€²\nğ‘£\n. The\nselection of ğ‘  will be discussed in Table 5 of this paper.\nIn order to achieve complementary fusion between shallow and\ndeep layers, this paper finely integrates the features of ğ‘€\nâ€²\nğ‘ â„ğ‘ğ‘™ğ‘™ğ‘œğ‘¤ \nand\nğ‘€\nâ€²\nğ‘‘ğ‘’ğ‘’ğ‘ \nobtained from Eqs. (6) and (7) through linear addition and\nelement-wise multiplication, resulting in the final high-quality feature\nrepresentation. This provides strong support for accurately locating the\ntarget region. The specific calculation is described in Eq. (8).\nğ‘€\nâ€²\nğ‘“ ğ‘¢ğ‘ ğ‘–ğ‘œğ‘› \n= (ğ‘€\nâ€²\nğ‘ â„ğ‘ğ‘™ğ‘™ğ‘œğ‘¤ \n+ ğ‘€\nâ€²\nğ‘‘ğ‘’ğ‘’ğ‘\n) Ã— ğ‘€\nâ€²\nğ‘‘ğ‘’ğ‘’ğ‘ \n(8)\nBased on the above operations, the texture information from shal-\nlow layers can be effectively integrated with the semantic information\nfrom deep layers, as shown in Fig. 8. This is mainly benefited from\nnot only considering the importance of different activation layers but\nalso considering the importance of different spatial positions. In this\npaper, the measurement of positional importance is achieved by using\nthe deep activation maps as weights for spatial positions.\n3.2. Texture knowledge extraction module\nIn weakly supervised image segmentation tasks, due to the lack of\npixel-level annotation information, only image-level category labels,\nboundary labels, etc., can be utilized for image segmentation. This\noften makes algorithms struggle to capture small textures and details\nin the image, thereby affecting the segmentation accuracy. Introducing\ntexture knowledge in weakly supervised image segmentation tasks can\nhelp algorithms better understand the local structure in the image,\nthereby improving the accuracy of segmentation. Therefore, a novel\ntexture knowledge extraction module is designed in this paper. This\nmodule can effectively extract and utilize texture knowledge.\nFirstly, the input image is subjected to edge extraction using the\nPrewitt operator to obtain a complete edge image. Let the image be\nğ¼(ğ‘¥, ğ‘¦), and the Prewitt operator uses the following two convolution\nkernels:\nFig. 2. Texture knowledge extraction module.\nHorizontal direction:\nğ¾ğ‘’ğ‘Ÿğ‘›ğ‘’ğ‘™\nğ‘¥ \n=\nâ¡\nâ¢\nâ¢\nâ£\nâˆ’1 0 1\nâˆ’1 0 1\nâˆ’1 0 1\nâ¤\nâ¥\nâ¥\nâ¦\n(9)\nVertical direction:\nğ¾ğ‘’ğ‘Ÿğ‘›ğ‘’ğ‘™\nğ‘¥ \n=\nâ¡\nâ¢\nâ¢\nâ£\n1 1 1\n0 0 0\nâˆ’1 âˆ’1 âˆ’1\nâ¤\nâ¥\nâ¥\nâ¦\n(10)\nThe edge magnitude is computed as:\nâˆ‡ğ‘“ =\nâˆš\n(ğ¼ âˆ— ğ¾ğ‘’ğ‘Ÿğ‘›ğ‘’ğ‘™\nğ‘¥\n)\n2 \n+ (ğ¼ âˆ— ğ¾ğ‘’ğ‘Ÿğ‘›ğ‘’ğ‘™\nğ‘¦\n)\n2 \n(11)\nwhere âˆ— denotes the convolution operation, and âˆ‡ğ‘“ represents the\nresulting edge image.\nSecondly, the complete edge image is multiplied element-wise with\nthe mask generated by fuzzy K-Means [37]. Finally, the texture features\nof the target region are obtained. These texture features can enhance\nthe activation of edge regions in the class feature mapping. As shown\nin Fig. 2.\nTexture in images is manifested as irregular local regions. It not\nonly reflects the grayscale statistical information of the image but\nalso exhibits spatial distribution and structural characteristics. GLCM\nconsiders spatial relationships between voxels and can reflect subtle\nstructural changes in brain tissue [38]. Therefore, GLCM can be utilized\nto extract texture features to compensate for the missing edge texture\nfeatures in the pseudo-labels generated by CAMs.\nIn this study, to optimize the selection of texture features, consid-\neration is given to the five most widely used texture features in brain\nimage analysis [39]: Mean, Standard Deviation (Std), Contrast (CON),\nDissimilarity (DIS) and Entropy (Ent). The best-performing texture\nfeatures are selected through comparative experiments to be used as\ninputs for the fuzzy clustering algorithm.\nNext, the normalized ğ‘€\nâ€²\nğ‘“ ğ‘¢ğ‘ ğ‘–ğ‘œğ‘› \nis used as prior knowledge to deter-\nmine the membership degrees ğœ‡\nğ‘–ğ‘— \n(0) of the texture feature matrix ğ‘‡\nto each mode class, establishing the initial membership matrix ğ‘ˆ (0) =\nBiomedical Signal Processing and Control 113 (2026) 108948\n4\n\nG. Wang et al.\n[ğœ‡\nğ‘–ğ‘— \n(0)]. The clustering centers ğ‘\nğ‘–\n(ğ¾) for each class are then computed\nbased on the membership matrix, as shown in Eq. (12).\nğ‘\nğ‘–\n(ğ¾) =\nâˆ‘\nğ‘\nğ‘—=1\n[ğœ‡\nğ‘–ğ‘—(ğ¾)\n]\nğ‘š\nğ‘‡\nğ‘—\nâˆ‘\nğ‘\nğ‘—=1\n[ğœ‡\nğ‘–ğ‘—(ğ¾)\n]\nğ‘š \n, ğ‘– = 1, 2, â€¦ , ğ¶ (12)\nwhere, ğ¾ denotes the number of iterations, ğ¶ represents the number of\ncategories, ğ‘‡\nğ‘— \ndenotes the ğ‘—ğ‘¡â„ element in the texture feature matrix, ğ‘\nrepresents the number of samples, and the hyperparameter ğ‘š(ğ‘š â‰¥ 2) is\na constant controlling the degree of fuzziness in the clustering results.\nThe value of ğ‘š is discussed in the Section 4.\nTo compute the new membership matrix ğ‘ˆ (ğ¾ + 1), the method for\ncalculating the matrix elements is as follows:\nğ‘ˆ (ğ¾ + 1) = \n1\nâˆ‘\nğ¶\nğ‘=1\n( \nğ·\nğ·\nâ€² \n) \n2\nğ‘šâˆ’1\n(13)\nwhere, ğ· is the set of ğ‘‘\nğ‘–ğ‘— \n, ğ‘‘\nğ‘–ğ‘— \nis the distance from the ğ‘—ğ‘¡â„ element in the\ntexture feature matrix to the ğ‘–ğ‘¡â„ cluster center ğ‘\nğ‘–\n(ğ¾) upon completion\nof the ğ¾ğ‘¡â„ iteration. ğ‘ âˆˆ [1, ğ¶] represents the ğ‘ğ‘¡â„ class. ğ·\nâ€² \nis the\nset of ğ‘‘\nğ‘ğ‘— \n. ğ‘‘\nğ‘ğ‘— \nis the distance from the ğ‘—ğ‘¡â„ element in the texture\nfeature matrix to the ğ‘ğ‘¡â„ cluster center ğ‘\nğ‘\n(ğ¾) upon completion of the\nğ¾ğ‘¡â„ iteration. The division operation between the sets ğ· and ğ·\nâ€² \nis\nconducted element-wise, that is, each corresponding pair of elements\nis divided individually.\nThe convergence condition is defined as shown in Eq. (14). If the\nconvergence condition is not satisfied, return to Eq. (12).\nmax\nğ‘–ğ‘— \n[ğœ‡\nğ‘–ğ‘— \n(ğ¾ + 1) âˆ’ ğœ‡\nğ‘–ğ‘— \n(ğ¾)] â‰¤ ğœ€ (14)\nwhere, ğœ€ is the boundary value for the convergence of fuzzy clustering.\nWhen the convergence condition is satisfied, the membership matrix\nğœ‡\nğ‘–ğ‘— \n(ğ¾ + 1) âˆˆ ğ‘ˆ (ğ¾ + 1) is partitioned according to the principle of\nmembership, as follows:\nğœ‡\nğ‘–ğ‘— \n(ğ¾ + 1) = max\n1â‰¤ğ‘â‰¤ğ¶ \nğœ‡\nğ‘ğ‘— \n(ğ¾ + 1), ğ‘— = 1, 2, â€¦ , ğ‘ (15)\ntherefore\nğ‘‡\nğ‘— \nâˆˆ ğ¶\nğ‘ \n(16)\nwhere, ğ¶\nğ‘ \ndenotes the ğ‘ğ‘¡â„ class in the set ğ¶. According to the principle\nof maximum membership, the sample ğ‘‡\nğ‘— \nis assigned to class ğ¶\nğ‘\n.\nFor the obtained texture features of the target region in this article,\nthey are encoded as ground truth of texture consistency loss to guide\nthe learning of the cross-guided module.\nğ‘Œ =\n{\n1, ğµ âŠ— ğ‘Š > ğ›¿\n0, ğµ âŠ— ğ‘Š â‰¤ ğ›¿ \n(17)\nwhere, ğ‘Œ represents the ground truth used for texture consistency\nloss, ğµ represents the texture image generated by Prewitt, and ğ‘Š\nrepresents the mask generated by Eq. (16). âŠ— signifies element-wise\nmultiplication. The value of ğ›¿ is 0.5.\n3.3. Cross-guided module\nAlthough the position of the target region can be accurately located\nby ğ‘€\nâ€²\nğ‘“ ğ‘¢ğ‘ ğ‘–ğ‘œğ‘›\n, the shape of the target may not be represented precisely.\nThis is because during the fusion of ğ‘€\nâ€²\nğ‘ â„ğ‘ğ‘™ğ‘™ğ‘œğ‘¤ \nand ğ‘€\nâ€²\nğ‘‘ğ‘’ğ‘’ğ‘\n, the guidance\nof shallow features through element-wise multiplication is relatively\nrough. Segmentation errors will be introduced due to unnecessary\ndetails if the shallow CAM contains too much noise. Therefore, it\nis necessary to provide targeted guidance information to refine the\ndetails of useful shallow features to prevent the generation of incorrect\nsegmentation regions. Considering the correlation between semantic in-\nformation and texture information (semantic information represents the\nhigh-dimensional feature of the target region, while texture information\nprovides much detailed representation of semantic information) [20].\nTherefore, the cross-guided module is designed from the perspective of\nFig. 3. Cross-guided module.\nthe cross-coordination between semantic information and texture infor-\nmation to remove irrelevant features in the shallow CAM. Meanwhile,\ndetailed features in ğ‘€\nâ€²\nğ‘“ ğ‘¢ğ‘ ğ‘–ğ‘œğ‘› \nare compensated and the deep fusion of\nğ‘€\nâ€²\nğ‘“ ğ‘¢ğ‘ ğ‘–ğ‘œğ‘› \nand ğ‘€\nâ€²\nğ‘ â„ğ‘ğ‘™ğ‘™ğ‘œğ‘¤ \nis guided by this module, as shown in Fig. 3.\nFirst, ğ‘“\nğ‘ â„ğ‘ğ‘™ğ‘™ğ‘œğ‘¤ \nand ğ‘“\nğ‘“ ğ‘¢ğ‘ ğ‘–ğ‘œğ‘› \nare generated by extracting feature in-\nformation of ğ‘€\nâ€²\nğ‘“ ğ‘¢ğ‘ ğ‘–ğ‘œğ‘› \nand ğ‘€\nâ€²\nğ‘ â„ğ‘ğ‘™ğ‘™ğ‘œğ‘¤ \nat different scales using dilated\nconvolutions with different dilation rates, batch normalization and\nReLU, composed of ğ·ğ¶ğµğ‘…(â‹…). Next, ğ‘€\nâ€²\nğ‘“ ğ‘¢ğ‘ ğ‘–ğ‘œğ‘› \nand ğ‘€\nâ€²\nğ‘ â„ğ‘ğ‘™ğ‘™ğ‘œğ‘¤ \nare concate-\nnated and then fed into layer to enrich feature information of target\nregion, as shown in Eq. (18) to (20):\nğ‘“\nğ‘ â„ğ‘ğ‘™ğ‘™ğ‘œğ‘¤ \n= ğ·ğ¶ğµğ‘…(ğ‘€\nâ€²\nğ‘ â„ğ‘ğ‘™ğ‘™ğ‘œğ‘¤\n) (18)\nğ‘“\nğ‘“ ğ‘¢ğ‘ ğ‘–ğ‘œğ‘› \n= ğ·ğ¶ğµğ‘…(ğ‘€\nâ€²\nğ‘“ ğ‘¢ğ‘ ğ‘–ğ‘œğ‘›\n) (19)\nğ» = ğ·ğ¶ğµğ‘…(ğ¶(ğ‘€\nâ€²\nğ‘ â„ğ‘ğ‘™ğ‘™ğ‘œğ‘¤\n, ğ‘€\nâ€²\nğ‘“ ğ‘¢ğ‘ ğ‘–ğ‘œğ‘›\n)) (20)\nSubsequently, ğ‘€\nâ€²\nğ‘ â„ğ‘ğ‘™ğ‘™ğ‘œğ‘¤ \nand ğ‘€\nâ€²\nğ‘“ ğ‘¢ğ‘ ğ‘–ğ‘œğ‘› \nare activated by ğ·ğ¶ğµğ‘…(â‹…) and\nvector ğ» in turn to obtain ğ¹\nğ‘ â„ğ‘ğ‘™ğ‘™ğ‘œğ‘¤ \nand ğ¹\nğ‘“ ğ‘¢ğ‘ ğ‘–ğ‘œğ‘›\n. Then, the vectors\nğ¹\nğ‘ â„ğ‘ğ‘™ğ‘™ğ‘œğ‘¤\n, ğ», ğ¹\nğ‘“ ğ‘¢ğ‘ ğ‘–ğ‘œğ‘› \nrich in target region features are concatenated.\nFinally, the feature fusion is performed by the ğ¶ğµğ‘…(â‹…) composed of con-\nvolution, batch normalization and ReLU, resulting in the final segmen-\ntation result represented as vector ğ¹ . This process can be represented\nby Eqs. (21) to (23).\nğ¹\nğ‘ â„ğ‘ğ‘™ğ‘™ğ‘œğ‘¤ \n= ğ·ğ¶ğµğ‘…(ğ‘€\nâ€²\nğ‘ â„ğ‘ğ‘™ğ‘™ğ‘œğ‘¤\n) âŠ— ğœ(ğ») (21)\nğ¹\nğ‘“ ğ‘¢ğ‘ ğ‘–ğ‘œğ‘› \n= ğ·ğ¶ğµğ‘…(ğ‘€\nâ€²\nğ‘“ ğ‘¢ğ‘ ğ‘–ğ‘œğ‘›\n) âŠ— ğœ(ğ») (22)\nğ¹ = ğ¶ğµğ‘…(ğ¶(ğ¹\nğ‘ â„ğ‘ğ‘™ğ‘™ğ‘œğ‘¤\n, ğ», ğ¹\nğ‘“ ğ‘¢ğ‘ ğ‘–ğ‘œğ‘›\n)) (23)\nwhere, ğ¶(â‹…) is represented as concatenation operation; âŠ— is represented\nas element-wise multiplication; ğœ(â‹…) is represented as ğ‘†ğ‘–ğ‘”ğ‘šğ‘œğ‘–ğ‘‘.\n3.4. Loss function\nDue to the limited information provided by labels, fully supervised\nsegmentation effects cannot be achieved by segmentation networks.\nTherefore, some form of equivariance constraint [40â€“43] is sought\nby researchers to approximate fully supervised segmentation results.\nBiomedical Signal Processing and Control 113 (2026) 108948\n5\n\nG. Wang et al.\nFig. 4. Segmentation loss function.\nTherefore, the loss functions in the weakly supervised segmentation\nnetwork model are redesigned in this paper. Among them, the classifi-\ncation loss function trains the backbone network to obtain CAMs, the\nsemantic consistency loss is used to ensure semantic consistency be-\ntween the predicted results and Step-CAM, and the texture consistency\nloss is used to activate the edge regions of target objects. As shown\nin Fig. 4. During the network training process, we conduct training in\ntwo steps. Firstly, medical image classification training is performed to\nobtain class activation map of the target region. Then, medical image\nsegmentation training is carried out to iteratively optimize the CAMs to\ngenerate the final mask. The loss functions used in these two training\nprocesses will be introduced separately.\n3.4.1. Classification loss function\nDuring training, image-level labels are the only manually annotated\nsupervision data available for use in the network. A fully connected\nlayer is employed at the end of the network for image classification, and\nbinary cross-entropy loss is utilized for network training. The specific\ncalculation is as follows:\nğ¿\nğ‘ğ‘™ğ‘ \n(ğ‘¦,Ì‚ğ‘¦ ) = âˆ’(ğ‘¦ğ‘™ğ‘œğ‘”(Ì‚ğ‘¦ ) + (1 âˆ’ ğ‘¦)ğ‘™ğ‘œğ‘”(1 âˆ’Ì‚ğ‘¦ )) (24)\nwhere, ğ‘¦ represents the ground truth of the image, and Ì‚ğ‘¦ represents the\npredicted labels.\n3.4.2. Segmentation loss function\nTexture consistency loss: To alleviate the issue of inaccurate seg-\nmentation caused by blurred target edges, local texture consistency loss\nis introduced in this paper. This loss mainly promotes activation in edge\nregions, improving the segmentation performance of class activation\nmapping.\nğ¿\nğ‘¡ğ‘’ğ‘¥ğ‘¡ğ‘¢ğ‘Ÿğ‘’ \n= âˆ’ \n1\nğ‘\nğ‘\nâˆ‘\nğ‘›=1\nğ¶\nâˆ‘\nğ‘=0\nğ‘¦\nğ‘›ğ‘ \nlog(Ì‚ğ‘¦ \nğ‘›ğ‘ \n) (25)\nwhere, ğ‘¦\nğ‘›ğ‘ \nâˆˆ [0, 1] denotes the one-hot encoding when voxel ğ‘› is of class\nğ‘. Ì‚ ğ‘¦ \nğ‘›ğ‘ \nâˆˆ [0, 1] is the probability that voxel ğ‘› is the predicted value of\nthe segmentation network output. ğ‘ is the total number of voxels, ğ¶\nis the number of categories, including the background.\nSemantic consistency loss: The accuracy of segmentation targets is\nensured by introducing texture consistency loss in this paper. However,\nusing only texture consistency loss may generate edges of non-target\nregions, so this requires semantic consistency constraints. In this paper,\nL1 loss is added between two activation mappings to ensure semantic\nconsistency.\nğ¿\n1 \n= â€–ğ‘›ğ‘œğ‘Ÿğ‘š(ğ¹ ) âˆ’ ğ‘€\nâ€²\nğ‘“ ğ‘¢ğ‘ ğ‘–ğ‘œğ‘›\nâ€–\n1 \n(26)\nwhere, ğ‘›ğ‘œğ‘Ÿğ‘š(â‹…) is normalized, ğ¹ is the prediction result of TKG-Net, and\nğ‘€\nâ€²\nğ‘“ ğ‘¢ğ‘ ğ‘–ğ‘œğ‘› \nis the output result of Step-CAM.\nIn summary, the segmentation loss function of the network can be\nrepresented as:\nğ¿\nğ‘ ğ‘’ğ‘” \n= ğ¿\n1 \n+ ğ¿\nğ‘¡ğ‘’ğ‘¥ğ‘¡ğ‘¢ğ‘Ÿğ‘’ \n(27)\n4. Experiments and results\n4.1. Dataset and metrics\nBraTS2019 [44â€“46] is a MRI dataset used for segmenting brain\ntumors. The training set comprises 259 HGG cases, 76 LGG cases,\nand their respective ground truth labels. Each case consists of four\nmodalities including T1, T1ce, T2, and FLAIR. The size of these datasets\nis 155 Ã— 240 Ã— 240. In this study, T1ce, T2, and FLAIR are used\nas model inputs because T1 contains less tumor region information.\nThe work of this paper involves accomplishing a binary segmentation\ntask, distinguishing between healthy and non-healthy classes. Hence,\ndifferent tumor regions are merged into a common non-healthy class.\nTo minimize the impact of background regions on the results, the\nspatial dimensions of all MRI axial slices are set to 160 Ã— 160.\nWe randomly select 80% of the data from the dataset for model\ntraining, and the remaining 20% of the data is used to select the best\ntraining model. Subsequently, we will use the segmentation results\nreturned by the online verification platform to verify the proposed\nmethod.\nINSTANE2022 [47,48] is a dataset of CT images used for segment-\ning intracranial hemorrhage (ICH). The dataset comprises non-contrast\nhead CT data from 200 clinically diagnosed patients with various\ntypes of ICH. The official training set includes CT data and ICH an-\nnotation data from 100 cases. The official validation and test sets\nare respectively comprised of data from 30 and 70 cases, with their\nlabels undisclosed. The size of the CT volumes in these datasets is\nN Ã— 512 Ã— 512, where N ranges from 20 to 70, with a pixel spacing of\n0.42 mmÃ— 0.42 mm Ã— 5 mm. Voxel-level segmentation labels are pro-\nvided (0: background, 1: ICH). To alleviate the impact of background\nregions on the results, the spatial dimensions of all CT axial slices are\nset to 360 Ã— 360. Due to the limited quantity of the dataset, 5-fold\ncross-validation is executed, and the average results from five runs are\nreported.\nEvaluation metrics: Combined with the actual clinical application,\nwe will use the Dice Similarity Coefficient (DSC) and the Hausdorff\nDistance (HD) as evaluation metrics.\nğ·ğ‘†ğ¶ = \n2ğ‘‡ ğ‘ƒ\n2ğ‘‡ ğ‘ƒ + ğ¹ ğ‘ƒ + ğ¹ ğ‘ \n(28)\nğ»ğ·(ğ‘‹, ğ‘Œ ) = ğ‘šğ‘ğ‘¥[ğ‘ ğ‘¢ğ‘\nğ‘¥âˆˆğ‘‹ \nğ‘–ğ‘›ğ‘“\nğ‘¦âˆˆğ‘Œ \nğ‘‘(ğ‘¥, ğ‘¦), ğ‘ ğ‘¢ğ‘\nğ‘¦âˆˆğ‘Œ \nğ‘–ğ‘›ğ‘“\nğ‘¥âˆˆğ‘‹ \nğ‘‘(ğ‘¥, ğ‘¦)] (29)\nwhere, ğ‘‡ ğ‘ƒ represents the modelâ€™s correct predictions of positive class\nresults. ğ¹ ğ‘ƒ signifies the model incorrectly predicting negative class as\npositive. ğ¹ ğ‘ denotes the modelâ€™s incorrect predictions of positive class\nas negative. ğ‘‹ and ğ‘Œ are two genuine subsets of metric space ğ‘€, ğ‘ ğ‘¢ğ‘\nstands for the upper bound and ğ‘–ğ‘›ğ‘“ stands for the lower bound. ğ‘‘(ğ‘¥, ğ‘¦)\ndenotes the Euclidean distance between ğ‘¥ and ğ‘¦.\nBiomedical Signal Processing and Control 113 (2026) 108948\n6\n\nG. Wang et al.\nTable 1\nComparison between the TKG-Net and other methods on\nthe BraTS2019 online validation dataset (The larger the\nDSC, the better; The smaller the HD, the better).\nMethod DSC HD\nLayerCAM [36] 0.691 26.71\nSSE-WSSN [10] 0.724 23.84\nCMER [22] 0.741 18.68\nC-CAM [25] 0.739 19.35\nHAMIL [18] 0.731 20.59\nHSL-CH [16] 0.728 21.74\nOurs 0.754 15.65\nTable 2\nComparison between the TKG-Net and other methods on the INSTANCE2022\ndataset (The larger the DSC, the better; The smaller the HD, the better).\nMethod DSC p-value HD p-value\nLayerCAM [36] 0.784 Â± 0.019 5.61 Ã— 10\nâˆ’4 \n8.96 Â± 1.32 4.86 Ã— 10\nâˆ’5\nSSE-WSSN [10] 0.789 Â± 0.023 5.83 Ã— 10\nâˆ’5 \n5.45 Â± 0.73 2.41 Ã— 10\nâˆ’3\nCMER [22] 0.793 Â± 0.015 7.23 Ã— 10\nâˆ’3 \n3.22 Â± 0.54 8.28 Ã— 10\nâˆ’3\nC-CAM [25] 0.817 Â± 0.011 4.82 Ã— 10\nâˆ’2 \n1.78 Â± 0.51 7.12 Ã— 10\nâˆ’2\nHAMIL [18] 0.814 Â± 0.016 3.28 Ã— 10\nâˆ’2 \n2.11 Â± 0.36 4.51 Ã— 10\nâˆ’2\nHSL-CH [16] 0.802 Â± 0.025 9.07 Ã— 10\nâˆ’4 \n4.38 Â± 0.68 9.54 Ã— 10\nâˆ’3\nOurs 0.829 Â± 0.009 âˆ’âˆ’ 1.21 Â± 0.23 âˆ’âˆ’\n4.2. Experimental details\nAll our experiments are conducted using medical image data pro-\nvided by the BraTS 2019 and INSTANCE2022 datasets. The exper-\nimental platform for this paper is Ubuntu, the graphics card is an\nNvidia RTX3090Ti with 24G of RAM. The models in this paper are\nimplemented using the PyTorch framework [49].\nBefore training, BraTS2019 and INSTANCE2022 are respectively\npreprocessed in this paper. For the BraTS2019 dataset, firstly, we per-\nform cropping operations on Flair, T1ce, and T2 images individually,\nresizing their dimensions to 155 Ã— 160 Ã— 160. Secondly, we conduct Z-\nscore normalization, after which the mean value of standardized data\nis 0 and the variance is 1. Next, we perform a slicing operation with\nchannel length 1 along the normal direction of the Axial plane. Then,\nwe obtain Flair, T1ce, and T2 images with dimensions of 160 Ã— 160\nrespectively. Finally, we concatenate these three images to obtain an\ninput vector of size 3 Ã— 160 Ã— 160. For the INSTANCE2022 dataset,\nfirstly, we conduct cropping operations to resize the image dimensions\nto N Ã— 360 Ã— 360. Secondly, we apply a filtering operation with a\nwindow level of 50 and a window width of 100. Then, we perform\nstandardization with a mean of 0 and a variance of 1. After that, we\nconduct slicing along the normal direction of the Axial plane with a\nchannel length of 1. Finally, we obtain an input vector with dimensions\nof 1 Ã— 360 Ã— 360.\nFirstly, ResNet50 [50] is utilized as the backbone network for\ntraining in a classification task to generate initial CAM. Secondly, the\nalready trained ResNet50 is frozen, and training is conducted on the\ncross-guided module. Lastly, to obtain the final segmentation mask, the\nprediction results of all methods are binarized with a threshold of 0.5.\nFor all the aforementioned network models, the parameters are kept\nconsistent. The batch size is set to 32, the number of epochs is 50, and\nAdam is used as the optimization method. The initial learning rate ğ‘™ğ‘Ÿ\n0 \nis\n1.0Ã—10\nâˆ’4\n, the learning rate changed as the number of training iterations\nincreased, as shown in Eq. (25) :\nğ‘™ğ‘Ÿ = ğ‘™ğ‘Ÿ\n0 \nÃ— (1 âˆ’ \nğ‘’ğ‘ğ‘œğ‘â„\n100 \n)\n0.9 \n(30)\n4.3. Comparative experiment\n4.3.1. Comparing with state-of-the-art methods\nIn this section, the proposed TKG-Net is compared with other state-\nof-the-art segmentation methods based on image labels, including: Lay-\nerCAM [36], SSE-WSSN [10], CMER [22], Câ€“CAM [25], HAMIL [18]\nTable 3\nComparison of Step-CAM with other classical CAM methods.\nMethod BraTS2019 INSTANCE2022\nDSC HD DSC HD\nCAM 0.711 7.25 0.704 10.88\nGradCAM 0.735 3.04 0.757 2.70\nLayerCAM 0.747 5.95 0.784 8.96\nStep-CAM 0.758 2.83 0.787 1.41\nTable 4\nAblation experiments for each part of TKG-Net (The data is the average after\n5-fold cross-validation, Baseline: CAM of layer3_2 in ResNet50, TE: Texture\nknowledge extraction module, CG: cross-guided module).\nModel BraTS2019 INSTANCE2022\nBaseline ğ‘€\nâ€²\nğ‘“ ğ‘¢ğ‘ ğ‘–ğ‘œğ‘› \nTE CG DSC HD DSC HD\nâˆš \n0.735 3.04 0.757 2.70\nâˆš âˆš \n0.758 2.83 0.787 1.41\nâˆš âˆš âˆš \n0.797 2.75 0.806 1.34\nâˆš âˆš âˆš âˆš \n0.812 2.63 0.829 1.21\nand HSL-CH [16]. The TKG-Net method proposed in this paper achieves\nthe best performance in terms of DSC and HD on the BraTS2019 and\nINSTANCE2022 datasets, as shown in Tables 1 and 2. In Table 1,\nthe TKG-Net proposed in this paper achieves a DSC of 0.754 and an\nHD of 15.6 mm. Compared to the advanced method CMER [22], it\nshows an improvement of 1.3% in DSC and a decrease of 3.03 mm\nin HD. In Table 2, the TKG-Net proposed in this paper obtains a DSC\nof 0.829 and an HD of 1.21 mm. Compared to the advanced method\nCâ€“CAM [25], it demonstrates an increase of 1.2% in DSC and a decrease\nof 0.57 mm in HD. By comparing the HD metric, it can be observed\nthat the edges generated by the pseudo-labels of our method are closer\nto the edges of the ground truth labels compared to other methods.\nThis further indicates that utilizing texture knowledge to guide medical\nimage segmentation in this paper is effective.\nDue to the lack of publicly available data labels in the validation set\nof the BraTS2019 dataset, the DSC and HD values for each 2D predicted\nslice cannot be obtained, making it impossible to conduct t-tests on this\ndataset. Therefore, t-tests are only conducted on the INSTANCE2022\ndataset. Our method demonstrates p-values less than 0.05 compared\nto LayerCAM [36], SSE-WSSN [10], CMER [22], HAMIL [18] and\nHSL-CH [16], indicating a significant improvement in segmentation ac-\ncuracy with TKG-Net on this dataset. Although TKG-Net failed to meet\nthe standard of significant difference when compared with Câ€“CAM [25]\nin terms of HD, the ğ‘-value of Câ€“CAM was lower than 0.05 in DSC,\nshowing a significant difference in segmentation between TKG-Net and\nCâ€“CAM.\nTo better illustrate the advantages of our method, Figs. 5 and 6\ndepict some qualitative results of the current state-of-the-art methods\nbased on weakly supervised learning. For images with target regions\nof complex shapes, the TKG-Net proposed in this paper is closer to\nthe ground truth. Additionally, for target regions of varying sizes, the\nmethod proposed in this paper can extract texture features at the edges,\navoiding issues of under-activation or over-activation.\n4.3.2. The comparison of step-CAM\nFurthermore, to validate the contribution of Step-CAM to perceptual\nsystems, comparisons are made with classic CAM, GradCAM, and Layer-\nCAM, as shown in Table 3. Step-CAM exhibits higher DSC and lower\nHD compared to CAM, GradCAM, and Layer-CAM. This indicates that\nthe translation transformation of the CAM in the deep layer can reduce\nthe interference of inconsistent position. Secondly, by using the deep\nCAM as the weight, the noise can be removed specifically, so that the\nbackground region can be suppressed and the foreground region can be\neffectively activated.\nBiomedical Signal Processing and Control 113 (2026) 108948\n7\n\nG. Wang et al.\nFig. 5. Segmentation results of the TKG-Net and other methods on the BraTS2019 dataset.\nFig. 6. Segmentation results of the TKG-Net and other methods on the INSTANCE2022 dataset.\n4.4. Ablation analysis\n4.4.1. Effectiveness of TKG-Net\nFrom the experimental results in Table 4, it can be observed that\nwhen the multi-layer activation maps are fused, the segmentation\nperformance of the model is improved on the BraTS2019 and IN-\nSTANCE2022 datasets. Specifically, in the BraTS2019 dataset, the DSC\nis increased by 2.3%, and the HD is decreased by 0.21 mm. In the\nINSTANCE2022 dataset, the DSC is increased by 3%, and the HD is\ndecreased by 1.29 mm. It shows the benefits of refining the activation\nrange of ğ‘€\nâ€²\nğ‘‘ğ‘’ğ‘’ğ‘ \nafter fusing the shallow CAM.\nWith the addition of the texture knowledge extraction module, the\nsegmentation performance of the model further improves on these two\ndatasets. Specifically, in the BraTS2019 dataset, the DSC is increased\nby 3.9%, and the HD is decreased by 0.08 mm. In the INSTANCE2022\ndataset, the DSC is increased by 1.9%, and the HD is decreased by\n0.07 mm. It can be seen that the texture knowledge extraction module\ncaptures the texture of the target edges by clustering texture features\nwithin the target region.\nAfter introducing the cross-guided module, there is still a certain im-\nprovement in the segmentation performance of the model on these two\ndatasets. Specifically, in the BraTS2019 dataset, the DSC is increased\nby 1.5%, and the HD is decreased by 0.12 mm. In the INSTANCE2022\ndataset, the DSC is increased by 2.3%, and the HD is decreased by\n0.13 mm. It can be seen that the edge texture information is effectively\nTable 5\nAblation experiments with different ğ‘  (L1-L5 represents layer1_1, layer2_3,\nlayer3_2, layer3_4, layer4_2 in ResNet50).\nSetting Shallow layer\n[1, ğ‘ ]\nDeep layer\n[ğ‘  + 1, 5]\nBraTS2019 INSTANCE2022\nDSC HD DSC HD\nğ‘  = 1 L1 L2,L3,L4,L5 0.730 2.86 0.780 1.47\nğ‘  = 2 L1,L2 L3,L4,L5 0.758 2.83 0.785 1.44\nğ‘  = 3 L1,L2,L3 L4,L5 0.727 2.93 0.787 1.41\nğ‘  = 4 L1,L2,L3,L4 L5 0.679 3.22 0.763 1.52\nsupplemented by the cross-guided module, addressing the issue of in-\nsufficient edge feature extraction and enhancing the effective activation\nof the edge region. Additionally, qualitative results indicate that the\nmethod proposed in this paper can improve segmentation performance\nand achieve satisfactory experimental results, which is consistent with\nthe quantitative comparisons in Table 4, as shown in Fig. 7.\n4.4.2. Selection of hyperparameters\nIn the TKG-Net method, we use several hyperparameters to enhance\nthe modelâ€™s performance. To study their effectiveness, ablation studies\nare conducted on them in this paper.\nAnalysis of ğ‘  in Step-CAM: A series of comparative experiments\nare conducted to find the most suitable value of ğ‘ , as shown in Table 5.\nFrom the table, it can be observed that the value of ğ‘  varies for different\nBiomedical Signal Processing and Control 113 (2026) 108948\n8\n\nG. Wang et al.\nFig. 7. Visualization of ablation experiment of weakly supervised medical im-\nage segmentation framework guided by texture knowledge. (a) original image,\n(b) baseline model CAM, (c) ğ‘€\nâ€²\nğ‘“ ğ‘¢ğ‘ ğ‘–ğ‘œğ‘›\n, (d) the texture knowledge extraction\nmodule, (e) the cross-guided module, (f) ground truth.\nFig. 8. Comparison of CAM at different stages. (a) original image, (b) L1,\n(c) L2, (d) L3, (e) L4, (f) L5, (g) ğ‘€\nâ€²\nğ‘“ ğ‘¢ğ‘ ğ‘–ğ‘œğ‘›\n, (h) Ground Truth (L1-L5 represents\nlayer1_1, layer2_3, layer3_2, layer3_4, layer4_2 in ResNet50).\ndatasets. Specifically, for the BraTS2019 dataset, the optimal segmenta-\ntion performance is achieved when ğ‘  = 2; while for the INSTANCE2022\ndataset, the optimal segmentation performance is achieved when ğ‘  = 3.\nAnalysis of ğ‘š in K-Means: Fig. 9 illustrates the DSC under different\nvalues of ğ‘š on the BraTS2019 and INSTANCE2022 datasets. It can be\nobserved that the detection and segmentation performance are optimal\nwhen ğ‘š = 4. Therefore, in the proposed method in this paper, the ğ‘š is\nset to 4.\nAnalysis of the iteration number ğ¾ in K-Means: During the actual\ntraining process, to improve training efficiency, the influence of the\nhyperparameter ğ¾ on the performance of the model in this paper is\nvalidated. Comparative experiments are conducted within the range of\n[1, 6] with a step size of 1, as illustrated in Fig. 10. For the Brats2019\nFig. 9. Variation trend of segmentation performance for different values of ğ‘š.\nFig. 10. Variation trend of segmentation performance under different number\nof iterations ğ¾.\nFig. 11. Different segmentation performance contrast between GLCM feature.\ndataset, the DSC value remains relatively stable as the value of ğ¾\nincreases. For the INSTANCE2022 dataset, the DSC value also increases\nwith the increase in ğ¾, but after ğ¾ = 4, the DSC value no longer\nBiomedical Signal Processing and Control 113 (2026) 108948\n9\n\nG. Wang et al.\nFig. 12. Different segmentation performance contrast between GLCM feature.\nTable 6\nInference time and number of parameters for each slice.\nMethod Backbone Params (M) BraTS2019 INSTANCE2022\nInference time(s) Inference time(s)\nCMER [22] ResNet50 94.08 0.263 0.439\nC-CAM [25] ResNet50 287.55 0.311 0.492\nHAMIL [18] ResNet50 25.51 0.295 0.468\nWeakMedSAM [51] UNet 31.04 0.311 0.473\nAC-CAM [52] Vision Transformers 86.57 0.425 0.570\nTKG-Net ResNet50 23.57 0.270 0.451\nTable 7\nComparison of texture knowledge extraction methods.\nMethod DSC FLOPs\nGMM 0.784 2.5 Ã— 10\n7\nSpectral Clustering 0.793 2.0 Ã— 10\n10\nFCM 0.781 2.0 Ã— 10\n6\nchanges. Therefore, the model achieves optimal performance on both\ndatasets when ğ¾ = 5. Consequently, we set the value of the ğ¾ to 5.\nTexture feature selection analysis: To verify the impact of texture\nfeature selection on the performance of the proposed model in this\npaper, we conduct comparative experiments using five sets of texture\nfeatures that perform well in medical image segmentation. From Fig.\n11, we can observe that the DSC score of the mean feature is the\nhighest and exceeds the segmentation results on the original grayscale\nimages. The performance of the other texture features is inferior and\ndoes not surpass the segmentation results on the original grayscale\nimages. Therefore, only the mean feature is selected as the input for\nfuzzy K-Means in this paper.\n4.4.3. Selection of edge detection operators\nEdge operator selection experiments are conducted in this paper.\nFrom Fig. 12, it can be observed that the Robert operator and Laplacian\noperator both failed to successfully extract the edges of lesion areas.\nThe Canny operator introduces some false edges. Compared to the Pre-\nwitt operator, the Sobel operator significantly enhances the brightness\nof the entire region. In our actual experiments, the segmentation results\nof Sobel and Prewitt are consistent. The Prewitt operator is chosen for\nexperimentation.\n5. Discussion\nIn this paper, a texture knowledge-guided weakly supervised brain\nmedical image segmentation method is proposed, aiming to achieve\nthe desired medical image segmentation performance while reducing\nannotation costs. Meanwhile, experimental results indicate that the\nproposed method is feasible for weakly supervised segmentation of\nbrain medical images using image-level labels.\nMeanwhile, Table 6 demonstrates the performance of our method in\nterms of parameter count and inference time. Compared to the meth-\nods [18,22,25,51,52] with better segmentation results, our method\nachieves optimal performance in terms of parameter count. This\nachievement is mainly attributed to our model architecture design.\nParticularly noteworthy is that the texture knowledge extraction mod-\nule and Step-CAM structure do not introduce additional parameters,\nsignificantly reducing the overall model parameter size. Furthermore,\ndilated convolutions with different dilation rates are employed in\nthe cross-guided module, a strategy that enhances the modelâ€™s ability\nto capture multi-scale information without increasing the number of\nparameters. In terms of inference time, our method performs well,\ndemonstrating performance similar to the state-of-the-art method [22].\nIn order to extract texture knowledge efficiently, we conducted a\ncomparative experiment on the BraTS2019 dataset. It can be seen from\nTable 7 that Spectral Clustering performs slightly better than other\nmethods with a DSC of 0.793, but its FLOPs is significantly higher\nthan that of other methods. It is worth noting that although the DSC\nof FCM and Gaussian Mixture Model (GMM) differs by only 0.003, its\ncomputational efficiency is the best among the three. Therefore, we\nselect FCM for extracting texture knowledge.\nTo discuss the application of this method in multi-class segmenta-\ntion, new experiments are conducted on the BraTS2019 dataset. The\nmulti-class labels in the BraTS2019 dataset are ET, ED, and NET. Sub-\nsequently, classification experiments are performed to generate class\nactivation maps with object localization capabilities, as shown in Fig.\n13. It can be observed that the class activation maps for ET, ED, and\nNET are not activated at their respective locations. This phenomenon\nis mainly due to two reasons: firstly, the overlapping and intertwining\nnature of ET, ED, and NET leads to the neural networkâ€™s inability\nto fully capture the features of each label. Secondly, there is a lack\nof clear boundaries between the class activation maps of different\nlabels, making subsequent optimization of the class activation maps\ndifficult. In the future, we will train using a small number of scribble-\nlevel labels and combine them with multi-instance learning methods\nto locate features for each class, so that it can segment multi-class\nmedical images, which can benefit the proposed method. Additionally,\nthis method has some limitations when applied to medical images\nwith less texture-rich lesion regions. In the near future, this issue will\nBiomedical Signal Processing and Control 113 (2026) 108948\n10\n\nG. Wang et al.\nFig. 13. Class activation maps generated by labels of different classes. (a) Flair, (b) T1ce, (c) T2, (d) ET, (e) NET, (f) ED, (g) Ground Truth.\nbe thoroughly addressed to ensure compatibility with medical image\ndatasets that lack texture richness.\n6. Conclusion\nThe weakly supervised brain image segmentation guided by texture\nknowledge studied in this paper provides an innovative approach to\naddressing the issues of incomplete and inaccurate labels in brain med-\nical image segmentation. In this method, a stepped fusion algorithm\nfor CAM is designed, which achieves precise localization of the target\nregion by fully exploiting the characteristics of class activation maps at\ndifferent layers. The texture knowledge extraction module is introduced\nin this paper, which adds texture structural features to generate more\nprecise segmentation results. Then, the cross-guided module combines\nsemantic consistency loss and texture consistency loss to better fuse\nthe preliminary merged class activation maps with shallow CAM for\nachieving more accurate segmentation. Additionally, it is believed that\nthis work may increase peopleâ€™s interest in optimizing segmentation\ntargets using texture knowledge. Finally, potential limitations of the\nproposed method have been identified, which will serve as directions\nfor further improvement to better adapt to more complex medical\nscenarios.\nCRediT authorship contribution statement\nGuizeng Wang: Writing â€“ original draft, Visualization, Valida-\ntion, Software, Methodology, Investigation, Formal analysis, Data cu-\nration. Huimin Lu: Writing â€“ review & editing, Project administration,\nMethodology, Investigation, Funding acquisition. Songzhe Ma: Writ-\ning â€“ review & editing, Visualization, Validation, Software. Niya Li:\nWriting â€“ review & editing, Visualization, Validation. Pengcheng Sang:\nWriting â€“ review & editing, Visualization, Data curation. Yilong Wang:\nVisualization, Formal analysis, Data curation.\nDeclaration of competing interest\nThe authors declare that they have no known competing finan-\ncial interests or personal relationships that could have appeared to\ninfluence the work reported in this paper.\nAcknowledgments\nThis research is supported by the Fundamental Research Funds\nfor the Central Universities, China, JLU (No. 93K172022K15), Indus-\ntrial Technology Research and Development Special Project of Jilin\nProvincial Development and Reform Commission, China in 2023 (No.\n2023C042-6), and Key Project of Science and Technology Research\nPlan of Jilin Provincial Department of Education, China in 2023 (No.\nJJKH20230763KJ).\nData availability\nData will be made available on request.\nReferences\n[1] X. Chen, X. Wang, K. Zhang, K.-M. Fung, T.C. Thai, K. Moore, R.S. Mannel,\nH. Liu, B. Zheng, Y. Qiu, Recent advances and clinical applications of deep\nlearning in medical image analysis, Med. Image Anal. 79 (2022) 102444, http:\n//dx.doi.org/10.1016/j.media.2022.102444.\n[2] W. Shen, Z. Peng, X. Wang, H. Wang, J. Cen, D. Jiang, L. Xie, X. Yang, Q. Tian,\nA survey on label-efficient deep image segmentation: Bridging the gap between\nweak supervision and dense prediction, IEEE Trans. Pattern Anal. Mach. Intell.\n26 (2023) 9284â€“9305, http://dx.doi.org/10.1109/TPAMI.2023.3246102.\n[3] M.K. Ghalati, A. Nunes, H. Ferreira, P. Serranho, R. Bernardes, Texture analysis\nand its applications in biomedical imaging: A survey, IEEE Rev. Biomed. Eng.\n15 (2021) 222â€“246.\n[4] J. Morano, Ã.S. Hervella, J. Rouco, J. Novo, J.I. FernÃ¡ndez-Vigo, M. Ortega,\nWeakly-supervised detection of AMD-related lesions in color fundus images using\nexplainable deep learning, Comput. Methods Programs Biomed. 229 (2023)\n107296, http://dx.doi.org/10.1016/j.cmpb.2022.107296.\n[5] X. Liu, Z. Liu, Y. Zhang, M. Wang, J. Tang, Weakly-supervised localization and\nclassification of biomarkers in OCT images with integrated reconstruction and\nattention, Biomed. Signal Process. Control. 79 (2023) 104213, http://dx.doi.org/\n10.1016/j.bspc.2022.104213.\n[6] R. Liu, S. Zhou, Y. Guo, Y. Wang, C. Chang, U2F-GAN: weakly supervised\nsuper-pixel segmentation in thyroid ultrasound images, Cogn. Comput. 13 (2021)\n1099â€“1113, http://dx.doi.org/10.1007/s12559-021-09909-7.\n[7] X. Liu, Q. Yuan, Y. Gao, K. He, S. Wang, X. Tang, J. Tang, D. Shen, Weakly\nsupervised segmentation of COVID19 infection with scribble annotation on\nCT images, Pattern Recognit. 122 (2022) 108341, http://dx.doi.org/10.1016/\nj.patcog.2021.108341.\n[8] G. Valvano, A. Leo, S.A. Tsaftaris, Learning to segment from scribbles using\nmulti-scale adversarial attention gates, IEEE Trans. Med. Imaging 40 (8) (2021)\n1990â€“2001, http://dx.doi.org/10.1109/TMI.2021.3069634.\n[9] F. Lyu, M. Ye, J.F. Carlsen, K. Erleben, S. Darkner, P.C. Yuen, Pseudo-label guided\nimage synthesis for semi-supervised covid-19 pneumonia infection segmentation,\nIEEE Trans. Med. Imaging 42 (3) (2022) 797â€“809, http://dx.doi.org/10.1109/\nTMI.2022.3217501.\n[10] M. Yu, M. Han, X. Li, X. Wei, H. Jiang, H. Chen, R. Yu, Adaptive soft erasure\nwith edge self-attention for weakly supervised semantic segmentation: thyroid\nultrasound image case study, Comput. Biol. Med. 144 (2022) 105347, http:\n//dx.doi.org/10.1016/j.compbiomed.2022.105347.\n[11] F. Gao, M. Hu, M.-E. Zhong, S. Feng, X. Tian, X. Meng, Z. Huang, M. Lv, T. Song,\nX. Zhang, et al., Segmentation only uses sparse annotations: Unified weakly and\nsemi-supervised learning in medical images, Med. Image Anal. 80 (2022) 102515,\nhttp://dx.doi.org/10.1016/j.media.2022.102515.\n[12] A.B. Hamida, M. Devanne, J. Weber, C. Truntzer, V. DerangÃ¨re, F. Ghiringhelli,\nG. Forestier, C. Wemmert, Weakly supervised learning using attention gates for\ncolon cancer histopathological image segmentation, Artif. Intell. Med. 133 (2022)\n102407, http://dx.doi.org/10.1016/j.artmed.2022.102407.\n[13] K. Zhang, X. Zhuang, Cyclemix: A holistic strategy for medical image segmenta-\ntion from scribble supervision, in: Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition, 2022, pp. 11656â€“11665.\nBiomedical Signal Processing and Control 113 (2026) 108948\n11\n\nG. Wang et al.\n[14] X. Ma, Z. Ji, S. Niu, T. Leng, D.L. Rubin, Q. Chen, MS-CAM: Multi-scale\nclass activation maps for weakly-supervised segmentation of geographic atrophy\nlesions in SD-OCT images, IEEE J. Biomed. Heal. Inform. 24 (12) (2020)\n3443â€“3455, http://dx.doi.org/10.1109/JBHI.2020.2999588.\n[15] P. Chikontwe, H.J. Sung, J. Jeong, M. Kim, H. Go, S.J. Nam, S.H. Park,\nWeakly supervised segmentation on neural compressed histopathology with self-\nequivariant regularization, Med. Image Anal. 80 (2022) 102482, http://dx.doi.\norg/10.1016/j.media.2022.102482.\n[16] Q. Meng, L. Liao, S. Satoh, Weakly-supervised learning with complementary\nheatmap for retinal disease detection, IEEE Trans. Med. Imaging 41 (8) (2022)\n2067â€“2078, http://dx.doi.org/10.1109/TMI.2022.3155154.\n[17] W. Xie, C. Jacobs, J.-P. Charbonnier, B. van Ginneken, Dense regression activa-\ntion maps for lesion segmentation in CT scans of COVID-19 patients, Med. Image\nAnal. 86 (2023) 102771, http://dx.doi.org/10.1016/j.media.2023.102771.\n[18] L. Zhong, G. Wang, X. Liao, S. Zhang, HAMIL: High-resolution activation maps\nand interleaved learning for weakly supervised segmentation of histopathological\nimages, IEEE Trans. Med. Imaging 42 (2023) 2912â€“2923, http://dx.doi.org/10.\n1109/TMI.2023.3269798.\n[19] C.-C. Hsu, K.-J. Hsu, C.-C. Tsai, Y.-Y. Lin, Y.-Y. Chuang, Weakly supervised\ninstance segmentation using the bounding box tightness prior, Adv. Neural Inf.\nProcess. Syst. 32 (2019).\n[20] H. Zhou, B. Qiao, L. Yang, J. Lai, X. Xie, Texture-guided saliency distilling\nfor unsupervised salient object detection, in: Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition, 2023, pp. 7257â€“7267.\n[21] C. Yu, J. Wang, C. Gao, G. Yu, C. Shen, N. Sang, Context prior for scene\nsegmentation, in: Proceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition, 2020, pp. 12416â€“12425, http://dx.doi.org/10.1016/j.\nmedia.2023.102771.\n[22] G. Patel, J. Dolz, Weakly supervised segmentation with cross-modality equivari-\nant constraints, Med. Image Anal. 77 (2022) 102374, http://dx.doi.org/10.1016/\nj.media.2022.102374.\n[23] H. Du, Q. Dong, Y. Xu, J. Liao, Weakly-supervised 3D medical image segmenta-\ntion using geometric prior and contrastive similarity, IEEE Trans. Med. Imaging\n42 (2023) 2936â€“2947, http://dx.doi.org/10.1109/TMI.2023.3269523.\n[24] C. Zotti, Z. Luo, A. Lalande, P.-M. Jodoin, Convolutional neural network\nwith shape prior applied to cardiac mri segmentation, IEEE J. Biomed. Heal.\nInformatics 23 (3) (2018) 1119â€“1128, http://dx.doi.org/10.1109/JBHI.2018.\n2865450.\n[25] Z. Chen, Z. Tian, J. Zhu, C. Li, S. Du, C-cam: Causal cam for weakly\nsupervised semantic segmentation on medical image, in: Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern Recognition, 2022, pp.\n11676â€“11685.\n[26] Y. Li, Y. Liu, L. Huang, Z. Wang, J. Luo, Deep weakly-supervised breast tumor\nsegmentation in ultrasound images with explicit anatomical constraints, Med.\nImage Anal. 76 (2022) 102315, http://dx.doi.org/10.1016/j.media.2021.102315.\n[27] A.V. Dalca, J. Guttag, M.R. Sabuncu, Anatomical priors in convolutional net-\nworks for unsupervised biomedical segmentation, in: Proceedings of the IEEE\nConference on Computer Vision and Pattern Recognition, 2018, pp. 9290â€“9299.\n[28] J. Wang, B. Xia, Bounding box tightness prior for weakly supervised image\nsegmentation, in: International Conference on Medical Image Computing and\nComputer-Assisted Intervention, Springer, 2021, pp. 526â€“536.\n[29] Y. Meng, X. Chen, H. Zhang, Y. Zhao, D. Gao, B. Hamill, G. Patri, T. Peto, S.\nMadhusudhan, Y. Zheng, Shape-aware weakly/semi-supervised optic disc and cup\nsegmentation with regional/marginal consistency, in: International Conference on\nMedical Image Computing and Computer-Assisted Intervention, Springer, 2022,\npp. 524â€“534.\n[30] H. Kervadec, J. Dolz, M. Tang, E. Granger, Y. Boykov, I.B. Ayed, Constrained-\nCNN losses for weakly supervised segmentation, Med. Image Anal. 54 (2019)\n88â€“99, http://dx.doi.org/10.1016/j.media.2019.02.009.\n[31] H. Kervadec, J. Dolz, S. Wang, E. Granger, I.B. Ayed, Bounding boxes for weakly\nsupervised segmentation: Global constraints get close to full supervision, in:\nMedical Imaging with Deep Learning, PMLR, 2020, pp. 365â€“381.\n[32] Z. Jia, X. Huang, I. Eric, C. Chang, Y. Xu, Constrained deep weak supervision for\nhistopathology image segmentation, IEEE Trans. Med. Imaging 36 (11) (2017)\n2376â€“2388, http://dx.doi.org/10.1109/TMI.2017.2724070.\n[33] B. Zhang, J. Xiao, Y. Wei, K. Huang, S. Luo, Y. Zhao, End-to-end weakly\nsupervised semantic segmentation with reliable region mining, Pattern Recognit.\n128 (2022) 108663, http://dx.doi.org/10.1016/j.patcog.2022.108663.\n[34] Z. Chen, T. Wang, X. Wu, X.-S. Hua, H. Zhang, Q. Sun, Class re-activation maps\nfor weakly-supervised semantic segmentation, in: Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition, 2022, pp. 969â€“978.\n[35] M. Meng, T. Zhang, Q. Tian, Y. Zhang, F. Wu, Foreground activation maps\nfor weakly supervised object localization, in: Proceedings of the IEEE/CVF\nInternational Conference on Computer Vision, 2021, pp. 3385â€“3395.\n[36] P.-T. Jiang, C.-B. Zhang, Q. Hou, M.-M. Cheng, Y. Wei, Layercam: Exploring\nhierarchical class activation maps for localization, IEEE Trans. Image Process.\n30 (2021) 5875â€“5888, http://dx.doi.org/10.1109/TIP.2021.3089943.\n[37] T.H. Lee, M.F.A. Fauzi, R. Komiya, Segmentation of CT brain images using K-\nmeans and EM clustering, in: 2008 Fifth International Conference on Computer\nGraphics, Imaging and Visualisation, IEEE, 2008, pp. 339â€“344, http://dx.doi.org/\n10.1109/CGIV.2008.17.\n[38] L. Wu, S. Hu, C. Liu, MR brain segmentation based on DE-ResUnet combining\ntexture features and background knowledge, Biomed. Signal Process. Control. 75\n(2022) 103541, http://dx.doi.org/10.1016/j.bspc.2022.103541.\n[39] J. Ra, S. Db, MRI brain abnormality detection using conventional neural network\n(CNN), Smart Intell. Comput. Commun. Technol. 38 (2021) 439.\n[40] Y. Wang, J. Zhang, M. Kan, S. Shan, X. Chen, Self-supervised equivariant atten-\ntion mechanism for weakly supervised semantic segmentation, in: Proceedings\nof the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2020,\npp. 12275â€“12284.\n[41] J. Qin, J. Wu, X. Xiao, L. Li, X. Wang, Activation modulation and recalibration\nscheme for weakly supervised semantic segmentation, in: Proceedings of the\nAAAI Conference on Artificial Intelligence, vol. 36, (2) 2022, pp. 2117â€“2125.\n[42] S. Yu, B. Zhang, J. Xiao, E.G. Lim, Structure-consistent weakly supervised salient\nobject detection with local saliency coherence, in: Proceedings of the AAAI\nConference on Artificial Intelligence, vol. 35, (4) 2021, pp. 3234â€“3242.\n[43] J. Chen, W. Lu, Y. Li, L. Shen, J. Duan, Adversarial learning of object-\naware activation map for weakly-supervised semantic segmentation, IEEE Trans.\nCircuits Syst. Video Technol. 33 (8) (2023) 3935â€“3946, http://dx.doi.org/10.\n1109/TCSVT.2023.3236432.\n[44] S. Bakas, H. Akbari, A. Sotiras, M. Bilello, M. Rozycki, J.S. Kirby, J.B. Freymann,\nK. Farahani, C. Davatzikos, Advancing the cancer genome atlas glioma MRI\ncollections with expert segmentation labels and radiomic features, Sci. Data 4\n(1) (2017) 1â€“13.\n[45] S. Bakas, M. Reyes, A. Jakab, S. Bauer, M. Rempfler, A. Crimi, R.T. Shinohara,\nC. Berger, S.M. Ha, M. Rozycki, et al., Identifying the best machine learning\nalgorithms for brain tumor segmentation, progression assessment, and overall\nsurvival prediction in the BRATS challenge, 2018, arXiv preprint arXiv:1811.\n02629.\n[46] S. Bakas, H. Akbari, A. Sotiras, M. Bilello, M. Rozycki, J. Kirby, J. Freymann,\nK. Farahani, C. Davatzikos, Segmentation labels and radiomic features for the\npre-operative scans of the TCGA-LGG collection, Cancer Imaging Arch. 286\n(2017).\n[47] X. Li, G. Luo, K. Wang, H. Wang, S. Li, J. Liu, X. Liang, J. Jiang, Z. Song,\nC. Zheng, et al., The state-of-the-art 3D anisotropic intracranial hemorrhage\nsegmentation on non-contrast head CT: The INSTANCE challenge, 2023, arXiv\npreprint arXiv:2301.03281.\n[48] X. Li, G. Luo, W. Wang, K. Wang, Y. Gao, S. Li, Hematoma expansion context\nguided intracranial hemorrhage segmentation and uncertainty estimation, IEEE\nJ. Biomed. Heal. Inform. 26 (3) (2021) 1140â€“1151, http://dx.doi.org/10.1109/\nJBHI.2021.3103850.\n[49] A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan, T. Killeen, Z. Lin,\nN. Gimelshein, L. Antiga, et al., Pytorch: An imperative style, high-performance\ndeep learning library, Adv. Neural Inf. Process. Syst. 32 (2019).\n[50] K. He, X. Zhang, S. Ren, J. Sun, Deep residual learning for image recognition, in:\nProceedings of the IEEE Conference on Computer Vision and Pattern Recognition,\n2016, pp. 770â€“778.\n[51] H. Wang, L. Huai, W. Li, L. Qi, X. Jiang, Y. Shi, Weakmedsam: Weakly-supervised\nmedical image segmentation via sam with sub-class exploration and prompt\naffinity mining, IEEE Trans. Med. Imaging (2025).\n[52] H. Wang, J. Wu, C. Wang, M. Zhao, D. Zhang, R. Xu, AC-CAM: Affinity-aware\ncontrast CAM for weakly-supervised semantic segmentation on MRI brain tumor,\nin: 2024 IEEE International Conference on Bioinformatics and Biomedicine,\nBIBM, IEEE, 2024, pp. 3768â€“3771.\nBiomedical Signal Processing and Control 113 (2026) 108948\n12",
    "version": "5.3.31"
  },
  {
    "numpages": 18,
    "numrender": 18,
    "info": {
      "PDFFormatVersion": "1.7",
      "Language": "en",
      "EncryptFilterName": null,
      "IsLinearized": true,
      "IsAcroFormPresent": false,
      "IsXFAPresent": false,
      "IsCollectionPresent": false,
      "IsSignaturesPresent": false,
      "Creator": "Elsevier",
      "Custom": {
        "CrossMarkDomains[1]": "elsevier.com",
        "CrossmarkMajorVersionDate": "2010-04-23",
        "CreationDate--Text": "13th January 2025",
        "ElsevierWebPDFSpecifications": "7.0.1",
        "robots": "noindex",
        "doi": "10.1016/j.heliyon.2024.e40148",
        "CrossMarkDomains[2]": "sciencedirect.com",
        "CrossmarkDomainExclusive": "true"
      },
      "ModDate": "D:20250113064915Z",
      "Author": "Valentin Comte",
      "Title": "Deep cascaded registration and weakly-supervised segmentation of fetal brain MRI",
      "Keywords": "Registration,Segmentation,Cascade,Deep learning,Fetal brain",
      "CreationDate": "D:20250109160527Z",
      "Producer": "Acrobat Distiller 8.1.0 (Windows)",
      "Subject": "Heliyon, 11 (2025) e40148. doi:10.1016/j.heliyon.2024.e40148"
    },
    "metadata": {
      "ali:license_ref": "http://creativecommons.org/licenses/by/4.0/",
      "crossmark:crossmarkdomains": "elsevier.comsciencedirect.com",
      "crossmark:crossmarkdomainexclusive": "true",
      "crossmark:doi": "10.1016/j.heliyon.2024.e40148",
      "crossmark:majorversiondate": "2010-04-23",
      "dc:format": "application/pdf",
      "dc:identifier": "10.1016/j.heliyon.2024.e40148",
      "dc:publisher": "Elsevier Ltd",
      "dc:description": "Heliyon, 11 (2025) e40148. doi:10.1016/j.heliyon.2024.e40148",
      "dc:subject": [
        "Registration",
        "Segmentation",
        "Cascade",
        "Deep learning",
        "Fetal brain"
      ],
      "dc:title": "Deep cascaded registration and weakly-supervised segmentation of fetal brain MRI",
      "dc:creator": [
        "Valentin Comte",
        "Mireia Alenya",
        "Andrea Urru",
        "Judith Recober",
        "Ayako Nakaki",
        "Francesca Crovetto",
        "Oscar Camara",
        "Eduard GratacÃ³s",
        "Elisenda Eixarch",
        "Fatima Crispi",
        "Gemma Piella",
        "Mario Ceresa",
        "Miguel A. GonzÃ¡lez Ballester"
      ],
      "jav:journal_article_version": "VoR",
      "pdf:creationdate--text": "13th January 2025",
      "pdf:producer": "Acrobat Distiller 8.1.0 (Windows)",
      "pdf:keywords": "Registration,Segmentation,Cascade,Deep learning,Fetal brain",
      "pdfx:creationdate--text": "13th January 2025",
      "pdfx:crossmarkdomains": "sciencedirect.comelsevier.com",
      "pdfx:crossmarkdomainexclusive": "true",
      "pdfx:crossmarkmajorversiondate": "2010-04-23",
      "zifnomgiontf9mmagzdv-osnnmweplt6lmwugmdf9y.n7ndn-owv.o9eqn9iknm2nn9qtma": "",
      "pdfx:doi": "10.1016/j.heliyon.2024.e40148",
      "pdfx:robots": "noindex",
      "prism:aggregationtype": "journal",
      "prism:copyright": "Â© 2024 The Authors. Published by Elsevier Ltd.",
      "prism:coverdate": "2025-01-15",
      "prism:coverdisplaydate": "15 January 2025",
      "prism:doi": "10.1016/j.heliyon.2024.e40148",
      "prism:issn": "2405-8440",
      "prism:number": "1",
      "prism:pagerange": "e40148",
      "prism:publicationname": "Heliyon",
      "prism:startingpage": "e40148",
      "prism:url": "https://doi.org/10.1016/j.heliyon.2024.e40148",
      "prism:volume": "11",
      "tdm:policy": "https://www.elsevier.com/tdm/tdmrep-policy.json",
      "tdm:reservation": "1",
      "xmp:createdate": "2025-01-09T16:05:27",
      "xmp:creatortool": "Elsevier",
      "xmp:metadatadate": "2025-01-13T06:49:15",
      "xmp:modifydate": "2025-01-13T06:49:15",
      "xmpmm:documentid": "uuid:1b499bed-4ce8-4c0c-b682-0d58baae1cbe",
      "xmpmm:instanceid": "uuid:b6ed5266-03cb-4855-90c0-636283357f42",
      "xmprights:marked": "True"
    },
    "text": "Research article\nDeep cascaded registration and weakly-supervised segmentation\nof fetal brain MRI\nValentin Comte \na,g,*\n, Mireia Alenya \na\n, Andrea Urru \na\n, Judith Recober \na\n,\nAyako Nakaki \nb,c\n, Francesca Crovetto \nb\n, Oscar Camara \na\n, Eduard Gratac\nÂ´\nos \nb,c,d\n,\nElisenda Eixarch \nb,c,d\n, Fatima Crispi \nb,c,d\n, Gemma Piella \na\n, Mario Ceresa \na,f\n,\nMiguel A. Gonz\nÂ´\nalez Ballester \na,e\na \nBCN MedTech, Department of Information and Communication Technologies, Universitat Pompeu Fabra, Barcelona, Spain\nb \nBCNatal, Fetal Medicine Research Center (Hospital ClÃ­nic and Hospital Sant Joan de D\nÂ´\neu), University of Barcelona, Barcelona, Spain\nc \nInstitut dâ€™Investigacions Biom\nÂ´\nediques August Pi i Sunyer (IDIBAPS), Barcelona, Spain\nd \nCentre for Biomedical Research on Rare Diseases (CIBERER), Barcelona, Spain\ne \nICREA, Barcelona, Spain\nf \nEuropean Commission, Joint Research Centre (JRC), Ispra, Italy\ng \nEuropean Commission, Joint Research Centre (JRC), Geel, Belgium\nA R T I C L E I N F O\nKeywords:\nRegistration\nSegmentation\nCascade\nDeep learning\nFetal brain\nA B S T R A C T\nDeformable image registration is a cornerstone of many medical image analysis applications,\nparticularly in the context of fetal brain magnetic resonance imaging (MRI), where precise\nregistration is essential for studying the rapidly evolving fetal brain during pregnancy and\npotentially identifying neurodevelopmental abnormalities. While deep learning has become the\nleading approach for medical image registration, traditional convolutional neural networks\n(CNNs) often fall short in capturing fine image details due to their bias toward low spatial fre-\nquencies. To address this challenge, we introduce a deep learning registration framework\ncomprising multiple cascaded convolutional networks. These networks predict a series of incre-\nmental deformation fields that transform the moving image at various spatial frequency levels,\nensuring accurate alignment with the fixed image. This multi-resolution approach allows for a\nmore accurate and detailed registration process, capturing both coarse and fine image structures.\nOur method outperforms existing state-of-the-art techniques, including other multi-resolution\nstrategies, by a substantial margin. Furthermore, we integrate our registration method into a\nmulti-atlas segmentation pipeline and showcase its competitive performance compared to nnU-\nNet, achieved using only a small subset of annotated images as atlases. This approach is partic-\nularly valuable in the context of fetal brain MRI, where annotated datasets are limited. Our\npipeline for registration and multi-atlas segmentation is publicly available at https://github.com/\nValBcn/CasReg.\nAbbreviations: MAS, multi-atlas segmentation; MRI, magnetic resonance imaging; SRR, super-resolution reconstruction; CC, cross-correlation;\nNCC, normalized cross-correlation; DL, deep learning; CNN, convolutional neural network; DF, deformation field; MSE, mean square error; SSIM,\nstructural similarity measure; LWV, local weighted voting; CSF, Cerebro-Spinal Fluid; CGM, Cortical Grey Matter; WM, White Matter; VTC, ven-\ntricles; CRB, cerebellum; THA, thalamus; BS, brain stem.\n* Corresponding author. BCN MedTech, Department of Information and Communication Technologies, Universitat Pompeu Fabra, Barcelona,\nSpain\nE-mail address: valentin.comte@ec.europa.eu (V. Comte).\nContents lists available at ScienceDirect\nHeliyon\njournal homepage: www.cell.com/heliyon\nhttps://doi.org/10.1016/j.heliyon.2024.e40148\nReceived 15 February 2024; Received in revised form 28 September 2024; Accepted 4 November 2024\nHeliyon 11 (2025) e40148\nAvailable online 19 November 2024\n2405-8440/Â© 2024 The Authors. Published by Elsevier Ltd. This is an open access article under the CC BY license\n(http://creativecommons.org/licenses/by/4.0/).\n\n1. Introduction\nFetal brain Magnetic Resonance Imaging (MRI) is an important asset in the early diagnosis of neurodevelopment abnormalities,\nsuch as those derived from Intrauterine Growth Restriction, Corpus Callosum Agenesis, or Ventriculomegaly [1â€“5]. In comparison with\nthe adult brain, acquiring fetal brain MRI is a challenging procedure both at a clinical and technical level. A major difficulty is the\nappearance of motion artifacts caused by the fetal movements and the motherâ€™s breathing. To tackle this issue, snapshot imaging\ntechniques, such as T2-weighted Single Shot Fast Spin Echo, are preferred to 3D volumetric imaging. These techniques produce\nlow-resolution images with minimized motion artifacts that can be acquired along all planes. 3D Super-Resolution Reconstruction\n(SRR) algorithms, such as proposed by Ebner et al. [6] or Kuklisova-Murgasova et al. [7], are then applied tocombine the low reso-\nlution images and form high resolution 3D volumes. Once they are reconstructed, the analysis of fetal brain MRI involves the appli-\ncation of advanced techniques, including image registration and multi-class tissue segmentation. Image registration, which consists of\naligning a pair of images, serves as a critical tool for gaining insights into the evolution of brain structures throughout the course of\npregnancy. It can be used to model the fetal brain development and help clinicians to identify potential neurodevelopmental ab-\nnormalities. Similarly, the accurate annotation of fetal brain tissues using multi-class segmentation is critical to characterize and\nquantify the rapidly evolving morphological features of the fetal brain.\n2. Related works\nClassical registration methods use intensity-based similarity metrics, such as cross-correlation (CC), mutual information, or sum of\nsquare distance, to measure the degree of alignment between the fixed image and the warped moving image, and maximize the\nsimilarity metric by iterative optimization methods. These methods are effective, but can be computationally intensive and time-\nconsuming. In recent years, deep learning (DL)-based registration methods have been proposed as a faster alternative to classical\nmethods, achieving similar results in a shorter amount of time. Most of these DL-based approaches update their parameters using a loss\nfunction based on the aforementioned intensity-based similarity metrics, and an additional regularization term to promote smooth and\nreversible transformation. DL has also revolutionized segmentation methods, excelling in both segmentation accuracy and compu-\ntational efficiency. Numerous DL-based techniques for medical image segmentation have been proposed, often leveraging convolu-\ntional neural networks (CNNs) with an encoder-decoder structure, such as U-Net [8]. These approaches take advantage of the\ndeconvolution concept [9] and incorporate skip connections between layers of the same dimension on either side of the network in\norder to preserve fine details in the feature maps. Many of the DL-based registration methods proposed in the recent years employ\nspatial transformer networks [10], introducing differentiable spatial transformation layers to generate dense deformation fields (DFs).\nFor example, Li and Fan [11] presented a self-supervised method for registering pairs of 3D brain MRI scans, differing from traditional\nmethods by eliminating network training. It relies on a Fully Convolutional Network optimized using Normalized Cross-Correlation\n(NCC) and a total variation term for DF regularization. Balakrishnan et al. [12] presented VoxelMorph, a CNN autoencoder with a\nUNet-like structure, featuring ReLU activation and skip connection between the feature maps. They showed the efficiency of their\nmethods on 3D brain MRI scans, yielding superior results compared to traditional methods [13] in a much shorter amount of time. In\nthe same trend, Chen et al. [14] introduced TransMorph, a hybrid CNN-Transformer model, replacing traditional convolutional blocks\nwith Swin Transformer blocks. An original approach was proposed by Krebs et al. [15] to bypass traditional regularization techniques.\nThey utilized a conditional variational autoencoder to learn low-dimensional probabilistic deformations from the latent space rep-\nresentations of images, and converted them into DFs using an exponentiation layer. Within this landscape, the Deep Learning Image\nRegistration (DLIR) framework, authored by [16], emerged as an innovative unsupervised approach for training CNNs in medical\nimage registration tasks. They performed B-Spline registration with transposed convolutions, reducing the memory cost and speeding\nup the registration process, and used a bending energy penalty to regularize the DF. Another key innovation of their methods is the fact\nthat they introduced a multi-stage CNNs for registration. Their model starts with a first network for affine registration followed by\nseveral CNNs for deformable registration, with different grid dimension of the B-Spline in order to perform coarse-to-fine registration.\nWith the same inspiration, Zhao et al. [17] proposed recursive cascaded networks. Their model consists of several cascaded networks,\ntrained simultaneously, that warp progressively the moving image into the fixed image. They further enforced a synergy between the\ncascaded networks by computing the loss function only on the final output of the model.\nSimilarly, Wang et al. [18] proposed an unsupervised end-to-end recursive cascaded parallel network for image registration. Their\nmethod includes a combination unit that merges the warped image with the moving image at each iteration. However, these cascaded\nTable 1\nOverview of state-of-the-art DL cascaded registration methods for medical imaging, including image modality, region of interest, use of a multi-\nresolution approach, and code availability.\nReference Image modality Region of interest Multi-resolution Code available\nDe Vos et al. (2019) MRI, CT Heart, Lungs Yes Yes\nZhao et al. [17] MRI, CT Brain, Liver No Yes\nLara et al.[39] CT Cardiac No No\nZhu et al. [19] fMRI Brain No No\nHu et al. [40] MRI Brain Yes No\nWang et al. [18] MRI Brain Yes No\nCai et al. [41] CT Liver Yes No\nV. Comte et al. \nHeliyon 11 (2025) e40148\n2\n\napproaches do not incorporate a multi-resolution strategy, which would enable the model to process images at varying spatial reso-\nlutions for more accurate alignment. Such an approach was proposed by Zhu et al. [19] introduced an identity-mapping cascaded\nnetwork for fMRI registration. In their model, multi-resolution processing is enforced by employing parallel branches of progressively\nlower resolution as the cascaded networks proceed. An overview of DL-based cascaded registration approaches is given in Table 1.\nAtlas-based image segmentation, supported by registration techniques, was regarded as the most accurate and reliable method for\nmedical image segmentation before the rise of DL-based segmentation. Early atlas-based image segmentation techniques involved\naligning a single atlas with the target image through the use of registration. This was primarily due to the limited availability of\nmanually annotated images and to the high computational cost of registration at the time. Once the atlas was registered with the target\nimage, the resulting transformation was used to propagate the atlas segmentation labels onto the target image [20â€“22]. However, a\nsingle registration may not encompass all the anatomical variation from one brain to another. In this context, MAS gained popularity\nthanks to the contributions of researchers such as Rohlfing et al. [23], Klein et al. [24], and Heckemann et al. [25]. MAS employs\nmultiple atlases, or reference images, which are registered to the target image to guide the segmentation process. Manually annotated\nlabels from the atlases are then transferred onto the fixed image and combined using label fusion techniques to obtain a refined\nsegmentation. Crucially, the registration process in MAS is regularized, which produces smoother and more invertible transformation,\naddressing potential topological inconsistencies that might arise with some automatic segmentation methods. Rohlfing et al. [23] were\nthe first to apply multiple registrations followed by label fusion techniques for the segmentation of three-dimensional microscopy\nimages of beeâ€™s brains. Their work demonstrated that MAS was far superior to single atlas-based segmentation. Building on this\nresearch, Klein et al. [24] showed the effectiveness of MAS for segmenting human brain MRI scans. Their results indicated that using a\nlarger number of atlases in MAS can help capture a wider range of anatomical variability and improve segmentation accuracy.\nTypically, intensity-based registration tools such as Avants et al. [13], Klein et al. [26], or Rueckert et al. [27] are used to compute one\nindependent registration between each atlas and the target. In the context of perinatal brain segmentation, Urru et al. [28] exploited\nthe idea first proposed by Wang et al. [29] of pre-computed registration: rather than registering the image to segment with all the\natlases, their pipeline solely registers the image with a common template that has been computational time. Despite its accuracy, MAS\nhas some notable drawbacks, including extensive computational time, as multiple registrations are time-consuming using classical\nmethods. However, the potential of applying DL-based registration techniques within the framework of MAS remains relatively un-\nexplored, despite their capacity to speed up the registration process, significantly improve the segmentation performances, and enable\nunsupervised or weakly supervised segmentation, thanks to the use of annotated atlases. This presents a distinct advantage over\nDL-segmentation methods [8,30], which are typically supervised and rely heavily on large amounts of annotated data. Our proposed\nmethod improves upon existing cascaded registration techniques by utilizing a model that reduces information loss through the\naccumulation of the DFs produced by the cascades, rather than relying on a composition of successive transformations. The model\nprocesses the image at various resolutions, going from low spatial frequencies for the first cascade, towards higher spatial frequencies\nmoving forward through the cascades. This multi-resolution is further enforced by a multi-scale image similarity loss that simulta-\nneously computes Normalized Cross Correlation (NCC) for various patches sizes, allowing the model to focus both coarse and fine\nimage details at different scales. Moreover, we go beyond existing DL registration works by implementing a derived MAS approach and\ndemonstrate its competitive performances compared to nnU-Net [30]. The proposed MAS method, being inherently a\nweakly-supervised segmentation methodology, by its capability to segment images using only a small subset of annotated images as\natlases, offers a considerable advantage over supervised DL segmentation techniques, which usually require large annotated training\nsets.\n3. Materials and methods\n3.1. Cascaded registration\nIn mathematical terms, the process of registering two 3D images can be expressed as follows: let X\nmv \nand X\nfx \nbe the moving and fixed\nimage, of size H Ã— W Ã— L, defined over a 3-dimensional domain Î©âŠ‚R\n3\n. The goal of registration is to align the moving image with the\nfixed image using a spatial transformation Ï† : R\n3\nâ†’R\n3\n. The warped moving image X\nwp \nthat is generated by this transformation should\nbe as similar as possible to the fixed image X\nfx\n:\nX\nwp \n= X\nmv \nâˆ˜ Ï† â‰ˆ X\nfx \n(1)\nExecuting this procedure in a sequential manner to optimize the registration process aligns with the traditional paradigm of image\nregistration, which often comprises multiple stages. These stages typically start with affine registration and then progress through a\nseries of refinement steps in deformable image registration, following a coarse-to-fine strategy. This hierarchical multi-stage approach\nhas been empirically shown to improve conventional iterative image registration [31]. In the domain of DL-based registration, a\nsimilar approach has been put into practice, as demonstrated by [16]. They proposed a multi-stage registration training utilizing\nstacked CNNs with varying grid sizes for B-Spline registration, enabling progressive and refined registration. In contrast [17], ad-\ndresses this limitation by stacking multiple CNNs and training them concurrently. They compute the image similarity previously\nregistered with all the atlases. This approach requires only one registration, which significantly reduces loss only on the final output,\nencouraging a collaborative synergy among the cascaded networks. In our present work, we build on this idea of collaborating\ncascaded networks. However, instead of progressively warping the moving image, our model accumulates the successive DFs\ngenerated by the cascades. This approach minimizes information loss caused by repeated interpolations. Our cascaded registration\nV. Comte et al. \nHeliyon 11 (2025) e40148\n3\n\nmodel decomposes the DF into smaller and simpler transformations, enabling image processing at various spatial frequencies. Fig. 1a\nillustrates the functioning of our cascaded model. The first network takes the moving image X\nmv \nand fixed image X\nfx \nas inputs and\nproduces a dense DF, Ï†\n1\n, which partially aligns the moving image with the fixed image. Subsequently, this first warped image, X\nwp \n0\n, is\nfed to the second network, together with X\nfx \n, to generate Ï†\n2\n, which is then summed with Ï†\n1 \ntowarp X\nmv \ninto X\nwp\n1 \n. This process is\nrepeated with successive networks in a recursive manner, resulting in the final DF that fully aligns the moving image with the fixed\nimage (Fig. 2):\nX\nwp\nn \n= X\nmv \nâ—¦ \nâˆ‘\nn\ni=1\nÏ†\ni\n. (2)\nFor this work, we employ a loss function derived from Balakrishnan et al. [12], which is comprised of two main components: a\nsimilarity loss and a regularization loss. The similarity loss quantifies the degree of alignment between the warped image Xwp and the\nfixed image Xfx. This component is critical, as it drives the registration process to ensure that the two images are closely matched. We\nutilize negative local cross-correlation (NCC) as the measure for similarity, formulated as:\nL\nsim \n= \u0000 CC\n\u0000 \nX\nfx \n, X\nwp\n) \n= \u0000 \nâˆ‘\npâˆˆÎ©\n(\nâˆ‘\np\ni\n\u0000 \nX\nfx\n(p\ni \n) \u0000 \nX\nfx \n(p)\n)\u0000 \nX\nwp\n(p\ni \n) \u0000 \nX\nwp\n(p)\n)\n)\n2\nâˆ‘\np\ni\n\u0000 \nX\nfx \n(p\ni\n) \u0000 \nX\nfx \n(p)\n)\n2 \nâ‹… \nâˆ‘\np\ni\n\u0000 \nX\nwp\n(p\ni\n) \u0000 \nX\nwp\n(p)\n)\n2 \n, (3)\nwhere p\ni \niterates over the volume d\n3 \naround the voxel p, X\nfx\n(p) and \nX\nwp\n(p) are the local mean intensities over the volume d\n3 \nof the fixed\nand warped images, respectively. To achieve a robust evaluation, we analyze patches of varying sizes (ranging from 5 Ã— 5 Ã— 5 to 11 Ã—\n11 Ã— 11 voxels) around each voxel. This multi-scale approach enables us to capture both fine details and broader structural features,\nFig. 1. (a) The first partial DF Ï†1 is predicted using the fixed and moving images (blue arrows), and applied to the moving image in order to form\nthe first warped image Xwp,1 (red arrows). Subsequently, the warped and fixed images are used to predict the second partial DF Ï†2 (blue arrows),\nwhich is summed with Ï†1 and applied to the moving image to form the second warped image (red arrows). This recursive pattern continues\nthroughout the rest of the model (b) VoxelMorph architecture. (b) The proposed contracted architecture.\nV. Comte et al. \nHeliyon 11 (2025) e40148\n4\n\nthereby enhancing the accuracy of the registration. The regularization loss is designed to promote smoothness in the deformation field.\nWe express the regularization loss as:\nL\nsmooth \n= \nâˆ‘\npâˆˆÎ©\nâ€–âˆ‡Ï†(p)â€–\n2 \n(4)\nHere, âˆ‡Ï†(p) represents the local gradient of the deformation field, and the loss penalizes significant discontinuities. By minimizing this\nterm, we encourage the deformation to be smooth and continuous, which helps maintain the anatomical integrity of the structures\ninvolved. The complete loss function integrates both components as follows:\nL = \u0000 CC\n\u0000 \nX\nfx\n, X\nwp\n) \n+ Î»\nâˆ‘\npâˆˆÎ©\nâ€–âˆ‡Ï†(p)â€–\n2 \n(5)\nIn this formulation, Î» serves as a weighting factor that balances the influence of the similarity and regularization losses. By fine-tuning\nthis parameter, we aim to achieve optimal registration and segmentation accuracy while ensuring that the transformations applied are\nsmooth and mostly invertible. Importantly, both terms of the loss functions are computed on the final outputs of the networks, X\nwp\nn \nand\nÏ† respectively, ensuring a synergy between the cascades\n3.1.1. Contracted architecture\nOne of the main challenges of using cascaded networks is that they can become resource-intensive as the number of cascades\nincreases. To address this issue, we adopted an alternative designed to reduce memory consumption (Fig. 1b and c). This alternative\narchitecture presents higher number of feature maps for the hidden layers, enabling to encode more information for a marginal in-\ncrease in resources, and it eliminates the final convolution layer on the full-sized image, leading to a substantial decrease in memory\nconsumption. Those modifications enable to achieve equivalent performance while consuming fewer memory resources, as later\ndiscussed in Section 3.1.\n3.2. Multi-atlas segmentation\nMAS can be divided into four main steps: image registration, atlas selection, label propagation and label fusion. During the image\nregistration step, each atlas is aligned with the target image using a registration algorithm. This step is crucial because it allows ac-\ncurate propagation of the atlases labels onto the target image. The atlas selection step involves choosing which atlases to use based on\nfactors such as the similarity between the atlases and the target image before or after registration, and the relevance of the atlases to the\nspecific task at hand. By combining multiple atlases and carefully selecting the most suitable ones, it is possible to achieve improved\nFig. 2. Illustration of the successive partial DFs produced by the cascaded networks (bottom) and the final DF (top). The color mapping is done by\ntranslating the three components of the deformation into RGB colors. We observe the increasing spatial frequency of the DFs, processing different\nlevels of details.\nV. Comte et al. \nHeliyon 11 (2025) e40148\n5\n\nsegmentation accuracy and efficiency. During the label propagation step, the labels from the selected atlases are transferred onto the\ntarget image using the DFs obtained from the image registration step. Finally, in the label fusion step, the propagated labels are\ncombined to create the final segmentation of the target image. Based on our cascaded registration model, we propose a simple MAS\npipeline for fetal brain MRI segmentation. This approach involves using a subset of annotated images, which are used as atlases and\nregistered to the target image. This multi-subject atlas is composed of 20 subjects between 28 and 44 gestational weeks, that have been\nselected within the cohort to maximize variability in terms of age, brain development and shape [28]. The best aligned atlases are then\nselected based on their image similarity with the target image, ensuring that the most suitable atlas images are used to construct the\nfinal segmentation. Finally, we propagate the labels of the selected atlases and we combine them using a local weighted voting (LWV)\nstrategy. The overview of the proposed MAS method is shown on Fig. 3.\n3.2.1. Atlas selection\nThe main motivations for selecting specific atlases for use in MAS are, on one hand, to use the more suitable atlases for the task at\nhand, discarding the ones that overly differ in termsof appearance or morphology, and that ultimately may lower the accuracy of the\nsegmentation, and on the other hand to reduce the computational time. Different similarity measures can be used for atlas selection,\nsuch as NCC, Mean Square Error (MSE), and Structural Similarity Index Measure (SSIM). We evaluated those similarity measures and\nchose the most suitable one for the atlas selection in Section 3.3.\n3.2.2. Label fusion\nLabel fusion is the last critical phase of MAS; it consists of combining the propagated labels from the atlases in order to generate a\nrefined segmentation of the target image. The most straightforward method is Majority Voting [32]. This strategy consists of selecting,\nfor each voxel, the label that is most commonly assigned among all the propagated labels. While this approach is simple and easy to\nimplement, it does not take into account the local similarity between the transformed atlases and the target image, possibly resulting in\na less accurate segmentation. In this work, we adopt a technique of label fusion based on LWV [33], which assigns more weight to\nlabels that correspond to areas of high local similarity between the transformed atlases and the target image. By taking local similarity\ninto account, the resulting segmentation is more accurate and better capture the complex structure and variability within the target\nimage. LWV assigns a weight w to each voxel i of the propagated label k as follows:\nÏ‰\nk,i \n= |m(i âˆˆ Î©)|\ng \n(6)\nwhere \nÏ‰ is a region of size d\n3 \naround the voxel i, m is the average local similarity metrics in the region Ï‰, and g is the gain factor, which\ncan take different values depending on thesimilarity metrics used.\n3.3. Evaluation\nLet Y\nmv \nand Y\nfx \nbe the ground-truth labels of the moving and fixed images, respectively. The propagated labels of the moving images\nare formed by applying a DF Ï† to the labelsof the moving image Y\nmv\n:\nFig. 3. Overview of the MAS method. Registration: all atlases are registered with the target image, with their labels propagated accordingly. Atlas\nselection: the best-aligned atlases are chosen based on their normalized cross-correlation (NCC with the target image. Label fusion: the selected\npropagated labels are comined together using local weighted voting (LWV) to generate the final labels.\nV. Comte et al. \nHeliyon 11 (2025) e40148\n6\n\nY\nwp \n= Y\nmv \nâˆ˜ Ï† â‰ˆ Y\nfx \n(7)\nWe evaluate the performance of our registration model and MAS pipeline using the Dice score [34], and the Hausdorff Distance.\nThe Dice score between the labels Yfxand Ywp is given by:\nDSC = 2 â‹…\nâƒ’âƒ’\n \nY\nfx \nâˆ© Y\nwp\nâƒ’âƒ’âƒ’âƒ’\nY\nfx\nâƒ’âƒ’\n \n+|Y\nwp\nâƒ’âƒ’\n \n(8)\nThe one-sided Hausdorff Distance (HD) from Yfx to Ywp is defined as:\nhd\n\u0000 \nY\nfx \n, Y\nwp\n) \n= max\ny\n1 \nâˆˆY\nfx\nmin\ny\n2 \nâˆˆY\nwp\nâ€–y\n1 \n\u0000 y\n2\nâ€–\n2 \n(9)\nand from Ywp to Yfx:\nhd\n\u0000 \nY\nwp\n, Y\nfx\n) \n= max\ny\n2 \nâˆˆwp \nmin\ny\n1 \nâˆˆY\nfx\nâ€–y\n1 \n\u0000 y\n2\nâ€–\n2 \n(10)\nThe bidirectional HD is then:c\nHD\n\u0000 \nY\nfx\n, Y\nwp\n) \n= max\n\u0000 \nhd\n\u0000 \nY\nfx\n, Y\nwp\n)\n, hd\n\u0000 \nY\nwp\n, Y\nfx\n)) \n(11)\nWe opted to evaluate our model using the 95th percentile of the HD, denoted HD95, because it provides a more robust measure of\nlabel alignment, by discarding the worst-case errors. All our results are given with the associated standard error, given by:\nÏƒ\nx \n= \nÏƒ\nÌ…Ì…Ì…\nn\nâˆš \n(12)\nWhere n is the size of the sample, and \nÏƒ its standard deviation.\n3.4. Experimental settings\nThe model was developped using Python 3.8, Pytorch 1.11, and a GPU Nvidia Titan Xp with 12 GB of memory. The baseline ar-\nchitecture of the cascaded networks is a contracted version of Balakrishnan et al. [12], designed to limit memory usage (see Section\n2.1). The networks were trained for EP\nmax \n= 500 epochs with 100 iterations per epoch using the Adam optimizer, an exponentially\ndecaying learning rate lr\nEP \n= 3 â‹… 10\n\u0000 4 \nâ‹… e\n3EP/EP\nmax \nand a batch size of 2. Our experiments were conducted using two fetal brain MRI\ndatasets:\nâ€¢ The IMPACT dataset [28,35]: composed of 170 fetal brain MRI scans between 32 and 39 gestational weeks. The single-shot fast\nspin-echo T2-weighted were performed in Hospital San Joan de Deu (3T Philips Ingenia MR scanner), and Hospital Clinic (3T\nSiemens Magneton Vida MR scanner). The low resolution stacks were reconstructed using the Ebner et al. [6] super-resolution\nreconstruction (SRR) pipeline. All fetuses included in this study did not have any major malformation. The dataset is split into\n140-10-20 for training, validation and test sets, respectively.\nâ€¢ The FeTa dataset [36]: publicly available dataset composed of 160 T2-weighted fetal brain MRI scans between 20 and 35 gesta-\ntional weeks, including neurotypical and pathological subjects. The scans were collected from four institutions: University Chil-\ndrenâ€™s Hospital Zurich (1.5T and 3T GE scanners), General Hospital Vienna/Medical University of Vienna (1.5T and 3T Philips\nscanners), Lausanne University Hospital (1.5T Siemens scanner), and University of California San Francisco (3T GE scanner).\nHigh-resolution 3D volumes were reconstructed using four different super-resolution methods ([6,7]; Dong et al., 2016; [37]). The\ndataset is split into 130-10-20 for training, validation and test sets, respectively.\nThe seven anatomical labels are included for the validation and test sets (although they are not required for the validation set, as the\nweights of the model are saved solely based onthe image similarity loss): cerebro-spinal fluid (CSF), cortical grey matter (CGM), white\nmatter (WM), ventricles (VTC), cerebellum (CRB), thalamus (THA), and brain stem (BS). As a preprocessing step, the images were\ncropped to remove the non-brain regions, resized to 128 Ã— 128 Ã— 128 voxels, and normalized between 0 and 1.\n4. Results\n4.1. Cascaded registration\nIn this section, we share the outcomes of our experiments concerning the development and optimization of our cascaded regis-\ntration model. We start by addressing the regularizationof the DF, which plays a crucial role in minimizing voxel folding by promoting\na smooth and reversible transformation. Following this, we present the results linked to the contractedarchitecture that serves as a\nbaseline network for the cascades. We showcase how this architecture optimizes memory utilization, enabling the stacking of addi-\ntional cascades and resulting in overall performance improvements. Subsequently, we introduce the multi-resolution similarity loss\nand demonstrate its beneficial impact on the registration process. Lastly, we present the results obtained from various iterations of our\nV. Comte et al. \nHeliyon 11 (2025) e40148\n7\n\nmodel, compared with three state-of-the-art DL registration methods [12,14,16].\n4.1.1. Quality of the transformation\nIdeally, the transformation that aligns the moving image with the fixed image should be smooth and invertible. Non-invertible\ntransformations often contain multiple regions offolding, which can be quantified by the percentage of negative determinant of the\ntransformationâ€™s Jacobian. One way to reduce the amount of folding is to use diffeomorphic registration, although this method is\ngenerally associated with lower registration accuracy. A key factor in minimizing folding is the regularization parameter Î» in Equation\n(6), which helps to ensure smooth deformation by minimizing the average local gradient. By selecting the value of Î», it is possible to\nTable 2\nAverage Dice scores and percentage of negative Jacobian determinant for different regularization parameters.\nÎ» %|JÏ•|<0 Dice HD95\n10â€“4 0.458 Â± 0.167 0.849 Â± 0.021 1.87 Â± 0.12\n10â€“1 0.178 Â± 0.091 0.859 Â± 0.020 1.81 Â± 0.11\n1 0.062 Â± 0.021 0.866 Â± 0.020 1.70 Â± 0.08\n2 0.012 Â± 0.003 0.52 Â± 0.026 1.75 Â± 0.08\nTable 3\nAverage Dice scores, HD95, percentage of negative Jacobian determinant, and memory usage for the original VoxelMorph architecture and the\ncontracted architecture, for different number of cascades. The last two rows for the original architecture are empty because GPUâ€™s memory was maxed\nout.\nOriginal architecture Contracted architecture\nN cascades Dice HD95 %|JÏ•|<0 GPU(GB) Dice HD95 %|JÏ•|<0 GPU(GB)\n1 0.818 Â± 0.028 2.11 Â± 0.14 0.223 Â± 0.067 4.6 0.805 Â± 0.029 2.15 Â± 0.17 0.243 Â± 0.096 3.3\n2 0.844 Â± 0.023 1.98 Â± 0.13 0.156 Â± 0.045 7.9 0.836 Â± 0.023 2.05 Â± 0.15 0.168 Â± 0.057 4.5\n3 0.856 Â± 0.021 1.89 Â± 0.13 0.100 Â± 0.036 11.1 0.854 Â± 0.021 1.90 Â± 0.12 0.110 Â± 0.043 5.6\n4 â€“ â€“ â€“ >12 0.859 Â± 0.020 1.76 Â± 0.09 0.072 Â± 0.032 6.7\n5 â€“ â€“ â€“ >12 0.866 Â± 0.020 1.70 Â± 0.08 0.062 Â± 0.021 8.1\nFig. 4. Example of the registration of a pair of images with different numbers of cascades. The multi-resolution DFs are summed together to form the\nfinal DF and warp the moving image. The maps of the local NCC between the fixed and warped images demonstrate the better alignment with the\nincreasing number of cascades.\nV. Comte et al. \nHeliyon 11 (2025) e40148\n8\n\nbalance the need for smooth deformations with the accuracy ofthe registration. Table 2 shows the percentage of negative Jacobian\ndeterminant of the transformation for different values of Î», as well as the average Dice score and HD95 obtained. Based on these results,\nwe adopted a regularization parameter of Î» = 1, which provides the best evaluation metrics and significantly reduces the percentage of\nnegative Jacobian values. Fig. 5 illustrates the effect of the regularization parameter Î» on the smoothness of the DF. The figure includes\na warped image with a grid overlay, a map of the determinant of the Jacobian, and a zoomed-in view of two folding regions for\ndifferent values of the regularization parameter. It visually demonstrates that higher values of Î» lead to smoother DFs and fewer regions\nof voxel folding (i.e., regions with negative Jacobian determinant).\n4.1.2. Contracted architecture and number of cascaded networks\nAs explained in Section 2.1, we developed a modified version of the VoxelMorph [12] architecture in order to reduce the memory\ncost and enable the use of more cascades. This modified architecture is referred to as the contracted architecture. Table 3 presents the\naverage Dice scores and HD95 obtained with both the original Voxelmorph architecture and the contracted architecture for different\nnumbers of cascades, as well as the GPU usage during inference. The results presented in the table demonstrate that, while the\nFig. 5. Example of a warped image with different regularization parameter Î», with the deformation grid superimposed, the corresponding Jacobian\ndeterminant map and a zoom of the deformation grid on two regions.\nV. Comte et al. \nHeliyon 11 (2025) e40148\n9\n\ncontracted architecture yields slightly lower scores for a given number of cascades in comparison to the original architecture, it enables\nthe utilization of more cascades due to the reduced memory cost, resulting in enhanced overall performance. We also observe that the\npercentage of negative Jacobian values decreases with the increasing number of cascades for both architectures. This suggests that the\ncascades are able to decompose the transformation field into smaller and simpler transformations, which are more likely to be\ninvertible. To visually illustrate the effect of varying number of cascades, we provide an example in Fig. 4, which showcases the\nregistration of an image pair using different number of cascaded networks. The multi-resolution DFs are aggregated to construct the\nfinalDF, which is then applied to warp the moving image. The accompanying maps plot the local NCC between the fixed and warped\nimages, visibly highlighting improved alignment asthe number of cascades increases.\n4.2. Multi-resolution loss\nAs elaborated in Section 2.1, the loss function is primarily driven by the NCC (Eq. (4)), which promotes similarity between the\nwarped image generated by the model and the fixedimage. To further encourage the cascaded networks to engage in multi-resolution\nimage analysis, we use an average similarity measure (NCC) over patches of different sizes, with the objective of minimizing image\ndissimilarities across a spectrum of scales. Fig. 6 illustrates the progression of the image similarity loss, during the training of two 5-\ncascade networks. In the first network, training relies on an NCC loss with a single patch size of d = 9, while the second network\nincorporates multiple patch sizes, specifically d = 5, 7, 9, 11. The bottom part of the figure showcases evolution of average local\nnegative NCC for each patch size, highlighting a significant gap between the two methods, with the difference becoming more pro-\nnounced for smaller window sizes. On the top plot, the overall NCC, computed by averaging the local negative NCC values across all\npatch sizes, reveals an improvement of approximately 10 % in the multi-resolution version.\nFig. 6. Evolution of the negative NCC during training for a standard 5-cascade model (gold) and a 5-cascade model with multi-resolution image\nsimilarity loss (purple). The subplots show the evolution of the negative NCC when computed over different patch sizes (d = 5, 7, 9, 11). On the top,\nthe value on the negative NCC is computed by averaging the multi-scale values.\nV. Comte et al. \nHeliyon 11 (2025) e40148\n10\n\nTable 4Average Dice scores and HD95 obtained using ANTs rigid registration [13], VoxelMorph [12], TransMorph [14], DLIR [16], 2\n \nÃ—\n \n10 VTN [17] and the proposed method, with and without DF accumulation.\nRigid\n \nVoxelMorph\n \nTransMorph\n \n2\n \nÃ—\n \n10 VTN\n \nDLIR\n \nCasReg-NA\n \nCasreg\n \nCasReg-MR\nLabels\n \nDice\n \nHD95(mm)\nDice\n \nHD95(mm)\nDice\n \nHD95(mm)\nDice\n \nHD95(mm)\nDice\n \nHD95(mm)\nDice\n \nHD95(mm)\nDice\n \nHD95(mm)\nDice\n \nHD95\nCSF\n \n0.516\n \nÂ±\n0.008\n4.06\n \nÂ±\n0.16\n0.802\n \nÂ±\n0.007\n2.35\n \nÂ±\n0.08\n0.796\n \nÂ±\n0.009\n2.46\n \nÂ±\n0.07\n0.801\n \nÂ±\n0.07\n1.67\n \nÂ±\n0.07\n0.777\n \nÂ±\n0.012\n1.99\n \nÂ±\n0.19\n0.823\n \nÂ±\n0.007\n1.76\n \nÂ±\n0.07\n0.855\n \nÂ±\n0.008\n1.87\n \nÂ±\n0.06\n0.863\n \nÂ±\n0.005\n1.84\n \nÂ±\n0.07\nCGM\n \n0.501\n \nÂ±\n0.003\n3.63\n \nÂ±\n0.17\n0.724\n \nÂ±\n0.009\n1.69\n \nÂ±\n0.04\n0.725\n \nÂ±\n0.011\n1.78\n \nÂ±\n0.06\n0.646\n \nÂ±\n0.05\n1.32\n \nÂ±\n0.05\n0.591\n \nÂ±\n0.025\n2.58\n \nÂ±\n0.30\n0.754\n \nÂ±\n0.009\n1.35\n \nÂ±\n0.04\n0.781\n \nÂ±\n0.006\n1.28\n \nÂ±\n0.04\n0.802\n \nÂ±\n0.006\n1.26\n \nÂ±\n0.04\nWM\n \n0.663\n \nÂ±\n0.003\n3.98\n \nÂ±\n0.17\n0.782\n \nÂ±\n0.005\n2.44\n \nÂ±\n0.07\n0.780\n \nÂ±\n0.007\n2.57\n \nÂ±\n0.08\n0.779\n \nÂ±\n0.02\n1.95\n \nÂ±\n0.05\n0.638\n \nÂ±\n0.026\n3.14\n \nÂ±\n0.25\n0.823\n \nÂ±\n0.007\n2.00\n \nÂ±\n0.05\n0.850\n \nÂ±\n0.004\n1.94\n \nÂ±\n0.04\n0.853\n \nÂ±\n0.003\n1.97\n \nÂ±\n0.05\nVT\n \n0.495\n \nÂ±\n0.011\n4.37\n \nÂ±\n0.23\n0.761\n \nÂ±\n0.017\n2.45\n \nÂ±\n0.12\n0.751\n \nÂ±\n0.016\n2.49\n \nÂ±\n0.15\n0.790\n \nÂ±\n0.06\n1.81\n \nÂ±\n0.07\n0.620\n \nÂ±\n0.023\n5.07\n \nÂ±\n1.06\n0.812\n \nÂ±\n0.012\n1.65\n \nÂ±\n0.09\n0.812\n \nÂ±\n0.012\n1.63\n \nÂ±\n0.05\n0.831\n \nÂ±\n0.008\n1.70\n \nÂ±\n0.11\nCRB\n \n0.784\n \nÂ±\n0.010\n4.67\n \nÂ±\n0.26\n0.881\n \nÂ±\n0.009\n2.42\n \nÂ±\n0.13\n0.873\n \nÂ±\n0.014\n2.47\n \nÂ±\n0.10\n0.915\n \nÂ±\n0.02\n1.62\n \nÂ±\n0.07\n0.907\n \nÂ±\n0.012\n2.42\n \nÂ±\n0.17\n0.899\n \nÂ±\n0.007\n1.83\n \nÂ±\n0.07\n0.932\n \nÂ±\n0.005\n1.85\n \nÂ±\n0.07\n0.939\n \nÂ±\n0.003\n1.68\n \nÂ±\n0.05\nTHL\n \n0.812\n \nÂ±\n0.004\n3.68\n \nÂ±\n0.18\n0.871\n \nÂ±\n0.008\n1.82\n \nÂ±\n0.07\n0.871\n \nÂ±\n0.007\n1.77\n \nÂ±\n0.05\n0.895\n \nÂ±\n0.03\n1.67\n \nÂ±\n0.06\n0.801\n \nÂ±\n0.017\n3.80\n \nÂ±\n0.39\n0.884\n \nÂ±\n0.007\n1.89\n \nÂ±\n0.06\n0.915\n \nÂ±\n0.004\n1.74\n \nÂ±\n0.04\n0.927\n \nÂ±\n0.002\n1.61\n \nÂ±\n0.03\nBS\n \n0.767\n \nÂ±\n0.005\n3.61\n \nÂ±\n0.25\n0.856\n \nÂ±\n0.011\n1.57\n \nÂ±\n0.10\n0.853\n \nÂ±\n0.012\n1.56\n \nÂ±\n0.10\n0.908\n \nÂ±\n0.03\n1.45\n \nÂ±\n0.07\n0.868\n \nÂ±\n0.019\n1.95\n \nÂ±\n0.16\n0.889\n \nÂ±\n0.009\n1.78\n \nÂ±\n0.07\n0.919\n \nÂ±\n0.004\n1.57\n \nÂ±\n0.07\n0.928\n \nÂ±\n0.004\n1.27\n \nÂ±\n0.04\nAverage\n \n0.648\n \nÂ±\n0.054\n4.00\n \nÂ±\n0.34\n0.811\n \nÂ±\n0.023\n2.11\n \nÂ±\n0.14\n0.807\n \nÂ±\n0.023\n2.16\n \nÂ±\n0.15\n0.819\n \nÂ±\n0.021\n1.64\n \nÂ±\n0.09\n0.743\n \nÂ±\n0.018\n2.99\n \nÂ±\n0.30\n0.837\n \nÂ±\n0.021\n1.81\n \nÂ±\n0.08\n0.866\n \nÂ±\n0.020\n1.70\n \nÂ±\n0.08\n0.878\n \nÂ±\n0.021\n1.61\n \nÂ±\n0.09\nV. Comte et al. \nHeliyon 11 (2025) e40148\n11\n\n4.2.1. Registration accuracy\nFollowing the cascaded registration method presented in Section 2.1, we evaluate three versions of our registration model on the\nIMPACT dataset, and compare the results with three state-of-the-art DL registration methods. Specifically, â€CasRegâ€ represents a 5-\ncascade network incorporating the accumulation of DFs through the cascades. Conversely, â€CasReg-NAâ€ (No Accumulation) warps\nthe moving image successively, with regularization computed on the composition of successive DFs. â€CasReg-MRâ€ (Multi-Resolution)\ncombines the accumulation of the DF with a multi-resolution image similarity loss. Quantitative assessments after registering a\nconsistent set of 50 distinct image pairs are detailed in Table 4, which includes the average Dice scores and H95 of the propagated\nlabels with the ground-truth labels of the fixed images for all seven anatomical structures. It is worth noting that our network was\ntrained to maximize image similarity between the fixed and warped images using trilinear interpolation, while propagated labels were\ngenerated by warping the moving imageâ€™s labels with Ï† using nearest neighbour interpolation. The evaluation metrics are presented\nalongside results obtained using VoxelMorph [12], TransMorph [14], and DLIR [16]. Additionally, the Dice score and HD95 after rigid\nregistration are provided for reference. Notably, â€Cas-Regâ€ outperforms the other methods significantly, achieving an average Dice\nscore of 0.837 Â± 0.021 and an average HD95 of 1.81 Â± 0.08. The accumulated strategy surpasses these results by a substantial margin,\nobtaining an average Dice score of 0.866 Â± 0.020 and an average HD95 of 1.70 Â± 0.08. Furthermore, the multi-resolution version\nâ€CasReg-MRâ€ exhibits additional improvement, with an average Dice score of 0.878 Â± 0.021 and an average HD95 of 1.61 Â± 0.08.\nAlthough the error ranges for the different variants of our method overlap in terms of Dice score and HD95, they do not intersect with\nthe results of the other registration methods tested. This observation underscores the superior registration performance of our\napproach. Furthermore, paired t-tests were conducted among all methods to establish statistical significance, revealing that our\nmethod demonstrates statistically significant differences compared to each of the other methods. Additionally, all three variants of our\nmethod exhibit statistically significant differences between themselves. A comparison of the qualitative results of our method with\nVoxelMorph [12] is shown in Fig. 7. It clearly establishes that the cascaded registration produces better and more realistic results. The\nappearance of the DF is also substantially different for both methods. This is a consequence of the cascaded approach, which computes\nrecursive DFs at different scales, rather than a single, global DF. Hence, the final transformation is able to capture more accurately the\nlocal variations in the shape and structure of the fixed image.\nFig. 7. Comparison of the qualitative results of VoxelMorph (top) and our proposed method (bottom). The axial, sagittal and coronal views of the\nmoving, warped and fixed images are shown, as well as the corresponding DF.\nV. Comte et al. \nHeliyon 11 (2025) e40148\n12\n\n4.3. Multi-atlas segmentation\nBuilding on our cascaded registration model, we designed a MAS pipeline and evaluated its performances compared to previous\nMAS methods for fetal brain MRI segmentation [28,38], and to nnU-Net[30]. Our MAS pipeline relies on the unsupervised training of\nthe cascaded registration model and uses a small subset of 20 annotated images as atlases. This methodology can thus be classified as a\nweakly-supervised segmentation technique, in contrast to typical DL segmentation methods that draw from extensive annotated\ndatasets. In the following, we first present the atlas selection strategy we opted for, based on the NCC-selection of atlases\npost-registration. We then discuss the label fusion strategy that we employed to merge to propagated labels into a refined segmen-\ntation. Finally, we present our results compared with the aforementioned methods for two fetal brain MRI datasets.\n4.3.1. Atlas selection\nAs mentioned in Section 2.2, the selection of atlases for MAS is an important factor that can affect the accuracy of the segmentation.\nBy carefully choosing atlases that are similar to the target image, it is possible to improve the accuracy of the segmentation by\neliminating atlases that are not relevant. With our approach, a pairwise registration is completed in approximately 0.2 s, which allows\nus to register a large number of atlases with the target image and select the most suitable ones without increasing exceedingly the\noverall computation time. We examined the effect of atlas selection using three different similarity metrics: NCC, MSE and SSIM. To\nevaluate the performance of each metric, we calculated the Dice score of the propagated labels and plotted the results in Fig. 8a-c. The\nfigure shows that there is a stronger correlation between higher values of NCC and Dice scores, with a correlation coefficient of r =\n0.54, compared with a much lower correlations with r = 0.22 and r = 0.18 for MSE and SSIM, respectively. It is worth noting that these\nvaluesare influenced by the networkâ€™s training, which was guided by NCC. The outcomes might have varied if an alternative loss\nfunction had been employed. Nevertheless, we adoptedNCC as the similarity metric for atlas selection. In addition, we conducted an\nexperiment on the FeTA dataset to further demonstrate the effectiveness of NCC-based atlas selection. The FeTA dataset includes\nfetuses at various gestational ages, which provides a good opportunity to test the ability of our method to select appropriate atlases,\nbased on the gestational age deviation with the target image. Fig. 8d compares the distribution of gestational age difference between\nthe NNC-selected atlases and the target image to the distribution obtained without selection. Each target image was registered to the\nother images of the test set, and ten atlases were selected based on NCC in the first case, whereas ten atlases were randomly chosen in\nthe second case. The figure shows that the NCC-selection significantly reduces the age gap between the selected atlases and the target\nFig. 7. (continued).\nV. Comte et al. \nHeliyon 11 (2025) e40148\n13\n\nimages, compared to the random selection. The selected atlases mostly stand within a range of Â±5 gestational week difference from the\ntarget image, compared to a much more spread out distribution for the random selection. This result indicates that our atlas selection\nmethod not only leads to better alignment of the propagated labels, but also favors atlases that are relatively close in terms of\ngestational age and morphology.\n4.3.2. Label fusion\nThe label fusion strategy we adopted is based on LWV, as discussed in Section 2.2. LWV, compared to MV, represents a small\nimprovement, with an average Dice score of 0.930 Â± 0.012 versus 0.915 Â± 0.012, respectively. Fig. 9 illustrates the label propagation\nand label fusion using LWV. For each propagated labels, a disagreement map with the ground-truthlabels is shown. In this case, the\naverage Dice score over all seven anatomical labels ranges from 0.891 to 0.900 for the propagated labels, and reaches 0.933 after LWV.\n4.3.3. Segmentation accuracy\nIn this section, we present the segmentation results of our MAS method using cascaded registration. We provide a comparison with\ntwo classical MAS approaches [28,36] for the IMPACT dataset. Additionally, we compare our segmentation results with nnU-Net [30]\nfor both IMPACT and FeTa datases. For both datasets, a subset of 20 annotated images is used as atlas images, and will be referred as\nsuch is the following. The procedure involves first to register each of the atlas images to the target image using the CasReg-MR method\ndetailed in Section 2.1. The ten best aligned atlases are then selected based on the NCC of the warped images with the target image and\ntheir corresponding labels are propagated (Section 2.2). Finally, a LWV is applied to combine the propagated labels and form the final\nsegmentation (Section 2.2). Table 5a presents the segmentation results on the 20 fetal brain MRI scans of the IMPACT test set, nnU-Net\n[30] when trained and tested on the same sets of images, and two classical MAS methods that are the dHCP pipeline for neonatal brain\nsegmentation [38] and the perinatal brain segmentation pipeline [28]. Our method obtains similar results than nnU-Net for all\nanatomical labels, achieving an average Dice score of 0.930 Â± 0.012 and HD95 of 0.87 Â± 0.05 versus 0.920 Â± 0.014 and 0.95 Â± 0.07.\nThe results also demonstrate the superiority of our method compared to classical MAS pipelines for fetal and neonatal brain MRI scans,\nwhich obtain an average Dice score significantly lower with 0.87 Â± 0.02 for the perinatal brain segmentation pipeline and 0.81 Â± 0.03\nfor the dHCP pipeline for neonatal segmentation. To assess the statistical significance of these results, we conducted paired t-tests\nbetween our method and the different comparison methods, considering a p-value of less than 0.05 to indicate statistical significance.\nFig. 8. Dice score as a function of the similarity metrics for Normalized Cross Correlation (a), Mean Square Error (b), and Structural Similarity\nIndex Measure (c). (d) Histogram of the gestational age difference between the target image and the selected atlases (orange), compared to a\nrandom selection (blue).\nV. Comte et al. \nHeliyon 11 (2025) e40148\n14\n\nThe results showed a p-value of 0.102 between our method and nnU-Net, suggesting no significant difference in performance. The\ncomparison with the perinatal pipeline and the dHCP pipeline yielded p-values of 2.17 Ã— 10\u0000 2 and 3.99 Ã— 10\u0000 2, respectively,\nindicating a statistically significant difference in favor of our method. The other advantage of our MAS approach when compared to the\nclassical methods is the drastically reduced computational time. For the same single segmentation task, classical approaches require\nfrom 30 min up to 1 h, whereas our method segments the image in approximately 60 s, in the same range as nnU-Net. Table 5b\nshowcases the segmentation outcomes on 20 fetal brain MRI scans from the FeTa test set [36], obtained using our MAS method and\nnnU-Net [30]. In this case, nnU-Net slightly outperforms our approach for 4 out of the 7 anatomical labels, achieving a higher average\nDice score of 0.866 Â± 0.013 compared to 0.855 Â± 0.012 for our method, and HD95 of 3.94 Â± 0.36 versus 4.12 Â± 0.56, respectively.\nThe comparatively lower performance on the FeTa dataset can be attributed to several factors. Firstly, the FeTa dataset exhibits a\ngreater morphological variability due to the inclusion of fetuses with pathologies that impact the shapes of certain brain regions, such\nas ventricles in the case of VM. Moreover, the dataset encompasses a broader range of gestational ages, from 20 to 35 weeks, as opposed\nto the 32 to 37-week range in the IMPACT dataset. Secondly, the FeTa dataset is characterized by a higher degree of heterogeneity as\nthe fetal brain MRI scans were acquired from multiple centers using different scanners and 3D SR techniques, resulting in diverse image\nappearances within the dataset. This increased variability in morphology and appearance complicates the task of establishing cor-\nrespondences between image pairs, thereby posing challenges for registration-based segmentation. Although nnU-Net marginally\noutperformed our method on the FeTa dataset, a notable advantage of our approach is its ability to effectively utilize a limited number\nof annotated images for MAS, without the need for large amounts of labeled data during network training. This weakly-supervised\napproach can lead to considerable savings in terms of time and resources, particularly in scenarios where annotated data is scarce.\nFig. 9. Example of the label selection by LWV. Each column represents one of the NCC-selected atlas, from top to bottom: warped atlas, propagated\nlabels, disagreement between the propagated labels and ground-truth labels. After label fusion, from left to right: final labels, disagreement between\nthe final labels and the ground-truth labels, ground-truth labels.\nV. Comte et al. \nHeliyon 11 (2025) e40148\n15\n\n5. Discussion\nIn this work, we introduce a registration model based oncascaded networks and a derived MAS pipeline for fetal brain MRI seg-\nmentation. Our registration model decomposes the DF into a series of simpler transformations that operate at different spatial scales,\nallowing for more efficient and accurate alignment of the atlases with the target image. The key aspects of our approach are on one\nhand the accumulation of the DFs produced by the cascaded network, which avoids the loss of information that can occur with\nsuccessive warping and interpolation, and on the other hand the multi-scale image similarity loss that promotes a multi-resolution\nprocessing of the images and enable the cascaded networks to capture deformations at different spatial resolutions. Each network\nfocuses on a different level of the registration, from capturing global deformations in the earlier stages to smaller deformations in the\nlater stages. This hierarchical processing helps the networks to capture more complex and non-linear relationships between the images,\nwhich are often present in medical image registration problems. To evaluate the performance of our method, we conducted experi-\nments on two fetal brain MRI datasets and compared the results with several state-of-the-art methods [12,14,16]. Our results show that\nour approach outperforms these methods in both quantitative and qualitative measures. In addition, the derived MAS method achieves\nperformance similar to one of the most robust state-of-the-art segmentation method, nnU-Net [30], despite not requiring any labeled\ntraining data. This weakly-supervised segmentation methodology, utilizing a selected subset of annotated data in lieu of traditional\natlases, offers a viable alternative for contexts where the amount of annotated data is limited. Another valuable advantage of the MAS\napproach over DL-based segmentation methods, is its inherent ability to enforce spatial and topological consistency, ensuring\nsmoother transformations and reducing folding or unrealistic distortion in the anatomical structures. This is particularly important in\nmedical image segmentation, where maintaining the anatomical structures and their relationships is crucial for accurate diagnosis and\nanalysis. Furthermore, our MAS method can adapt to different annotation standards and requirements and can be used in multi-center\nstudies, where different centers may have their own specific annotation protocols. It would be therefore possible to incorporate data\nfrom multiple centers without the need to standardize the annotation process, potentially saving time and resources. We look forward\nto exploring further improvements to our framework, particularly through the local regularization of the deformation field by\nincorporating locally weighted constraints based on biomechanical models. This approach could enhance the accuracy of the spatial\ntransformations, making them more aligned with the underlying anatomical and physiological characteristics of the tissues being\nanalyzed. Future investigations will also focus on analyzing longitudinal data from fetal and neonatal brains. In this context, precise\nregistration is essential for understanding the dynamic changes in brain structures over time and can significantly improve biome-\nchanical models of perinatal brain growth, aiding in early detection of potential developmental issues. Additionally, we will apply our\nregistration framework for the reidentification of nodules in lung CT scans. This application will allow us to track the progression or\nregression of lung nodules over time, which is critical for effective patient management and treatment planning. We also plan to study\ncine-cardiac MRI using this registration framework. This will enable us to analyze the dynamics of moving cardiac structures and\ndetect potential pathologies, contributing to a better understanding of cardiac function and disease. Moreover, we aim to adapt and\nextend our method for various medical imaging applications, particularly in scenarios where obtaining annotated training data is a\nchallenge.\nCRediT authorship contribution statement\nValentin Comte: Writing â€“ review & editing, Writing â€“ original draft, Visualization, Validation, Software, Methodology,\nTable 5\n(a) Average Dice scores and HD95 obtained using our approach and nnU-Net [30] on the IMPACT dataset. The Dice scores obtained by perinatal brain\nsegmentation pipeline PP [28], and the dHCP pipeline for neonatal segmentation [38], were obtained on the same dataset and are reported from [28].\n(b) Average Dice scores and HD95 obtained using our approach, and nnU-Net [30], on the FeTa dataset [36].\n(a) (b)\nOurs nnU-Net PP dHCP Ours nnU-Net\nLabels Dice HD95 Dice HD95 Dice Dice Dice HD95 Dice HD95\nCSF 0.931 Â±\n0.006\n0.76 Â±\n0.04\n0.897 Â±\n0.011\n1.10 Â±\n0.20\n0.83 Â±\n0.04\n0.79 Â±\n0.02\n0.907 Â±\n0.008\n2.48 Â±\n0.12\n0.878 Â±\n0.012\n3.42 Â±\n0.18\nCGM 0.885 Â±\n0.006\n0.86 Â±\n0.08\n0.871 Â±\n0.012\n1.01 Â±\n0.09\n0.85 Â±\n0.03\n0.76 Â±\n0.05\n0.751 Â±\n0.009\n2.75 Â±\n0.13\n0.762 Â±\n0.021\n2.67 Â±\n0.12\nWM 0.924 Â±\n0.006\n1.00 Â±\n0.08\n0.919 Â±\n0.006\n1.20 Â±\n0.17\n0.90 Â±\n0.02\n0.85 Â±\n0.02\n0.916 Â±\n0.003\n3.32 Â±\n0.19\n0.914 Â±\n0.006\n3.97 Â±\n0.34\nVTC 0.902 Â±\n0.005\n0.83 Â±\n0.07\n0.879 Â±\n0.011\n0.96 Â±\n0.13\n0.73 Â±\n0.05\n0.66 Â±\n0.07\n0.825 Â±\n0.023\n4.76 Â±\n0.56\n0.902 Â±\n0.014\n3.56 Â±\n0.32\nCRB 0.964 Â±\n0.004\n0.99 Â±\n0.10\n0.965 Â±\n0.004\n0.84 Â±\n0.09\n0.93 Â±\n0.02\n0.89 Â±\n0.03\n0.900 Â±\n0.008\n3.47 Â±\n0.35\n0.897 Â±\n0.008\n3.75 Â±\n0.34\nTHL 0.952 Â±\n0.002\n0.91 Â±\n0.06\n0.957 Â±\n0.004\n0.85 Â±\n0.09\n0.92 Â±\n0.02\n0.88 Â±\n0.03\n0.859 Â±\n0.009\n6.13 Â±\n0.83\n0.866 Â±\n0.012\n5.67 Â±\n0.62\nBS 0.955 Â±\n0.003\n0.79 Â±\n0.05\n0.953 Â±\n0.006\n0.79 Â±\n0.05\n0.91 Â±\n0.01\n0.90 Â±\n0.02\n0.824 Â±\n0.006\n5.96 Â±\n0.89\n0.841 Â±\n0.013\n4.54 Â±\n0.45\nAverage 0.930 Â±\n0.012\n0.87 Â±\n0.05\n0.920 Â±\n0.014\n0.95 Â±\n0.23\n0.87 Â±\n0.02\n0.81 Â±\n0.03\n0.855 Â±\n0.006\n4.12 Â±\n0.56\n0.866 Â±\n0.013\n3.94 Â±\n0.36\nV. Comte et al. \nHeliyon 11 (2025) e40148\n16\n\nInvestigation, Formal analysis, Data curation, Conceptualization. Mireia Alenya: Validation, Resources, Investigation. Andrea Urru:\nValidation, Methodology, Investigation. Judith Recober: Resources, Investigation. Ayako Nakaki: Investigation, Data curation.\nFrancesca Crovetto: Data curation. Oscar Camara: Supervision. Eduard Gratac\nÂ´\nos: Data curation. Elisenda Eixarch: Data curation.\nFatima Crispi: Data curation. Gemma Piella: Writing â€“ review & editing, Supervision. Mario Ceresa: Writing â€“ review & editing,\nSupervision. Miguel A. Gonz\nÂ´\nalez Ballester: Writing â€“ review & editing, Supervision.\nDeclaration of competing interest\nThe authors declare that they have no known competing financial interests or personal relationships that could have appeared to\ninfluence the work reported in this paper.\nAcknowledgements\nThis publication is part of the project PCI2021-122044- 2A, funded by the project ERA-NET NEURON Cofund2, by MCIN/AEI/\n10.13039/501100011033/and by the European Union â€œNextGenerationEUâ€/PRTR. G. Piella is supported by ICREA under the ICREA\nAcademia programme. We also thank the study participants for their personal time and commitment to the IMPACT BCN Trial, and all\nthe medical staff, residents, midwives, nurses, MR platform, and researchers of BCNatal especially Annachiara Basso, MD and Kilian\nVellv\nÂ´\ne, MD for their support in the MR data collection. IMPACT BCN Trial was partially funded by a grant from â€œla Caixaâ€ Foundation\n(LCF/PR/GN18/10310003); Cerebra Foundation for the Brain Injured Child (Carmarthen, Wales, UK); ASISA Foundation; AGAUR\nunder grant 2017 SGR No. 1531 and Instituto de Salud Carlos III (ISCIII), PI18/00073, co-funded by the European Union. A. Nakaki has\nreceived the support of a fellowship from la Caixa Foundation under grant number LCF/BQ/DR19/11740018. F.Crovetto reports a\npersonal fee from Centro de Investigaciones Biom\nÂ´\nedicas en Red sobre Enfermedades Raras (CIBERER).\nReferences\n[1] O.M. Benkarim, N. Hahner, G. Piella, E. Gratacos, M.A. Gonz\nÂ´\nalez Ballester, E. Eixarch, G. Sanroma, Cortical folding alterations in fetuses with isolated non-\nsevere ventriculomegaly, Neuroimage: Clinical 18 (2018) 103â€“114.\n[2] O. Benkarim, G. Piella, I. Rekik, N. Hahner, E. Eixarch, D. Shen, G. Li, M.A. Gonz\nÂ´\nalez Ballester, G. Sanroma, A novel approach to multiple anatomical shape\nanalysis: application to fetal ventriculomegaly, Med. Image Anal. 64 (2020) 101750.\n[3] N. Hahner, O. Benkarim, M. Aertsen, M. Perez-Cruz, G. Piella, G. Sanroma, N. Bargallo, J. Deprest, M.A. Gonz\nÂ´\nalez Ballester, E. Gratacos, et al., Global and\nregional changes in cortical development assessed by MRI in fetuses with isolated nonsevere ventriculomegaly correlate with neonatal neurobehavior, American\nJournal of Neuro- radiology 40 (2019) 1567â€“1574.\n[4] J. Torrents-Barrena, G. Piella, N. Masoller, E. Gratac\nÂ´\nos, E. Eixarch, M. Ceresa, M.A. Gonz\nÂ´\nalez Ballester, Segmentation and classification in MRI and USfetal\nimaging: recent trends and future prospects, Med. Image Anal. 51 (2019) 61â€“88.\n[5] O.M. Benkarim, G. Sanroma, V.A. Zimmer, E. Mu\nËœ\nnoz-Moreno, N. Hahner, E. Eixarch, O. Camara, M.A. Gonz\nÂ´\nalez Ballester, G. Piella, Toward the automatic\nquantification of in utero brain development in 3d structural MRI: a review, Hum. Brain Mapp. 38 (2017) 2772â€“2787.\n[6] M. Ebner, G. Wang, W. Li, M. Aertsen, P.A. Patel, R. Aughwane, A. Melbourne, T. Doel, S. Dymarkowski, P. De Coppi, et al., An automated framework for\nlocalization, segmentation and super-resolution reconstruction of fetal brain MRI, Neuroimage 206 (2020) 116324.\n[7] M. Kuklisova-Murgasova, G. Quaghebeur, M.A. Rutherford, J.V. Hajnal, J.A. Schnabel, Reconstruction of fetal brain MRI with intensity matching and complete\noutlier removal, Med. Image Anal. 16 (2012) 1550â€“1564.\n[8] O. Ronneberger, P. Fischer, T. Brox, U-net: convolutional networks for biomedical image segmentation, in: International Conference on Medical Image\nComputing and Computer-Assisted Intervention, Springer, 2015, pp. 234â€“241.\n[9] M.D. Zeiler, R. Fergus, Visualizing and understanding convolutional networks, in: European Conference on Computer Vision, Springer, 2014, pp. 818â€“833.\n[10] M. Jaderberg, K. Simonyan, A. Zisserman, et al., Spatial transformer networks, Adv. Neural Inf. Process. Syst. 28 (2015).\n[11] H. Li, Y. Fan, Non-rigid image registration using self-supervised fully convolutional networks without training data, in: 2018 IEEE 15th International Symposium\non Biomedical Imaging (ISBI 2018), IEEE, 2018, pp. 1075â€“1078.\n[12] G. Balakrishnan, A. Zhao, M.R. Sabuncu, J.V. Guttag, A.V. Dalca, Voxelmorph: a learning framework for deformable medical image registration, CoRR abs/1809\n(2018) 05231. URL: http://arxiv.org/abs/1809.05231,arXiv:1809.05231.\n[13] B.B. Avants, N.J. Tustison, G. Song, P.A. Cook, A. Klein, J.C. Gee, A reproducible evaluation of ANTs similarity metric performance in brain image registration,\nNeuroimage 54 (2011) 2033â€“2044.\n[14] J. Chen, E.C. Frey, Y. He, W.P. Segars, Y. Li, Y. Du, Transmorph: transformer for unsupervised medical image registration, Med. Image Anal. (2022) 102615.\n[15] J. Krebs, H. Delingette, B. Mailh\nÂ´\ne, N. Ayache, T. Mansi, Learning a probabilistic model for diffeomorphic registration, IEEE Trans. Med. Imag. 38 (2019)\n2165â€“2176.\n[16] B.D. De Vos, F.F. Berendsen, M.A. Viergever, H. Sokooti, M. Staring, I. Isgum, A deep learning framework for unsupervised affine and deformable image\nregistration, Med. Image Anal. 52 (2019) 128â€“143.\n[17] S. Zhao, Y. Dong, E.I. Chang, Y. Xu, et al., Recursive cascaded networks for unsupervised medical image registration, in: Proceedings of the IEEE/CVF\nInternational Conference on Computer Vision, 2019, pp. 10600â€“10610.\n[18] L. Wang, H. Shao, X. Deng, An unsupervised end-to-end recursive cascaded parallel network for image registration, Neural Process. Lett. 55 (2023) 8255â€“8268.\n[19] Q.Y. Zhu, H. Bai, Y. Wu, Y.J. Zhou, Q. Feng, Identity-mapping cascaded network for fmri registration, Phys. Med. Biol. 66 (2021) 225011.\n[20] G.E. Christensen, S.C. Joshi, M.I. Miller, Volumetric transformation of brain anatomy, IEEE Trans. Med. Imag. 16 (1997) 864â€“877.\n[21] D.L. Collins, C.J. Holmes, T.M. Peters, A.C. Evans, Automatic 3-d model-based neuroanatomical segmentation, Hum. Brain Mapp. 3 (1995) 190â€“208.\n[22] J. Lancaster, L. Rainey, J. Summerlin, C. Freitas, P. Fox, A. Evans, A. Toga, J. Mazziotta, Automated labeling of the human brain: a preliminary report on the\ndevelopment and evaluation of a forward-transform method, Hum. Brain Mapp. 5 (1997) 238â€“242.\n[23] T. Rohlfing, R. Brandt, R. Menzel, C.R. Maurer Jr, Evaluation of atlas selection strategies for atlas-based image segmentation with application to confocal\nmicroscopy images of bee brains, Neuroimage 21 (2004) 1428â€“1442.\n[24] A. Klein, B. Mensh, S. Ghosh, J. Tourville, J. Hirsch, Mindboggle: automated brain labeling with multiple atlases, BMC Med. Imag. 5 (2005) 1â€“14.\n[25] R.A. Heckemann, J.V. Hajnal, P. Aljabar, D. Rueckert, A. Hammers, Automatic anatomical brain MRI segmentation combining label propagation and decision\nfusion, Neuroimage 33 (2006) 115â€“126.\n[26] S. Klein, M. Staring, K. Murphy, M.A. Viergever, J.P. Pluim, Elastix: a toolbox for intensity-based medical image registration, IEEE Trans. Med. Imag. 29 (2009)\n196â€“205.\n[27] D. Rueckert, L.I. Sonoda, C. Hayes, D.L. Hill, M.O. Leach, D.J. Hawkes, Nonrigid registration using free-form deformations: application to breast mr images, IEEE\nTrans. Med. Imag. 18 (1999) 712â€“721.\nV. Comte et al. \nHeliyon 11 (2025) e40148\n17\n\n[28] A. Urru, A. Nakaki, O. Benkarim, F. Crovetto, L. Segal\nÂ´\nes, V. Comte, N. Hahner, E. Eixarch, E. Gratacos, F. Crispi, G. Piella, M.A. Gonz\nÂ´\nalez Ballester, An automatic\npipeline for atlas-based fetal and neonatal brain segmentation and analysis, Comput. Methods Progr. Biomed. 230 (2023) 107334, https://doi.org/10.1016/j.\ncmpb.2023.107334. URL: https://www.sciencedirect.com/science/article/pii/S0169260723000019.\n[29] H. Wang, A. Pouch, M. Takabe, B. Jackson, J. Gorman, R. Gorman, P.A. Yushkevich, Multi-atlas segmentation with robust label transfer and label fusion, in:\nInformation Processing in Medical Imaging: 23rd International Conference, IPMI 2013, Asilomar, CA, USA, June 28â€“July 3, 2013. Proceedings 23, Springer,\n2013, pp. 548â€“559.\n[30] F. Isensee, P.F. Jaeger, S.A. Kohl, J. Petersen, K.H. Maier-Hein, nnu-net: a self-configuring method for deep learning-based biomedical image segmentation, Nat.\nMethods 18 (2021) 203â€“211.\n[31] J.A. Schnabel, D. Rueckert, M. Quist, J.M. Blackall, A.D. Castellano-Smith, T. Hartkens, G.P. Penney, W.A. Hall, H. Liu, C.L. Truwit, et al., A generic framework\nfor non-rigid registration based on non-uniform multi-level free-form deformations, in: Medical Image Computing and Computer-Assisted Interventionâ€“MICCAI\n2001: 4th International Conference Utrecht, The Netherlands, October 14â€“17, 2001 Proceedings 4, Springer, 2001, pp. 573â€“581.\n[32] J. Kittler, Combining Classifiers: A Theoretical Framework. Pattern Analysis and Applications, vol. 1, 1998, pp. 18â€“27.\n[33] X. Artaechevarria, A. Munoz-Barrutia, C. Ortiz-de Solorzano, Combination strategies in multi-atlas image segmentation: application to brain mr data, IEEE\nTrans. Med. Imag. 28 (2009) 1266â€“1277.\n[34] L.R. Dice, Measures of the amount of ecologic association between species, Ecology 26 (1945) 297â€“302.\n[35] F. Crovetto, F. Crispi, R. Casas, A. MartÃ­n-Asuero, R. Borr\n`\nas, E. Vieta, R. Estruch, E. Gratac\nÂ´\nos, C. Paules, A. Nakaki, et al., Effects of mediterranean diet or\nmindfulness-based stress reduction on prevention of small-for-gestational age birth weights in newborns born to at-risk pregnant individuals: the impact bcn\nrandomized clinical trial, JAMA 326 (2021) 2150â€“2160.\n[36] K. Payette, P. de Dumast, H. Kebiri, I. Ezhov, J.C. Paetzold, A. Iqbal, R. Khan, R. Kottke, P. Grehten, et al., An automatic multi-tissue human fetal brain\nsegmentation benchmark using the fetal tissue annotation dataset, Sci. Data 8 (2021) 1â€“14.\n[37] S. Tourbier, X. Bresson, P. Hagmann, J.P. Thiran, R. Meuli, M.B. Cuadra, An efficient total variation algorithm for super-resolution in fetal brain mri with\nadaptive regularization, Neuroimage 118 (2015) 584â€“597.\n[38] A. Makropoulos, E.C. Robinson, A. Schuh, R. Wright, S. Fitzgibbon, J. Bozek, S.J. Counsell, J. Steinweg, K. Vecchiato, J. Passerat-Palmbach, et al., The\ndeveloping human connectome project: a minimal processing pipeline for neonatal cortical surface reconstruction, Neuroimage 173 (2018) 88â€“112.\n[39] Lara et al., (2021).\n[40] Hu et al., (2022).\n[41] Cai et al., (2023).\nV. Comte et al. \nHeliyon 11 (2025) e40148\n18",
    "version": "5.3.31"
  },
  {
    "numpages": 11,
    "numrender": 11,
    "info": {
      "PDFFormatVersion": "1.7",
      "Language": "EN-US",
      "EncryptFilterName": null,
      "IsLinearized": true,
      "IsAcroFormPresent": false,
      "IsXFAPresent": false,
      "IsCollectionPresent": false,
      "IsSignaturesPresent": false,
      "Author": "Yuren Hu",
      "CreationDate": "D:20250919163027+05'30'",
      "Creator": "Elsevier",
      "Custom": {
        "CrossMarkDomains[1]": "sciencedirect.com",
        "CrossMarkDomains[2]": "elsevier.com",
        "CrossmarkDomainExclusive": "true",
        "CrossmarkMajorVersionDate": "2010-04-23",
        "ElsevierWebPDFSpecifications": "7.0.1",
        "doi": "10.1016/j.hest.2025.04.003",
        "robots": "noindex"
      },
      "Keywords": "Intracerebral hemorrhage,Weakly supervised learning,Transformer,Medical image segmentation,Class activation map",
      "ModDate": "D:20250927173634+05'30'",
      "Producer": "Acrobat DC",
      "Subject": "Brain Hemorrhages, 6 (2025) 195-205. doi:10.1016/j.hest.2025.04.003",
      "Title": "Transformer-based weakly supervised intracerebral hemorrhage segmentation using image-level labels",
      "Trapped": { "name": "False" }
    },
    "metadata": {
      "dc:format": "application/pdf",
      "dc:identifier": "doi:10.1016/j.hest.2025.04.003",
      "dc:publisher": "International Hemorrhagic Stroke Association",
      "dc:description": "Brain Hemorrhages, 6 (2025) 195-205. doi:10.1016/j.hest.2025.04.003",
      "dc:subject": [
        "Intracerebral hemorrhage",
        "Weakly supervised learning",
        "Transformer",
        "Medical image segmentation",
        "Class activation map"
      ],
      "dc:title": "Transformer-based weakly supervised intracerebral hemorrhage segmentation using image-level labels",
      "dc:creator": [
        "Yuren Hu",
        "Zengqiang Yan",
        "Zhuo Kuang",
        "Xianbo Deng",
        "Li Yu"
      ],
      "prism:aggregationtype": "journal",
      "prism:publicationname": "Brain Hemorrhages",
      "prism:copyright": "Â© 2025 International Hemorrhagic Stroke Association. Publishing services by Elsevier B.V. on behalf of KeAi Communications Co. Ltd.",
      "prism:issn": "2589-238X",
      "prism:volume": "6",
      "prism:number": "5",
      "prism:coverdisplaydate": "October 2025",
      "prism:pagerange": "195-205",
      "prism:startingpage": "195",
      "prism:endingpage": "205",
      "prism:doi": "10.1016/j.hest.2025.04.003",
      "prism:url": "https://doi.org/10.1016/j.hest.2025.04.003",
      "crossmark:majorversiondate": "2010-04-23",
      "crossmark:crossmarkdomainexclusive": "true",
      "crossmark:doi": "10.1016/j.hest.2025.04.003",
      "crossmark:crossmarkdomains": "elsevier.comsciencedirect.com",
      "jav:journal_article_version": "VoR",
      "pdfx:doi": "10.1016/j.hest.2025.04.003",
      "pdfx:robots": "noindex",
      "pdfx:crossmarkmajorversiondate": "2010-04-23",
      "pdfx:crossmarkdomainexclusive": "true",
      "w8arlmdv7m93_ztegodn.y8nnzdenlwj9odygmgenz9mrm9esnd-ro9eqn9iknm-qn96tma": "",
      "pdfx:crossmarkdomainsâ†‚005b1â†‚005d": "sciencedirect.com",
      "pdfx:crossmarkdomainsâ†‚005b2â†‚005d": "elsevier.com",
      "pdfx:crossmarkdomains": "sciencedirect.comelsevier.com",
      "xmp:creatortool": "Elsevier",
      "xmp:modifydate": "2025-09-27T17:36:34+05:30",
      "xmp:createdate": "2025-09-19T16:30:27+05:30",
      "xmp:metadatadate": "2025-09-27T17:36:34+05:30",
      "xmprights:marked": "True",
      "tdm:reservation": "1",
      "tdm:policy": "https://www.elsevier.com/tdm/tdmrep-policy.json",
      "pdf:producer": "Acrobat DC",
      "pdf:trapped": "False",
      "pdf:keywords": "Intracerebral hemorrhage,Weakly supervised learning,Transformer,Medical image segmentation,Class activation map",
      "xmpmm:documentid": "uuid:be49fd9c-16ec-4734-ad18-b131057eee99",
      "xmpmm:instanceid": "uuid:7f9b7b98-165f-4f4a-9c45-e787853bcfdb",
      "ali:license_ref": "http://creativecommons.org/licenses/by-nc-nd/4.0/"
    },
    "text": "Research Article\n1. Introduction\nIntracerebral hemorrhage (ICH) is a potentially fatal cerebrovas-\n(CT) has become the most frequently-used imaging modality\nBrain Hemorrhages 6 (2025) 195â€“205\nCHINESE ROOTS\nGLOBAL IMPACT\nContents lists available at ScienceDirect\nBrain Hemorrhages\nj o u r n a l h o m e p a g e : w w w . k e a i p u b l i s h i n g . c o m / e n / j o u r n a l s / b r a i n - h e m o r r h a g e s /\nTransformer-based weakly supervised intracerebral hemorrhage\nsegmentation using image-level labels\nYuren Hu \na\n,\na \nHuazhong University of Science and Technology, Wuhan, China\nZengqiang Yan \na\n, Zhuo Kuang \na\n, Xianbo Deng \nb,\nâ\n,\nc \nResearch Institute of Huazhong University of Science and Technology in Shenzhen, China\nLi Yu \na,c,\nâ\nb \nWuhan Union Hospital, Wuhan, China\na r t i c l e i n f o\nArticle history:\nReceived 12 December 2024\nReceived in revised form 24 March 2025\nAccepted 29 April 2025\nAvailable online 29 July 2025\nKeywords:\nIntracerebral hemorrhage\nWeakly supervised learning\nTransformer\nMedical image segmentation\nClass activation map\na b s t r a c t\nObjective: The segmentation of intracerebral hemorrhage (ICH) lesions in brain CT scans is of paramount\nimportance for the diagnosis and treatment of stroke. Given the tremendous challenge of pixel-wise\nannotation in intracerebral hemorrhage, weakly supervised segmentation for ICH based on image-level\nlabels has drawn great attention. Typical methods constructed based on convolutional neural networks\noften suffer from insufficient global perception, making it difficult to address ICH lesion diversity.\nTherefore, vision transformer, building pair-wise global dependency, becomes a popular alternative.\nUnfortunately, the data-hungry nature of vision transformer hinders its full exploitation given relatively\nlimited medical imaging data, resulting in over-smoothing issue.\nMethods: In this paper, based on the observation that most patches/tokens tend to build pair-wise depen-\ndency with intracerebral hemorrhage lesion, we propose weighted attention fusion (WAF) to fully utilize\nover-smoothing attention maps produced by ViT under conditions of limited training data. Compared to\nexisting research, no additional parameters or computational complexity is introduced by WAF when\nincorporating target-relevant information. In addition, to recall low-confidence/-salient regions in seg-\nmentation, a patch-erasing re-activation mechanism is proposed by forcing the model to explore more\nclass-specific regions.\nResults: Experimental results on three datasets, i.e., INSTANCE2022, LocalBrainCT and BraTS2021 demon-\nstrates the effectiveness of the proposed ICH weakly supervised segmentation framework. Compared to\nthe previous works on the weakly supervised sementation, the proposed architecture obtains the state-\nof-the-art performance on intracerebral hemorrhage segmentation (Dice of 72.39).\nConclusion: This study focus on weakly supervised intracerebral hemorrhage segmentation, and propose\na transformer-based framework with weighted attention fusion module and patch-erasing re-activation\nmechanism. It achives superior performance than previous methods under various settings.\nÂ© 2025 International Hemorrhagic Stroke Association. Publishing services by Elsevier B.V. on behalf of\nKeAi Communications Co. Ltd. This is an open access article under the CC BY-NC-ND license (http://creati-\nvecommons.org/licenses/by-nc-nd/4.0/).\ncular disorder, covering 10â€“15% stroke cases.\n1 \nThe outcome of ICH\ndepends on the volume of bleeding, growing rapidly within the\nfirst few hours\n2 \nwhich may result in a high risk of secondary brain\ninjuries or even death without prompt treatment. Upon hospital\nadmission, timely and precise detection/quantification of ICH is\ncrucial to determine suitable medical interventions and thus\ndecrease mortality rates. Therefore, computerized tomography\nthanks to its rapid imaging capabilities and widespread availability\ncompared to magnetic resonance imaging.\nAccurate and automated identification and localization of brain\nhemorrhage lesions play a critical role in subsequent clinical diag-\nâ \nCorresponding authors at: Wuhan Union Hospital, Wuhan, China (X. Deng);\nHuazhong University of Science and Technology, Wuhan, China (L. Yu).\nE-mail addresses: dengxianbo@hotmail.com (X. Deng), hustlyu@hust.edu.cn\n(L. Yu).\nhttps://doi.org/10.1016/j.hest.2025.04.003\n2589-238X/Â© 2025 International Hemorrhagic Stroke Association. Publishing services by Elsevier B.V. on behalf of KeAi Communications Co. Ltd.\nThis is an open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/4.0/).\nnosis and prognosis.\n3,4 \nWhile deep-learning-based automated seg-\nmentation methods have offered a feasible solution, most existing\nstudies rely on pixel-wise annotation-based fully supervised\nframeworks. Although fully supervised segmentation frameworks\ncan achieve higher accuracy in lesion segmentation, they require\nsubstantial pixel-level annotated data for training, which severely\nlimits the modelâ€™s generalization performance. Additionally, these\nframeworks pose more training challenges and exhibit slower\ninference speeds compared to weakly supervised segmentation\nmethods.\n5 \nSuch limitations make them impractical for clinical\n\n2. Weighted attention fusion to extract CAMs from over-\nmoothing attention maps, which is lightweight and easy to\n3.\nuppress \nfalse positives and ensure segmentation\n4.\nupervised medical image segmentation approaches for ICH\nThe rest of this paper is organized as follows. Section 2 summa-\nriz\nde\n2.1. Intracerebral Hemorrhage segmentation based on CT images\nEarly ICH segmentation studies on CT images employed statisti-\nmachine learning approaches, such as random forests and fuzzy C-\n19â€“23\nderive object localization maps from image-level labels, which\nusually only activates the most discriminative object regions with\ntation maps obtained from CAM seeds becomes the main research\napplications demanding rapid diagnosis, while also hindering their\nwidespread deployment. In clinical practice, ICH is further classi-\nfied into five sub-types, including intraventricular (IVH), intra-\nparenchymal (IPH), subarachnoid (SAH), epidural (EDH), and\nsubdural (SDH), where sub-types vary dramatically in morpholog-\nical structures and even share similar CT values with brain tissues\nin some cases,\n6 \nmaking pixel-wise ICH annotation exceptionally\nchallenging and time-consuming. Therefore, weakly supervised\nlearning, pursuing dense pixel-level segmentation based on\nimage-/slice-level labels, has become a promising solution for\nICH segmentation. Weakly supervised segmentation frameworks\nexhibit significantly lower costs in annotation, training, and infer-\nence, making them more suitable for clinical deployment. Existing\nweakly supervised approaches mainly focus on the class activation\nmaps (CAMs)\n7 \nproduced by convolutional neural networks\n(CNNs),\n8â€“10 \nto localize targets contributing to image-level labels.\nDue to the relatively limited receptive fields of CNNs, CAMs typi-\ncally localize only the most discriminative regions instead of pur-\nsuing complete segmentation. How to produce high-quality\nCAMs becomes the main focus\n11â€“13 \nof weakly supervised image\nsegmentation.\nTo complement global contextual information, vision trans-\nformer,\n14 \nwith strong capability in modeling long-range depen-\ndency, has been extensively studied in various visual tasks,\nincluding weakly supervised medical image segmentation.\n15,16\nY. Hu, Z. Yan, Z. Kuang et al. Brain Hemorrhages 6 (2025) 195â€“205\nHowever, given limited medical imaging data, training a well-\nconverged transformer presents a challenge, due to its data-\nhungry nature. The fixed anatomical structures and specific CT\nvalue distribution patterns in brain imaging contrast markedly\nwith the highly heterogeneous characteristics of natural images\nin texture, structural complexity, and other aspects. Consequently,\ncompared to natural images, Transformers processing brain CT\ndata encounter feature extraction bottlenecks due to limited diver-\nsity in input samples, manifesting as premature over-smoothing in\nattention maps, itâ€™s also a critical limitation that existing\napproaches have not adequately addressed through targeted archi-\ntectural adaptations. Consequently, directly using such imperfect\naffinity/attention maps may fail to compensate for naive CAMs in\nmedical image applications, making transformer-based weakly\nsupervised learning sub-optimal.\nIn this work, we propose a weakly supervised intracerebral\nhemorrhage (ICH) segmentation framework utilizing image-level\nannotations, where our Weighted Attention Fusion (WAF) module\naddresses the previously overlooked attention over-smoothing\nissue in prior works, while Patch Erasing Mechanism (PEM) allevi-\nates under-segmentation issues. This method achieves high-\nperformance ICH segmentation with minimal computational and\nannotation costs, enabling efficient clinical deployment while sub-\nstantially lowering the barriers for practical implementation. The\nmain contributions are summarized as follows:\n1. A novel high-performance weakly supervised framework for\nbrain hemorrhage segmentation offers a clinically viable low-\ncost solution for ICH lesion localization and segmentation.\ns\nimplement.\nPatch-erasing re-activation penalized by contrastive learning to\ns\ncompleteness.\nSuperior performance against the state-of-the-art weakly\ns\nvalidated on publicly-available datasets.\nes related works on medical image segmentation and Section 3\nscribes our approach in detail. We present a thorough evaluation\n196\nin Section 4 and ablation studies in Section 5. Section\n 6 concludes\nthis paper.\n2. Related work\ncal shape models\n17 \nand atlas-based techniques.\n18 \nAdditionally,\nmeans clustering, were explored. Recent years, CNN-based\nICH segmentation methods were proposed.\n18,23â€“25 \nICHNet\n18 \nwas\none of the first to be introduced, leveraging a multi-layer perceptron\nwith three fully connected layers for hemorrhage segmentation.\nFocusing on ICH caused by traumatic brain injury (TBI), Kuo et al.\n23\nproposed a patch-based FCN. Yu et al.\n25 \ndevelop a novel dimension\nreduction UNet(DR-UNet) segmentation model that applies the\nreduced dimensional residual convolution unit to replace the tradi-\ntional convolution layer. Based on Mask-RCNN,\n26 \nChang et al.\n24\nintroduced a 2.5D method utilizing five CT slices as multi-channel\ninputs to generate prediction outputs. However, the irregular shapes\nand complex distributions of hemorrhages pose significant chal-\nlenges for achieving high accuracy. Moreover, due to the labor-\nintensive nature of pixel-wise annotation for intracranial hemor-\nrhages, fully supervised methods often struggle to acquire sufficient\ntraining data, further limiting their performance.\n2.2. Weakly supervised segmentation using image-level labels\nExisting approaches mainly rely on class activation mapping\n7 \nto\ncoarse boundaries. Therefore, how to improve the pseudo segmen-\nfocus. One straightforward solution is adversarial erasing\n27,28,12 \nto\nmake CAM activate more undiscriminating object regions, which\nmay introduce additional training complexity. Comparatively,\nGrad-CAM\n11 \nused the gradients of any target concept, flowing into\nthe final convolution layer to produce a coarse localization map\nand highlighting important regions for predicting the concept.\nLayerCAM\n29 \nsimultaneously combined shallow and deep feature\nmaps to generate reliable CAMs. ScoreCAM\n30 \ngot rid of the depen-\ndence on gradients and achieved better visual performance and\nfairness for interpreting the decision-making process. Despite the\neffectiveness of these approaches in natural image processing,\nthere is a notable scarcity of research focusing on medical image\nsegmentation. To complement prior knowledge in medical imag-\ning,\n31 \nutilized the estimation of cancerous regions within each\nimage as an additional area constraint,\n32 \nforced the segmentation\noutput to satisfy a given region size, and Chen et al.\n33 \nintroduced\nboth category and anatomy causality into weakly supervised med-\nical image segmentation. Despite the effectiveness, such\napproaches unavoidably inherit the locality of CNNs, limiting their\nperformance ceilings.\nOwing to the capability of capturing long-range dependencies,\nvision transformer (ViT) has been widely studied over various\nvisual tasks.\n14,34 \nTherefore, to complement global information,\nViT was also introduced to weakly supervised image segmenta-\ntion\n35â€“38 \nrecently. Inspired by the fact that pair-wise dependency\nof the class token in vanilla ViT can be rearranged to a class-\nagnostic attention map, TS-CAM\n35 \nproposed to combine CAMs with\nthe attention map for more accurate object localization.\nMCTformer\n37 \nintroduced multiple class tokens to disentangle con-\ntext information among different object categories. Consequently,\nlearned attention from class tokens to patch tokens forms class-\nspecific attention maps. Specifically,\n39,38 \nproposed to address the\n\nweakly supervised image segmentation, which is typically realized\nthrough erasing.\n27,12,40 \nSpecifically, Hou et al.\n28 \nintroduced back-\nground prior knowledge, aiming to drive attention networks into\na self-erasing state to prohibit attention from spreading to unex-\nAn overview of the proposed transformer-based weakly super-\nvised medical image segmentation framework is illustrated in\nbased network trained by image-level labels. We first extract initial\nencoder and then generate refined CAMs by merging attention\ntency becomes unreliable due to the statistical distribution charac-\nspecifically, the emergence of over-smoothed attention maps. To\nA \nc \nA 1 2 N 1 3\nover-smoothing issue of ViT in weakly supervised image segmen-\ntation. In medical imaging, Qian et al.\n15 \nexplored the combination\nof transformer and multiple instance learning for weakly super-\nvised histopathology image segmentation. It should be noted such\ntransformer-based approaches are still in the early exploration\nstage and weakly supervised medical image segmentation is far\nfrom well-addressed.\n2.3. Re-activation in weakly supervised segmentation\nAs discussed above, mining more non-discriminate regions, also\nknown as re-activation, is crucial to pursue\n completeness\n in\nY. Hu, Z. Yan, Z. Kuang et al. \nBrain Hemorrhages 6 (2025) 195â€“205\npected background regions, while complementary image pairs\nwere utilized to activate different object regions in.\n41,42\n3. Method\n3.1. Overview\nFig. 1, where classification is implemented via a transformer-\nlocalization maps from class-to-patch attention in the transformer\nmaps through a weighted attention fusion (WAF) module. The final\nCAMs are produced through the refinement process of patch-\nerasing re-activation.\n3.2. Weighted attention fusion\nPrevious methods (e.g.MCT-Former,\n37 \nToCo\n38\n) optimize class\nactivation maps (CAMs) by leveraging the consistency between\nattention maps and semantic affinity maps.\n36 \nAs shown in Fig. 2,\nexperimental results on the ICH dataset reveal that this consis-\nFig. 1. Overview of the proposed weakly supervised framework, where L is the number of transformer blocks in ViT and H is the number of self-attention heads in each\ntransformer block. Black arrows represent the initial input process, while blue arrows depict the re-activation process.\n197\nteristics and diversity limitations of brain imaging dataâ€”\nfully utilize these over-smoothed\n attention maps, we propose\nWAF, a method that improves CAM quality without modifying\nmodel architecture or increasing parameters. A vanilla ViT encoder\nis trained by image-level classification labels to capture the long-\nrange dependencies across patch tokens. Specifically, given one\ntraining CT slice, it is split and tokenized into \npatch tokens\nand then combined with a learnable class token to form a sequence\nof input tokens \nwhere d is the embedding dimen-\nsion. Denoting \nas the input of the l\n-th transformer\nblock, refined tokens \nafter self-attention in the\n l-th transformer\nblock is computed as:\nT \nl \nSoftmax \nT \nl \nh\nl\nq \nT \nl \nh\nl\nk\nT\nd \nT \nl\nh\nl\nv\nA \nl \nT \nl\nh\nl\nv\n1\nwhere and denote the projection matrices to and V\nrespectively in ViT, T is the matrix transpose operator, and\ns the sum of the attention maps of all heads in\nthe l-th transformer block. To simultaneously acquire both high-\nlevel discriminative representations and low-level visual informa-\ntion, attention maps from all transformer layers are aggregated by\nA \n1\nL\nL\nl\nA \nl \n2\nwhere L is the total number of transformer blocks. Based on A, class\nattention A\nc \ncontributing to image-level labels is extracted by\n2\nN \nN\nT\ninput \nR \n1 N\n2 \nd\n,\nT\nl \nR \n1 N\n2 \nd\nT \nl\nh\nl\nq \nh\nl\nk \nh\nl\nv \nQ K,\nA\nl \nR \n1 N\n2 \n1 N\n2\ni\n\nand the initial CAMs can be obtained by\nover-smoothing attention matrices without introducing additional\nA kA 1 k A 9\nthe poor performance of the baseline method TSCAM, \nas can be\nclearly observed in Fig. 4. Inspired by re-activation, we introduce\nadversarial erasure and design a patch-erasing mechanism. Given\nan input image processed through the ViT encoder and the WAF\nFig. 2. Attention maps generated by different layers of ViT in natural images and brain CT scans.\nY. Hu, Z. Yan, Z. Kuang et al. Brain Hemorrhages 6 (2025) 195â€“205\nA\ncls \nArrange A \nc \n4\nwhere is the operator used to reshape the matrix to\nIn addition to class attention of the class token, pat to-\npatch att \non is valuable as it indicates the importance of image\npatches in long-range dependence.\n43 \nThus, we further extract\npatch-to-patch attention \nby\nA A 2 N\n2 \n1 2 N\n2 \n1 5\np2p\nIt should be noted that token-wise/patch-to-patch attention in ViT\nis sparse with particular patterns even in shallow layers on medical\nimages, denoted as over-smoothing.\n39,38 \nIn other words, simply\nrelying on attention maps of partial tokens may be useless or even\ncounter-productive. Therefore, we propose a weighted attention\nfusion (WAF) module to discover the tremendous potential of such\ncomputational complexity. Specifically, given \nas each row rep-\nresents the attention scores of a specific patch/token to all other\npatches/tokens, we can obtain \npatch-attention maps by reshap-\ning as shown in Fig. 3. Then, the attention or importance of\n i-th\nimage patch, denoted as \nis computed by\nA \ni\np2p \nArrange A \np2p \ni 1 N\n2 \n6\nIt is observed that, penalized by classification, all patches tend to\nactivate foreground patches in self-attention. By aggregating atten-\ntion maps of all patches, it is possible to further localize the fore-\nground regions. In WAF, we first derive the fusion weights W of\nall patches by\nW Normalize\nN\n2\nj 1\nA \nj\np2p \n1 N \n7\nwhere denotes column-wise summation. Then, the refined\nattention map \ns calculated by\nA\nwaf\nN\n2\ni 1\nW \ni \nA \ni\np2p \n8\nwhere is the fusion weight of the i-th patch/to-\nken, and the final refined CAMs could be obtained by\n198\nrefine cls waf\nwhere s a balancing parameter.\nThrough the aforementioned process, the semantic information\ncontained in the over-smoothed attention maps can be utilized to\nfurther optimize the class activation maps. Existing methods, such\nas MCTformer\n37 \nand ToCo,\n38 \ndo not account for the over-smoothing\nissue in ViT when processing medical images, which significantly\nlimits the effectiveness of their optimization strategies.\n3.3. Patch-erasing re-activation\nDespite the effectiveness of WAF in obtaining enhanced CAMs\ndirectly from over-smoothing attention maps, itâ€™s still possible to\noverlook less discriminative regions. Since ViT performs feature\nextraction by dividing patches, obtaining high-resolution CAMs\nrequires a larger number of patches. However, an increased num-\nber of patches reduces the efficiency of ViTâ€™s attention mechanism,\nleading to under-segmentation issues. This is the main reason for\n35\nmodule, a sequence of output patch tokens \nand the refine\nCAMs \nare obtained. With the guidance of\n \nthe most con-\nfident foreground patches could be effectively localized for subse-\nquent patch-erasing. To prevent the dispersion of attention maps\ninto the background during patch-erasing process, thus resulting\nin a decline in the localization capability of the initial attention\ngenerator, the foreground patches are calculated by:\nA\naux \nA\ncls \nA\nwaf \n10\nwhere denotes element-wise multiplication. As shown in Fig. 1,\nduring re-activation, is binarized and used as a mask for\npatch-erasing. After erasing, the masked image is once again used\nas input to the ViT encoder, producing another series of patch\ntokens F\naux \nR\nN\n2 \nD \nand a newly refined attention map A \naux \naccord-\ning to Eq. 10. It is evident that, as shown in Fig. 1, A \naux \nwill activate\npotential regions complementary to A\naux \n. By minimizing the feature\ndifference between A \naux \nand A\naux \n, representations of entire fore-\nground regions would be more consistent, making it possible to\nachieve completeness in segmentation. In addition, maximizing\nthe feature difference between foreground and background regions\nhelps alleviate false positives. To accomplish this, given A \naux \nand\nA\naux \n, we obtain paired foreground tokens p and q from F and\nch-\nenti\nA\ncls\nArrange\nN N.\nA\np2p \nR\nN\n2 \nN\n2\nA\np2p\n,\nN\n2\nA\ni\np2p \nR\nN N \n,\n1 N\nA\nwaf \nR\nN N \ni\nW \ni \ni 1 2 N\n2\nki\nF \nR\nN\n2 \nD\nA\nWAF \nA\nwaf \n,\nA\naux\n\nFig. 3. Overview of the proposed WAF module, where L is the number of transformer blocks in ViT and H is the number of self-attention heads in each transformer block.\nq N reperents the number of the common low-activation\nregions. , and \ns represents the temperature.\nOne potential issue of such an erasing strategy is tha tention\nions will gradually expand to non-ob reg s during train-\ning. When the majority of patches belonging to the target regions\nare erased, classification scores after reactivation tend to be lower.\nY. Hu, Z. Yan, Z. Kuang et al. \nBrain Hemorrhages 6 (2025) 195â€“205\nFig. 4. Qualitative results of different approaches on intracranial hemorrhage segmentation of INSTANCE2022. WAF is short for weighted attention fusion, and PEM denotes\nthe patch-erasing re-activation mechanism.\nrespectively and paired background tokens from the shared\nlow-activation regions, and adopt an NCE loss defined as\nL\nctr \nlog\n1\nN\nq\ne \np q s\n1\nN\nq\ne \ns \n1\nN \nq\ne \np q s\n11\n199\nwhere N represents the total number of positive pairs from p and\np q\nt at\nreg ject ion\nF\naux \n,\nL\nctr\n\nvents attention leakage to background regions, addressing the lim-\nhybrid approaches on publicly-available datasets.\ngeneralization capability of the algorithm proposed in this paper,\ntion are selected for evaluation.\ndation, and testing following an 8:1:1 split ratio. All slices follow\nINSTANCE2022 for ICH segmentation contains 200 non-contrast\n0.42 mm \n0.42 mm 5 mm, hence the volume is anisotropic with\nshare the same resolution of \npixels and are\n(DSC), intersection over union (IoU), 95-th percentile of Hausdorff\nSE \nTP\nTP \nFN \n17\nSP \nTN \n18\nitives, true negatives, false positives, and false negatives\n4.4. Evaluation\nTSCAM and MCTformer, have been included for comparison.\non input images and primarily utilize feature maps of the final con-\nY. Hu, Z. Yan, Z. Kuang et al. Brain Hemorrhages 6 (2025) 195â€“205\nTo address this, we introduce a straightforward gated classification\nloss to prevent attention from focusing on unintended background\nregions. Given an input \nthe final classification loss is computed\nas:\ni i\nL\ni\ns\nL\ncls \nL\naux \nO\ni\ncls \nO\ni\naux \nO \nT\nL\ni\nO\ni\ncls \nO \nT \nand O\ni\naux \nO \nT\n2\nwhere and represent the binary cross-entropy loss for clas-\nsification, \nand are the normalized outputs before and after\npatch-erasing, and \nis a pre-defined threshold to ensure the qual-\nity of re-activation set as 0.5 in practice. The overall loss is defined\nas\nL L\ncls \nL\nctr \n13\nThe patch contrastive loss enables the PEM mechanism to mine\nmore foreground regions with just a single erasure operation, sig-\nnificantly reducing training costs compared to existing adversarial\nerasing methods such as.\n27,28 \nSimultaneously, the gated loss pre-\nitations of conventional erasure methods.\n4. Evaluation\nIn this section, we conduct extensive comparison experiments\nagainst the state-of-the-art CNN-based and transformer-based/-\n4.1. Dataset\nThe datasets, including INSTANCE2022,\n44,45 \nLocalBrainCT and\nRSNA2019\n46 \nfor intracranial hemorrhage segmentation/detection\nare selected for evaluation. To validate both the effectiveness and\nMRI-modality dataset BraTS2021\n47,48 \nfor brain tumor segmenta-\nRSNA2019 contains 752,803 CT slices each of which is anno-\ntated only with ICH sub-type labels. A subset of RSNA containing\n247,176 slices is sampled and randomly divided for training, vali-\nthe same resolution as \npixels.\nhead CT volumes of clinically diagnosed patients with different\nkinds of ICH, including subdural hemorrhage, epidural hemor-\nrhage, intraventricular hemorrhage, intraparenchymal hemor-\nrhage, and subarachnoid hemorrhage. We use the official training\nset consisting of 100 cases (3008 slices) as an independent valida-\ntion set.the pixel spacing of a CT volume is\ninter-slice resolution much smaller than the within-slice resolu-\ntion. All CT volumes share a similar resolution of\npixels where Nis in the range of\n To avoid\nunnecessary computations, we crop out the zero-value \npixels\naround CT scans, making all CT slices share the same resolution\nof \npixels.\nLocalBrainCT is one locally collected clinical dataset, which\nincludes 110 cases and 2085 slices. All the ground truth were anno-\ntated by three experienced radiologists with maximum consis-\ntency. The preprocessing of this dataset is consistent with that of\nINSTANCE2022.\nBraTS2021 contains 2,040 cases, each of which includes four 3D\nvolumes from four different MRI modalities (i.e., T1, post-contrast\nenhanced T1 (T1-CE), T2, and T2 fluid attenuated inversion recov-\nery (T2-FLAIR)) and a segmentation ground-truth mask. We split\nthe data into training and testing following.\n49 \nAll MRI volumes\ncropped to \npixels for training and evaluation. We inte-\ngrated multi-modal data into multi-channel images as input for\nmodel training and validation.\n200\nIt should mentioned that we directly adopt the ICH classifier\ntrained by RSNA2019 onto INSTANCE2022 and LocalBrainCT for\nsegmentation testing, as only INSTANCE2022 and LocalBrainCT\nhave pixel-wise annotations. The BraTS dataset includes both\npixel-level and image-level annotations, so our classification train-\ning and segmentation inference are conducted on the same data-\nset. All publicly available datasets withhold baseline information\nto protect privacy. To ensure the fairness of experiments, the four\ndatasets used in this paper rely solely on image data for experi-\nmentation. And we applied regularization preprocessing to align\nthe statistical distribution consistency across all data.\n4.2. Implementation details\nAll networks were implemented within the PyTorch framework\nand trained for 100 epochs using an Adam optimizer with an initial\nlearning rate of 5e-4 and a batch size of 64. For data augmentation,\ncontrast adjustment, random rotation, and erasing were adopted.\nIn terms of post-processing, we first normalized \nby Minâ€“\nMax normalization and then generated pseudo masks with a fixed\nthreshold of 0.5 for the evaluation of single-stage segmentation.\nWe used an 8-layers ViT as the experimental transformer back-\nbone. For multi-stage segmentation, such pseudo labels are used\nto train a U-Net\n50 \nin a fully supervised way.\n4.3. Metrics\nThe most commonly-used metrics, including Dice coefficient\ndistance (HD95), sensitivity (SE), and specificity (SP), are adopted\nfor evaluation. Detailed definitions are as follows:\nDSC X Y \n2 X Y\nX Y \n14\nIoU X Y \nX Y\nX Y \n15\nHD95 max\np95\nd X Y d Y X 16\nTN FP\nwhere X and Y denote the prediction and ground truth respectively,\nd is the Euclidean distance, and TP, TN, FP, and FN refer to true pos-\nrespectively.\n4.4.1. Approaches for comparison\nFour CNN-based weakly supervised image segmentation meth-\n51 30 2951 30 2951 30 29\nods, including GradCAM++, ScoreCAM, LayerCAM and\nCfdCAM,\n49 \nand two transformer-based approaches, including\n35 3735 37\nIn addition, nn-UNet\n52 \nunder fully supervised learning is imple-\nmented for reference.\n4.4.2. Quantitative evaluation\nQuantitative comparison results of different methods on\nINSTANCE2022 and LocalBrainCT are summarized in Tables 1 and\n2. CNN-based approaches typically perform 32 down-sampling\n1\ncl\ncls\nX\ni \n,\nL\ni\ncls \nL\ni\naux\nO\ncls \nO\naux\nO\nT\n512 \n512\n512 \n512 N 20 70 .\n4\n48 \n448\n240 240 155\n192\n 192\nA\nrefine\n\nimage of a higher resolution (e.g. \npixels), they would\nthe best-performing LayerCAM, \nthe proposed method achieves\nperformance over TSCAM\n35 \nwithout a notable increase in inference\ntime.\npreservation.\nQualitative segmentation results of different methods on\nof object localization maps by fusing attention maps from the last\nn layers in the ViT encoder as summarized in Table 6. In general,\naggregating attention maps from more layers helps capture both\nY. Hu, Z. Yan, Z. Kuang et al. \nBrain Hemorrhages 6 (2025) 195â€“205\nvolutional layer to generate CAMs. Consequently, given an input\nencounter severe information loss during the up-sampling process.\nComparatively, transformer-based approaches achieve better seg-\nmentation performance, benefiting from global perception and no\nimage down-sampling. For single-stage segmentation as summa-\nrized in Table 1, the performance gap between CNN-based and\ntransformer-based approaches is more than 13.14% on\nINSTANCE2022 and 11.12% on LocalBrainCT in DSC. Compared to\n29\nan average improvement of 28.06% and 22.63% in DSC on two data-\nsets. Compared to the SOTA transformer-based approach, i.e,\nTSCAM,\n35 \nour method obtains an average improvement of 8.14%\nand 8.21% in DSC respectively. More impor ly, given the same\nbaseline, i.e., ViT, introducing either WAF o EM (i.e., the Patch-\nErasing Mechanism) helps outperform other SOTA approaches,\ndemonstrating the effectiveness of the proposed weakly super-\nvised segmentation framework.\nMulti-stage segmentation results on INSTANCE2022 through\nretraining based on pseudo-labels are summarized in Table 2.\nBased on better pseudo labels, the proposed approach consistently\noutperforms SOTA methods across all evaluation metrics, leading\nto at least an average increase of 7.18% in DSC and approaching\nthe performance of nnU-Net under full supervision.\nQuantitative comparison results of different methods on\nBraTS2021 are summarized in Table 3. In BraTS2021, different\nbrain gliomas regions, including GD-enhancing tumor (ET), per-\nitumoral edema (ED), and necrotic and non-enhancing tumor\ncore (NET), exhibit varying characteristics, and certain regions\nmay exhibit more pronounced features in one modality (e.g. ED\nis relatively more easy-to-segment in T2-FLAIR compared to\nT1). For a fair evaluation, segmentation performance is evalu-\nated on the whole tumor (i.e., ET + ED + NET). CNN-based meth-\nods, tending to over-segment foreground regions, would activate\nmore non-salient areas, outperforming transformer-based\nmethods. Comparatively, the proposed framework consistently\noutperforms comparison approaches including those CNN-\nbased methods. It is mainly due to the patch-erasing re-\nactivation mechanism for recalling more non-salient regions\nduring classification. Compared to brain hemorrhage lesions,\nbrain tumor data exhibits richer modality information and more\ndistinct edge boundaries, enabling the proposed weakly super-\nvised approach to achieve performance comparable to that of\nfully supervised methods.\nThe training and inference speed comparisons of all methods\nare presented in Table 4. Compared with fully supervised methods\n(nnUNet,\n52 \nMask R-CNN,\n26 \nand ICHNet\n18\n), weakly supervised\napproaches require significantly lower annotation costs while\nachieving substantially faster training and inference speeds. This\nreduces the practical challenges of clinical deployment. Further-\nmore, our proposed method demonstrates improved segmentation\n4.4.3. Qualitative evaluation\nQualitative segmentation results of different methods on\nINSTANCE2022 are illustrated in Fig. 4. Consistent with previous\nanalysis, CNN-based methods tend to overestimate positive\nregions, resulting in extensive false positives. In addition, CNN-\nbased methods would suffer from poor boundary localization due\nto multi-stage down-sampling. Benefiting from the capability of\nestablishing long-range dependencies through ViT, TSCAM\n35 \nand\nMCTformer\n37 \neffectively avoid false-positive issues while localizing\nmore hemorrhage areas. However, they struggle to activate large\nTable 1\nQuantitative segmentation results of single-stage segmentation on INSTANCE2022 and LocalBrainCT dataset.\nSupervision Backbone Method INSTANCE2022 LocalBrainCT\nDSC (%) HD95 ; IoU (%) DSC (%) HD95 ; IoU (%)\nWeak \nResNet \nGrad CAM++\n51 \n34.69 52.16 22.72 36.32 54.89 25.11\nScore CAM\n30 \n26.01 67.72 16.07 31.27 57.41 28.37\nLayer CAM\n29 \n37.36 42.12 24.52 44.31 36.15 39.04\nCfd-CAM\n49 \n32.95 50.43 22.75 36.78 44.65 28.86\nViT TS-CAM\n35 \n57.28 36.76 42.22 58.73 31.67 40.93\nMCTformer\n37 \n50.50 52.79 35.61 55.43 31.56 39.96\nViT + WAF 60.86 31.93 46.27 62.91 33.51 47.14\nViT + PEM 61.64 38.84 47.48 64.01 37.13 50.14\nViT + WAF + PEM 65.42 26.35 51.57 66.94 24.93 53.05\nFull ResNet nn-Unet\n52 \n79.53 21.57 63.76 82.55 18.83 72.71\n201\nlesion regions, resulting in incomplete segmentation with low\nconfidence.\nComparatively, either WAF or PEM is beneficial for more com-\nplete segmentation. Specifically, WAF would help activate more\nforeground regions but may also introduce more false positives,\nwhile PEM could effectively suppress irrelevant regions but suffers\nfrom incompleteness. By jointly using WAF and PEM, the proposed\napproach achieves the best overall segmentation performance,\nleading to â€˜â€˜cleanerâ€ segmentation with better boundary/shape\nBraTS2021 are shown in Fig. 5. Compared to CNN-based\napproaches, our method well addresses the issue of over-\nsegmentation by suppressing irrelevant regions through weighted\nattention fusion. Compared to transformer-based methods, the\nproposed framework achieves more complete segmentation, bene-\nfiting from the patch-erasing re-activation mechanism. The perfor-\nmance of both WAF and PEM is consistent with that of\nINSTANCE2022.\n5. Discussion\n5.1. Component-wise ablation study\n5.1.1. On the effectiveness of WAF\nTo further analyze the improvements brought by weighted\nattention fusion, quantitative segmentation results of\nINSTANCE2022 under various settings are summarized in Table 5\n.\nSpecifically, \nclass attention A\ncls\n37 \nis used as the baseline, and\npseudo masks are directly used for evaluation following a fixed\nthreshold of 0.5 onto generated CAMs. When evaluating A\nwaf\ninstead of class attention, an average increase of 9.61% in DSC is\nachieved. It is observed that simply introducing WAF to ViT already\nsurpasses TSCAM as summarized in Table 1. Meanwhile, WAF may\nalso erroneously activate background regions, as illustrated in\nFig. 4. Therefore, fusing the attention maps from all transformer\nencoding layers would strike a better balance between precision\nand recall on the generated object localization maps.\nFollowing previous research,\n37 \nwe further evaluate the quality\ntant\nr P\n448 448\n\nlow-level representations and high-level semantic information,\neight layers of attention maps may introduce more false positives,\nsmaller T, more background regions may be wrongly included in\nducing WAF is beneficial to effectively fuse multi-stage attention\ntion, similar to previous observations, jointly using both WAF\nY. Hu, Z. Yan, Z. Kuang et al. Brain Hemorrhages 6 (2025) 195â€“205\nTable 2\nQuantitative segmentation results of multi-stage segmentation on INSTANCE2022 dataset.\nSupervision Backbone Method DSC (%) HD95 (%); IoU (%)\nWeak ResNet Grad CAM++\n51 \n43.05 44.61 28.74\nScore CAM\n30 \n40.81 52.14 24.34\nLayer CAM\n29 \n47.74 38.73 33.34\nCfd-CAM\n49 \n41.91 42.21 26.50\nViT TS-CAM\n35 \n65.21 25.78 52.37\nMCTformer\n37 \n60.16 36.24 46.78\nOurs 72.39 22.44 60.03\nFull ResNet nn-Unet\n52 \n79.53 21.57 63.76\nTable 3\nQuantitative segmentation results of single-stage segmentation on BraTS2021 dataset.\nSupervision Backbone Method DSC (%) HD95 (%); IoU (%)\nWeak ResNet Grad CAM++\n51 \n47.90 44.15 36.87\nScore CAM\n30 \n55.12 27.26 40.42\nLayer CAM\n29 \n63.17 22.07 48.20\nCfd-CAM\n49 \n66.51 19.51 50.46\nViT TS-CAM\n35 \n63.24 22.08 48.63\nMCTformer\n37 \n64.23 22.16 49.67\nOurs 73.33 14.24 58.97\nFull ResNet nn-Unet\n52 \n87.84 9.73 80.28\nTable 4\nTraining and inference time comparison of various methods on the INSTANCE2022 dataset.\nTrain Inference\nTime(hours) Device Time(millisecond) Device\nICHNet\n18 \n30 1 GTX 1080Ti 112 1 GTX 1080Ti\nMask-RCNN\n26 \n9 4 GTX Titan X 78 1 GTX Titan X\nnn-Unet\n52 \n7.4 1 RTX 2080Ti 41 1 RTX 2080Ti\nTS-CAM\n35 \n4 22\nours 4.5 \n23\nleading to better segmentation performance. Though merging all\nit brings performance improvement in DSC and IoU.\n5.1.2. On the effectiveness of PEM\nAs analyzed above, patch-erasing re-activation is to activate\nnon-salient regions and pursue more complete segmentation.\nGiven the same baseline, adopting patch-erasing brings even\ngreater improvement com ed to WAF, leading to an average\nincrease of 11.14% in DSC as summarized in Table 5. As demon-\nstrated in Fig. 4, PEM is capable of addressing th under-\nsegmentation issue of standard class attention without signifi-\ncantly introducing foreground over-segmentation. Therefore, when\ncoupling with WAF, it further alleviates the over-segmentation\nproblem of WAF by maximizing foreground-background differ-\nence, leading to an average increase of 14.21% in DSC compared\nto the baseline. Conclusively, jointly adopting all components\nachieves the best overall segmentation performance, demonstrat-\ning the effectiveness of each component.\nFollowing the ablation studies in Table 6, we re-conduct the\nsame ablation studies by using both WAF and PEM as summarized\nin Table 7. Empowered by PEM, the performance gap between\nusing \nand \nis further increased from 3.00% to 7.73% in\nDSC, demonstrating the value of PEM in refining attention maps\nand the necessity of jointly using WAF and PEM.\n5.2. Ablation studies on varying T for binarization\nTo analyze the potential effect of the threshold T in binarization\nfor segmentation evaluation, a series of ablation studies under var-\nious settings of T are conducted as summarized in Table 8. Given a\nsegmentation. On the contrary, given a larger T, fewer foreground\nregions can be preserved, leading to incomplete segmentation. In\ngeneral, the performance of all approaches will be improved with\nthe increase of T and gradually degrade along with the increase T\nafter a certain setting. It is noticed that, different from other\napproaches, TS-CAM achieves the best segmentation performance\ngiven the smallest T \n0 3, indicating that it tends to under-\nactivate regions and features are less discriminative. Consequently,\nregions detected by TS-CAM are of low confidence, requiring a\nsmaller T for segmentation. Comparatively, the proposed frame-\nwork is more robust to the setting of T, consistently outperforming\ncomparison methods under most settings.\n202\n5.3. Validation on more backbones\nFor a more comprehensive evaluation, we further introduce\nboth WAF and PEM to various backbones including ViT\n14 \nand Con-\nformer\n53 \nas summarized in Table 9. Across both backbones, intro-\nmaps, leading to consistent performance improvements. In addi-\nand PEM achieves the most performance improvements, with an\naverage increase of 14.92% and 14.96% in DSC for ViT and Con-\nformer respectively.\n6. Conclusion\nIn this paper, we focus on weakly supervised intracerebral hem-\norrhage (ICH) segmentation and propose a transformer-based\nframework. The framework achieves fully automated lesion local-\npar\ne\nn \n1 \nn 8\n\nweighted attention fusion module to jointly utilize token-/patch-\nre-activation mechanism.\nTable 5\nAttention WAF PEM\nU 50.50 52.79 52.69 99.92\nU U\nU U\nU U\nU U U\nization and segmentation while requiring minimal annotation\ncosts and computa nal resources compared to existing methods.\nWe introduce two key innovationsâ€”the Weighted Attention Fusion\n(WAF) module and the Patch-Erasing reactivation Mechanism\n(PEM)â€”to enhance the performance of weakly supervised models\nfor ICH segmentation. Firstly, considering the over-smoothing\nissue of vision transformers where attention maps tend to be less\nY. Hu, Z. Yan, Z. Kuang et al. \nBrain Hemorrhages 6 (2025) 195â€“205\nFig. 5. Qualitative results of different approaches on brain tumor segmentation of BraTS2021. WAF is short for weighted attention fusion, and PEM denotes the patch-erasing\nEvaluation of different object CAMs on INSTANCE2022.\nMethod DSC (%) HD95 SE (%) SP (%)\nU 60.11 31.31 58.73 97.76\n60.86 31.93 62.38 97.74\n61.64 38.84 66.39 99.93\n64.71 31.24 68.65 97.75\n65.42 26.35 70.78 97.74\nTable 6\nEvaluation of WAF by fusing the class token attention maps from the last n transformer layers on INSTANCE2022\nn Metrics\nDSC (%) HD95 SE (%) SP (%)\n8 60.11 31.31 58.73 97.76\n7 59.54 35.94 58.29 98.87\n6 58.82 41.66 57.39 98.86\n5 57.36 38.96 57.08 97.77\n4 56.70 40.97 56.97 98.86\n3 58.51 38.18 59.52 98.85\n2 51.47 64.24 62.48 93.39\n1 57.11 59.72 68.05 98.81\n203\ninformative due to relatively limited brain CT data, we propose a\nspecific dependency. It is based on the observation that most\npatches/tokens tend to build pair-wise dependency with fore-\nground regions. Secondly, to activate more non-salient class-\nrelevant regions while avoiding redundant inference processes,\nwe design a patch-erasing re-activation mechanism to recall more\ntio\n\nagainst the current state-of-the-art methods. We believe our work\nsegmentation.\nNatural Science Foundation of Hubei Province (2024AFB659), and\nConceptualization. Xianbo Deng: Writing â€“ review & editing, Visu-\nComputations were conducted on the HPC Platform of Huaz-\nRe\n1.\nTable 9\nDSC (%) HD95 SE (%) SP (%)\nViT14 50.50 52.79 52.69 99.92\nfalse negatives while suppressing false positives. Experiments con-\nducted on two intracerebral hemorrhage CT datasets and one brain\ntumor MRI dataset, including INSTANCE2022, LocalBrainCT and\nBraTS2021, demonstrate the superiority of the proposed approach\nY. Hu, Z. Yan, Z. Kuang et al. Brain Hemorrhages 6 (2025) 195â€“205\nTable 7\nEvaluation of both WAF and PEM by fusing the class token attention maps from the last n transformer layers on INSTANCE2022\nn Metrics\nDSC (%) HD95 SE (%) SP (%)\n8 64.71 31.24 68.65 97.75\n7 63.78 34.87 67.30 97.74\n6 63.54 38.60 65.58 99.94\n5 62.42 35.82 63.99 99.95\n4 61.66 33.25 63.23 99.95\n3 61.21 32.85 61.49 99.96\n2 60.94 36.53 63.09 99.94\n1 56.98 36.37 54.75 99.96\nTable 8\nQuantitative segmentation measured in i.e. DSC (%) of brain tumor segmentation on INSTANCE2022 under varying thresholds T for binarization.\nT Method\nGrad-CAM++ LayerCAM TS-CAM MCTformer WAF Ours\n0.3 22.01 19.40 57.36 45.59 45.95 49.18\n0.4 26.97 23.95 57.28 52.44 60.09 61.11\n0.5 31.63 33.40 51.99 50.50 64.71 65.28\n0.6 34.85 36.96 42.15 41.78 61.12 62.17\n0.7 34.69 37.36 29.84 29.05 49.93 52.12\n0.8 28.50 31.58 17.05 15.99 32.74 35.48\n0.9 13.94 17.57 6.67 5.54 12.06 14.25\nValidation on different backbones for weakly supervised segmentation on INSTANCE2022, including ViT\n14 \nand Conformer.\n53\nBackbone Metrics\n+WAF 60.86 31.93 62.38 97.74\n+WAF + PEM 65.42 26.35 70.78 97.74\nConformer\n53 \n47.46 18.36 43.49 99.97\n+WAF 52.09 18.81 47.79 98.88\n+WAF + PEM \n62.42 16.02 74.64 99.88\nwill shed new light on transformer-based weakly supervised ICH\nDeclaration of Competing Interest\nThe authors declare that they have no known competing finan-\ncial interests or personal relationships that could have appeared\nto influence the work reported in this paper. All the authors have\nconsented to the publication of this manuscript.\nEthical statement\nThis study utilized publicly available datasets from\nINSTANCE2022, LocalBrainCT, RSNA2019, and BraTS2021. All data\nwere anonymized and obtained in accordance with the ethical\nstandards of the original studies. Ethical approval and informed\nconsent for the original data generation were addressed by the\nrespective contributing institutions in the original publications.\nNo additional ethical approval was required for this secondary\nanalysis. All clinical practices and observations were conducted\nin accordance with the Declaration of Helsinki.\nFunding\nThis work was supported in part by grants from the National\nNatural Science Foundation of China (62271220, 62202179), the\n204\nthe Shenzhen Foundation Research Fund (2024534319).\nCRediT authorship contribution statement\nYuren Hu: Writing â€“ original draft, Methodology, Formal anal-\nysis, Data curation, Conceptualization. Zengqiang Yan: Data cura-\ntion, Conceptualization. Zhuo Kuang: Methodology, Investigation,\nalization, Supervision, Conceptualization. Li Yu: Writing â€“ review &\nediting, Visualization, Supervision, Conceptualization.\nAcknowledgments\nhong University of Science and Technology.\nferences\nVan Asch Charlotte JJ, Luitse Merel JA, Rinkel GabriÃ«l JE, van der Tweel Ingeborg,\nAlgra Ale, Klijn Catharina JM. Incidence, case fatality, and functional outcome of\nintracerebral haemorrhage over time, according to age, sex, and ethnic origin: a\nsystematic review and meta-analysis. Lancet Neurol. 2010;9(2):167â€“176.\n\n13.\n17.\n19.\n30.\n39.\n48.\n52.\n53.\net al. Conformer: local features coupling global representations for visual\nrecognition. In: Proceedings of the IEEE/CVF international conference on\ncomputer vision; 2021. p. 367â€“76.\nY. Hu, Z. Yan, Z. Kuang et al. \nBrain Hemorrhages 6 (2025) 195â€“205\n2. Qureshi AI, Palesch YY. Antihypertensive treatment \nof acute cerebral\nhemorrhage (atach) ii: design, methods, and rationale. Neurocrit Care.\n2011;15:559â€“576.\n3. Gautam Anjali, Raman Balasubramanian. Automatic segmentation of\nintracerebral hemorrhage from brain ct images\nMachine intelligence and signal\nanalysis. Springer; 2019:753â€“764.\nMa Yang, Zhang Ping, Tang Yingxin, Pan Chao, Li Gaigai, Liu Na, Yang Hu, Tang4.\nZhouping. Artificial intelligence: the dawn of a new era for cutting-edge\ntechnology based diagnosis and treatment for stroke. Brain Hemorrhages.\n2020;1(1):1â€“5.\nRasoulian Amirhossein, Salari Soorena, Xiao Yiming, et al. Weakly supervised5.\nintracranial hemorrhage segmentation using head-wise gradient-infused self-\nattention maps from a swin transformer in categorical learning. Machine\nlearning for biomedical imaging. 2023;2(MLCN 2022 special issue):338â€“360.\n6. Qureshi Adnan I, Tuhrim Stanley, Broderick Joseph P, Hunt Batjer H, Hondo\nDaniel F, Hanley Daniel F. Spontaneous intracerebral hemorrhage.\n N Engl J Med.\n2001;344(19):1450â€“1460.\n7. Zhou Bolei, Khosla Aditya, Lapedriza Agata, Oliva Aude, Torralba Antonio.\nLearning deep features for discriminative localization\nProceedings of the IEEE/\nCVF conference on computer vision and pattern recognition:2921â€“2929.\n8. Yan Zengqiang, Yang Xin, Cheng Kwang-Ting. A three-stage deep learning\nmodel for accurate retinal vessel segmentation. IEEE J Biomed Health Inform.\n2019;23(4):1427â€“1436.\n9. Shi Junjie, Yu Li, Cheng Qimin, Yang Xin, Cheng Kwang-Ting, Yan Zengqiang.\nM\n2\nftrans: modality-masked fusion transformer for incomplete multi-modality\nbrain tumor segmentation. IEEE J Biomed Health Inform. 2023;1â€“12.\n10. Xiaoben Jiang Yu, Zhu Yatong Liu, Wang Nan, Yi Lei. Mc-dc: an mlp-cnn based\ndual-path complementary network for medical image segmentation. Comput\nMethods Prog Biomed. 2023;242:107846.\nSelvaraju Ramprasaath R, Cogswell Michael, Das Abhishek, Vedantam11.\nDevi, Parikh Devi, Batra Dhruv. Grad-cam: visual explanations from deep\nnetworks via gradient-based localizationProceedings of the IEEE/CVF\ninternational conference on computer vision:618â€“626.\n12. Zhang Xiaolin, Wei Yunchao, Feng Jiashi, Yang Yi, Huang Thomas S. Adversarial\ncomplementary learning for weakly supervised object localizationProceedings of\nthe IEEE/CVF conference on computer vision and pattern recognition:1325â€“1334.\nZhang Tianqiao, Wei Qiaoqian, Li Zhenzhen, Meng Wenjing, Zhang Mengjiao,\nZhang Zhengwei. Segmentation of paracentral acute middle maculopathy\nlesions in spectral-domain optical coherence tomography images through\nweakly supervised deep convolutional networks. Comput Biol Med.\n2023;240:107632.\n14. Dosovitskiy Alexey et al.. An image is worth 16x16 words: transformers for\nimage recognition at scale\nProceedings the international conference on learning\nrepresentations.\n15. Qian Ziniu et al.. Transformer based multiple instance learning for weakly\nsupervised histopathology image segmentation\nMedical image computing and\ncomputer assisted interventionâ€“MICCAI:160â€“170.\n16. Rasoulian Amirhossein, Salari Soorena, Xiao Yiming. Weakly supervised\nintracranial hemorrhage segmentation using hierarchical combination of\nattention maps from a swin transformer. arXiv:2304.04902. 2022;63â€“72.\nGillebert Celine R, Humphreys Glyn W, Mantini Dante. Automated delineation\nof stroke lesions using brain ct images. NeuroImage: Clin. 2014;4:540â€“548.\n18. Boers AM, Zijlstra IA, Gathier CS, Van Den Berg R, Slump Cornelis H, Marquering\nCB, Majoie CB. Automatic quantification of subarachnoid hemorrhage on\nnoncontrast ct. Am J Neuroradiol\n. 2014;35(12):2279â€“2286\n.\nScherer Moritz, Cordes Jonas, Younsi Alexander, Sahin Yasemin-Aylin, GÃ¶tz\nMarkus, MÃ¶hlenbruch Markus, Stock Christian, BÃ¶sel Julian, Unterberg Andreas,\nMaier-Hein Klaus, \net al. Development and validation of \nan automatic\nsegmentation algorithm \nfor quantification of intracerebral hemorrhage.\nStroke. 2016;47(11):2776â€“2782.\nBhanu Prakash KN, \nZhou Shi, Morgan Tim C, Hanley Daniel F, Nowinski20.\nWieslaw L. Segmentation and quantification of intra-ventricular/cerebral\nhemorrhage in ct scans by modified distance regularized level set evolution\ntechnique. Int J Comput Assisted Radiol Surg 2012;7:785â€“98.\nKuang Hulin, Najm Mohamed, Menon Bijoy K, Qiu Wu. Joint segmentation of21.\nintracerebral hemorrhage and infarct from non-contrast ct images of post-\ntreatment acute ischemic stroke patientsMedical image computing and computer\nassisted interventionâ€“MICCAI. Springer; 2018:681â€“688.\nMuschelli John, Sweeney Elizabeth M, Ullman Natalie L, Vespa Paul, Hanley22.\nDaniel F, Crainiceanu Ciprian M. Pitchperfect: primary intracranial hemorrhage\nprobability estimation using random forests on ct. NeuroImage: Clin.\n2017;14:379â€“390\n.\n23. \nKuo Weicheng, HÃ¤ne Christian, Yuh Esther, Mukherjee Pratik, Malik Jitendra.\nPatchfcn for intracranial hemorrhage detection. arXiv preprint\narXiv:1806.03265; 2018.\n24. Chang Peter D, Kuoy Edward, Grinband Jack, Weinberg Brent D, Thompson\nMatthew, Homo Richelle, et al. Hybrid 3d/2d convolutional neural network for\nhemorrhage evaluation on head ct. Am J Neuroradiol 2018;39(9): 1609â€“16.\n25. Nannan Yu, He Yu, Li Haonan, Ma Nannan, Chunai Hu, Wang Jia. A robust deep\nlearning \nsegmentation method for hematoma volumetric detection in\nintracerebral hemorrhage. Stroke. 2022;53(1):167â€“176.\n26. He Kaiming, Gkioxari Georgia, DollÃ¡r Piotr, Girshick Ross. Mask r-\ncnnProceedings of the IEEE/CVF international conference on computer\nvision:2961â€“2969.\n27. Yunchao Wei, Jiashi Feng, Xiaodan Liang, Ming-Ming Cheng, Yao Zhao,\nShuicheng Yan. Object region mining with adversarial erasing: a simple\n205\nclassification to semantic segmentation approach. In: Proceedings of the IEEE/\nCVF conference on computer vision and pattern recognition 2017;p. 1568â€“76..\nHou Qibin, Jiang PengTao. Yunchao Wei, and Ming-Ming Cheng. Self-erasing28.\nnetwork for integral object attentionProceedings of the conference and workshop\non neural information processing systems:31.\nJiang Peng-Tao, Zhang Chang-Bin, Hou Qibin, Cheng Ming-Ming, Wei Yunchao.29.\nLayercam: exploring hierarchical class activation maps for localization. IEEE\nTrans Image Process. 2021;30:5875â€“5888.\nHaofan Wang, Zifan Wang, Mengnan Du, Fan Yang, Zijian Zhang, Sirui Ding,\nPiotr Mardziel, and Xia Hu. Score-CAM: Score-weighted visual explanations for\nconvolutional neural networks. In: Proceedings of the IEEE/CVF conference on\ncomputer vision and pattern recognition workshop; 2020. p. 24â€“5.\nJia Zhipeng, Huang Xingyi, Eric I, Chang Chao, Xu Yan. Constrained deep weak\n31.\nsupervision for histopathology image segmentation. IEEE Trans Med Imag.\n2017;36(11):2376â€“2388.\n32. Kervadec Hoel et al.. Constrained-CNN losses for weakly supervised\nsegmentation. Med Image Anal\n. 2019;54:88â€“99.\nChen Zhang, Tian Zhiqiang, Zhu Jihua, Li Ce, Shaoyi Du. C-cam: causal cam33.\nfor weakly supervised semantic segmentation on medical imageProceedings\nof the IEEE/CVF conference on computer vision and pattern\nrecognition:11676â€“11685.\n34. Strudel Robin, Garcia Ricardo, Laptev Ivan, Schmid Cordelia. Segmenter:\ntransformer for semantic segmentationProceedings of the IEEE/CVF\ninternational conference on computer vision:7262â€“7272.\n35. Gao Wei et al.. Ts-cam: token semantic coupled attention map for weakly\nsupervised object localizationProceedings of the IEEE/CVF international\nconference on computer vision:2886â€“2895.\nLixiang Ru, Zhan Yibing, Baosheng Yu, Bo Du. Learning affinity from attention:\n36.\nend-to-end \nweakly-supervised semantic segmentation with\ntransformersProceedings of the IEEE/CVF conference on computer vision and\npattern recognition:16846â€“16855.\nLian Xu, Ouyang Wanli, Bennamoun Mohammed, Boussaid Farid, Dan Xu.37.\nMulti-class token transformer for \nweakly supervised semantic\nsegmentationProceedings of the IEEE/CVF conference on computer vision and\npattern recognition:4310â€“4319.\n38. Lixiang Ru, Zheng Heliang, Zhan Yibing, Bo Du. Token contrast for weakly-\nsupervised semantic segmentationProceedings of the IEEE/CVF conference on\ncomputer vision and pattern recognition:3093â€“3102.\nHe Jingxuan et al. Mitigating undisciplined over-smoothing in transformer for\nweakly supervised semantic segmentation. arXiv:2305.03112; 2023.\nLee Jungbeom, Kim Eunji, Lee Sungmin, Lee Jangho, Yoon Sungroh. Ficklenet:40.\nweakly and semi-supervised semantic image segmentation using stochastic\ninferenceProceedings of the IEEE/CVF conference on computer vision and pattern\nrecognition:5267â€“5276.\n41. Zhang Fei, Chaochen Gu, Zhang Chenyue, Dai Yuchao. Complementary patch for\nweakly supervised semantic segmentationProceedings of the IEEE/CVF\ninternational conference on computer vision:7242â€“7251.\n42. Chunmeng Liu, Guangyao Li, Yao Shen, Ruiqi Wang. MECPformer: multi-\nestimations complementary patch with cnn-transformers for weakly\nsupervised semantic segmentation. arXiv:2303.10689; 2023.\n43. Liu Yuhang, Wang Han, Chen Zugang, Huangliang Kehan, Zhang Haixian.\nTransunet+: redesigning the skip connection to enhance features in medical\nimage segmentation. Knowl Based Syst. 2022;256:109859.\n44. Li Xiangyu et al.. Hematoma expansion context guided intracranial hemorrhage\nsegmentation and uncertainty estimation. IEEE J Biomed Health Inform. 2021;26\n(3):1140â€“1151\n.\n45. \nLi X et al. The state-of-the-art 3D anisotropic intracranial hemorrhage\nsegmentation on non-contrast head CT: The INSTANCE challenge.\narXiv:2301.03281; 2023.\n46. Flanders Adam E et al.. Construction of a machine learning dataset through\ncollaboration: the rsna 2019 brain ct hemorrhage challenge. Radiology.\n Artif\nIntell. 2020;2(3):e190211.\nUjjwal Baid et al. The rsna-asnr-miccai brats 2021 benchmark on brain tumor47.\nsegmentation and radiogenomic classification. arXiv:2107.02314; 2021.\nMenze Bjoern H et al.. The multimodal brain tumor image segmentation\nbenchmark (BRATS). IEEE Trans Med Imag. 2014;34(10):1993â€“2024.\n49. Yu-Jen Chen, Yiyu Shi, and Tsung-Yi Ho. A novel confidence induced class\nactivation mapping for mri brain tumor segmentation. arXiv:2306.05476;\n2023.\n50. Ronneberger Olaf, Fischer Philipp, Brox Thomas. U-net: Convolutional networks\nfor biomedical image segmentationMedical image computing and computer\nassisted interventionâ€“MICCAI:234â€“241.\nChattopadhay Aditya, Sarkar Anirban, Howlader Prantik, Balasubramanian51.\nVineeth N. Grad-cam++: generalized gradient-based visual explanations for\ndeep convolutional networksProceedings of the IEEE/CVF winter conference on\napplications of computer vision\n:839â€“847.\nIsensee Fabian et al.. nnU-Net: a self-configuring method for deep learning-\nbased biomedical image segmentation. Nat Methods. 2021;18(2):203â€“211.\nPeng Zhiliang, Huang Wei, Gu Shanzhi, Xie Lingxi, Wang Yaowei, Jiao Jianbin,",
    "version": "5.3.31"
  },
  {
    "numpages": 6,
    "numrender": 6,
    "info": {
      "PDFFormatVersion": "1.4",
      "Language": null,
      "EncryptFilterName": null,
      "IsLinearized": false,
      "IsAcroFormPresent": false,
      "IsXFAPresent": false,
      "IsCollectionPresent": false,
      "IsSignaturesPresent": false,
      "Producer": "GPL Ghostscript 9.06",
      "CreationDate": "D:20250814112407+03'00'",
      "ModDate": "D:20250814112407+03'00'",
      "Title": "Microsoft Word - ETASR_V15_N5_pp-27312-27317",
      "Creator": "PScript5.dll Version 5.2.2",
      "Author": "pc"
    },
    "metadata": {
      "xmp:modifydate": "2025-08-14T11:24:07+03:00",
      "xmp:createdate": "2025-08-14T11:24:07+03:00",
      "xmp:creatortool": "PScript5.dll Version 5.2.2",
      "dc:title": "Microsoft Word - ETASR_V15_N5_pp-27312-27317",
      "dc:creator": ["pc"]
    },
    "text": "Engineering, Technology & Applied Science Research Vol. 15, No. 5, 2025, 27312-27317 \n27312\nwww.etasr.com Omarov & Ikram: Brain Stroke Diagnosis Using Auxiliary Branch Guided Swin Transformer with â€¦\nBrain Stroke Diagnosis Using Auxiliary Branch\nGuided Swin Transformer with Pseudo-\nSegmentation Supervision\nBatyrkhan Omarov\nNarxoz University, Kazakhstan | International Information Technology University, Kazakhstan | Kh.\nDosmukhamedov Atyrau State University, Kazakhstan\nbatyahan@gmail.com (corresponding author)\nZhanseri Ikram\nNarxoz University, Kazakhstan\nzhanserikaz@gmail.com\nReceived: 8 June 2025 | Revised: 22 July 2025 | Accepted: 29 July 2025\nLicensed under a CC-BY 4.0 license | Copyright (c) by the authors | DOI: https://doi.org/10.48084/etasr.12602\nABSTRACT\nAccurate and timely diagnosis of brain stroke is critical for effective clinical intervention and long-term\npatient outcomes. In this study, a novel deep learning-based framework for automated stroke diagnosis is\nproposed, utilizing an Auxiliary Branch Guided Swin Transformer with pseudo-segmentation supervision.\nThe proposed architecture combines the hierarchical representation power of the Swin Transformer with\na parallel auxiliary segmentation branch to enhance lesion-specific attention and spatial awareness. To\naddress the scarcity of detailed annotations in clinical datasets, we employ pseudo-labels generated from\nbounding boxâ€“level supervision, enabling the model to learn lesion localization without full pixel-wise\nsegmentation masks. The model was trained and validated on the ISLES 2024 dataset, which includes\nmultimodal brain MRI scans. Quantitative results demonstrate that the proposed model achieves 94.6%\naccuracy, 94.3% precision, 94% recall, and an F1-score of 94%, outperforming existing CNN-based and\ntransformer-based approaches. The auxiliary branch not only facilitates better feature refinement but also\nimproves generalization by promoting regularization during training. This study highlights the\neffectiveness of transformer-based architectures in medical image analysis and introduces a practical\nsolution for weakly-supervised stroke detection, offering a promising tool for clinical decision support and\nautomated neuroimaging diagnostics.\nKeywords-stroke diagnosis; swin transformer; auxiliary branch; pseudo-segmentation; brain MRI; deep\nlearning; ISLES 2024\nI. INTRODUCTION\nThe rapid and accurate diagnosis of brain stroke [1] type\nand location is critical for initiating timely therapeutic\ninterventions, which directly influence clinical outcomes [2]. In\nrecent years, neuroimaging modalities, particularly Diffusion-\nWeighted Imaging (DWI) and Perfusion-Weighted Imaging\n(PWI), have become the cornerstone for ischemic stroke\ndiagnosis and tissue viability assessment [3]. However, the\ninterpretation of such multimodal datasets is both time-\nconsuming and highly dependent on expert radiological input,\nnecessitating the development of intelligent, automated\ndiagnostic tools. Artificial intelligence (AI) and, more\nspecifically, Deep Learning (DL) have transformed the\nlandscape of medical image analysis by enabling automatic\nfeature extraction and lesion localization with remarkable\naccuracy [4]. Convolutional Neural Networks (CNNs) have\ndemonstrated success in stroke lesion segmentation, but their\nlimited receptive field and inability to model long-range\ndependencies present significant limitations [5]. To overcome\nthese challenges, transformer-based architectures, such as the\nSwin Transformer, have emerged as promising alternatives by\nintegrating hierarchical feature representation with shifted\nwindows to capture contextual relationships at multiple scales\n[6]. Their ability to model global context makes them suitable\nfor complex medical tasks like ischemic stroke diagnosis.\nWhile segmentation models have achieved notable\nperformance, many require extensive pixel-wise annotations,\nwhich are scarce in real-world clinical datasets [7]. To address\nthis, pseudo-segmentation supervision has been proposed as an\neffective surrogate strategy, using weak or generated labels to\napproximate segmentation maps during training [8]. Moreover,\nauxiliary branches in neural architectures have been shown to\nenhance learning by injecting intermediate supervision and\nencouraging more discriminative feature representations [9].\nIntegrating these concepts, this study proposes an auxiliary\n\nEngineering, Technology & Applied Science Research Vol. 15, No. 5, 2025, 27312-27317 \n27313\nwww.etasr.com Omarov & Ikram: Brain Stroke Diagnosis Using Auxiliary Branch Guided Swin Transformer with â€¦\nbranch guided Swin Transformer enhanced with pseudo-\nsegmentation supervision for stroke diagnosis. The proposed\nmodel was trained and evaluated on the ISLES 2024 dataset\n[10], a benchmark for stroke lesion segmentation and outcome\nprediction using multimodal MRI. Our architecture aims to\nbridge the gap between clinical feasibility and computational\nprecision by delivering accurate lesion localization without\nrequiring exhaustive manual annotations. This study\ncontributes to the growing field of AI-assisted neuroradiology\nby demonstrating how hybrid transformer frameworks can\nyield state-of-the-art performance in challenging diagnostic\nscenarios.\nII. MATERIALS AND METHODS\nThis section outlines the experimental design, dataset\ncharacteristics, model architecture, and evaluation criteria\nemployed in this study to develop and assess the proposed\nstroke diagnosis framework. The primary objective is to\nleverage an Auxiliary Branch Guided Swin Transformer\narchitecture enhanced with pseudo-segmentation supervision to\naccurately classify and localize stroke lesions from brain MRI\ndata. This section provides a comprehensive description of the\nneural network components, the preprocessing pipeline applied\nto the ISLES 2024 dataset, and the quantitative metrics used to\nevaluate performance. All methodological choices were guided\nby clinical relevance, reproducibility, and the need for robust\ngeneralization across diverse imaging scenarios.\nA. Proposed Model Architecture\nThe architecture of the proposed model is illustrated in\nFigure 1. The input to the system is a single-channel axial slice\nof a brain MRI scan with dimensions BÃ—1Ã—224Ã—224, where B\ndenotes the batch size. The primary objective of the\narchitecture is to perform three synergistic tasks: stroke\nclassification, lesion localization, and pseudo-segmentation,\nunder a multi-task learning framework guided by an auxiliary\nbranch.\nFig. 1. Proposed Swin Transformer architecture with an auxiliary branch and pseudo-segmentation supervision.\n1) Patch Embedding and Backbone\nInitially, the input image is processed through a patch\nembedding layer consisting of a convolutional operation with a\nkernel size k=4 and stride s=4. This operation transforms the\ninput into a lower resolution feature map of shape\nBÃ—96Ã—56Ã—56, which is then flattened and fed into the Swin\nTransformer backbone. The backbone comprises four\nhierarchical stages, each containing multiple Swin Transformer\nblocks. Each block utilizes Shifted Window-based Multi-head\nSelf-Attention (SW-MSA) and Multilayer Perceptron (MLP)\nmodules, enabling the model to capture both local and global\nspatial relationships. After four stages, the output tensor has\ndimensions of BÃ—768Ã—7Ã—7, which is globally averaged to yield\na compact feature representation \n768ï‚´\nïƒ \nB\nRf .\n2) Classification and Localization Heads\nThe classification head maps the global feature vector f to a\nbinary prediction through a fully connected layer followed by a\nsigmoid activation:\nï€¨ ï€©\nccclass \nbfWy ï€«ï€½ï³\nâŒ¢ \n(1)\nwhere \n7681ï‚´\nïƒ RW\nc \n, Rb\nc \nïƒ , and\nï³ \ndenotes the sigmoid\nfunction.\n\nEngineering, Technology & Applied Science Research Vol. 15, No. 5, 2025, 27312-27317 \n27314\nwww.etasr.com Omarov & Ikram: Brain Stroke Diagnosis Using Auxiliary Branch Guided Swin Transformer with â€¦\nFor the localization task, a parallel localization head\npredicts spatial coordinates of lesion centroids or bounding\nboxes using mean squared error loss. The predicted coordinates\nn\nloc \nRy ïƒ\nâŒ¢ \n(depending on formulation) are regressed from the\nsame feature vector \nf \n, using:\nllloc \nbfWy ï€«ï€½\nâŒ¢ \n(2)\nwhere \n768ï‚´\nïƒ \nn\nl \nRW \nand \nn\nl \nRb ïƒ \n. Localization accuracy is\nsupervised using the Mean Squared Error (MSE) loss:\nï€¨ ï€©\n2\n1\n1 \nË†\nMSE\nn\ni i\ni\ny y\nn \nï€½\nï€½ ï€­\nïƒ¥ \n(3)\nwhere \ni\ny and \ni\ny\nË† \nrepresent the true and predicted coordinates,\nrespectively.\n3) \nAuxiliary Branch and Pseudo-Segmentation Head\nA distinguishing component of the proposed architecture is\nthe auxiliary branch, which taps into the output of the third\nstage of the Swin Transformer, yielding a feature map of shape\nBÃ—1Ã—224Ã—224. This is then fed into an auxiliary mask head,\nwhich generates a pseudo-segmentation mask supervised by a\nbox-guided pseudo ground truth. The segmentation loss is\ncomputed using the Dice Loss [11]:\nïƒ¥ ïƒ¥\nïƒ¥ \nï€«ï€«\nï€­ï€½\ni i \nii\ni \nii\ndice \ngp\ngp\nL\nï¥\n2\n1 \n(4)\nwhere \ni\np and \ni\ng are predicted and pseudo ground truth mask\nvalues, respectively, and\nï¥ \nis a small constant to avoid\ndivision by zero.\nThe auxiliary supervision facilitates early feature\nrefinement and enforces spatial sensitivity, helping the model\ndistinguish between lesion and non-lesion regions even with\nweak supervision.\n4) \nMulti-Task Loss Function\nThe total loss is defined as a weighted sum of the Binary\nCross-Entropy (BCE) loss for classification, the MSE loss for\nlocalization, and the Dice loss for segmentation [12]:\ndicelocbcetotal \nLLLL \n321\nï¬ï¬ï¬ ï€«ï€«ï€½ (5)\nwhere \n1\nï¬ \n, \n2\nï¬ \n, \n3\nï¬ are tunable weights controlling the\ninfluence of each task.\nThis architecture effectively combines global and local\ncontextual information while leveraging weakly labeled\nsegmentation masks to enhance lesion understanding. The\nauxiliary branch provides intermediate supervision and spatial\ncues, ultimately contributing to robust stroke diagnosis from\nmultimodal MRI data.\nB. \nDataset Description\nIn this study, the ISLES 2024 dataset was utilized. ISLES\n2024 is a benchmark collection curated for ischemic stroke\nlesion segmentation and outcome prediction tasks based on\nmultimodal Magnetic Resonance Imaging (MRI). The dataset\ncomprises pre-processed brain scans from patients with acute\nischemic stroke, incorporating key imaging modalities such as\nDiffusion Weighted Imaging (DWI), Apparent Diffusion\nCoefficient (ADC), and Time-to-maximum (Tmax) perfusion\nmaps [13-15]. Each case is presented in a standardized voxel\nresolution and spatial alignment, facilitating consistent model\ntraining and evaluation. The dataset includes voxel-wise\nannotated lesion masks, which serve as the primary ground\ntruth for segmentation benchmarking, although such labels are\noften sparse or partially available. To address the inherent\nannotation limitations and enable broader utilization of the\navailable imaging data, we employed a pseudo-segmentation\nsupervision approach, generating bounding boxâ€“guided soft\nmasks that simulate lesion boundaries and serve as weak labels\nfor the auxiliary segmentation head. The dataset was split into\ntraining (70%), validation (15%), and test (15%) subsets,\nensuring a balanced distribution of lesion sizes, locations, and\npatient demographics. All images were resampled to a common\nspatial resolution of 224Ã—224 pixels and intensity-normalized\nto the [0,1] range. Data augmentation techniques, including\nrandom horizontal and vertical flips, affine transformations,\nand Gaussian noise injection, were applied to improve the\nmodelâ€™s generalization and robustness to imaging variance\n[16]. Figure 2 presents representative samples from the dataset,\nshowcasing the diversity of lesion patterns across different\nmodalities and highlighting the complexity of ischemic stroke\ndetection. These samples illustrate the necessity for\narchitectures capable of extracting both fine-grained anatomical\ndetails and contextual information across modalities. The\nISLES 2024 dataset, due to its challenging multimodal\nstructure and clinical relevance, provides a rigorous benchmark\nto assess the efficacy of our proposed auxiliary branch guided\nSwin Transformer with pseudo-segmentation supervision.\nFig. 2. Sample multimodal MRI slices from the ISLES 2024 dataset with\ncorresponding lesion annotations.\nC. Evaluation Metrics\nTo quantitatively assess the performance of the proposed\nmodel, we employed a set of standard evaluation metrics\ntailored to each of the three core tasks: classification,\nlocalization, and segmentation. For the classification task, we\nused accuracy, precision, recall, and F1-score to evaluate the\nmodelâ€™s ability to distinguish stroke from non-stroke cases.\nThese metrics are defined as follows:\nAccuracy \nTP TN\nTP TN FP FN\nï€«\nï€½ \nï€« ï€« ï€« \n(6)\n\nEngineering, Technology & Applied Science Research Vol. 15, No. 5, 2025, 27312-27317 \n27315\nwww.etasr.com Omarov & Ikram: Brain Stroke Diagnosis Using Auxiliary Branch Guided Swin Transformer with â€¦\nPrecision \nTP\nTP FP\nï€½ \nï€« \n(7)\nRecall \nTP\nTP FN\nï€½ \nï€« \n(8)\nprecisionÃ—recall\nF1-score 2 \nprecision+recall\nï€½ ïƒ— \n(9)\nwhere \nTP, \nTN \n, FP , and \nFN \nrepresent the true positives,\ntrue negatives, false positives, and false negatives, respectively.\nThese metrics provide a comprehensive view of model\nperformance under imbalanced class distributions, as\ncommonly encountered in stroke diagnosis tasks [17].\nAdditionally, for the localization task, we employed the\nMSE [18] between the predicted and actual lesion centroid\ncoordinates. Collectively, these metrics provide a holistic\nevaluation framework for analyzing the multi-task capabilities\nof our auxiliary branch guided Swin Transformer architecture.\nIII. \nRESULTS\nThis section presents the experimental results obtained from\nevaluating the proposed Auxiliary Branch Guided Swin\nTransformer model on the ISLES 2024 dataset. The\nperformance is analyzed across multiple tasks, including stroke\nclassification, lesion localization, and pseudo-segmentation.\nQuantitative metrics such as accuracy, precision, recall, and\nF1-score were used to assess the classification effectiveness,\nwhile visual examples and loss curves are used to further\nvalidate the modelâ€™s learning behavior and diagnostic accuracy.\nComparative results against existing state-of-the-art methods\nare also discussed to demonstrate the superiority and robustness\nof the proposed approach.\nFigure 3 illustrates the model's training and testing accuracy\nover 200 epochs. The training accuracy shows a consistent\nupward trend, reaching near-perfect performance around epoch\n120, with minor oscillations. The test accuracy follows a\nsimilar trajectory, stabilizing at approximately 91%, indicating\nstrong generalization capability. The convergence behavior\nsuggests that the model effectively learns discriminative\nfeatures for stroke classification, with no signs of significant\noverfitting. The performance gap between training and test\ncurves remains narrow, supporting the robustness of the\nauxiliary branch guided Swin Transformer architecture\nenhanced with pseudo-segmentation supervision in the stroke\ndiagnosis task. Figure 4 shows the training and testing loss of\nthe proposed model over 200 epochs. It can be seen that in both\ncases the loss is almost stabilized at low values after epoch 120.\nFigure 5 showcases qualitative results from the proposed\nmodel, highlighting its capability to accurately localize stroke\nlesions in brain MRI slices. The red bounding boxes represent\nthe modelâ€™s predictions, while the green boxes denote the\nground truth annotations. In all cases, the predicted regions\nclosely align with the true lesion locations, even in instances\nwith small or diffuse ischemic areas. Predicted stroke\nprobabilities ranging from 0.88 to 0.94 indicate strong model\nconfidence. These visualizations demonstrate the effectiveness\nof the auxiliary branch and pseudo-segmentation supervision in\nenhancing spatial localization and diagnostic reliability for\nautomated stroke detection.\nFig. 3. Training and testing accuracy of the proposed model over 200\nepochs.\nFig. 4. Training and testing loss of the proposed model over 200 epochs.\nTable I presents the performance comparison of several DL\nmodels applied to stroke diagnosis, evaluated across various\ndatasets. The proposed Auxiliary Branch Guided Swin\nTransformer achieves the highest overall performance, with an\naccuracy of 94.6%, precision of 94.3%, recall of 94%, and F-\nscore of 94% on the ISLES 2024 dataset. This indicates its\nstrong capability in capturing both spatial and contextual\nfeatures relevant for stroke lesion classification. Other models,\nincluding CNN-ViT integrations, traditional CNN-based\narchitectures, and boosting machines, show lower or partial\nmetric reporting. For instance, several models reported high\naccuracyâ€”such as 90.2% and 93.3%â€”yet lacked full\ndisclosure of precision, recall, or F-score, which limits\ncomprehensive performance assessment. One hybrid CNN\nmodel demonstrated an accuracy of 81% with an F-score of\n73%, indicating a weaker balance between FP and FN. Models\nutilizing lighter architectures or ensemble techniques show\nmoderate to high recall but often compromise on precision. The\nconsistent and superior performance of the proposed model\nacross all four metrics highlights its robustness and suitability\nfor stroke diagnosis using multimodal neuroimaging, especially\nwhen benchmarked against existing methods.\n\nEngineering, Technology & Applied Science Research Vol. 15, No. 5, 2025, 27312-27317 \n27316\nwww.etasr.com Omarov & Ikram: Brain Stroke Diagnosis Using Auxiliary Branch Guided Swin Transformer with â€¦\nFig. 5. Sample stroke localization results with predicted and ground truth bounding boxes on MRI slices.\nTABLE I. PERFORMANCE COMPARISON OF DEEP LEARNING MODELS FOR STROKE DIAGNOSIS ACROSS VARIOUS DATASETS\nReference Model Dataset Accuracy Precision Recall F-score\nProposed\nAuxiliary Branch\nGuided Swin\nTransformer\nISLES 2024 94.6% 94.3% 94% 94%\n[19]\nAn integration of\nCNN and Vision\nTransformers\nOwn data\n87% for single slice-level\nprediction and 92% for\npatient-wise prediction\n- - -\n[20] CNN-based model Kaggle CT Images 81% 76% 82% 73%\n[21] VGG-16 \nMoroccan MRI\nScans \n90% - - -\n[22] ResNet50 \nMoroccan MRI\nScans \n87% - - -\n[23] \nJaccard_Residual\nSqueezeNet (CNN)\nBrain CT images\n(IoT-enhanced\npipeline)\n90.2% 91.6% 89.6% 90.6%\n[24]\nHybrid Ensemble\nDeep Learning\nModel\nOwn collected\ndataset of 10,000\nimages\n93.3% - - -\nIV. CONCLUSION\nIn this study, a novel deep learning framework for brain\nstroke diagnosis based on an Auxiliary Branch Guided Swin\nTransformer architecture integrated with pseudo-segmentation\nsupervision was proposed. The model was trained and\nevaluated in the ISLES 2024 dataset, which includes\nmultimodal MRI scans representative of real-world stroke\ncases. By combining hierarchical transformer-based feature\nextraction with an auxiliary segmentation pathway, the\nproposed model effectively captured both global contextual\ncues and fine-grained spatial patterns essential for accurate\nlesion classification and localization. The incorporation of\npseudo-labels alleviated the need for dense manual annotations,\nmaking the approach more practical for clinical deployment.\nExperimental results demonstrated that our model\noutperformed several existing deep learning methods,\nachieving high accuracy, precision, recall, and F1-score across\nmultiple evaluation settings. The auxiliary branch not only\nimproved training stability but also enhanced lesion-specific\nattention during inference. This multi-task learning strategy\nproved effective in addressing the challenges associated with\nvariable lesion morphology and low-contrast imaging artifacts.\nOverall, the proposed method offers a promising direction for\nrobust and efficient stroke diagnosis, with the potential to assist\nradiologists in early detection and treatment planning. Future\nwork will aim to validate the model on larger and more\nheterogeneous clinical datasets and explore real-time\ndeployment possibilities in hospital systems.\nACKNOWLEDGMENT\nThis work was supported by the Science Committee of the\nMinistry of Higher Education and Science of the Republic of\nKazakhstan within the framework of grant AP23489899\n\"Applying Deep Learning and Neuroimaging Methods for\nBrain Stroke Diagnosis.\"\nREFERENCES\n[1] J. Chaki and M. WoÅºniak, \"Deep Learning and Artificial Intelligence in\nAction (2019â€“2023): A Review on Brain Stroke Detection, Diagnosis,\nand Intelligent Post-Stroke Rehabilitation Management,\" IEEE Access,\nvol. 12, pp. 52161â€“52181, 2024,\nhttps://doi.org/10.1109/ACCESS.2024.3383140.\n[2] S. H. Lee et al., \"Audio-guided implicit neural representation for local\nimage stylization,\" Computational Visual Media, vol. 10, no. 6, pp.\n1185â€“1204, Sep. 2024, https://doi.org/10.1007/s41095-024-0413-5.\n[3] C.-F. Liu et al., \"Deep learning-based detection and segmentation of\ndiffusion abnormalities in acute ischemic stroke,\" Communications\nMedicine, vol. 1, no. 1, Dec. 2021, Î‘ÏÏ„. ÎÎ¿. 61,\nhttps://doi.org/10.1038/s43856-021-00062-8.\n[4] M. A. Saleem et al., \"Innovations in Stroke Identification: A Machine\nLearning-Based Diagnostic Model Using Neuroimages,\" IEEE Access,\nvol. 12, pp. 35754â€“35764, 2024,\nhttps://doi.org/10.1109/ACCESS.2024.3369673.\n[5] K. Mridha, S. Ghimire, J. Shin, A. Aran, Md. M. Uddin, and M. F.\nMridha, \"Automated Stroke Prediction Using Machine Learning: An\nExplainable and Exploratory Study With a Web Application for Early\nIntervention,\" IEEE Access, vol. 11, pp. 52288â€“52308, 2023,\nhttps://doi.org/10.1109/ACCESS.2023.3278273.\n[6] S. Altmann et al., \"Ultrafast Brain MRI with Deep Learning\nReconstruction for Suspected Acute Ischemic Stroke,\" Radiology, vol.\n\nEngineering, Technology & Applied Science Research Vol. 15, No. 5, 2025, 27312-27317 \n27317\nwww.etasr.com Omarov & Ikram: Brain Stroke Diagnosis Using Auxiliary Branch Guided Swin Transformer with â€¦\n310, no. 2, Feb. 2024, Art. no. e231938,\nhttps://doi.org/10.1148/radiol.231938.\n[7] S. R. Polamuri, \"Stroke detection in the brain using MRI and deep\nlearning models,\" Multimedia Tools and Applications, vol. 84, no. 12,\npp. 10489â€“10506, Apr. 2025, https://doi.org/10.1007/s11042-024-\n19318-1.\n[8] \nM. Rahardi, A. Aminuddin, F. F. Abdulloh, B. P. Asaddulloh, H. R.\nEnriquez, and K. Kusnawi, \"Analyzing the Impact of Data Resampling\non Stroke Prediction using Machine Learning,\" Engineering, Technology\n& Applied Science Research, vol. 15, no. 2, pp. 20790â€“20797, Apr.\n2025, https://doi.org/10.48084/etasr.9736.\n[9] \nA. A. Abujaber, Y. Imam, I. Albalkhi, S. Yaseen, A. J. Nashwan, and N.\nAkhtar, \"Utilizing machine learning to facilitate the early diagnosis of\nposterior circulation stroke,\" BMC Neurology, vol. 24, no. 1, p. 156,\nMay 2024, https://doi.org/10.1186/s12883-024-03638-8.\n[10] \"Ischemic Stroke Lesion Segmentation Challenge 2024 - Grand\nChallenge.\" [Online]. Available: https://isles-24.grand-challenge.org/.\n[11] T. Rohini and P. Praveen, \"An Intuitive Approach on Transfer Learning\nwith an IBF+IHP Model for Stroke Classification and Prediction,\"\nEngineering, Technology & Applied Science Research, vol. 15, no. 1,\npp. 19655â€“19660, Feb. 2025, https://doi.org/10.48084/etasr.9031.\n[12] \nF. Yousaf, S. Iqbal, N. Fatima, T. Kousar, and M. Shafry Mohd Rahim,\n\"Multi-class disease detection using deep learning and human brain\nmedical imaging,\" Biomedical Signal Processing and Control, vol. 85,\nAug. 2023, Art. no. 104875, https://doi.org/10.1016/j.bspc.2023.104875.\n[13] J. Wei et al., \"Deep learning-based automatic ASPECTS calculation can\nimprove diagnosis efficiency in patients with acute ischemic stroke: a\nmulticenter study,\" European Radiology, vol. 35, no. 2, pp. 627â€“639,\nFeb. 2025, https://doi.org/10.1007/s00330-024-10960-9.\n[14] \nB. Borsos, C. G. Allaart, and A. van Halteren, \"Predicting stroke\noutcome: A case for multimodal deep learning methods with tabular and\nCT Perfusion data,\" Artificial Intelligence in Medicine, vol. 147, Jan.\n2024, Art. no. 102719, https://doi.org/10.1016/j.artmed.2023.102719.\n[15] M. Anil Inamdar et al., \"A Dual-Stream Deep Learning Architecture\nWith Adaptive Random Vector Functional Link for Multi-Center\nIschemic Stroke Classification,\" IEEE Access, vol. 13, pp. 46638â€“46658,\n2025, https://doi.org/10.1109/ACCESS.2025.3550344.\n[16] Y. Yang and Y. Guo, \"Ischemic stroke outcome prediction with diversity\nfeatures from whole brain tissue using deep learning network,\" Frontiers\nin Neurology, vol. 15, 2024, Art. no. 1394879,\nhttps://doi.org/10.3389/fneur.2024.1394879.\n[17] \nM. Shakunthala and K. HelenPrabha, \"Classification of ischemic and\nhemorrhagic stroke using Enhanced-CNN deep learning technique,\"\nJournal of Intelligent & Fuzzy Systems, vol. 45, no. 4, pp. 6323â€“6338,\nOct. 2023, https://doi.org/10.3233/JIFS-230024.\n[18] S. L. J M and S. P, \"Unveiling the potential of machine learning\napproaches in predicting the emergence of stroke at its onset: a\npredicting framework,\" Scientific Reports, vol. 14, no. 1, Aug. 2024, Art.\nno. 20053, https://doi.org/10.1038/s41598-024-70354-1.\n[19] \nR. Raj, J. Mathew, S. K. Kannath, and J. Rajan, \"StrokeViT with\nAutoML for brain stroke classification,\" Engineering Applications of\nArtificial Intelligence, vol. 119, Mar. 2023, Art. no. 105772,\nhttps://doi.org/10.1016/j.engappai.2022.105772.\n[20] \n\"Deep Learning-Enabled Brain Stroke Classification on Computed\nTomography Images,\" Computers, Materials and Continua, vol. 75, no.\n1, pp. 1431â€“1446, Jan. 2023, https://doi.org/10.32604/cmc.2023.034400.\n[21] W. Abbaoui, S. Retal, S. Ziti, B. E. Bhiri, and H. Moussif, \"Ischemic\nStroke Classification Using VGG-16 Convolutional Neural Networks: A\nStudy on Moroccan MRI Scans,\" International Journal of Online and\nBiomedical Engineering (iJOE), vol. 20, no. 02, pp. 61â€“77, Feb. 2024,\nhttps://doi.org/10.3991/ijoe.v20i02.44845.\n[22] \nH. Yu et al., \"Prognosis of ischemic stroke predicted by machine\nlearning based on multi-modal MRI radiomics,\" Frontiers in Psychiatry,\nvol. 13, 2022, Art. no. 1105496,\nhttps://doi.org/10.3389/fpsyt.2022.1105496.\n[23] \nA. B. Sreekumari and A. T. Yesudasan Paulsy, \"Hybrid deep learning\nbased stroke detection using CT images with routing in an IoT\nenvironment,\" Network: Computation in Neural Systems, vol. 0, no. 0,\npp. 1â€“40, https://doi.org/10.1080/0954898X.2025.2452280.\n[24] \nR. Qasrawi et al., \"Hybrid Ensemble Deep Learning Model for\nAdvancing Ischemic Brain Stroke Detection and Classification in\nClinical Application,\" Journal of Imaging, vol. 10, no. 7, Jul. 2024, Art.\nno. 160, https://doi.org/10.3390/jimaging10070160.",
    "version": "5.3.31"
  },
  {
    "numpages": 12,
    "numrender": 12,
    "info": {
      "PDFFormatVersion": "1.6",
      "Language": null,
      "EncryptFilterName": null,
      "IsLinearized": false,
      "IsAcroFormPresent": true,
      "IsXFAPresent": false,
      "IsCollectionPresent": false,
      "IsSignaturesPresent": true,
      "Keywords": "",
      "Creator": "LaTeX with hyperref package",
      "ModDate": "D:20260203075645-08'00'",
      "CreationDate": "D:20241226192617+05'30'",
      "Producer": "Acrobat Distiller 24.0 (Windows); modified using iText 4.2.0 by 1T3XT",
      "Subject": "IET Image Processing 2025.19:e13289",
      "Custom": {
        "WPS-PROCLEVEL": "3",
        "WPS-JOURNALDOI": "10.1049/(ISSN)1751-9667",
        "WPS-ARTICLEDOI": "10.1049/ipr2.13289"
      },
      "Author": "",
      "Title": "Weakly supervised brain tumour segmentation with label propagation and level set loss"
    },
    "metadata": {
      "dc:format": "application/pdf",
      "dc:identifier": "doi:10.1049/ipr2.13289",
      "prism:doi": "10.1049/ipr2.13289",
      "prism:url": "https://doi.org/10.1049/ipr2.13289",
      "crossmark:majorversiondate": "2024-12-12",
      "crossmark:crossmarkdomainexclusive": "true",
      "crossmark:crossmarkdomains": "ietresearch.onlinelibrary.wiley.com",
      "crossmark:doi": "10.1049/ipr2.13289",
      "pdfx:crossmarkdomains": "ietresearch.onlinelibrary.wiley.com",
      "pdfx:crossmarkdomainexclusive": "true",
      "pdfx:doi": "10.1049/ipr2.13289",
      "pdfx:crossmarkmajorversiondate": "2024-12-12",
      "jav:journal_article_version": "VoR"
    },
    "text": "Received: 11 May 2024 Revised: 13 August 2024 Accepted: 25 October 2024 IET Image Processing\nDOI: 10.1049/ipr2.13289\nORIGINAL RESEARCH\nWeakly supervised brain tumour segmentation with label\npropagation and level set loss\nFatemeh-Sadat Abadian-Zadeh Mohammad Reza Mohammadi Mohsen Soryani\nSchool of Computer Engineering, Iran University of\nScience and Technology, Tehran, Iran\nCorrespondence\nMohsen Soryani, School of Computer Engineering,\nIran University of Science and Technology, Tehran,\nIran.\nEmail: soryani@iust.ac.ir\nAbstract\nEarly diagnosis of brain tumors significantly enhances treatment success. However, accu-\nrate detection and segmentation of tumors, essential for diagnosis, rely heavily on costly\nmanual annotation by experts. To mitigate these costs, weakly supervised methods have\ngained traction. This paper introduces a novel weakly supervised brain tumor segmen-\ntation approach utilizing point and scribble supervision. Experts annotate only the slice\nwith the largest tumor area by marking a single point near the tumor center or draw-\ning a scribble within the tumor region. The method operates in two phases. First, labels\nare propagated to unlabelled pixels, generating a pseudo-ground-truth with three labels:\ntumor, non-tumor, and marginal pixels (unlabelled pixels surrounding the initial segmen-\ntation). Second, a segmentation model is trained using the pseudo-ground-truth and a loss\nfunction combining level-set and binary cross-entropy losses. Marginal pixels contribute\nto level-set loss computation, refining the segmentation process. The approach is vali-\ndated on 3D magnetic resonance imaging (MRI) volumes from BraTS2020, BraTS2021,\nand BraTS2023 benchmark datasets. Experimental results show Dice scores comparable\nto fully supervised methods for whole tumor segmentation, demonstrating the effective-\nness of the proposed weakly supervised strategy. This method reduces annotation effort\nwhile maintaining competitive segmentation performance, making it valuable for clinical\napplications.\n1 INTRODUCTION\nSegmenting a brain tumour in the treatment process is crucial to\nkeep normal brain tissues safe while resecting the tumour part\n[1â€“4]. Current clinical procedures perform manual detection\nand segmentation of the tumours using MR images, which\nis time-consuming and requires expert knowledge. Multiple\ncomplimentary 3D magnetic resonance imaging (MRI) modal-\nities â€“ such as T1, T1 with contrast agent (T1ce), T2, and\nFluid Attenuation Inversion Recover (FLAIR) â€“ are obtained\nto display distinctive tissue properties and the area in which\ntumour spreads [5].\nRecently, deep learning techniques, especially convolutional\nneural networks, have become popular for automatic analysis of\nmedical images [6, 7]. These networks need a large amount of\nlabelled data for training to achieve a desired performance [8].\nThis is an open access article under the terms of the Creative Commons Attribution-NonCommercial License, which permits use, distribution and reproduction in any medium, provided\nthe original work is properly cited and is not used for commercial purposes.\nÂ© 2024 The Author(s). IET Image Processing published by John Wiley & Sons Ltd on behalf of The Institution of Engineering and Technology.\nImage annotation is very time-consuming and needs domain\nknowledge. To decrease the necessity of fully labelled data,\nsemi-supervised and weakly supervised algorithms have been\ndeveloped for image segmentation [9]. Weakly supervised learn-\ning is based on partially labelled images or image-level labels in\nsegmentation. It can reduce the dependency on full pixel/voxel\nlabelled data [10]. Recent works have combined level-set meth-\nods, which are based on the evolution of initial contours,\nwith convolutional neural networks to improve medical image\nsegmentation [11]. Distance Regularized Level Set Evolution\n(DRLSE) is a type of level-set evolution methods [12]. In ref-\nerence [13], to propagate labels through unlabelled pixels in\nultrasound images, DRLSE has been modified to an adaptive\nregion-growing task. Additionally, the energy functional of the\nactive contour methods has been utilised in deep neural network\nloss functions [14â€“17]. To the best of our knowledge, this is the\nIET Image Process. 2025;19:e13289. wileyonlinelibrary.com/iet-ipr \n1 of 12\nhttps://doi.org/10.1049/ipr2.13289\n\n2 of 12 ABADIAN-ZADEH ET AL .\nfirst paper aiming to perform 3D segmentation on brain MR\nimages with weak supervision and deep convolutional neural\nnetworks. The main contributions of this paper are:\nâˆ™ \nWe introduce a new weakly supervised method for 3D\nsegmentation of brain tumours using point and scribble\nsupervision.\nâˆ™ \nIn our method, we generate a new pseudo-ground-truth with\na marginal area for weakly labelled data (point and scribble)\nbased on region-growing methods.\nâˆ™ \nWe use a loss function based on the active contour methods\nin a novel approach.\nâˆ™ \nWe use multiple modalities of MR images to generate ini-\ntial segmentation, and we also restrict the uncertainty by\nintroducing a marginal area.\nThe paper is organised as follows. In Section 2, related work\nis presented. Section 3, describes the details of our method.\nIn Section 4, experimental results and system performance on\nBraTS benchmark datasets are given. Finally, in Section 5, the\ndiscussion and conclusion are presented and we point out the\nfuture work.\n2 RELATED WORK\n2.1 \nWeakly supervised learning\nDeep neural networks, and especially Unet [18], have made a\nsignificant contribution to medical image analysis, but their per-\nformance highly depends on the amount of fully labelled data\nprovided. In references [19, 20] different levels of supervision\nare compared according to the time and cost of annotation.\nFor example, image-level class labelling takes 20 s per sam-\nple, while pixel-level annotation takes 1.5 h. for each image.\nWeakly supervised learning aims to create robust models using\nsparse annotation and less manual effort. In image-level weakly\nsupervised segmentation, models work based on the simi-\nlarity between images of each class. Other hints, like the\nlocation of objects (bounding-box supervision), one pixel for\neach instance (point-level supervision), and a marker for each\ninstance (scribble-based supervision), can be used as weak labels\n[21].\nImage-level supervision means that a binary vector for each\ninput training image is considered, in which 1 or 0 indicates\nwhether a particular label exists in the image or not [22]. The\nmajority of research with this level of supervision employed\nClass Activation Map (CAM) as the first step in developing seg-\nmentation models. However, it is not practically applicable due\nto deriving a noisy initial segmentation proposal, especially in\n3D images [23]. To address this issue, attention CAM is intro-\nduced in reference [23]. They applied average pooling to CAMâ€™s\nslices along each dimension to learn their attention score while\ntraining. These scores indicate the importance of each slice of\nthe CAM. The CAM is used as a weak label in references [24, 25]\nfor multi-class segmentation of brain tumours. The latter used\nmultiple low-level greyscale features extracted from MR images\nwhich are fused with high-level semantic information extracted\nby a neural network as a feature map.\nScribble/RECIST is another level of supervision utilised in\nweakly supervised segmentation. In reference [26], the RECIST\nslice was used with the GrabCut method [27] to generate a\npseudo-mask. They used a weakly supervised co-segmentation\nmodel, which was an attention-based convolutional neural net-\nwork, in order to segment a pair of CT images. Furthermore,\nthey used channel attention vectors (CA) and computed the\nmean value at each spatial location over feature maps to gener-\nate spatial attention maps (SA), which was inspired by reference\n[28]. Then, Conditional Random Field (CRF) post-processing\nrefined the final result. CRF has been used for brain tumour\nsegmentation and has shown noticeable performance [29]. In\nreference [30], sparse labels are used to segment brain tumours.\nThey used probabilistic labelling to annotate unlabelled data\nusing sparse positive labels, automatically. Some post-processing\nmethods are also used in reference [30] to improve the final\nresults.\nIn reference [9], a two-step scribble-based hierarchical weakly\nsupervised model for brain tumour segmentation was pre-\nsented. In the first step, scribble pixels were augmented using\nGraph Cut, and a Unet called Unet-WT was trained to get\nthe whole tumour (WT) mask. Then, a k-means clustering\nwas applied to the 3D WT region to get the initial sub-\nregions of different parts of the tumour for the second Unet\nmodel (Unet-Sub). A novel framework to handle both nod-\nule detection and segmentation was introduced in reference\n[31], which combined Simple linear iterative clustering (SLIC).\n[32] super voxel segmentation with Convolutional Neural Net-\nwork (CNN) classification. The only supervised annotation\nprovided was the voxel coordinates for each nodule. In ref-\nerence [33], a two-stage method was proposed to segment\nnuclei in histopathology images using point-level supervi-\nsion. It used super voxels inspired by reference [28] and\ninter-cluster variance to propagate labels to unlabelled super\nvoxels.\nIn references [34] and [35] point-level annotation was used\nto segment Covid-19 CT images. The former integrated weakly\nsupervised and active learning methods and the latter used the\nadvantages of self-supervised learning. Researchers in reference\n[36] used box annotation to generate a rough segmentation to\ntrain deep networks. Then fine segmentation was obtained with\ngraph search methods and DenseCRF [37].\nHere, we use region-growing methods to generate a new\npseudo-ground-truth by expanding the initial labelled region\nand compensating for the limitation of weak labels. The pseudo-\nground-truth includes a marginal area to restrict uncertainty.\n2.2 Level-set methods\nLevel-set methods are based on showing a contour as the zero\nlevel set of a higher dimensional function (the level-set func-\ntion [LSF]) and formulating the change of the contour as the\nevolution of the LSF [12]. This idea is used in references [38,\n39] for image segmentation. In reference [12], the level-set\n17519667, 2025, 1, Downloaded from https://ietresearch.onlinelibrary.wiley.com/doi/10.1049/ipr2.13289 by Algeria Hinari NPL, Wiley Online Library on [03/02/2026]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License\n\nABADIAN-ZADEH ET AL . 3 of 12\nevolution (called DRLSE) is considered a gradient flow that\naims to minimise an energy functional with a distance regu-\nlarisation term and an external energy term. The energy term\ncontrols the movement of the zero level set into the desired\narea. In image segmentation, the energy term can be deter-\nmined through region-based [40â€“42] or edge-based [12, 43â€“45]\nimage formation.\nIn reference [13], DRLSE with edge-based energy term is\ncombined with the Gaussian mixture model (GMM) to gener-\nate pre-segmentation with given initial seeds at the centre of the\nbreast tumour. The active contour concept was used in medi-\ncal applications to improve model performance in supervised\nmethods [11, 14, 46, 47]. In some works for unsupervised vessel\nsegmentation, the loss function was modified by incorporating\nan active contour loss, leading to an enhancement in the final\nDice score [15, 48]. Here, the level-set concept is utilised in the\nloss function along with weakly supervised loss to achieve more\nprecise borders for the tumours.\nTo the best of our knowledge, we are the first to imple-\nment 3D tumour segmentation in brain MR images util-\nising point-level supervision. Existing methods of weakly\nsupervised segmentation have typically depended on sparse\nlabelling for model training. Our innovative approach takes\ninto account sparsely labelled training data, while simulta-\nneously propagating labelling information to the unlabelled\nsections of the 3D volumes. This process generates a\nnew pseudo-ground-truth, effectively addressing the limita-\ntions associated with weak labels. The marginal area plays\na crucial role in limiting uncertainty, and we integrate the\nlevel-set concept into the loss function alongside weakly super-\nvised loss to enhance the precision of tumour boundary\ndelineation.\n3 MATERIALS AND METHODS\n3.1 \nDataset\nBraTS is a multimodal MRI study that focuses on evaluat-\ning advanced methods for brain tumour segmentation. It uses\npre-processing routines like co-registration, interpolation, and\nskull stripping. All images are manually segmented by one to\nfour raters, followed by ground truth annotations approved\nby neuroradiologists [49]. BraTS 2020 uses multi-institutional\npre-operative MRI scans to segment intrinsically heterogeneous\nbrain tumours, specifically gliomas.\nWe initially used the BraTS2020 dataset to evaluate our\nproposed method. The training dataset contains 369 and the\nvalidation dataset contains 125 3D brain MRI images in 4\nmodalities: Flair, T1 weighted, T1ce weighted, and T2 weighted,\nand the ground-truth. The ground-truth images include whole,\ncore, and enhanced tumours. All volumes of the dataset are\naligned and normalised to the same space and have 240 Ã—\n240 Ã— 155 dimensions [50]. Then we repeat our experiments\non more recent datasets, BraTS2021 and BraTS2023. The last\ntwo datasets have the same modalities and image dimensions\nas BraTS2020. However, BraTS2021 contains 1251 images,\nFIGURE 1 A sample slice of a brain tumour from the BraTS dataset in\ndifferent modalities. Upper Left: T2 weighted, Upper Right: T1 weighted,\nLower Left: T1-weighted contrast enhanced, Lower Right: FLAIR, Middle:\nT1-weighted contrast enhanced with labelmap overlay. Red: Enhanced tumour\n(ET); Blue: non-enhanced tumour; Blue and Red: tumour core (TC); Yellow,\nRed, and Blue: whole tumour (WH).\nand BraTS2023 has 1470 images. We have used all training\nand validation sets of the BraTS 2020, 2021, and 2023 in\nour experiments.\nFigure 1 shows different modalities and tumour sub-regions\nfor a sample slice of the BraTS2020 dataset.\nThe tumour core (TC) includes the tumourâ€™s enhancing and\nnon-enhancing (solid) parts. The appearance of the tumour core\nis typically hypo-intense in T1ce when compared to T1. The\nwhole tumour (WT) shows full extent of the disease. It con-\ntains the TC and the edema (ED), typically depicted by the\nhyper-intense signal in FLAIR. The TC describes the mass of\nthe tumour, which is what typically cuts out [50, 51].\n3.2 The proposed weakly supervised\nsegmentation method\nHere, we present a novel method for 3D segmentation of brain\ntumour in MR images with weak labelling. This method has\ntwo steps. The first step propagates the label to unlabelled pix-\nels, and the second step trains an Unet model using the weakly\nlabelled training data (Figure 2).\nIn order to propagate labels through unlabelled pixels, we\nused the level-set method (DRLSE) presented in reference [12].\nHere, the optimisation function is an energy functional with a\ndistance regularisation term and an external energy term. Since\nregion-based models fail in images with intensity inhomogeneity\n[52], we applied the edge-based image formation to the external\nenergy term. Therefore, g which is the edge indicator of image\n17519667, 2025, 1, Downloaded from https://ietresearch.onlinelibrary.wiley.com/doi/10.1049/ipr2.13289 by Algeria Hinari NPL, Wiley Online Library on [03/02/2026]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License\n\n4 of 12 ABADIAN-ZADEH ET AL .\nFIGURE 2 Block diagram of the proposed method.\nI over domain Î© is defined as:\ng â‰œ \n1\n1 + \n|\nâˆ‡G\nğœ \nâˆ— I \n|\n2 \n, (1)\nwhere G\nğœ \nis a Gaussian filter with standard deviation ğœ which\nsmooths image I for noise reduction. The energy function is\nminimised when the zero level set is placed at the desired loca-\ntion. For the LSF ğœ™ âˆ¶ Î© â†’ â„œ the energy functional is defined\nas:\nîˆ± (ğœ™) = ğœ‡îˆ¾\np\n(ğœ™) + ğœ†îˆ¸\ng \n(ğœ™) + ğ›¼îˆ­\ng \n(ğœ™), (2)\nwhere ğœ‡, ğœ† > 0 and ğ›¼ âˆˆ â„œ are constant coefficients. ğœ™ is the\nLSF, îˆ¾\np\n(ğœ™) is the distance regularisation term, and îˆ¸\ng \n(ğœ™) and\nîˆ­\ng \n(ğœ™) are external energy terms which are defined by\nîˆ¸\ng \n(ğœ™) â‰œ \nâˆ«\nÎ©\ngğ›¿(ğœ™)|âˆ‡ğœ™|d x (3)\nand\nîˆ­\ng \n(ğœ™) â‰œ \nâˆ«\nÎ©\ngH (âˆ’ğœ™)d x, (4)\nwhere ğ›¿ and H are the Dirac delta function and the Heaviside\nfunction, respectively. îˆ¸\ng \n(ğœ™) calculates the line integral along\nzero level set and îˆ­\ng \n(ğœ™) calculates the weighted area inside lev-\nels set. According to the initial contour, which is placed inside\nor outside of the object, the coefficient of the weighted area\nterm ğ›¼ takes a negative or positive value, respectively. ğ›¿ and H\nare approximated by the following smooth functions in many\nlevel-set methods [12]:\nğ›¿ \nğœ€ \n(x) =\n{ \n1\n2ğœ€\n[\n1 + cos\n( \nğœ‹x\nğœ€\n)]\n, |x| â‰¤ ğœ€\n0, |x| > ğœ€ \n(5)\nand\nH\nğœ€ \n(x) =\nâ§\nâª\nâ¨\nâª\nâ©\n1\n2\n(\n1 + \nx\nğœ€ \n+ \n1\nğœ‹ \nsin\n( \nğœ‹x\nğœ€\n))\n, |x| â‰¤ ğœ€\n1, x > ğœ€\n0, x < âˆ’ğœ€\n. (6)\nThe parameter ğœ€ is usually set to 1.5. By replacing (5) and (6)\nin (2) and according to [12], the energy functional equation is\napproximated by:\nîˆ±\nğœ€ \n(ğœ™) = ğœ‡ \nâˆ«\nÎ©\np(|âˆ‡ğœ™|)d x + ğœ† \nâˆ«\nÎ©\ngğ›¿ \nğœ€ \n(ğœ™)|âˆ‡ğœ™|d x\n+ğ›¼ \nâˆ«\nÎ©\ngH\nğœ€ \n(âˆ’ğœ™)d x, (7)\nand can be optimised by minimising the following gradient flow:\nğœ•ğœ™\nğœ•t \n= ğœ‡ div \n(\nd \np\n(|âˆ‡ğœ™|)âˆ‡ğœ™\n) \n+ ğœ†ğ›¿ \nğœ€ \n(ğœ™) div\n(\ng \nâˆ‡ğœ™\n|âˆ‡ğœ™|\n)\n+ ğ›¼gğ›¿ \nğœ€ \n(ğœ™).\n(8)\nIn the 3D DRLSE method, initial volumes should be given and\nthey will be extended iteratively. Here, two types of volumes\nare considered. The first is constructed by extending a given\nseed point. The seed point is placed approximately at the mid-\ndle of the tumour. Then we consider a cube centred on the\n17519667, 2025, 1, Downloaded from https://ietresearch.onlinelibrary.wiley.com/doi/10.1049/ipr2.13289 by Algeria Hinari NPL, Wiley Online Library on [03/02/2026]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License\n\nABADIAN-ZADEH ET AL . 5 of 12\nALGORITHM 1 Label propagation algorithm.\nfunction Label\\_PropagationIV , Input â–¹ where IV:\ninitial point (scribble), Input: Patient MRI modalities\nb, d , w, h = size of input array â–¹ where b: number of\nmodalities, d,w,h: depth, width and height of each volume\nGenerate the initial volume (extend point or scribble)\nMasks = Zeros(b, d , w, h)\nFinal _Mask = Zeros(d , w, h)\nfor i = 1 to b do\nMasks[i] = DRLSE[IV [i]]\nend for\nfor x, y, z in Final _Mask do\nif x, y, z voxel is 1, in at least two modalities then\nFinal _Mask[x, y, z] = 1\nend if\nend for\ntd , tw\n, th = maximum dimensions of the tumour\nmargine = add _margine(td , tw, th) â–¹ the final mask is dilated\nusing a structuring element with dimensions td, tw, th which are\n20% of the largest diametres of axial, sagittal and coronal views\nof the final mask\nFinal _Mask[margine] = 255\nend Function\noriginal seed as the initial DRLSE volume. The second type\nof initial volume is produced by extending a given scribble. We\nask experts to draw a scribble inside the tumour on the slice\nwith the largest tumour area. Then, the scribble is dilated and\nrepeated in several neighbouring slices and is considered as the\ninitial DRLSE volume.\nThe initial volumes, along with four image modalities, are\ngiven to DRLSE to produce label propagation results (one\npseudo-ground-truth for each modality). In practice, manual\nlabelling, label propagation, and segmentation are done using\nall modalities, since tumour sub-regions have different visual\ncharacteristics in different modalities. We finalise the label prop-\nagation phase by selecting pixels that are specified as parts of\na tumour sub-region in at least two modalities. As we get far-\nther from the initial volume, the chance of pixels being tumours\nis reduced. Therefore, we consider a margin around the ini-\ntial tumour as an unlabelled area, and the rest of the image\nis considered non-tumour. Algorithm 1 shows the process of\npseudo-ground-truth generation.\nA sample slice of the pseudo-ground-truth for the core\ntumour is shown in Figure 3. The yellow and the blue areas\nare the results of the label propagation algorithm (tumour and\nmarginal area, respectively). After label propagation, we train\nour model with the prepared data.\nWe used the 3D Unet architecture\n1 \n[53] with some modifica-\ntions as our model (Figure 4). This network has four stages, each\nconsisting of two 3 Ã— 3 Ã— 3 convolutions which are followed\n1 \nhttps://github.com/lescientifik/open_brats2020\nFIGURE 3 Initial segmentation of a sample slice of a volume. From left\nto right: Original image, Ground-Truth core tumour (Green and red), Our\ninitial segmentation (Yellow: core tumour, Blue: margin).\nby a normalisation layer and a non-linear activation. The first\nconvolution increases the number of filters, while the second\nkeeps the output channels unchanged. Spatial downsampling\nis performed by a MaxPool layer, and filters are doubled. The\ndecoder stage is symmetrical to the encoder, and spatial upsam-\npling is done using trilinear interpolation. Shortcut connections\nbetween stages are made through concatenation. The decoder\nstage with the lowest spatial resolution has only one 3 Ã— 3 Ã— 3\nconvolution. The final convolutional layer uses a 1 Ã— 1 Ã—1 kernel\nwith one output channel and a sigmoid activation.\nThe modifications made include: one modality instead of\nfour is considered as the input and output is compared with\nthe pseudo-ground-truth generated with the weak labels (core\nor whole). The loss function is changed to level-set loss plus\nbinary cross-entropy (BCE) loss. Level-set loss is calculated for\nthe marginal area and BCE loss for the rest. A similar network\nis used for four different modalities.\nThen we utilises a more advanced network, 3D attention\nUnet, using the Convolutional Block Attention Module [54]\nwhich is added at the end of each encoder stage. Figure 5 shows\nthe block.\nWe implemented five distinct training scenarios based on the\nlevels of supervision and loss functions used and compared\ntumour segmentation results.\n1. Point-level supervision only.\n2. Point-level supervision using extended pseudo-ground-\ntruth.\n3. Scenario II plus probabilistic margin loss.\n4. Scenario II plus active contour loss.\n5. Scribble-level supervision using extended pseudo-ground\ntruth plus active contour loss.\nDetails of each scenario are as follows:\nâˆ™ \nScenario I: Point-level supervision only\nWe train our model with only one labelled point inside\nthe tumour core in order to examine the networkâ€™s capability.\nBCE is used as the loss function.\nL\nBCE \n(p, q) = âˆ’ \n1\n|I |\nâˆ‘\n[q(i ) log(p(i ))\n+(1 âˆ’ q(i )) log(1 âˆ’ p(i ))], (9)\nwhere p and q are ground truth and network outputs,\nrespectively. \n|\nI \n| \nis the set of pixels with labels (0, 1).\n17519667, 2025, 1, Downloaded from https://ietresearch.onlinelibrary.wiley.com/doi/10.1049/ipr2.13289 by Algeria Hinari NPL, Wiley Online Library on [03/02/2026]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License\n\n6 of 12 ABADIAN-ZADEH ET AL .\nFIGURE 4 The proposed modified 3D Unet architecture. One modality is considered as input and the output is compared with the pseudo-ground-truth\ngenerated with the weak labels (core or whole). The loss function is level-set loss plus BCE loss.\nFIGURE 5 Convolutional Block Attention Module [54].\nFIGURE 6 Scribble annotation for two different samples of BraTs. Blue\ncontours are tumour borders and green lines are scribbles.\nâˆ™ \nScenario II: Point-level supervision using extended pseudo-\nground-truth\nHere, we provide more labelled data (propagated labels)\nfor the model. Only tumour and non-tumour labels are\napplied in this step and margin labels are ignored (Figure 3).\nThe loss function is left as in scenario I.\nâˆ™ \nScenario III: Scenario II plus probabilistic margin loss\nHere, the margin labels are considered using a probabilis-\ntic approach. Since pixels near the DRLSE output are more\nlikely to be parts of the tumour than those farther, a higher\nweight is considered in the loss function for wrong predic-\ntions. The total loss for this scenario is the sum of the BCE\nloss for pixels with labels and Equation (10) for marginal\npixels:\nL\npro \n= \n1\n|\ndis\ni \n| \n(1 âˆ’ q \ni \n), (10)\nwhere dis\ni \nis the distance of a margin point i from the DRLSE\noutput and q \ni \nis the network output for that point. If a pixel\nnear the DRLSE output is predicted as non-tumour, the total\nloss is increased.\nâˆ™ \nScenario IV (proposed method with point-supervision):\nScenario II plus active contour loss\nSince we have used the weakly labelled ground-truth, the\nDice coefficient cannot be used as the loss function. So we\nintroduce a new loss function as (11). The loss function has\ntwo parts. The first part is for the labelled pixels where BCE\nis used. For non-labelled pixels (margin), active contour loss\nis considered.\nLoss = ğ›¾L\nBCE \n+ ğ›½L\nAC\n, (11)\nwhere ğ›¾ and ğ›½ are weights for each part. The active contour\nloss is:\nL\nAC \n= \n1\n|\nI\nu\n|\nâˆ‘\niâˆˆI\nu\n[\nğœ†\n1 \nâ‹… p(i ) â‹… \n|\ni âˆ’ m \nin\n|\n2 \n+ ğœ†\n2 \nâ‹… (1 âˆ’ p(i ))â‹…\n|\ni âˆ’ m \nout \n|\n2\n]\n, (12)\nwhere I\nu \nis the set of pixels of the margin, and Equation (12)\nis considered specifically for these pixels. m \nin \nand m \nout \nare the\n17519667, 2025, 1, Downloaded from https://ietresearch.onlinelibrary.wiley.com/doi/10.1049/ipr2.13289 by Algeria Hinari NPL, Wiley Online Library on [03/02/2026]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License\n\nABADIAN-ZADEH ET AL . 7 of 12\nmean values of the pixels in the original image, which cor-\nrespond to the pixels in the pseudo-ground-truth with labels\none and zero, respectively.\nâˆ™ \nScenario V (proposed method with scribble-supervision):\nScribble-level supervision using extended pseudo-ground-\ntruth plus active contour loss This scenario is the same as\nscenario IV, with one difference. We use more supervision by\nusing a scribble instead of a point.\n4 RESULTS\n4.1 \nInitial point/scribble selection\nFor â€˜point supervisionâ€™, since the ground-truth annotation of\nthe BraTS training dataset is available, the coordinates of the\nlabelled pixel are selected automatically. We consider slices with\nthe largest area of core and whole tumours separately. For the\ncore tumour, we place the point in the middle of the core part\nof the slice. The initial seed for the whole tumour is selected\nin two ways: either the selected seed for the core is considered\nfor the whole too, or a random point is selected in different\nwhole tumour sub-regions using a normal distribution. These\nseed points are expanded to form cubes as initial volumes\n(Algorithm 1).\nFor â€˜scribble supervisionâ€™, we asked experts to draw a scribble\non the slice with the largest tumour area on the axial view, for the\nwhole and the core tumour, separately (Figure 6). The scribbles\nare then dilated using a 2 Ã— 2 structuring element, and the results\nare extended on the three neighbouring slices in two directions\nto create the initial volumes.\n4.2 Label propagation\nDRLSE was executed for each modality with two different ini-\ntial volumes (for the core and the whole tumour). We used the\n3D version of Matlabâ€™s implementation of DRLSE,\n2 \nwhich has\nbeen developed by the authors of reference [13]. It takes 15 min\nfor all modalities of an image to generate the initial segmen-\ntation on a Core i7 CPU with 16G RAM. It certainly takes less\ntime if we use a GPU. The first step, as mentioned in Section 3.2,\nis to apply an edge detector to Image I using (1). Different\nvalues of ğœ as a smoothing factor were tested. According to\nFigure 7a, ğœ = 1.5 has the best Dice score.\nSince the DRLSE model is not sensitive to the values of ğœ†\nand ğœ‡ [12], we left the parameters (ğœ‡ = 0.2, ğœ† = 5) in (2) to\ntheir default values [12] excluding the balloon force coefficient\n(ğ›¼), which controls how much the contour expands. We tested\ndifferent ğ›¼ values in our experiments. As shown in Figure 7b,\nğ›¼ = âˆ’3 has the best Dice value for both whole and core tumour\nin BraTS dataset. Since the initial contour is placed inside the\ntumour area, negative values of ğ›¼ were tested.\nTable 1 shows the Dice similarity values of the final con-\ntours for each modality and different initial seeds. Having initial\n2 \nhttps://www.mathworks.com/matlabcentral/fileexchange/12711-level-set-for-image-\nsegmentation\nTABLE 1 Dice similarity values of DRLSE contours for each modality\nand different initial seeds for BraTS2020.\nModality Initial seeds\nWhole\ntumour (%)\nCore\ntumour (%)\nFlair Point in the\nmiddle\n40 48\nRandom point 44 âˆ’\nScribble 61 54\nT1 Point in the\nmiddle\n46 51\nRandom point 49 âˆ’\nScribble 59 53\nT1ce Point in the\nmiddle\n24 38\nRandom point 26 âˆ’\nScribble 42 48\nT2 Point in the\nmiddle\n32 46\nRandom point 34 âˆ’\nScribble 47 54\nhybrid Point in the\nmiddle\n42\nRandom point 45 âˆ’\nScribble 59 64\nsegmentation of different modalities, we considered a voting\nmethod for final segmentation. A pixel is marked as tumour if it\nis labelled as tumour in at least two modalities.\nThe â€˜hybridâ€™ rows in Table 1 show the result of the voting.\nIt is shown that voting improves the DRLSE initial seg-\nmentation for the core tumour, and therefore, we used the\nvoting method in our next experiments for the core sub-\nregion. According to the results of Table 1, the Flair modality\nis considered for the whole tumour experiments. Figure 8\ndisplays the final contours of each modality for two sample\nslices.\n4.3 Training the model\nWe implemented the proposed method (Figure 4) using\nPytorch. The models were trained from scratch on one Nvidia\nGeForce GTX 1080 Ti with 16G RAM. Each training phase\nhad 200 epochs with early stopping, and the learning rate for all\nexperiments was equal to 0.001. Each epoch took 440 s to finish.\nAs mentioned in Section 3.2 we have five scenarios. We\nstarted to train the model with T1ce modality to segment the\ncore tumour. In scenario I with point-level supervision, we\nachieved a mean Dice score of 52%. The Dice score experi-\nenced a 9.9% increase when pseudo-ground-truth was utilised in\nscenario II. Implementing the probabilistic loss in the marginal\nregion in scenario III led to an improvement of about 3.13% in\nthe Dice score.\n17519667, 2025, 1, Downloaded from https://ietresearch.onlinelibrary.wiley.com/doi/10.1049/ipr2.13289 by Algeria Hinari NPL, Wiley Online Library on [03/02/2026]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License\n\n8 of 12 ABADIAN-ZADEH ET AL .\nFIGURE 7 (a) The impact of different smoothing factors (ğœ) in segmentation by DRLSE. (b) The impact of balloon force in segmentation by DRLSE.\nTABLE 2 Dice similarity scores of the proposed point-level supervision\n(scenario IV) for core and whole tumour segmentation of BraTS2020.\nModality\nModality of\npseudo GT\nWhole\ntumour (%)\nCore\ntumour (%)\nFlair Flair 74.09 55.2\nHybrid 67.6 57.8\nT1 T1 64.7 53.7\nHybrid 63.1 69.6\nT1ce T1ce 57.4 65.2\nHybrid 56.3 70.9\nT2 T2 67.8 62.6\nHybrid 64.8 66.4\nTABLE 3 Comparison of the results of each scenario for T1ce modality\nand the core tumour of BraTS2020.\nScenario I II III IV V V + CRF\nDice score 52% 61.9% 65.03% 70.9% 75.6% 77.8%\nA 70.9% Dice score was achieved in scenario IV through the\nuse of the proposed point-level supervision method. In (11), we\nconsidered equal weights (ğ›¾ = 0.5 and ğ›½ = 0.5) for two parts of\nthe loss function. The parameters ğœ†\n1 \nand ğœ†\n2 \nwere established\nas 1 and 3, respectively, based on the information provided\nin reference [48] for (12). These values remained unchanged\nthroughout the subsequent experiments.\nFor point-level supervision, scenario IV was repeated for\ndifferent modalities to segment core and whole regions,\nseparately. For each modality, its corresponding pseudo-ground-\ntruth was considered. Finally, we trained the model with\nthe hybrid pseudo-ground-truth for all modalities. Table 2\nshows the results. It is shown that T1ce has the best Dice\nsimilarity for the core tumour using hybrid pseudo-ground-\ntruth. Although T1ce produced lower scores in DRLSE\n(Table 1), it established better results in the deep model\n(Table 2).\nWe used scribble-level supervision in scenario V. It improved\nthe Dice score to 75.6% for the core tumour. CRF post-\nprocessing increased the score to 77.8%. Table 3 summarises the\nTABLE 4 Comparison of the Dice similarity scores of the proposed\nmethods with a recent scribble-based method and a fully supervised method\nof BraTS2020.\nMethod Supervision\nCore\ntumour (%)\nWhole\ntumour (%)\n[53] Full supervision 85 91\n[9] Two scribbles 69.88 88.23\nProposed method Point level 70.9 74.09\nProposed method One scribble 75.6 88.13\nProposed method + CRF One scribble 77.8 88.38\nresults of each scenario for T1ce modality and the core tumour.\nTo segment the whole tumour with scribble-level super-\nvision, we utilised the Flair modality as the input and the\npseudo-ground-truth generated by Flair, as they yielded the\nhighest Dice score for point-level supervision. This resulted in\na Dice score of 88%. Table 4 compares the segmentation results\nof our proposed methods (for the core and the whole tumour)\nwith two recent fully supervised and weakly supervised methods\non the BraTS dataset.\nThe results presented in Table 4 demonstrate that, while [9]\nemployed two scribbles (one inside and one outside the tumour\nareas) for their annotations, we have achieved better outcomes\nfor the core and the whole using only one scribble annotation.\nMoreover, our proposed method for point-level supervision\noutperformed [9] for core tumour segmentation. Figure 9 shows\nthe segmentation results of the proposed methods for whole\nand core tumours. In Figure 10, 3D reconstruction of the\nsegmentation results is presented for the core tumour with\nscribble supervision. As can be seen, the segmentation out-\ncomes do not completely match with the ground-truth on\nborders and edges. We also report the results of using 3D\nattention Unet instead of regular 3D Unet on the BraTS2020\ndataset. Table 5 shows the final results of the Dice score\nfor the whole tumour. The achieved outcome narrows the\ndifference between our approach and the fully supervised\nmethod [53] across the whole tumour. It is our belief that\nthis progress can be credited to our innovative pseudo-ground-\ntruth, which takes into account the uncertainties present\n17519667, 2025, 1, Downloaded from https://ietresearch.onlinelibrary.wiley.com/doi/10.1049/ipr2.13289 by Algeria Hinari NPL, Wiley Online Library on [03/02/2026]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License\n\nABADIAN-ZADEH ET AL . 9 of 12\nFIGURE 8 DRLSE output for each modality and the hybrid version for\nthe core tumour in two sample slices of BraTs2020. Blue contours are\nground-truth and reds show DRLSE output.\nTABLE 5 Comparison of the Dice similarity scores of the proposed\nmethods with a fully supervised method using 3D AttUnet on BraTS2020.\nMethod Supervision\nWhole\ntumour (%)\n[53] with AttUnet Full supervision 90.37\nProposed method with\nAttUnet [53] + CRF\nScribble level (one) 89.18\nwith weakly labelled data and distinguishing it from other\nmethodologies.\nIn order to validate our method on different datasets, we\nrepeat the proposed method on BraTS2021 and BraTS2023\ndatasets for the whole tumours and obtain similar results for\nthem (Table 6).\nFIGURE 9 Segmentation results of the proposed methods for whole and\ncore tumours for a sample slice of BraTS.\nFIGURE 10 3D reconstruction of segmentation results of the proposed\nmethod for the core tumours of BraTS.\nTABLE 6 Segmentation results of the proposed method on BraTS2021\nand BraTS2023 datasets using AttUnet + CRF for the whole tumour.\nDataset Dice (scribble level) Dice (fully supervised)\nBraTS2021 90.09% 90.37% [55]\nBraTS2023 89.08% 90.3% [56]\n4.4 Computational efficiency\nAlthough the pre-processing phase of our method to gener-\nate initial segmentations is time-consuming (mentioned in the\nresult section), the other phases have reasonable times. More-\nover, the initial segmentations during the pre-processing stage\nare computed only once prior to training the models automat-\nically, eliminating the need for any human intervention. The\nexperts were requested to annotate the initial seeds (point or\nscribble) only once, with a minimal amount of time allocated\nfor this task. Then the generated initial segmentations in the pre-\nprocessing phase are used by all models for training. We should\nconsider that, after training the models, the inference time for\ntest cases is less than a second. We compare our modelâ€™s effi-\nciency with previous methods using the number of parameters\n17519667, 2025, 1, Downloaded from https://ietresearch.onlinelibrary.wiley.com/doi/10.1049/ipr2.13289 by Algeria Hinari NPL, Wiley Online Library on [03/02/2026]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License\n\n10 of 12 ABADIAN-ZADEH ET AL .\nTABLE 7 Comparison of modelsâ€™ efficiency in terms of number of\nparameters and times.\nModel # of parameters Time (ms)\n[53] (3D Unet) 34.07 M 8\n[53] (3D AttUnet) 41.25 M 12\n[9] (2D Unet) 31.04 M 4\nOurs (3D Unet) 34.07 M 7\nOurs (3D AttUnet) 41.24 M 11\nand times in Table 7. These metrics have been calculated on a\nhardware with a T4 GPU and 12G RAM.\n5 DISCUSSION AND CONCLUSION\nHere, we have presented a novel approach for segmenting brain\ntumours in MR images using a weakly supervised method. The\nmethodology comprises two primary steps: label propagation,\nwhich involves generating a pseudo-ground-truth, and segmen-\ntation. During the label propagation step, we employed an active\ncontour method called DRLSE to propagate the labelled pixels\nwithin the tumour across the unlabelled pixels. The marginal\nregion surrounding initial segmentation was designated as an\nundecided area without labels, while the remaining parts of the\nimage were labelled as non-tumour. This step was performed on\nvarious image modalities and tumour sub-regions to assess the\nimpact of modalities on the initial segmentation. Subsequently,\nwe introduced a modified 3D Unet with a new loss function\nfor tumour segmentation. This loss function consists of two\ndistinct components for labelled and marginal pixels. BCE loss\nwas utilised for labelled pixels, while the marginal region was\nsubjected to an active contour loss. We conducted different\nexperiments considering image modalities, tumour sub-regions,\ninitial seed locations, and types of supervision. To the best of\nour knowledge, this paper represents the first study that specif-\nically focuses on the 3D segmentation of tumours in brain MR\nimages using point-level supervision. Our proposed method,\nbased on point-level supervision, outperformed a state-of-the-\nart weakly supervised approach that relies on scribble-level\nsupervision in terms of segmenting the core tumour. We believe\nthat this improvement has several reasons.\nâˆ™ \nThe undecided area that the proposed method should make\na decision about was restricted by the marginal space we took\ninto account.\nâˆ™ \nSimultaneously, the loss function incorporated both super-\nvised and unsupervised elements.\nâˆ™ \nIn comparison to other modalities, the visual properties of\nthe T1ce modality stand out. The integration of these features\nwith active contour loss allowed the network to enhance its\nlearning process and make more informed decisions.\nOur approach utilises minimal labelling and outperforms\nmodels that rely on stronger labels. There is potential for\nenhancement to reach nearly supervised levels of accuracy.\nAdditionally, we can incorporate multiple modalities into our\ndeep learning model by making assumptions about the modal-\nitiesâ€™ weights in calculating the active contour component of\nthe loss function. Despite its effectiveness, our method has a\ndrawback in terms of time consumption. The label propaga-\ntion algorithm, which needs to be computed for all modalities\nand volumes of the dataset, is a time-consuming process. How-\never, it is important to note that this is only applicable during\nthe training phase of the algorithm. On the other hand, the test\nphase takes less than a second.\nAUTHOR CONTRIBUTIONS\nAll authors contributed to the study conception, design and\nanalysis. Material preparation and data collection were accom-\nplished by Fatemeh-Sadat Abadian-Zadeh. The first draft of the\nmanuscript was written by Fatemeh-Sadat Abadian-Zadeh and\nall authors commented on previous versions of the manuscript.\nAll authors read and approved the final manuscript.\nCONFLICT OF INTEREST STATEMENT\nThe authors declare no conflicts of interest.â€™\nDATA AVAILABILITY STATEMENT\nData openly available in a public repository that does not\nissue DOIs.\nORCID\nMohsen Soryani \nhttps://orcid.org/0000-0002-8555-9617\nREFERENCES\n1. Chen, W., Liu, B., Peng, S., Sun, J., Qiao, X.: S3D-UNet: separable 3D U-\nNet for brain tumor segmentation. In: International MICCAI Brainlesion\nWorkshop, pp. 358â€“368. Springer, Berlin (2018)\n2. Roy Choudhury, A., Vanguri, R., Jambawalikar, S.R., Kumar, P.: Seg-\nmentation of brain tumors using deeplabv3+. In: International\nMICCAI Brainlesion Workshop, pp. 154â€“167. Springer, Berlin\n(2018)\n3. Feng, X., Tustison, N.J., Patel, S.H., Meyer, C.H.: Brain tumor segmenta-\ntion using an ensemble of 3D U-Nets and overall survival prediction using\nradiomic features. Front. Comput. Neurosci. 14, 25 (2020)\n4. Naser, M.A., Deen, M.J.: Brain tumor segmentation and grading of lower-\ngrade glioma using deep learning in MRI images. Comput. Biol. Med. 121,\n103758 (2020)\n5. Myronenko, A.: 3D MRI brain tumor segmentation using autoencoder reg-\nularization. In: International MICCAI Brainlesion Workshop, pp. 311â€“320.\nSpringer, Berlin (2018)\n6. Litjens, G., Kooi, T., Bejnordi, B.E., Setio, A.A.A., Ciompi, F., Ghafoorian,\nM., et al.: A survey on deep learning in medical image analysis. Med. Image\nAnal. 42, 60â€“88 (2017)\n7. Zhou, C., Chen, S., Ding, C., Tao, D.: Learning contextual and\nattentive information for brain tumor segmentation. In: Interna-\ntional MICCAI brainlesion workshop, pp. 497â€“507. Springer, Berlin\n(2018)\n8. Abd Ellah, M.K., Awad, A.I., Khalaf, A.A.M., Hamed, H.F.A.: A review\non brain tumor diagnosis from mri images: Practical implications, key\nachievements, and lessons learned. Magn. Reson. Imaging 61, 300â€“318\n(2019)\n9. Ji, Z., Shen, Y., Ma, C., Gao, M.: Scribble-based hierarchical weakly super-\nvised learning for brain tumor segmentation. In: International Conference\non Medical Image Computing and Computer-Assisted Intervention, pp.\n175â€“183. Springer, Springer (2019)\n17519667, 2025, 1, Downloaded from https://ietresearch.onlinelibrary.wiley.com/doi/10.1049/ipr2.13289 by Algeria Hinari NPL, Wiley Online Library on [03/02/2026]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License\n\nABADIAN-ZADEH ET AL . 11 of 12\n10. Kervadec, H., Dolz, J., Tang, M., Granger, E., Boykov, Y., Ayed, I.B.:\nConstrained-CNN losses for weakly supervised segmentation. Med. Image\nAnal. 54, 88â€“99 (2019)\n11. Zhang, M., Dong, B., Li, Q.: Deep active contour network for medical\nimage segmentation. In: International Conference on Medical Image Com-\nputing and Computer-Assisted Intervention, pp. 321â€“331. Springer, Berlin\n(2020)\n12. Li, C., Xu, C., Gui, C., Fox, M.D.: Distance regularâ€™d level set evolution and\nits application to image segmentation. IEEE Trans. Image Process. 19(12),\n3243â€“3254 (2010)\n13. Kozegar, E., Soryani, M., Behnam, H., Salamati, M., Tan, T.: Mass segmen-\ntation in automated 3-D breast ultrasound using adaptive region growing\nand supervised edge-based deformable model. IEEE Trans. Med. Imaging\n37(4), 918â€“928 (2017)\n14. Chen, X., Williams, B.M., Vallabhaneni, S.R., Czanner, G., Williams, R.,\nZheng, Y.: Learning active contour models for medical image segmenta-\ntion. In: Proceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition, pp. 11632â€“11640 (2019)\n15. Gur, S., Wolf, L., Golgher, L., Blinder, P.: Unsupervised microvascular\nimage segmentation using an active contours mimicking neural net-\nwork. In: Proceedings of the IEEE/CVF International Conference on\nComputer Vision, pp. 10722â€“10731 (2019)\n16. Kim, B., Ye, J.C.: Mumford-Shah loss functional for image segmentation\nwith deep learning. IEEE Trans. Image Process. 29, 1856â€“1866 (2019)\n17. Kim, Y., Kim, S., Kim, T., Kim, C.: CNN-based semantic segmentation\nusing level set loss. In: 2019 IEEE Winter Conference on Applications of\nComputer Vision (WACV), pp. 1752â€“1760. IEEE, Piscataway, NJ (2019)\n18. Ronneberger, O., Fischer, P., Brox, T.: U-net: Convolutional networks for\nbiomedical image segmentation. In: International Conference on Medi-\ncal Image Computing and Computer-Assisted Intervention, pp. 234â€“241.\nSpringer, Berlin (2015)\n19. Lin, T.Y., Maire, M., Belongie, S., Hays, J., Perona, P., Ramanan, D., et al.:\nMicrosoft coco: Common objects in context. In: European Conference on\nComputer Vision, pp. 740â€“755. Springer, Berlin (2014)\n20. Cordts, M., Omran, M., Ramos, S., Rehfeld, T., Enzweiler, M., Benenson,\nR., et al.: The cityscapes dataset for semantic urban scene understanding.\nIn: Proceedings of the IEEE Conference on Computer Vision and Pattern\nRecognition, pp. 3213â€“3223 (2016)\n21. Simonyan, K., Vedaldi, A., Zisserman, A.: Deep inside convolutional\nnetworks: Visualising image classification models and saliency maps.\narXiv:13126034 (2013)\n22. Zhang, M., Zhou, Y., Zhao, J., Man, Y., Liu, B., Yao, R.: A survey of semi-\nand weakly supervised semantic segmentation of images. Artif. Intell. Rev.\n53(6), 4259â€“4288 (2020)\n23. Wu, K., Du, B., Luo, M., Wen, H., Shen, Y., Feng, J.: Weakly super-\nvised brain lesion segmentation via attentional representation learning. In:\nInternational Conference on Medical Image Computing and Computer-\nAssisted Intervention, pp. 211â€“219. Springer, Berlin (2019)\n24. Kuang, Z., Yan, Z., Yu, L.: Weakly supervised learning for multi-class med-\nical image segmentation via feature decomposition. Comput. Biol. Med.\n171, 108228 (2024)\n25. Li, Z.W., Xuan, S.B., He, X.D., Wang, L.: Global weighted average pooling\nnetwork with multilevel feature fusion for weakly supervised brain tumor\nsegmentation. IET Image Proc. 17(2), 418â€“427 (2023)\n26. Agarwal, V., Tang, Y., Xiao, J., Summers, R.M.: Weakly supervised lesion\nco-segmentation on CT scans. In: 2020 IEEE 17th International Sympo-\nsium on Biomedical Imaging (ISBI), pp. 203â€“206. IEEE, Piscataway, NJ\n(2020)\n27. Rother, C., Kolmogorov, V., Blake, A.: â€œgrabcutâ€ interactive foreground\nextraction using iterated graph cuts. ACM Trans. Graphics (TOG) 23(3),\n309â€“314 (2004)\n28. Chen, H., Huang, Y., Nakayama, H.: Semantic aware attention based deep\nobject co-segmentation. In: Asian Conference on Computer Vision, pp.\n435â€“450. Springer, Berlin (2018)\n29. Ullah, Z., Usman, M., Jeon, M., Gwak, J.: Cascade multiscale residual atten-\ntion CNNs with adaptive ROI for automatic brain tumo-r segmentation.\nInf. Sci. 608, 1541â€“1556 (2022)\n30. Wolf, D., Regnery, S., Tarnawski, R., Bobek Billewicz, B., PolaÂ´nska, J.,\nGÃ¶tz, M.: Weakly supervised learning with positive and unlabeled data for\nautomatic brain tumor segmentation. Appl. Sci. 12(21), 10763 (2022)\n31. Feng, Y., Hao, P., Zhang, P., Liu, X., Wu, F., Wang, H.: Supervoxel\nbased weakly-supervised multi-level 3d cnns for lung nodule detection and\nsegmentation. J. Ambient Intell. Hum. Comput. 1â€“11 (2019)\n32. Achanta, R., Shaji, A., Smith, K., Lucchi, A., Fua, P., SÃ¼sstrunk, S.: Slic\nsuperpixels compared to state-of-the-art superpixel methods. IEEE Trans.\nPattern Anal. Mach. Intell. 34(11), 2274â€“2282 (2012)\n33. Qu, H., Wu, P., Huang, Q., Yi, J., Riedlinger, G.M., De, S., et al.:\nWeakly supervised deep nuclei segmentation using points annotation in\nhistopathology images. In: International Conference on Medical Imaging\nwith Deep Learning, pp. 390â€“400. PMLR, New York (2019)\n34. Laradji, I., Rodriguez, P., Branchaud.Charron, F., Lensink, K.,\nAtighehchian, P., Parker, W., et al.: A weakly supervised region-based\nactive learning method for covid-19 segmentation in CT images.\narXiv:200707012 (2020)\n35. Laradji, I., Rodriguez, P., Manas, O., Lensink, K., Law, M., Kurzman,\nL., et al.: A weakly supervised consistency-based learning method for\ncovid-19 segmentation in CT images. In: Proceedings of the IEEE/CVF\nWinter Conference on Applications of Computer Vision, pp. 2453â€“2462\n(2021)\n36. Yang, L., Zhang, Y., Zhao, Z., Zheng, H., Liang, P., Ying, M.T., et al.:\nBoxnet: Deep learning based biomedical image segmentation using boxes\nonly annotation. arXiv:180600593 (2018)\n37. KrÃ¤henbÃ¼hl, P., Koltun, V.: Efficient inference in fully connected CRFs\nwith gaussian edge potentials. Adv. Neural Inf. Process. Syst. 24, 109â€“117\n(2011)\n38. Caselles, V., CattÃ©, F., Coll, T., Dibos, F.: A geometric model for active\ncontours in image processing. Numer. Math. 66(1), 1â€“31 (1993)\n39. Malladi, R., Sethian, J.A., Vemuri, B.C.: Shape modeling with front propa-\ngation: A level set approach. IEEE Trans. Pattern Anal. Mach. Intell. 17(2),\n158â€“175 (1995)\n40. Niu, S., Chen, Q., De Sisternes, L., Ji, Z., Zhou, Z., Rubin, D.L.: Robust\nnoise region-based active contour model via local similarity factor for\nimage segmentation. Pattern Recognit. 61, 104â€“119 (2017)\n41. Talu, M.F.: ORACM: Online region-based active contour model. Expert\nSyst. Appl. 40(16), 6233â€“6240 (2013)\n42. Dubrovina Karni, A., Rosman, G., Kimmel, R.: Multi-region active con-\ntours with a single level set function. IEEE Trans. Pattern Anal. Mach.\nIntell. 37(8), 1585â€“1601 (2014)\n43. Li, Q., Deng, T., Xie, W.: Active contours driven by divergence of gradient\nvector flow. Signal Process. 120, 185â€“199 (2016)\n44. Ciecholewski, M.: An edge-based active contour model using an infla-\ntion/deflation force with a damping coefficient. Expert Syst. Appl. 44,\n22â€“36 (2016)\n45. Liu, C., Liu, W., Xing, W.: An improved edge-based level set method com-\nbining local regional fitting information for noisy image segmentation.\nSignal Process. 130, 12â€“21 (2017)\n46. Zheng, Z., Oda, M., Mori, K.: Graph cuts loss to boost model accuracy\nand generalizability for medical image segmentation. In: Proceedings of\nthe IEEE/CVF International Conference on Computer Vision, pp. 3304â€“\n3313 (2021)\n47. Le, N., Bui, T., Vo Ho, V.K., Yamazaki, K., Luu, K.: Narrow band active\ncontour attention model for medical segmentation. Diagnostics 11(8),\n1393 (2021)\n48. Tang, Y., Cai, J., Yan, K., Huang, L., Xie, G., Xiao, J., et al.: Weakly-\nsupervised universal lesion segmentation with regional level set loss. In:\nInternational Conference on Medical Image Computing and Computer-\nAssisted Intervention, pp. 515â€“525. Springer, Berlin (2021)\n49. Bakas, S., Reyes, M., Jakab, A., Bauer, S., et al.: Identifying the best\nmachine learning algorithms for brain tumor segmentation, progression\nassessment, and overall survival prediction in the BRATS challenge. CoRR\nabs/1811.02629 (2018). http://arxiv.org/abs/1811.02629\n50. Menze, B.H., Jakab, A., Bauer, S., Kalpathy Cramer, J., Farahani, K., Kirby,\nJ., et al.: The multimodal brain tumor image segmentation benchmark\n(brats). IEEE Trans. Med. Imaging 34(10), 1993â€“2024 (2014)\n17519667, 2025, 1, Downloaded from https://ietresearch.onlinelibrary.wiley.com/doi/10.1049/ipr2.13289 by Algeria Hinari NPL, Wiley Online Library on [03/02/2026]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License\n\n12 of 12 ABADIAN-ZADEH ET AL .\n51. Bakas, S., Akbari, H., Sotiras, A., Bilello, M., Rozycki, M., Kirby, J.S.,\net al.: Advancing the cancer genome atlas glioma MRI collections with\nexpert segmentation labels and radiomic features. Sci. Data 4(1), 1â€“13\n(2017)\n52. Niu, S., Chen, Q., de Sisternes, L., Ji, Z., Zhou, Z., Rubin, D.:\nRobust noise region-based active contour model via local similar-\nity factor for image segmentation. Pattern Recognit. 61, 104â€“119\n(2017)\n53. Henry, T., CarrÃ©, A., Lerousseau, M., Estienne, T., Robert, C., Paragios, N.,\net al.: Brain tumor segmentation with self-ensembled, deeply-supervised\n3D U-Net neural networks: A brats 2020 challenge solution. In: Inter-\nnational MICCAI Brainlesion Workshop, pp. 327â€“339. Springer, Berlin\n(2020)\n54. Woo, S., Park, J., Lee, J., Kweon, I.S.: CBAM: Convolutional block attention\nmodule. CoRR abs/1807.06521 (2018). http://arxiv.org/abs/1807.06521\n55. Luu, H.M., Park, S.H.: Extending nn-UNet for brain tumor segmenta-\ntion. In International MICCAI brainlesion workshop, pp. 173â€“186. Cham:\nSpringer International Publishing (2022).\n56. BraTS2023. https://www.synapse.org/Synapse:syn51156910/wiki/\n622971. Accessed 27 Mar 2024\nHow to cite this article: Abadian-Zadeh, F.-S.,\nMohammadi, M.R., Soryani, M.: Weakly supervised\nbrain tumour segmentation with label propagation and\nlevel set loss. IET Image Process. 19, e13289 (2025).\nhttps://doi.org/10.1049/ipr2.13289\n17519667, 2025, 1, Downloaded from https://ietresearch.onlinelibrary.wiley.com/doi/10.1049/ipr2.13289 by Algeria Hinari NPL, Wiley Online Library on [03/02/2026]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License",
    "version": "5.3.31"
  },
  {
    "numpages": 13,
    "numrender": 13,
    "info": {
      "PDFFormatVersion": "1.7",
      "Language": "en-US",
      "EncryptFilterName": null,
      "IsLinearized": true,
      "IsAcroFormPresent": false,
      "IsXFAPresent": false,
      "IsCollectionPresent": false,
      "IsSignaturesPresent": false,
      "Creator": "Elsevier",
      "Custom": {
        "CrossMarkDomains[1]": "elsevier.com",
        "CrossmarkMajorVersionDate": "2010-04-23",
        "CreationDate--Text": "23rd May 2024",
        "ElsevierWebPDFSpecifications": "7.0",
        "robots": "noindex",
        "doi": "10.1016/j.patcog.2024.110471",
        "CrossMarkDomains[2]": "sciencedirect.com",
        "CrossmarkDomainExclusive": "true"
      },
      "ModDate": "D:20240523155103Z",
      "Author": "Hugo Oliveira",
      "Title": "Meta-learners for few-shot weakly-supervised medical image segmentation",
      "Keywords": "Meta-learning,Weakly supervised segmentation,Few-shot learning,Medical images,Domain generalization",
      "CreationDate": "D:20240523154555Z",
      "Producer": "Acrobat Distiller 8.1.0 (Windows)",
      "Subject": "Pattern Recognition, 153 (2024) 110471. doi:10.1016/j.patcog.2024.110471"
    },
    "metadata": {
      "crossmark:crossmarkdomains": "elsevier.comsciencedirect.com",
      "crossmark:crossmarkdomainexclusive": "true",
      "crossmark:doi": "10.1016/j.patcog.2024.110471",
      "crossmark:majorversiondate": "2010-04-23",
      "dc:format": "application/pdf",
      "dc:identifier": "10.1016/j.patcog.2024.110471",
      "dc:publisher": "Elsevier Ltd",
      "dc:description": "Pattern Recognition, 153 (2024) 110471. doi:10.1016/j.patcog.2024.110471",
      "dc:subject": [
        "Meta-learning",
        "Weakly supervised segmentation",
        "Few-shot learning",
        "Medical images",
        "Domain generalization"
      ],
      "dc:title": "Meta-learners for few-shot weakly-supervised medical image segmentation",
      "dc:creator": [
        "Hugo Oliveira",
        "Pedro H.T. Gama",
        "Isabelle Bloch",
        "Roberto Marcondes Cesar"
      ],
      "jav:journal_article_version": "VoR",
      "pdf:creationdate--text": "23rd May 2024",
      "pdf:producer": "Acrobat Distiller 8.1.0 (Windows)",
      "pdf:keywords": "Meta-learning,Weakly supervised segmentation,Few-shot learning,Medical images,Domain generalization",
      "pdfx:creationdate--text": "23rd May 2024",
      "pdfx:crossmarkdomains": "sciencedirect.comelsevier.com",
      "pdfx:crossmarkdomainexclusive": "true",
      "pdfx:crossmarkmajorversiondate": "2010-04-23",
      "lmfnsodmrnwunymqgmd2szcnny.mrlt7-ngegmwv.mpmmmgn.n9ipo9eqn9iknm-rotutma": "",
      "pdfx:doi": "10.1016/j.patcog.2024.110471",
      "pdfx:robots": "noindex",
      "prism:aggregationtype": "journal",
      "prism:copyright": "Â© 2024 Elsevier Ltd. All rights reserved.",
      "prism:coverdate": "2024-09-01",
      "prism:coverdisplaydate": "1 September 2024",
      "prism:doi": "10.1016/j.patcog.2024.110471",
      "prism:issn": "0031-3203",
      "prism:pagerange": "110471",
      "prism:publicationname": "Pattern Recognition",
      "prism:startingpage": "110471",
      "prism:url": "https://doi.org/10.1016/j.patcog.2024.110471",
      "prism:volume": "153",
      "tdm:policy": "https://www.elsevier.com/tdm/tdmrep-policy.json",
      "tdm:reservation": "1",
      "xmp:createdate": "2024-05-23T15:45:55",
      "xmp:creatortool": "Elsevier",
      "xmp:metadatadate": "2024-05-23T15:51:03",
      "xmp:modifydate": "2024-05-23T15:51:03",
      "xmpmm:documentid": "uuid:1b499bed-4ce8-4c0c-b682-0d58baae1cbe",
      "xmpmm:instanceid": "uuid:b6ed5266-03cb-4855-90c0-636283357f42",
      "xmprights:marked": "True"
    },
    "text": "Pattern Recognition 153 (2024) 110471\nAvailable online 8 April 2024\n0031-3203/Â© 2024 Elsevier Ltd. All rights reserved.\nContents lists available at ScienceDirect\nPattern Recognition\njournal homepage: www.elsevier.com/locate/pr\nMeta-learners for few-shot weakly-supervised medical image segmentation\nHugo Oliveira \na,b,\nâˆ—\n, Pedro H.T. Gama \nc\n, Isabelle Bloch \nd\n, Roberto Marcondes Cesar Jr. \na\na \nInstitute of Mathematics and Statistics, Universidade de SÃ£o Paulo, SÃ£o Paulo, Brazil\nb \nComputer Science Department, Universidade Federal de ViÃ§osa, ViÃ§osa, Brazil\nc \nDepartment of Computer Science, Universidade Federal de Minas Gerais, Belo Horizonte, Brazil\nd \nSorbonne UniversitÃ©, CNRS, LIP6, Paris, France\nA R T I C L E I N F O\nKeywords:\nMeta-learning\nWeakly supervised segmentation\nFew-shot learning\nMedical images\nDomain generalization\nA B S T R A C T\nMost uses of Meta-Learning in visual recognition are very often applied to image classification, with a relative\nlack of work in other tasks such as segmentation and detection. We propose a new generic Meta-Learning\nframework for few-shot weakly supervised segmentation in medical imaging domains. The proposed approach\nincludes a meta-training phase that uses a meta-dataset. It is deployed on an out-of-distribution few-shot target\ntask, where a single highly generalizable model, trained via a selective supervised loss function, is used as\na predictor. The model can be trained in several distinct ways, such as second-order optimization, metric\nlearning, and late fusion. Some relevant improvements of existing methods that are part of the proposed\napproach are presented. We conduct a comparative analysis of meta-learners from distinct paradigms adapted\nto few-shot image segmentation in different sparsely annotated radiological tasks. The imaging modalities\ninclude 2D chest, mammographic, and dental X-rays, as well as 2D slices of volumetric tomography and\nresonance images. Our experiments consider in total 9 meta-learners, 4 backbones, and multiple target organ\nsegmentation tasks. We explore small-data scenarios in radiology with varying weak annotation styles and\ndensities. Our analysis shows that metric-based meta-learning approaches achieve better segmentation results\nin tasks with smaller domain shifts compared to the meta-training datasets, while some gradient- and fusion-\nbased meta-learners are more generalizable to larger domain shifts. Guidelines learned from the comparative\nperformance assessment of the analyzed methods are summarized to support those readers interested in the\nfield.\n1. Introduction\nDespite the widespread use of deep learning in medical imaging,\nneural networks are still subject to a series of limitations that hamper\ntheir use in most real-world medical settings. The main hurdle in imple-\nmenting Deep Neural Networks (DNNs) [1] into clinical practice is the\ndata-driven nature of these models, which usually require hundreds, or\neven thousands of samples per class to fit properly, risking to suffer\nfrom underfitting in small-data scenarios. A compounding factor to\nthis problem is the presence of domain shifts in real-world cases. In\nmedical imaging, domain shift can be introduced to a task due to\nchanges in imaging equipment or settings, differences in the cohort\nof training/test samples, imaging and label modalities, etc. Regarding\nsegmentation tasks, an additional complication in using deep learning\nis the cost of producing the labels to be fed to the algorithms, as\ndense pixelwise labels are known to be expensive and require highly\nspecialized anatomical knowledge from the physician. In this case,\nsparse labels fed to weakly supervised segmentation algorithms can\nâˆ— \nCorresponding author at: Computer Science Department, Universidade Federal de ViÃ§osa, ViÃ§osa, Brazil.\nE-mail address: hugo.n.oliveira@ufv.br (H. Oliveira).\nURL: https://sites.google.com/view/oliveirahugo (H. Oliveira).\nbe a good compromise between annotation cost and performance [2].\nStill, the majority of works on one-/few-shot image segmentation do\nnot consider the case of sparsely labeled images, only focusing on the\ncase of fully labeled support sets [3,4].\nThe goal of the paper is to introduce a novel approach for Few-\nshot Weakly supervised Segmentation (FWS) of medical images in\ndifferent modalities. In other words, we are interested in mitigating\nthe limitations of semantic image segmentation with DNNs in problems\nwith small-data, weak supervision for segmentation, and completely\ndistinct image and label spaces during training and testing.\nOne common key point in few-shot learning is to introduce some\nform of prior knowledge into the models. A simple solution to this\nis to pretrain the model on a larger dataset. In RGB image domains,\nthere are several datasets with thousands or even millions of anno-\ntated samples, which can be used to produce feature extractors to\nbe leveraged in order to achieve good segmentation results with very\nfew samples. Recently, Self-Supervised Learning (SSL) has been proven\nhttps://doi.org/10.1016/j.patcog.2024.110471\nReceived 14 April 2023; Received in revised form 26 March 2024; Accepted 1 April 2024\n\nPattern Recognition 153 (2024) 110471\n2\nH. Oliveira et al.\nFig. 1. Overview of a meta-learner ğœ™ for FWS being pretrained in an episodic fashion on multiple related tasks {îˆ°\n1 \n, îˆ°\n2 \n, â€¦ , îˆ°\nğ‘› \n} pertaining to a meta-dataset (top) and being\ndeployed to a target few-shot task îˆ² with a sparsely annotated support set.\nto be a more robust initialization to such tasks [5] than supervised\nImageNet pretraining. However, such strategies are often limited to\nnatural images, not presenting considerable gains compared to random\ninitializations for domains such as medical imaging. Another approach\nto introduce prior knowledge is to use meta-learning training. Already\nbeing successfully used in other pattern recognition tasks, it has been\nemployed in few-shot image classification [6], with recent approaches\nfor semantic segmentation gaining popularity [7]. However, the major-\nity of works rely on ImageNet pretraining as feature extractors, while\nalso not conducting tests on weak labels. In this scenario, there is\ncurrently a gap in reliable methods for FWS on non-RGB images.\nWe propose a framework capable of strong pretraining using Meta-\nLearning on radiological images that does not assume task-specific\npriors, does not require previous pretraining of the backbone prior to\nour meta-training phase, and, therefore, can be generalized to any FWS\ntask in radiology. An overview of the proposed framework can be seen\nin Fig. 1. During its meta-training phase, it uses a meta-dataset îˆ° =\n{îˆ°\n1\n, îˆ°\n2\n, â€¦ , îˆ°\nğ‘›\n}, then it is deployed on an out-of-distribution (OOD)\nfew-shot target task îˆ² , where the single highly generalizable model ğœ™,\ntrained via a selective supervised loss function îˆ¸, is used as a predictor.\nAs discussed in the following sections, ğœ™ can be trained in several\ndistinct ways, such as second-order optimization, metric learning, and\nlate fusion. Differently from Domain Adaptation (DA) tasks, the solu-\ntions described in this work concern tasks wherein the target domain\nis related to the meta-training data, but not available during training.\nOur tasks are closer to Domain Generalization [8] scenarios, since\nour framework uses pixelwise labeled data from multiple radiology\ndomains (chest X-rays, mammographies, dental X-rays, etc.) aiming\nto be generalizable to other medical imaging tasks (e.g. computed\ntomography, magnetic resonance, positron emission tomography, etc.).\nWhile the meta-learners used in this study are not novel by them-\nselves, their application in Few-shot Weakly supervised Segmentation\n(FWS) is not straightforward. Thus, the main contributions of this work\nare the proposal of meta-learning algorithms to FWS and a thorough\nevaluation of the performance of multiple meta-learners in radiological\nimage segmentation. We highlight the other contributions of this work\nas:\nâ€¢ A new generalizable FWS method that may incorporate meta-\nlearners from multiple image classification paradigms;\nâ€¢ Relevant improvements of existing learners in real-world scenar-\nios achieved by our proposal of SSL pre-training coupled with\nmeta-learning;\nâ€¢ Guidelines and lessons learned from (1) detailed performance\nanalysis of the multiple Meta-Learning algorithms being adapted\nto multiple FWS in 2D radiology; (2) analysis of multiple annota-\ntion styles and sparsity parameters.\nThis work is organized as follows. Section 2 reviews the literature\nand discusses the taxonomy adopted in this paper to compare the\ndifferent approaches. Section 3 describes the assessed meta-learning\nmethods while the experimental setup is explained in Section 4. The\nexperimental results are presented and discussed in Section 5, with\nconclusions presented in Section 6.\n2. Related work\n2.1. Meta-learning for visual recognition\nMeta-learning methods leverage multi-task learning in order to\nimprove generalization for OOD tasks in image classification [6]. The\nidea is that, by generalizing for multiple datasets and tasks in the meta-\ntraining phase, the model is more well-equipped to deal with fully novel\nunseen tasks in the deployment phase. We highlight three distinct ap-\nproaches for achieving Meta-Learning that are important to this paper,\ndespite other paradigms (such as black-box or Bayesian approaches)\nalso being common: (1) gradient-based â€“ or optimization-based â€“ meth-\nods, which acquire task-specific parameters via optimization of first- [9,\n10] or second-order derivatives [9,11,12]; (2) metric learning [13,14],\nwherein instead of directly predicting class probabilities, methods fo-\ncus on learning distances across samples from similar and dissimilar\nclasses; (3) fusion-based approaches, which leverage the intermediary\nrepresentations of the support set to guide the predictions in the query\nset [15â€“17] via identity mapping (i.e. concatenation, multiplication,\naddition, etc.).\nGradient-based methods yield â€“ often through second-order op-\ntimization â€“ specific models for each task î‰€ in a meta-batch. Each\nsubtask î‰€ computes its own temporary parameters, optimized using\nthe task loss îˆ¸\nî‰€ \n. Finn et al. [9] introduce the MAML algorithm, a\nsecond-order framework, which updates its model parameters by taking\n\nPattern Recognition 153 (2024) 110471\n3\nH. Oliveira et al.\naverages of costâ€™s gradients of the specific task models evaluated on\nnew samples of the task î‰€ . Nichol et al. [10] propose the Reptile\nalgorithm, which only uses first-order gradient information by updating\nits weights in the direction of the difference between task-specific\nparameters and global parameters. Also optimized via second-order\nderivatives, MetaSGD [11] aims to automatically learn step sizes for\nthe SGD optimizer besides the model parameters. Raghu et al. [12] in-\ntroduced another second-order algorithm similar to the MAML. Named\nANIL, the method follows the MAML algorithm, with the novelty that\nin the inner loop, instead of updating all the parameters, this strategy\nonly updates the ones related to the network output head, e.g. the last\nclassification layers.\nMetric-based approaches train a single model ğœ™ in multiple tasks\nî‰€ . The objective of these approaches is to obtain an agnostic mapper to\nan embedding space where similar samples are closer than dissimilar\nones. Snell et al. [13] propose the Prototypical Networks (ProtoNets),\na model that tries to learn an embedding function that computes\nprototypes to a class (i.e. a ğ‘‘-dimensional vector that represents a class)\nand uses the distance to these prototypes for inference. For each class,\nfeatures extracted from samples of their support â€“ the labeled set of\nimages of a task â€“ are averaged to create its prototype vector. During\ninference, the feature extracted from a query image is compared with\nthe prototypes, and the class of the closest prototype, according to some\ndistance metric, is assigned to the query.\nFusion-based approaches, on the other hand, learn a single model\nğœ™ where information of support sets is used to enhance the predic-\ntion of the query images. Similarly to metric-based methods, fusion-\nbased approaches for meta-learning rely on an internal embedding\nfrom a neural network. However, instead of computing cross-sample\nsimilarities through a distance function (e.g. Euclidean, cosine, etc.)\non the embeddings, such methods perform some form of late fu-\nsion (e.g. concatenation, addition, multiplication, cross-attention, etc.)\non the support embeddings/labels and query samples. The Ridge Re-\ngression Differentiable Discriminator (R2D2) [16] uses the support\nembeddings and labels to train a fully tensorial logistic or ridge regres-\nsor using least-squares, which admits closed-form solutions. Similarly,\nMetaOptNet [15] leverages a highly discriminative embedding gen-\nerated from a neural network to train a differentiable SVM. In both\nmethods, the regressor obtained from the support data is then applied\nto the query samples through a simple matrix-matrix product, resulting\nin a few-shot classifier guided by the few labeled support examples.\n2.2. Meta-learning for image segmentation\nRecently, few-shot segmentation with meta-learning has evolved\nto alleviate the burden of the learners by filtering out irrelevant\nclasses [18] and mitigate the bias of learners to base classes seen during\nmeta-training [19], however most methods still do not fully explore\nweakly labeled support sets with a wide range of annotation modalities.\nThe most successful methods specifically designed for FWS are\nGuided Nets [17] and PANets [14]. Guided Nets rely on pretrained\nbackbones to extract features from both support and query data, and\napply late fusion in these embeddings to guide the prediction over\nthe query set from the support codes. By contrast, PANets rely on a\nframework similar to ProtoNets [13] to compute prototypes for each\nclass in the embedding space of the support set instead of leveraging\nlate fusion to guide the prediction over the query. PANets also intro-\nduce Prototype Alignment Regularization (PAR) in the training phase\nfor better label efficiency, wherein the query labels are also used to\ncompute prototypes to predict the segmentations of support samples.\nSimilarly to PANets, ProtoSeg [20] also use prototypes for conducting\nFWS, but instead of repurposing pretrained backbones, this approach\nis trained directly on related tasks from scratch in order to allow for\ninference over non-RGB images.\nHendryx et al. [21] adapt the Reptile [10] and First-Order-MAML\n(FOMAML) [9] to the problem of semantic segmentation. Their major\ncontribution, however, is introducing the EfficientLab architecture,\nwhich is a convolutional network for semantic segmentation. Weakly\nsupervised Segmentation Learning (WeaSeL) [22] applies the well-\nknown MAML second-order optimization-based framework [9] to FWS\nin medical imaging by training it directly on tasks related to radiology,\nwith the downside of being less efficient than first-order approaches.\nFor a more thorough discussion on the few-shot meta-learning seg-\nmentation methods discussed previously and other few-shot and/or\nweakly-supervised segmentation approaches, we refer the reader to\nrecent surveys [3,4,7,23].\nEven though multiple works on meta-learners specifically designed\nfor segmentation have appeared during the last few years, there is still\na gap in such methods that can work with weakly annotated images.\nAiming at encouraging a larger use of Meta-Learning for FWS tasks, in\nthe remainder sections of this text we propose generalizable pipelines\nfor porting meta-learners designed for image classification to weakly\nsupervised segmentation tasks and test these approaches in real-world\nmedical tasks.\n3. Meta-learners for weakly supervised segmentation\nWe use most of the problem definitions from Gama et al. [22].\nWe consider a training dataset îˆ° as a set of pairs (ğ±, ğ²) of images\nğ± âˆˆ R\nğ»Ã—ğ‘Š Ã—ğµ \nwith dimensions ğ» Ã— ğ‘Š and ğµ bands/channels, and\nsemantic labels ğ² âˆˆ R\nğ»Ã—ğ‘Š \n. For each batch fed to an algorithm is\npartitioned into two sets, named the support set (îˆ°\nğ‘ ğ‘¢ğ‘\n) and the query set\n(îˆ°\nğ‘ğ‘Ÿğ‘¦\n), such that îˆ°\nğ‘ ğ‘¢ğ‘ \nâˆ© îˆ°\nğ‘ğ‘Ÿğ‘¦ \n= âˆ…. We define a segmentation task î‰€ as a\ntuple î‰€ = {îˆ°\nğ‘ ğ‘¢ğ‘\n, îˆ°\nğ‘ğ‘Ÿğ‘¦\n, ğ‘¡} (or, î‰€ = {îˆ°, ğ‘¡}), where ğ‘¡ is a target class or set\nof classes. In our setting, all segmentation tasks are binary during both\nmeta-training and deployment, so ğ‘¡ is a single class that can be referred\nto as positive/foreground in opposition to the negative/background class.\nîˆ°\nğ‘ğ‘Ÿğ‘¦ \n= {ğ±\nğ‘ğ‘Ÿğ‘¦\n, ğ²\nğ‘ğ‘Ÿğ‘¦\n} is composed of a set of images (ğ±\nğ‘ğ‘Ÿğ‘¦\n) and associated\ndensely labeled segmentation ground truth (ğ²\nğ‘ğ‘Ÿğ‘¦\n), while the support set\nîˆ°\nğ‘ ğ‘¢ğ‘ \n= {ğ±\nğ‘ ğ‘¢ğ‘\n, ğ²\nğ‘ ğ‘¢ğ‘\n} contains another subset of images from the same\ndataset îˆ° as îˆ°\nğ‘ğ‘Ÿğ‘¦\n, but paired with a weakly supervised mask ğ²\nğ‘ ğ‘¢ğ‘\n.\nWe employ several distinct strategies detailed in the supplementary\nmaterial of this manuscript to procedurally acquire the weak labels ğ²\nğ‘ ğ‘¢ğ‘\nfrom the dense segmentation masks of the meta-training datasets.\nA few-shot semantic segmentation task îˆ² is a specific type of\nsegmentation task. The difference is that the samples of îˆ°\nğ‘ ğ‘¢ğ‘ \nhave their\nlabels sparsely annotated, and the labels in îˆ°\nğ‘ğ‘Ÿğ‘¦ \nare absent or unknown\nduring training/tuning. Moreover, the number of samples ğ‘˜ = |îˆ°\nğ‘ ğ‘¢ğ‘\n| is\nsmall, e.g., 20, 10 or even less.\nThe problem is then defined as follows. Given a few-shot task îˆ² and\na set of segmentation tasks î‰š = {î‰€\n1\n, î‰€\n2\n, â€¦ , î‰€\nğ‘›\n}, we want to segment the\nimages from îˆ°\nğ‘ğ‘Ÿğ‘¦\nîˆ² \nusing information from tasks in î‰š and information\nfrom îˆ°\nğ‘ ğ‘¢ğ‘\nîˆ² \n. We also require that no pair of image/labels of îˆ² is present\nin î‰š , in order to ensure that the only semantic information about îˆ² is\nin its support annotations.\nIn order to teach the model through supervised inputs during the\nmeta-training phase, we employ supervised loss functions îˆ¸\nğ‘ ğ‘¢ğ‘\n. As\nnot all pixels in an FWS task are labeled, we leverage the pixelwise\nSelective Cross-Entropy (SCE) loss function [20,22] in its binary form\nto conduct our supervised training on the labeled pixels in a ground\ntruth ğ² and prediction logits\nÌ‚\n ğ²:\nîˆ¸\nğ‘ ğ‘ğ‘’\n(ğ²,Ì‚ ğ²) = âˆ’ \n1\nğ‘\nğ‘\nâˆ‘\nğ‘—=1\n1\nğ‘—\n[\nğ²\nğ‘— \nlog\nÌ‚\n ğ²\nğ‘— \n+ (1 âˆ’ ğ²\nğ‘— \n) log(1 âˆ’\nÌ‚\n ğ²\nğ‘— \n)\n] \n. (1)\nIn Eq. (1), ğ‘ is the number of labeled pixels in ğ², ğ‘— is an index\niterating over all pixels,\nÌ‚\n ğ²\nğ‘— \nis the probability predicted for each pixel ğ‘—\nof being classified as pertaining from the positive class, and 1\nğ‘— \nâˆˆ {0, 1}\nis a flag indicating whether a pixel ğ‘— has a valid annotation or not.\nîˆ¸\nğ‘ ğ‘ğ‘’ \nis applicable to gradient-, metric- and fusion-based methods, with\nonly the form of computing the logits\nÌ‚\n ğ²\nğ‘— \nvarying across the different\nparadigms. We employ the SCE loss function in our experiments for all\nmethods, as we observed early on that other supervised segmentation\n\nPattern Recognition 153 (2024) 110471\n4\nH. Oliveira et al.\nFig. 2. Graphical illustration of one inner loop iteration for gradient-based FWS\nmethods.\nloss functions (i.e. Dice [24], Focal [25], etc.) did not achieve the same\nperformance as SCE.\nOur proposed pipeline for FWS via Meta-Learning was conceived as\na general strategy to convert algorithms originally designed for image\nclassification to semantic segmentation, in theory being suitable to\nany gradient-, metric- or fusion-based method. We leverage knowledge\ngathered from previous works [20,22] on FWS using MAML [9] and\nProtoNets [13] to generalize the frameworks to other state-of-the-art\nMeta-Learning algorithms [10â€“12,15,16]. Sections 3.1â€“3.3 discuss how\neach Meta-Learning paradigm can be ported to FWS tasks.\n3.1. Gradient-based meta-learning for FWS\nIn order to adapt gradient-based meta-learners to FWS we employ\nFCNs and Encoder-Decoder architectures, and we divide each network\narchitecture into two distinct parts: (1) a feature extraction component\nğœ™; and (2) a segmentation head â„. ğœ™ receives support images ğ±\nğ‘ ğ‘¢ğ‘ \nand\noutputs embedded representations of the pixels in these images ğŸ\nğ‘ ğ‘¢ğ‘ \n=\nğœ™(ğ±\nğ‘ ğ‘¢ğ‘\n), while â„ inputs ğŸ\nğ‘ ğ‘¢ğ‘ \nand outputs segmentation predictions\nÌ‚\n ğ²\nğ‘ ğ‘¢ğ‘\n.\nFor both FCNs and Encoder-Decoders, â„ is simply the last convolutional\nblock responsible for the final pixelwise classification, while the feature\nextractor ğœ™ comprises all previous layers in the architecture â€“ be it a\nsequence of Encoder/Decoder blocks or a CNN backbone.\nThe gradient-based pipeline can be observed graphically in Fig. 2 for\na single inner loop. As one can see, a support image ğ±\nğ‘ ğ‘¢ğ‘ \nis initially fed\nthrough ğœ™, generating features ğŸ\nğ‘ ğ‘¢ğ‘\n, which are then upscaled using an\ninterpolation function â†‘ and fed to â„, yielding a segmentation predic-\ntion\nÌ‚\n ğ²\nğ‘ ğ‘¢ğ‘\n. The prediction is then compared to the sparsely supervised\nground truth ğ²\nğ‘ ğ‘¢ğ‘\n, from the support set, in the few labeled points\navailable through îˆ¸\nğ‘ ğ‘¢ğ‘\n. The inner loop update function ğ‘ˆ\nğ‘–ğ‘› \noperates\non the gradients obtained through first- [10] or second-order [9,11]\noptimization, depending on the meta-learning algorithm of choice. ğ‘ˆ\nğ‘–ğ‘›\nreturns the task-specific feature extractor ğœ™\nâ‹† \nand segmentation head\nâ„\nâ‹†\n, which are then fed with the query image ğ±\nğ‘ğ‘Ÿğ‘¦\n, resulting in a pre-\ndiction\nÌ‚\n ğ²\nğ‘ğ‘Ÿğ‘¦ \nfor the query set.\nÌ‚\n ğ²\nğ‘ğ‘Ÿğ‘¦ \nand ğ²\nğ‘ğ‘Ÿğ‘¦ \nare then compared through\nîˆ¸\nğ‘ ğ‘¢ğ‘\n, yielding gradients that can be backpropagated to meta-models ğœ™\nand â„.\nIn a real-world implementation of this idea, this procedure is re-\npeated for all tasks randomly sampled in a meta-batch, each one with\ndistinct ğœ™\nâ‹† \nand â„\nâ‹† \nparameter sets, and yielding different gradients to\nbe backpropagated to ğœ™ and â„. The gradients can then be merged â€“\nusually via averaging â€“ to update ğœ™ and â„ to more generalist parameter\nsets, highly adaptable to multiple tasks at once. This update to ğœ™ and\nâ„ from the gradients obtained on the query set from the task-specific\nFig. 3. Graphical illustration of one inner loop iteration for metric-based FWS methods.\nmodels is the outer loop of the optimization-based meta-learning algo-\nrithm. Additionally, it is highly desirable that the support image â€“ or,\nrealistically, support batch of images â€“ is fed multiple times through\nğœ™ and â„, with the gradients being accumulated to generate ğœ™\nâ‹† \nand â„\nâ‹†\nbefore feeding the query set to the task-specific models. The number\nof iterations through the support set used to compute ğœ™\nâ‹† \nand â„\nâ‹† \nis a\nhyperparameter of gradient-based meta-learners, often being limited by\nthe amount of memory in the GPU and/or training time constraints.\n3.2. Metric-based meta-learning for FWS\nAs discussed in Section 2, metric-learning can also be used to\nachieve meta-learning through multi-task meta-training and some\nclever use of support set annotations [13,14,20] that are leveraged\ndistinctly from an optimization-based approach. Instead of using the\nsupport set to tune task-specific models that should perform well on the\nquery, metric-based methods instead use the labels from the support to\ncompute prototypes in the embedding space, which can then be used as\npivots to compute distances to query samples. This allows for extremely\nlow-shot learning regimes (e.g. one-shot) and even zero-shot learning,\nthe latter being unfeasible with most gradient-based methods.\nWe adapted metric-based meta-learners for FWS following the\npipeline presented in Fig. 3 for a single inner loop iteration. Differently\nfrom most gradient-based approaches, the feature extractor ğœ™ remains\nfrozen during the inner loops. The embedded spaces for the support\nâ†‘ ğŸ\nğ‘ ğ‘¢ğ‘ \nand query â†‘ ğŸ\nğ‘ğ‘Ÿğ‘¦ \nsets, both in R\nğ¶Ã—ğ»Ã—ğ‘Š \n, are fundamental to\npixelwise classification in this approach, assuming ğ¶ output channels\nto ğœ™. The few support labels available are used to compute pixel\nprototypes for each class, similarly to Snell et al. [13], which does this\nat an image level. As all tasks are binary in our approach, two centroids\nğœ‡\n0\n, ğœ‡\n1 \nâˆˆ R\nğ¶ \nare computed for the negative and positive classes,\nrespectively; resulting in an embedded representation ğ\nğ‘ ğ‘¢ğ‘\n. Prototypes\nonly consider the labeled pixels from ğ²\nğ‘ ğ‘¢ğ‘\n, ignoring the unannotated\nones from â†‘ ğŸ\nğ‘ ğ‘¢ğ‘\n.\nThe upscaled query set features â†‘ ğŸ\nğ‘ğ‘Ÿğ‘¦ \n= ğœ™(ğ±\nğ‘ğ‘Ÿğ‘¦\n) are then projected\nonto the space ğ\nğ‘ ğ‘¢ğ‘\n, where the distances of each query pixelwise feature\nvector can be computed in relation to ğœ‡\n0 \nand ğœ‡\n1 \naccording to some\ndistance metric ğ‘‘ (e.g. Euclidean [13,20], cosine [14], Mahalanobis,\nManhattan, etc.). Logits can then be computed for each query pixel\naccording to their distances to the centroids of the negative and positive\nclasses, allowing them to be fed to a supervised loss function îˆ¸\nğ‘ ğ‘¢ğ‘\n.\nThe gradients obtained from îˆ¸\nğ‘ ğ‘¢ğ‘\n(ğ²\nğ‘ğ‘Ÿğ‘¦\n,Ì‚ ğ²\nğ‘ğ‘Ÿğ‘¦\n) can then be backpropagated\nthrough the pipeline, reaching the trainable parameters ğœ™. Similarly\nto the gradient-based methods, multiple inner loops such as the one\nshown in Fig. 3 are conducted in each meta-training iteration, with the\ngradients âˆ‡\nğœ™\nîˆ¸\nğ‘ ğ‘¢ğ‘ \nbeing added or averaged before updating ğœ™ on the\nouter loop.\n\nPattern Recognition 153 (2024) 110471\n5\nH. Oliveira et al.\nFig. 4. Graphical illustration of one inner loop iteration for fusion-based FWS methods.\n3.3. Fusion-based meta-learning for FWS\nFusion-based methods work by using the support set images and\nlabels to â€˜â€˜guideâ€™â€™ [17,26] the classification or segmentation of the\ndesired classes on the query set samples via late feature fusion between\nfeature representations ğŸ\nğ‘ ğ‘¢ğ‘ \nand ğŸ\nğ‘ğ‘Ÿğ‘¦\n. Fig. 4 exemplifies the pipeline of a\nfusion-based algorithm along one inner loop, starting with the feature\nextraction from the support and query images via ğœ™ â€“ yielding ğŸ\nğ‘ ğ‘¢ğ‘\nand ğŸ\nğ‘ğ‘Ÿğ‘¦ \nâ€“ and the extraction of features from the sparse support mask\nğ²\nğ‘ ğ‘¢ğ‘\n, resulting in ğŸ\nğ‘š\n. Feature representations ğŸ\nğ‘š \nand ğŸ\nğ‘ ğ‘¢ğ‘ \nare then fused\nusing a function âŠ— and averaged into 1D vector ğŸ\nâŠ—\n, responsible for\nguiding ğœ™ in segmenting query images according to the background and\nforeground classes in ğ²\nğ‘ ğ‘¢ğ‘\n. The guiding vector is then fused to the query\nfeatures ğŸ\nğ‘ğ‘Ÿğ‘¦ \nusing another mapping âŠ• and resulting in ğŸ\nâŠ•\n. The fused\nfeature maps ğŸ\nâŠ• \ncan then be passed through a segmentation head â„,\nresulting in predictions\nÌ‚\n ğ²\nğ‘ğ‘Ÿğ‘¦\n, which is subsequently fed to îˆ¸\nğ‘ ğ‘¢ğ‘\n.\nAs shown in Fig. 4, some fusion-based approaches also extract fea-\ntures from the sparsely labeled support segmentation mask ğ²\nğ‘ ğ‘¢ğ‘ \nthrough\na generic model referred to as ğœ™\nğ‘š\n. Guided Nets [17], for instance,\nrepurpose the backbone ğœ™ to share parameters with ğœ™\nğ‘š\n, even though\nwe found that keeping distinct parameters sets for ğœ™ and ğœ™\nğ‘š \nresulted\nin much more stable pretraining using this strategy. Another important\naspect of fusion-based methods is the choice of the two functions: âŠ— â€“\nthat merges support image and label features; and âŠ• â€“ responsible for\nfusing the support and query feature representations. Common choices\nfor these functions are concatenation, matrix multiplication, addition,\npixelwise multiplication, or even trainable attention modules [3,4].\n4. Experimental setup\nOur experiments compare both well-known FWS algorithms in the\nliterature (PANets [14], Guided Nets [17], WeaSeL [22] â€“ referred to\nas MAML and ProtoSeg [20] â€“ referred to as ProtoNets) and novel\nalgorithms based on meta-learners designed for few-shot classification\nand ported to FWS (MetaSGD [11], ANIL [12], Reptile [10], R2D2 [16]\nand MetaOptNet [15]).\nWe explored different neural network architectures as backbones\nfor the meta-learners, including: U-Net (U) [20,27], EfficientLab-6-3\n(E) [21], DeepLabv3 (D) [28] and an FCN with a ResNet-12 (R) [15]\nbackbone.\nAiming to improve the performance of meta-learners by introducing\ninductive bias learned directly from related data, we also tested CNNs\npretrained with SSL previously to the meta-training phase. This strategy\naims at leveraging pretraining without human annotations to improve\nthe initial embeddings ğŸ\nğ‘ ğ‘¢ğ‘ \nand ğŸ\nğ‘ğ‘Ÿğ‘¦ \nfor the support and query sets\ngenerated by ğœ™, potentially improving the overall performance on the\ntarget FWS task during deploy. For that, we employed the SimSiam [29]\nalgorithm for pretraining a ResNet-18 and a ResNet-50 [30] on the\nradiological datasets used during meta-training, resulting in a backbone\nğœ™\nğ‘†ğ‘†ğ¿ \nand a pretext task prediction head â„\nğ‘†ğ‘†ğ¿\n. We then employ ğœ™\nğ‘†ğ‘†ğ¿\nas the starting point for our feature extractors ğœ™ and resume the\nmeta-training phase.\nAs the performance of few-shot algorithms is notoriously variable\naccording to the chosen support set samples, all results shown in Sec-\ntion 5 were computed according to a 5-fold cross-validation procedure,\nwith paired samples for each fold in all algorithms. As all of our\ntasks are binary by experimental design, the simple Intersection over\nUnion (IoU, also known as the Jaccard index) between the positive and\nnegative classes was our main measure for assessing the performance\nof FWS meta-learners.\nOur implementation of meta-learners for FWS was coded using the\nPytorch\n1 \nand learn2learn\n2 \nlibraries. Experiments were conducted in\nmachines running RTX 2070 GPUs with 8 GB of memory, so many\nhyperparameters for the meta-learning algorithms (e.g. meta batch size,\nnumber of inner loops, number of filters in the architectures, adaptation\nsteps in 2nd order gradient-based methods, etc.) were selected aiming\nto fit this capacity. Whenever possible, we set the default optimizer\nfor our algorithms as Adam [31], unless in methods that use quadratic\nprogram solvers, such as MetaOptNet [15] or R2D2 [16], which rely on\nstandard Stochastic Gradient Descent (SGD). As time limitation was our\nmain consideration in designing the experiments shown in this work,\nwe leveraged knowledge from previous works [20,22] to set most of\nthe other hyperparameters that do not directly affect the memory usage\nof meta-learners. Readers can refer to our official implementation\n3 \nfor\ndetails.\n4.1. Experiment organization\nIn order to test the efficacy of the meta-learners shown in Section 3\nfor FWS, we designed an experimental setup aiming to evaluate the\nperformance of each algorithm in multiple radiological image segmen-\ntation tasks. We use the points weak annotation style to assess the\nperformance of algorithms in very small data scenarios for two distinct\nfew-shot tasks: îˆ²\nğ‘–ğ‘‘ \nâ€“ the OpenIST dataset\n4 \nin lung segmentation task\n(OpenIST-lungs); and îˆ²\nğ‘œğ‘œğ‘‘ \nâ€“ the Panoramic Dental X-rays [32] in inferior\nmandible segmentation task (Panoramic-mandible). Information on the\nsource Chest X-ray (CXR), Dental X-ray (DXR), Mammographic X-ray\n1 \nhttps://pytorch.org/.\n2 \nhttp://learn2learn.net/.\n3 \nhttps://github.com/hugo-oliveira/fsws_metalearning.\n4 \nhttps://github.com/pi-null-mezon/OpenIST.\n\nPattern Recognition 153 (2024) 110471\n6\nH. Oliveira et al.\nFig. 5. Weak annotation styles (4 last columns) studied in this work for two distinct radiological tasks: (1) OpenIST-lungs (top row); and (2) Panoramic-mandible (bottom row).\n(MXR), and Digitally Reconstructed Radiograph (DRR) datasets used in\nthis research can be seen in the supplementary material accompanying\nthis manuscript.\nTask îˆ²\nğ‘–ğ‘‘ \nrepresents a relatively â€˜â€˜in-distributionâ€™â€™ (ID) task very\nclose to many domains used during meta-training, as both the Shen-\nzhen/Mongomery sets and the Chest X-ray 14 dataset are very similar\nto the OpenIST data and also contain lung annotations. Task îˆ²\nğ‘œğ‘œğ‘‘ \n, how-\never, is a very out-of-distribution task compared to the meta-training\ndomains, as no other dataset contains segmentation for the inferior\nmandible and only one other DXR dataset (IVisionLab) is used during\nthe meta-training for the quite distinct task of teeth segmentation. To\navoid the meta-learners simply overfitting on the target dataset [33],\nwe hold out each target domain in an experiment from the meta-\ntraining phase, assuring that the first time the methods/networks have\nseen each target dataset is during the test phase.\nAdditionally to the experiments on ID and OOD 2D images, we\ntested our meta-learners pretrained on the 2D CXRs, MXRs, and DXRs\nin four tasks from three volumetric datasets: (1) StructSeg [34] on\nthe Both Lungs and Heart tasks (StructSeg-lungs and StructSeg-heart );\n(2) Medical Imaging Segmentation Decathlon (MSD) [35] slices on the\nSpleen task (MSD-spleen); and (3) private data from Oliveira et al. [36]\nin Cerebellum segmentation on pediatric MRI data (STAP-cerebellum).\nWe selected these tasks as targets due to the relatively large size of\nthese organs in the images in comparison to the whole volumes. This\nselection was conducted in order to avoid the inherent difficulties\nof compensating for domain shift in datasets with imbalanced target\nclasses [37].\nWhile the meta-training is conducted with the largest variability\npossible for the choice of support/query set samples and weak support\nmasks, we fixed the seeds of all randomly chosen variables in the\nsparsification algorithms for the support set of the target task îˆ² , forcing\nthem to be the same on a pixel-level for all samples.\nTo evaluate the performance of the best meta-learners in other\nweakly supervised segmentation styles, we conduct a series of exper-\niments with grid, scribbles, and skeleton annotations, as depicted in\nFig. 5. These experiments are presented in Section 5.2 for Panoramic-\nmandible, OpenIST-lungs, StructSeg-lungs, StructSeg-heart, MSD-spleen and\nSTAP-cerebellum. At last, in Section 5.4 we exemplify how SSL pretrain-\ning [29] can benefit the OOD task performance of our meta-learners\nin the tasks of MSD-spleen and StructSeg-lungs for points and scribbles\nannotations.\n5. Results and discussion\n5.1. Results intra meta-learning paradigms\nWe present initially the results in the points annotation style for 1-\nand 5-shot scenarios, each one with 4 distinct label configurations: (1)\n1 annotated point (p = 1) with a dilation radius of 1 (r = 1); (2) p\n= 1/r = 3; (3) p = 5/r = 1; and (4) p = 5/r = 3. These experiments\ncorrespond to the Panoramic-mandible task, where the domain shift\nis larger than OpenIST-lungs, as the focus of this work is in Domain\nGeneralization instead of simple cross-dataset transfer learning. This\nresults in 8 distinct annotation settings, which is the number of result\ncolumns in Tables 1â€“3. Bold values in result tables represent the best\noverall results for each label configuration (column), while underlined\nvalues highlight the backbones, which had the best performance for a\nmeta-learner. All metrics within 0.01 of IoU difference from the best\nresult are highlighted in those tables in order to point not only to\nthe best overall algorithm/backbone pairs but also to other strategies\nwith comparable performance. Additional results for meta-learning al-\ngorithms in Panoramic-mandible and OpenIST-lungs, including statistical\nconfidence intervals, can be seen in the supplementary material that\naccompanies this manuscript.\nWe present results for algorithms in each Meta-Learning paradigm\nseparately in order to sort the more label-efficient algorithms for com-\nparisons cross-paradigms. Tables 1â€“3 show, respectively, the IoU met-\nrics for the gradient-, metric- and fusion-based methods for the 4\nbackbones analyzed in this work, as well as the baseline with ResNet-\n12.\nTable 1 shows IoU results for pure 2nd order [9,11], pure 1st\norder [10], as well as optimization-based strategies that use both\n2nd and 1st order gradients [12]. ANIL-D achieved the best overall\nresults in gradient-based methods, yielding the best performances in\nall FWS configurations. Hence, the hybrid strategies of mixing 1st\norder gradients to train the backbone and 2nd order gradients to train\nthe segmentation head were the most label-efficient gradient-based\nmethods by quite some margin. Most of this strategyâ€™s success can\nbe attributed to the larger amount of adaptation steps that can be\ncomputed to the segmentation head (10 adaptation steps in ANIL) in\ncomparison to applying 2nd order gradients to the whole backbone (2\nadaptation steps for MAML and MetaSGD). Thus, with additional GPU\nmemory, one could theoretically improve the performance of full 2nd\norder approaches in theory. However, we conducted early tests with up\nto 10 adaptation steps on MAML and MetaSGD and these approaches\ntended to apparently overfit on the few annotated pixels with more\nadaptation steps than 5, possibly due to the larger parameter capacity\ncompared to the amount of labeled data being fitted by the 2nd order\noptimization in the whole backbone.\nMetric-based methods shown in Table 2, in comparison to the base-\nline with ResNet-12 backbone, highlight the superiority of PANets [14]\ncompared to ProtoNets [13] in all scenarios. The better performance of\nPANets can be attributed to two factors: the use of the cosine distance\ninstead of Euclidean distance and the additional PAR regularization\nproposed by Wang et al. [14] â€“ further explained in Section 2.2.\nEfficientLab outperformed other backbones in all but the two most\nsparsely labeled scenarios (1-shot/p = 1), while the other architectures\nperformed quite well in 1-shot/p = 1/r = 1 and 1-shot/p = 1/r = 3,\nbut lacked the capacity of learning from more annotations as well as\nthe PANet-E.\n\nPattern Recognition 153 (2024) 110471\n7\nH. Oliveira et al.\nTable 1\nIoU results for four distinct gradient-based methods (MAML [9,22], MetaSGD [11], ANIL [12] and Reptile [10]) with the four\nsegmentation backbones pretrained on our meta-dataset and tuned on the few-shot weakly-supervised support samples with\nthe points annotation style.\nMethod 1-shot 5-shot\np = 1\nr = 1\np = 1\nr = 3\np = 5\nr = 1\np = 5\nr = 3\np = 1\nr = 1\np = 1\nr = 3\np = 5\nr = 1\np = 5\nr = 3\nBaseline R .312 .311 .478 .482 .443 .450 .556 .558\nMAML\nU .205 .239 .321 .360 .319 .336 .476 .473\nR .260 .269 .387 .394 .373 .358 .249 .243\nE .179 .185 .253 .254 .206 .207 .301 .304\nD .328 .335 .424 .431 .322 .337 .488 .501\nMetaSGD\nU .306 \n.351 .306 .304 .301 .300 .411 .435\nR .268 .277 .269 .260 .340 \n.390 .441 .473\nE .244 .246 .376 .388 .324 .320 .368 .366\nD .222 .234 .173 .191 .256 .272 .421 .431\nANIL\nU .327 .323 .369 .385 .361 .366 .475 .473\nR .187 .203 .216 .260 .314 .341 .218 .190\nE .361 .364 .504 .506 .484 .490 .507 .517\nD .454 \n.451 .532 .541 .546 .552 .619 .627\nReptile\nU .334 .341 .328 .326 .349 .355 .365 .386\nR .203 .205 .336 .354 .394 .414 .524 .536\nE .206 .210 .281 .318 .367 .387 .314 .305\nD .160 .161 .210 .205 .085 .080 .283 .317\nTable 2\nIoU results for two distinct metric-based methods (ProtoNets [13,20] and PANets [14]) with the four segmentation backbones\npretrained on our meta-dataset and tuned on the few-shot weakly supervised support samples with the points annotation\nstyle.\nMethod 1-shot 5-shot\np = 1\nr = 1\np = 1\nr = 3\np = 5\nr = 1\np = 5\nr = 3\np = 1\nr = 1\np = 1\nr = 3\np = 5\nr = 1\np = 5\nr = 3\nBaseline R .312 .311 .478 .482 .443 .450 .556 .558\nProtoNet\nU .371 .365 .364 .362 .454 .462 .558 .563\nR .363 .364 .397 .395 .460 .463 .533 .540\nE .319 .336 .445 .453 .448 .449 .521 .525\nD .382 .381 .403 .399 .458 .456 .556 .568\nPANet\nU .416 \n.411 .419 .432 .511 .509 .554 .561\nR .423 \n.418 .512 .510 .518 .522 .592 .600\nE .295 .303 .570 .581 .530 .539 .615 .622\nD .419 .421 .462 .462 .481 .484 .578 .584\nTable 3\nIoU results for three distinct fusion-based methods (Guided Nets [17], R2D2 [16], and MetaOptNet [15]) with the four\nsegmentation backbones pretrained on our meta-dataset and tuned on the few-shot weakly supervised support samples with\nthe points annotation style.\nMethod and\nbackbone\n1-shot 5-shot\np = 1\nr = 1\np = 1\nr = 3\np = 5\nr = 1\np = 5\nr = 3\np = 1\nr = 1\np = 1\nr = 3\np = 5\nr = 1\np = 5\nr = 3\nBaseline R .312 .311 .478 .482 .443 .450 .556 .558\nGuided Net\nU .015 .013 .020 .012 .000 .000 .000 .000\nR .450 .449 .447 .443 .450 .450 .449 .447\nE .295 .298 .286 .306 .272 .272 .272 .291\nD .150 .161 .140 .164 .254 .259 .260 .265\nR2D2\nU .320 .324 .292 .341 .390 .419 .531 .550\nR .365 .365 .425 .399 .487 .502 .596 .614\nE .268 .272 .385 .422 .529 .526 .677 .681\nD .362 \n.362 .415 .423 .436 .460 .706 .715\nMetaOptNet\nU .383 .379 .415 .408 .424 .423 .458 .458\nR .390 .382 .400 .382 .494 .489 .627 .640\nE .257 .262 .399 .444 \n.513 .515 .659 .670\nD .358 .377 .426 .448 .435 .486 .654 .662\nGuided Nets, mainly with a ResNet-12 backbone, had a strong\nstart in extremely low-data scenarios (e.g. 1-shot/p = 1), but were\nunable to evolve to learn from more annotations, maintaining their\nperformance close to 0.45 of IoU throughout all other experiments.\nAs for R2D2 and MetaOptNet, their most promising backbones reach\nrelatively similar performances in very sparsely annotated scenarios â€“\nfrom 0.36 to 0.39 in 1-shot/p = 1 â€“ with the considerable advantage\nthat these architectures can continue to learn from the larger amount\nof annotated data in 1-shot/p = 5 and 5-shot. We highlight that R2D2-\nE, R2D2-D and MetaOptNet-E, MetaOptNet-D show good performances\nwhenever larger amounts of annotated support pixels are provided.\n\nPattern Recognition 153 (2024) 110471\n8\nH. Oliveira et al.\nFig. 6. IoU results for multiple weakly-supervised annotation styles in 1-, 5- and 10-shots for the best baseline backbone (R) and the two overall best algorithm/backbone pairs\nin each paradigm (ANIL-D, PANets-E, R2D2-D and MetaOptNet-D). Rows reflect distinct weakly-supervised annotation styles, respectively: points, grid, scribbles, skeleton. The left\ncolumn represents results for the Panoramic-mandible task (large domain shift), while the rightmost column depicts results for the OpenIST-lungs. Dotted lines reflect the performance\nof segmentation algorithms on the weakly supervised support sets, while the dashed horizontal lines show the results tuned on the densely annotated support masks. Better viewed\nin color.\nFrom the results presented in Tables 1â€“3, we chose 7 distinct\nalgorithm/backbone pairs for further analysis in the following sections:\nBaseline-R, ANIL-E/D, PANets-R/E, R2D2-D and MetaOptNet-D.\n5.2. Weak annotation styles\nWhile Section 5.1 focused on results for points annotations, the\npresent section shows results for three additional weakly supervised\nmask styles: grid, scribbles and skeleton.\nFig. 6 depicts some broader trends applied to all experiments. As\nexpected, the OpenIST-lungs segmentation task â€“ wherein the domain\nshift is smaller both in the pixel- and label-space compared to the meta-\ndataset â€“ achieves considerably better segmentation performances than\nthe Panoramic-mandible segmentation. Even in very sparsely labeled\nsupport sets (i.e. points in 1-shot/p = 1), some algorithms reach 0.6\nor more of IoU, while the 5-shot/p = 1 scenario presents performances\nfor PANets with IoU larger than 0.8. All other annotation styles grid,\nscribbles and skeleton in their most sparsely annotated scenarios even\nreach 0.9 of IoU for OpenIST-lungs. Similarly to results reported by\nGama et al. [20], metric-based methods seem to be a much more\nsuitable predictor for tasks in target domains where the domain shift\nfrom the meta-dataset is small. ANIL-D follows PANets quite closely\nin this task, while fusion-based approaches reached a lower ceiling in\nperformance on OpenIST-lungs.\nOne should notice that for OpenIST only annotations in randomly\nselected points did not reach around 0.8 of IoU, while all of grid, scrib-\nbles, and skeleton did. This quickly reaching a ceiling in performance in\nall but one annotation style implies that points are a less label-efficient\nway of providing weak annotations.\nDepending on the meta-learner, points in its more sparse setting\n(1-shot/p = 1) achieves between around 0.45/0.70 of IoU for the\nPanoramic-mandible and OpenIST-lungs tasks, respectively, while the\nhighly efficient skeleton annotation style in its more sparse setting\n(1-shot/r = 2) yields 0.60/0.90 for MetaOptNet/PANets. In fact, an-\nnotations in skeleton and scribbles seem to be more efficient than points,\nmainly because the time spent in labeling single random points in an\nimage is similar to the time spent in delineating a scribble or a skeleton\nfor a commonly shaped organ, while also providing more annotated\npoints to tune the learner into the target task. Both draw-oriented\nannotations also provide a relatively clear guide for the algorithm\nindicating the organ borders, while randomly sampled points do not\nprovide this information.\nAnother clearly seen trend in Fig. 6 is the tiny performance gap\nbetween sparse (dotted lines) and dense (dashed horizontal lines) anno-\ntations. While the gap is considerably large in Panoramic-mandible for\nmore sparsely annotated scenarios in all annotation styles, for OpenIST-\nlungs in all scenarios and Panoramic-mandible in more densely annotated\nscenarios it is negligible for the best meta-learners.\n\nPattern Recognition 153 (2024) 110471\n9\nH. Oliveira et al.\nFig. 7. Qualitative results for FWS on two target 10-shot tasks: Panoramic-mandible (top 4 rows) and OpenIST-lungs (bottom 4 rows); for 4 distinct annotation styles: points (1st\nand 5th rows), grid (2nd and 6th rows), scribbles (3rd and 7th rows) and skeleton (4th and 8th rows). The positive class is represented in blue and the negative class is shown\nin red. For each task/annotation style, we show one sample of the query set (îˆ² \nğ‘ğ‘Ÿğ‘¦ \n). Segmentation predictions and sample IoU metrics for 4 of the best meta-learners (ANIL-D,\nPANet-E, MetaOptNet-D and R2D2-D) and Baseline-R are shown. (For interpretation of the references to color in this figure legend, the reader is referred to the web version of\nthis article.)\nFig. 7 shows sample segmentation predictions from 4 meta-learners\nand a baseline in 2D radiology data. Following the trends shown in\nFig. 6, qualitative segmentation predictions in the Panoramic-mandible\ntask are better performed by the fusion-based methods (R2D2-D and\nMetaOptNet-D). As for the task with a small domain shift, the metric-\nbased PANets-E work better in OpenIST-lungs, followed by the gradient-\nbased ANIL-D. One can also derive from the 4 top lines in Fig. 7\nthat the points style is the least effective weak annotation modality for\nboth Panoramic-mandible and OpenIST-lungs, while scribbles and skeleton\nlabels show overall superior performance for the best meta-learners in\neach scenario.\n5.3. Results for 2D slices from volumetric data\nThis section presents our experiments on 2D slices from 3D data,\nrepresenting fully OOD tasks in relation to the meta-training datasets.\nQuantitative results for four tasks (StructSeg-lungs, StructSeg-heart, MSD-\nspleen and STAP-cerebellum) are shown in Fig. 8 for two annotation\nstyles: grid and skeleton. As all 2D slice tasks are considerably more\nOOD compared to the meta-training datasets, we observe similar re-\nsults to the ones presented for the Panoramic-mandible task in Fig. 6,\nwith fusion-based methods achieving higher performance than their\ncounterparts. These best results obtained by R2D2 and MetaOptNet\nare followed by the optimization-based ANIL, with the similarity-based\nPANets showing performance close to the baseline in most tasks and\nannotation styles. Weak grid annotations seemed to perform better than\nthe skeleton labeling style in 2D slice tasks, possibly due to the spatial\nuniformity of such annotations in tasks with large domain shifts, even\nthough it is often more time-demanding to label a grid of points rather\nthan simply draw a skeleton and outline of an organ.\nAnother interesting trend shown in Figs. 8(b), 8(d), 8(f) and 8(h) is\nthat the baseline method was clearly outperformed by all meta-learners\n\nPattern Recognition 153 (2024) 110471\n10\nH. Oliveira et al.\nFig. 8. IoU results for multiple weakly-supervised annotation styles in 1-, 5- and 10-shots for the best baseline backbone (R) and the two overall best algorithm/backbone pairs\nin each paradigm (ANIL-D, PANets-E, R2D2-D and MetaOptNet-D). Rows reflect distinct tasks, respectively: StructSeg-lungs, StructSeg-heart, MSD-spleen and STAP-cerebellum. The\nleftmost column represents results for grid annotations, while the right column depicts skeleton results. Dotted lines reflect the performance of segmentation algorithms on the\nweakly supervised support sets, while the dashed horizontal lines depict the results tuned on the densely annotated support masks. Better viewed in color.\nFig. 9. Qualitative results for FWS on three target tasks: StructSeg-lungs on grid annotations (top row), MSD-spleen on scribbles annotations (middle row) and STAP-cerebellum on\nskeleton annotations (bottom row). Meta-learners and annotation styles are analogous to Fig. 7. Better viewed in color.\nin the skeleton annotation style for all tasks, but PANets in the StructSeg-\nlungs task. In contrast to that, the baseline had a very competitive\nperformance for grid annotations, even outperforming all meta-learners\nin most sparsity configurations in Figs. 8(c) (StructSeg-heart ) and 8(e)\n(MSD-spleen). Again, we attribute this better performance of the base-\nline to the higher density of annotations in grid. This implies that\nthe baseline is less adaptable than meta-learners in highly sparse and\nunorthodox annotation styles.\n\nPattern Recognition 153 (2024) 110471\n11\nH. Oliveira et al.\nTable 4\nIoU results for improved meta-learners in the MSD-spleen dataset for points (num_points = 5, radius = 3) and\nscribbles (proportion = 0.25, thickness = 4). Bold results represent the best results for each algorithm across\nnetwork architectures and underscored values indicate the best overall results in a whole column.\nMethod Points Scribbles\nShots Shots\n1 5 10 1 5 10\nBaseline \nR18+SS .145 .156 .219 .140 .166 .173\nR50+SS .176 .259 .318 .150 .180 .154\nANIL\nDLabv3 .337 .365 .385 .197 .233 .243\nR18+SS .300 .509 .477 .235 .486 .459\nR50+SS .403 .428 .375 .444 \n.556 .470\nPANet\nEffLab .200 .211 .212 .190 .164 .170\nR18+SS .544 .348 .398 .215 .207 .213\nR50+SS .249 .231 .252 .189 .194 .189\nR2D2\nDLabv3 .391 .447 .487 .285 .370 .366\nR18+SS .389 .457 .480 .201 .237 .252\nR50+SS .412 .470 .477 .234 .463 .411\nMetaOptNet\nDLabv3 .342 .443 .463 .216 .315 .347\nR18+SS .298 .470 .469 .183 .310 .334\nR50+SS .366 .492 .517 .272 .449 .456\nFinally, we show segmentation results for three of the 2D slice tasks\nin Fig. 9. As the domain shift is larger for these tasks, similar to the\nPanoramic-mandible task, R2D2 and MetaOptNet achieved the overall\nbest FWS results in comparison to the baseline and gradient-/metric-\nbased strategies. More specifically, the metric-based PANet performed\nparticularly poorly in these high domain-shifted tasks, segmenting\nother organs as the structure of interest in the majority of cases. This\nimplies that metric-based methods are unable to correctly deal with\nthe inherent ambiguity of FWS tasks, possibly due to the difficulties in\ncreating an embedding space that is discriminative to a wide range of\nreal-world unseen OOD tasks.\n5.4. Improving meta-learners with SSL\nAdditionally to the experiments with multiple algorithms within\nmeta-learning paradigms (Section 5.1), results and discussion about\nannotation styles (Section 5.2) and experiments with image slices from\n3D data (Section 5.3), in this section we also present ways of improving\nmeta-learning algorithms. As discussed in Section 4, we employed SSL\npretraining to aid meta-learners in achieving better results in OOD\ntasks. More specifically, we show results with backbones pretrained\nwith SimSiam (+SS) [29], more specifically, the ResNet-18 (R18) and\nResNet-50 (R50).\nTables 4 and 5 present these results for MSD-spleen and StructSeg-\nlungs, respectively; using simply SSL pretraining with standard fine-\ntuning (Baseline), ANIL, PANet, R2D2 and MetaOptNet. As additional\nbaseline comparisons to the larger R18 and R50 backbones, we show\nthe EfficientLab (EffLab) and DeepLabv3 (DLabv3) results.\nAll the best overall results in Table 4 are achieved by backbones\ntrained with SSL pretraining (R50+SS and R18+SS), mainly with\ngradient-based methods. In Table 5, overall best results for 5- and 10-\nshot are achieved using R2D2 paired with R50+SS. Also for StructSeg-\nlungs (Table 5), MetaOptNet outperforms other algorithms in extremely\nlabel-scarce 1-shot scenarios, once with R18+SS in the points annota-\ntion style and once with DLabv3 in scribbles annotations.\nIncluding all meta-learning approaches in our experiments â€“ ANIL,\nPANets, R2D2 and MetaOptNet â€“ the best results for each method in the\n1-, 5- and 10-shots of points and scribbles were achieved by R18+SS or\nR50+SS in 36 out of 48 experiments in both MSD-spleen and StructSeg-\nlungs. Even when +SS backbones lose by some amount to EffLab and\nDLabv3, it is commonly by a considerably small margin, reinforcing\nthe performance gains brought by SSL pretraining. These results make\nsuch strategies more suitable for generalizing to novel unseen data in\nreal-world applications of this research. We point, however, that the\nmeta-training phase using R50+SS coupled with gradient- and metric-\nbased methods presented some stability during training. This instability\nrenders R18+SS a safer approach, possibly given the smaller number\nof trainable parameters that better suit the MAML-based head of ANIL.\nAlso, the much larger computational resources required by R50, make\nR18 a considerably cheaper alternative for backbone choice with SSL\npretraining.\n6. Conclusion\nIn this work we introduced a new method for generalizing meta-\nlearners from three distinct paradigms (gradient-based [9â€“12], metric-\nbased [13,14] and fusion-based [15â€“17]) for FWS tasks without re-\nquiring ImageNet pretraining or strong domain-dependent priors. Some\nrelevant improvements to existing methods have proven to be impor-\ntant for the success of the proposed approach. We chose to apply\nour methodology to radiology images, even though the same methods\nshould also apply to other non-RGB domains such as histopathology,\nremote sensing, seismic images, or even 1D temporal signals. The ex-\nperimental results allowed for the proposition of guidelines to adopt the\nproposed methodology. A summary of our conclusions and observations\ncan be seen in Table 6.\nFor gradient-based methods, we observed that performance peaks\nwhen applying low-cost second-order gradient-based algorithms (i.e.\nANIL [12]) in the segmentation head. These methods are also more\ncomputationally effective than second-order optimization-based meth-\nods applied to the whole network (i.e. MAML [9] or MetaSGD [11]),\nwhich should allow them to be applied in scenarios that require infer-\nence on higher spatial resolution images or even in 3D radiology. Fully\nfirst-order methods (i.e. Reptile [10]), however, could not reach the\nsame performance as ANIL, even if they are relatively quicker and more\nscalable to larger backbones. Future works in gradient-based methods\nmight include alternative supervised loss functions (e.g. Dice [24] or\nFocal [25]).\nIn metric-based meta-learners, the cosine distance and prototype\nalignment regularization of PANets [14] proved to be more powerful\nthan the simple Euclidean distance of ProtoNets [13,20]. Similarity-\nbased methods, however, are only able to generalize to novel domains\nthat are quite close to the meta-training domains/tasks, possibly due to\nthe lack of global context in such models. Early experiments during this\nwork tried to insert pixel location information in the distance compu-\ntation with no visible effects on ProtoNets and PANets. Future research\ndirections might be more successful in integrating local information\nwith a global context in such a way that benefits segmentation tasks\n(e.g. via CRFs [38] or visual attention [39]).\n\nPattern Recognition 153 (2024) 110471\n12\nH. Oliveira et al.\nTable 5\nIoU results for improved meta-learners in the StructSeg-lungs dataset for points (num_points = 5, radius = 3)\nand scribbles (proportion = 0.25, thickness = 4). Bold results represent the best results for each algorithm\nacross network architectures and underscored values indicate the best overall results in a whole column.\nMethod Points Scribbles\nShots Shots\n1 5 10 1 5 10\nBaseline \nR18+SS .222 .396 .475 .178 .224 .224\nR50+SS .247 .447 .643 .304 .191 .169\nANIL\nDLabv3 .435 .518 .540 .481 .511 .520\nR18+SS .396 .493 .514 .527 .588 .595\nR50+SS .309 .398 .413 .445 .548 .581\nPANet\nEffLab .348 .579 .568 .209 .167 .167\nR18+SS .480 .662 .647 .167 .165 .166\nR50+SS .359 .641 .676 .168 .165 .165\nR2D2\nDLabv3 .338 .681 .718 .371 .381 .400\nR18+SS .395 .694 .740 .360 .305 .327\nR50+SS .379 .755 .794 .508 .639 .680\nMetaOptNet\nDLabv3 .395 .714 .763 .588 .531 .610\nR18+SS .462 .695 .741 .412 .361 .394\nR50+SS .371 .684 .776 .496 .506 .564\nTable 6\nSummary of conclusions on FWS meta-learners.\nAnnotation \nGrid âˆ™ Promising results, but closer to a costly dense annotation\nSkeletons & Scribbles âˆ™ More cost- and time-efficient annotations\nMethods \nMetric-based âˆ™ Easier implementation, straightforward convergence, but yields better\nresults with supervised pretraining [12,14]\nâˆ™ Works best in target tasks with small domain shifts\nGradient- & Fusion-based âˆ™ Better for OOD tasks with unknown domain shifts\nâˆ™ Fusion-based are less computationally expensive\nPre-training SSL âˆ™ Improved results in OOD target tasks (Tables 4 and 5)\nâˆ™ Considerable improvements for meta-learners in label-scarce scenarios\n(e.g. 1-shot/1-point in Tables 4 and 5)\nâˆ™ Unstable for larger architectures (e.g. ResNet50)\nPrevious works on FWS, such as Guided Nets [17] and PANets [14],\nproved unable to adapt to novel domains with large domain shifts in\nrelation to the meta-training datasets. In such scenarios, fusion-based\napproaches [15,16] were considered the optimal choices, followed\nby gradient-based methods [12]. These methods are also more com-\nputationally effective than second-order optimization-based methods\napplied to the whole network (e.g. MAML [9] or MetaSGD [11]), which\nshould allow them to be applied in scenarios that require inference on\nhigher spatial resolution images or even in 3D radiology.\nVery challenging segmentation tasks that require context and tex-\nture analysis, with organs that do not have clearly defined borders\n(such as STAP-cerebellum [36]) are still quite hard to learn from in few-\nshot settings. In future works, we intend to port the methods presented\nin this paper to fully 3D data (CTs, MRIs, and PET scans) instead\nof reducing these volumes to 2D slices. We hope that the additional\ncontext from the 3rd dimension will aid the algorithms in learning these\nchallenging 3D tasks, despite the higher computation cost involved in\nlearning 3D convolutional kernels.\nAt last, another major limitation of our pipeline is the need for\nannotated data from related tasks, restricting the application of the\nMeta-Learning pretraining to domains wherein labeled data is avail-\nable for multiple datasets. Aiming to mitigate this limitation, another\npromising direction might be to merge Meta-Learning with SSL pseudo\nlabels in order to eliminate the need for annotated datasets from related\ndomains.\nCRediT authorship contribution statement\nHugo Oliveira: Writing â€“ review & editing, Writing â€“ original draft,\nVisualization, Validation, Software, Methodology, Investigation, Data\ncuration, Conceptualization. Pedro H.T. Gama: Validation, Software,\nInvestigation, Conceptualization, Writing â€“ original draft, Writing â€“\nreview & editing. Isabelle Bloch: Funding acquisition, Project admin-\nistration, Resources, Supervision, Writing â€“ original draft, Writing â€“\nreview & editing. Roberto Marcondes Cesar Jr.: Funding acquisition,\nProject administration, Resources, Supervision, Writing â€“ original draft,\nWriting â€“ review & editing.\nDeclaration of competing interest\nThe authors declare that they have no known competing finan-\ncial interests or personal relationships that could have appeared to\ninfluence the work reported in this paper.\nData availability\nOne dataset is private (STAP/UC/USP), while all others are publicly\navailable. All datasets are public and referenced in the manuscript, but\nwe cannot redistribute them on our own.\nAcknowledgments\nThe authors would like to thank FAPESP, Brazil (grants #2015/22308-\n2, #2017/50236-1, #2020/06744-5 and #2022/15304-4), Serrapil-\nheira Institute, Brazil (grant #R-2011-37776), ANR, France (project\n#ANR-17-CE23-0021), CNPq, Brazil, CAPES, Brazil, FINEP, Brazil and\nMCTI, Brazil PPI-SOFTEX (TIC 13 DOU 01245.010222/2022-44) for\ntheir financial support for this research.\nAppendix A. Supplementary data\nSupplementary material related to this article can be found online\nat https://doi.org/10.1016/j.patcog.2024.110471.\n\nPattern Recognition 153 (2024) 110471\n13\nH. Oliveira et al.\nReferences\n[1] A. Krizhevsky, I. Sutskever, G.E. Hinton, ImageNet classification with deep\nconvolutional neural networks, in: NIPS, Vol. 25, 2012.\n[2] J. Peng, Y. Wang, Medical image segmentation with limited supervision: A review\nof deep network models, IEEE Access 9 (2021) 36827â€“36851.\n[3] H. Oliveira, R.M. Cesar, P.H. Gama, J.A. Dos Santos, Domain generalization\nin medical image segmentation via meta-learners, in: Conference on Graphics,\nPatterns and Images, Vol. 1, IEEE, 2022, pp. 288â€“293.\n[4] P.H.T. Gama, H. Oliveira, J.A. dos Santos, R.M. Cesar Jr., An overview on\nmeta-learning approaches for few-shot weakly-supervised segmentation, Comput.\nGraph. 113 (2023) 77â€“88.\n[5] L. Jing, Y. Tian, Self-supervised visual feature learning with deep neural\nnetworks: A survey, IEEE Trans. Pattern Anal. Mach. Intell. 43 (2020).\n[6] T. Hospedales, A. Antoniou, P. Micaelli, A. Storkey, Meta-learning in neural\nnetworks: A survey, IEEE Trans. Pattern Anal. Mach. Intell. 44 (9) (2021)\n5149â€“5169.\n[7] S. Luo, Y. Li, P. Gao, Y. Wang, S. Serikawa, Meta-seg: A survey of meta-learning\nfor image segmentation, Pattern Recognit. 126 (2022) 108586.\n[8] J. Wang, C. Lan, C. Liu, Y. Ouyang, T. Qin, W. Lu, Y. Chen, W. Zeng, P. Yu,\nGeneralizing to unseen domains: A survey on domain generalization, IEEE Trans.\nKnowl. Data Eng. 35 (8) (2022) 8052â€“8072.\n[9] C. Finn, P. Abbeel, S. Levine, Model-agnostic meta-learning for fast adaptation\nof deep networks, in: ICML, PMLR, 2017, pp. 1126â€“1135.\n[10] A. Nichol, J. Schulman, Reptile: A scalable meta-learning algorithm, 2018, p. 4,\narXiv preprint arXiv:1803.02999. 2 (3).\n[11] Z. Li, F. Zhou, F. Chen, H. Li, Meta-SGD: Learning to learn quickly for few-shot\nlearning, 2017, arXiv preprint arXiv:1707.09835.\n[12] A. Raghu, M. Raghu, S. Bengio, O. Vinyals, Rapid learning or feature reuse?\nTowards understanding the effectiveness of MAML, 2019, arXiv preprint arXiv:\n1909.09157.\n[13] J. Snell, K. Swersky, R. Zemel, Prototypical networks for few-shot learning, in:\nNeurIPS, Vol. 30, 2017.\n[14] K. Wang, J.H. Liew, Y. Zou, D. Zhou, J. Feng, PANet: Few-shot image semantic\nsegmentation with prototype alignment, in: CVPR, 2019, pp. 9197â€“9206.\n[15] K. Lee, S. Maji, A. Ravichandran, S. Soatto, Meta-learning with differentiable\nconvex optimization, in: CVPR, 2019, pp. 10657â€“10665.\n[16] L. Bertinetto, J.F. Henriques, P.H. Torr, A. Vedaldi, Meta-learning with\ndifferentiable closed-form solvers, in: ICLR, 2019.\n[17] K. Rakelly, E. Shelhamer, T. Darrell, A.A. Efros, S. Levine, Few-shot segmentation\npropagation with guided networks, 2018, arXiv preprint arXiv:1806.07373.\n[18] G. Cheng, C. Lang, J. Han, Holistic prototype activation for few-shot\nsegmentation, IEEE Trans. Pattern Anal. Mach. Intell. 45 (4) (2022) 4650â€“4666.\n[19] C. Lang, G. Cheng, B. Tu, C. Li, J. Han, Base and meta: A new perspective on\nfew-shot segmentation, IEEE Trans. Pattern Anal. Mach. Intell. 45 (2023).\n[20] P.H.T. Gama, H.N. Oliveira, J. Marcato, J. Dos Santos, Weakly supervised\nfew-shot segmentation via meta-learning, IEEE Trans. Multimed. 25 (2022).\n[21] S.M. Hendryx, A.B. Leach, P.D. Hein, C.T. Morrison, Meta-learning initializations\nfor image segmentation, 2019, arXiv preprint arXiv:1912.06290.\n[22] P.H. Gama, H. Oliveira, J.A. dos Santos, Learning to segment medical images\nfrom few-shot sparse labels, in: Conference on Graphics, Patterns and Images,\nIEEE, 2021, pp. 89â€“96.\n[23] Z. Chang, Y. Lu, X. Ran, X. Gao, X. Wang, Few-shot semantic segmenta-\ntion: A review on recent approaches, Neural Comput. Appl. 35 (25) (2023)\n18251â€“18275.\n[24] F. Milletari, N. Navab, S.-A. Ahmadi, V-net: Fully convolutional neural networks\nfor volumetric medical image segmentation, in: International Conference on 3D\nVision, IEEE, 2016, pp. 565â€“571.\n[25] T.-Y. Lin, P. Goyal, R. Girshick, K. He, P. DollÃ¡r, Focal loss for dense object\ndetection, in: ICCV, 2017, pp. 2980â€“2988.\n[26] X. Zhang, Y. Wei, Y. Yang, T.S. Huang, SG-one: Similarity guidance network for\none-shot semantic segmentation, IEEE Trans. Cybern. 50 (9) (2020) 3855â€“3865.\n[27] O. Ronneberger, P. Fischer, T. Brox, U-net: Convolutional networks for\nbiomedical image segmentation, in: MICCAI, Springer, 2015, pp. 234â€“241.\n[28] L.-C. Chen, G. Papandreou, F. Schroff, H. Adam, Rethinking atrous convolution\nfor semantic image segmentation, 2017, arXiv preprint arXiv:1706.05587.\n[29] X. Chen, K. He, Exploring simple siamese representation learning, in: CVPR,\n2021, pp. 15750â€“15758.\n[30] K. He, X. Zhang, S. Ren, J. Sun, Deep residual learning for image recognition,\nin: CVPR, 2016, pp. 770â€“778.\n[31] D.P. Kingma, J. Ba, Adam: A method for stochastic optimization, 2014, arXiv\npreprint arXiv:1412.6980.\n[32] A.H. Abdi, S. Kasaei, M. Mehdizadeh, Automatic segmentation of mandible in\npanoramic X-Ray, J. Med. Imaging 2 (4) (2015) 044003.\n[33] J. Rajendran, A. Irpan, E. Jang, Meta-learning requires meta-augmentation, in:\nNeurIPS, Vol. 33, 2020, pp. 5705â€“5715.\n[34] A.L. Simpson, M. Antonelli, S. Bakas, M. Bilello, K. Farahani, B. Van Ginneken,\nA. Kopp-Schneider, B.A. Landman, G. Litjens, B. Menze, et al., A large annotated\nmedical image dataset for the development and evaluation of segmentation\nalgorithms, 2019, arXiv preprint arXiv:1902.09063.\n[35] M. Antonelli, A. Reinke, S. Bakas, K. Farahani, A. Kopp-Schneider, B.A. Landman,\nG. Litjens, B. Menze, O. Ronneberger, R.M. Summers, et al., The medical\nsegmentation decathlon, Nat. Commun. 13 (1) (2022) 1â€“13.\n[36] H. Oliveira, L. Penteado, J.L. Maciel, S.F. Ferraciolli, M.S. Takahashi, I. Bloch,\nR.C. Junior, Automatic segmentation of posterior fossa structures in pediatric\nbrain MRIs, in: Conference on Graphics, Patterns and Images, IEEE, 2021, pp.\n121â€“128.\n[37] T.M.H. Hsu, W.Y. Chen, C.-A. Hou, Y.-H.H. Tsai, Y.-R. Yeh, Y.-C.F. Wang,\nUnsupervised domain adaptation with imbalanced cross-domain data, in: ICCV,\n2015, pp. 4121â€“4129.\n[38] S. Zheng, S. Jayasumana, B. Romera-Paredes, V. Vineet, Z. Su, D. Du, C. Huang,\nP.H. Torr, Conditional random fields as recurrent neural networks, in: ICCV,\n2015, pp. 1529â€“1537.\n[39] T. Hu, P. Yang, C. Zhang, G. Yu, Y. Mu, C.G. Snoek, Attention-based multi-context\nguiding for few-shot semantic segmentation, in: AAAI Conference on Artificial\nIntelligence, Vol. 33, 2019, pp. 8441â€“8448.\nHugo Oliveira is a Computer Science professor at Uni-\nversidade Federal de ViÃ§osa, with B.Sc. and M.Sc. degrees\nat Universidade Federal da ParaÃ­ba and Ph.D. at Univer-\nsidade Federal de Minas Gerais. Research areas include\nMachine/Deep Learning, Few-Shot Learning, Meta-Learning,\nSelfSupervised Learning, Domain Adaptation, Generative\nModels and Open Set Recognition.\nPedro Gama is currently a Doctoral candidate at UFMG,\nwith B.Sc. Computational and Applied Mathematics and\nMSc. degrees by UFMG. His research is focused on Deep\nLearning and Computer Vision, specifically in Semantic\nSegmentation, although his interests include General Ma-\nchine Learning, Pattern Recognition, Few-Shot Learning,\nMeta-Learning, and Remote Sensing.\nIsabelle Bloch received the Ph.D. degree from TÃ©lÃ©com\nParis. She has been a Professor at TÃ©lÃ©com Paris until\n2020 and is now a Professor at Sorbonne UniversitÃ©. Her\ncurrent research interests include mathematical morphol-\nogy, fuzzy set theory, graph-based, knowledge-based object\nrecognition, and medical imaging.\nRoberto M. Cesar Jr is a full-professor of the USP. Grad-\nuated in Computer Science from IBILCE-UNESP, MS from\nUNICAMP and Ph.D. from USP and is a Researcher at\nInovaUSP and member of the Coordination at FAPESP.\nExperience in computer science, computer vision, pattern\nrecognition, signal and image processing.",
    "version": "5.3.31"
  },
  {
    "numpages": 18,
    "numrender": 18,
    "info": {
      "PDFFormatVersion": "1.4",
      "Language": "en-US",
      "EncryptFilterName": null,
      "IsLinearized": true,
      "IsAcroFormPresent": false,
      "IsXFAPresent": false,
      "IsCollectionPresent": false,
      "IsSignaturesPresent": false,
      "CreationDate": "D:20250108152252+01'00'",
      "Creator": "Adobe InDesign 18.3 (Windows)",
      "Custom": {
        "CrossMarkDomains[1]": "springer.com",
        "CrossMarkDomains[2]": "springerlink.com",
        "CrossmarkDomainExclusive": "true",
        "CrossmarkMajorVersionDate": "2010-04-23",
        "doi": "10.1007/s00138-024-01643-y"
      },
      "ModDate": "D:20250115061146+05'30'",
      "Producer": "Adobe PDF Library 17.0",
      "Trapped": { "name": "False" }
    },
    "metadata": {
      "xmpmm:instanceid": "uuid:823c6dee-1ecc-46ea-bdb8-f6245dfef9c1",
      "xmpmm:originaldocumentid": "adobe:docid:indd:24bb93d9-bd1e-11dd-84db-a83b50d2aaad",
      "xmpmm:documentid": "xmp.id:5f1569ae-b00b-d843-ae16-5f8530f2ed60",
      "xmpmm:renditionclass": "proof:pdf",
      "xmpmm:versionid": "1",
      "xmpmm:derivedfrom": "xmp.iid:f706facd-829a-5d4a-b3d3-b59119fbf28cxmp.did:55b2f905-5d93-9847-87c7-9e0a10e3a401default",
      "xmpmm:history": "convertedfrom application/x-indesign to application/pdfAdobe InDesign 18.3 (Windows)2025-01-08T15:22:52+01:00converteduuid:fb0f610b-3220-44ba-838f-7045357e0090converted to PDF/A-2bpdfToolbox2025-01-15T06:11:00+05:30converteduuid:97d6e63f-f95b-4a2d-98f0-395ec99cbdcaconverted to PDF/A-2bpdfToolbox2025-01-15T06:11:46+05:30",
      "xmp:createdate": "2025-01-08T15:22:52+01:00",
      "xmp:modifydate": "2025-01-15T06:11:46+05:30",
      "xmp:metadatadate": "2025-01-15T06:11:46+05:30",
      "xmp:creatortool": "Adobe InDesign 18.3 (Windows)",
      "prism:aggregationtype": "journal",
      "crossmark:doi": "10.1007/s00138-024-01643-y",
      "crossmark:majorversiondate": "2010-04-23",
      "crossmark:crossmarkdomainexclusive": "true",
      "crossmark:crossmarkdomains": "springer.comspringerlink.com",
      "pdfx:crossmarkmajorversiondate": "2010-04-23",
      "pdfx:crossmarkdomainexclusive": "true",
      "pdfx:doi": "10.1007/s00138-024-01643-y",
      "pdfx:crossmarkdomains": "springer.comspringerlink.com",
      "jav:journal_article_version": "VoR",
      "dc:format": "application/pdf",
      "pdf:producer": "Adobe PDF Library 17.0",
      "pdf:trapped": "False",
      "pdfaid:part": "2",
      "pdfaid:conformance": "B",
      "pdfaextension:schemas": "http://ns.adobe.com/xap/1.0/mm/xmpMMXMP Media Management SchemainternalUUID based identifier for specific incarnation of a documentInstanceIDURIinternalThe common identifier for all versions and renditions of a document.OriginalDocumentIDURIinternalA reference to the original document from which this one is derived. It is a minimal reference; missing components can be assumed to be unchanged. For example, a new version might only need to specify the instance ID and version number of the previous version, or a rendition might only need to specify the instance ID and rendition class of the original.DerivedFromResourceRefIdentifies a portion of a document. This can be a position at which the document has been changed since the most recent event history (stEvt:changed). For a resource within an xmpMM:Ingredients list, the ResourceRef uses this type to identify both the portion of the containing document that refers to the resource, and the portion of the referenced resource that is referenced.http://ns.adobe.com/xap/1.0/sType/Part#stPartParthttp://ns.adobe.com/xap/1.0/t/pg/xmpTPgXMP Paged-TextinternalXMP08 Spec: An ordered array of plate names that are needed to print the document (including any in contained documents).PlateNamesSeq Texthttp://ns.adobe.com/pdf/1.3/pdfAdobe PDF SchemainternalA name object indicating whether the document has been modified to include trapping informationTrappedTexthttp://ns.adobe.com/pdfx/1.3/pdfxAdobe Document Info PDF eXtension SchemainternalID of PDF/X standardGTS_PDFXVersionTextinternalConformance level of PDF/X standardGTS_PDFXConformanceTextinternalCompany creating the PDFCompanyTextinternalDate when document was last modifiedSourceModifiedTextexternalMirrors crossmark:CrosMarkDomainsCrossMarkDomainsSeq TextexternalMirrors crossmark:CrossmarkDomainExclusiveCrossmarkDomainExclusiveTextexternalMirrors crossmark:MajorVersionDateCrossmarkMajorVersionDateTextinternalMirrors crossmark:DOIdoiTexthttp://www.aiim.org/pdfa/ns/id/pdfaidPDF/A ID SchemainternalPart of PDF/A standardpartIntegerinternalAmendment of PDF/A standardamdTextinternalConformance level of PDF/A standardconformanceTexthttp://crossref.org/crossmark/1.0/crossmarkcrossmarkinternalCrossMarkDomainsCrossMarkDomainsSeq TextinternalCrossmarkDomainExclusiveCrossmarkDomainExclusiveTextinternalUsual same as prism:doiDOITextexternalThe date when a publication was published.MajorVersionDateTexthttp://prismstandard.org/namespaces/basic/2.0/prismPrismexternalThe aggregation type specifies the unit of aggregation for a content collection. \nComment \nPRISM recommends that the PRISM Aggregation Type Controlled Vocabulary be used to provide values for this element. \nNote: PRISM recommends against the use of the #other value currently allowed in this controlled vocabulary. In lieu of using #other please reach out to the PRISM group at info@prismstandard.org to request addition of your term to the Aggregation Type Controlled Vocabulary. \n\naggregationTypeTexthttp://www.niso.org/schemas/jav/1.0/javNISOexternalValues for Journal Article Version are one of the following:\nAO = Authorâ€™s Original\nSMUR = Submitted Manuscript Under Review\nAM = Accepted Manuscript\nP = Proof\nVoR = Version of Record\nCVoR = Corrected Version of Record\nEVoR = Enhanced Version of Recordjournal_article_versionClosed Choice of Text"
    },
    "text": "RESEARCH\nMachine Vision and Applications (2025) 36:22\nhttps://doi.org/10.1007/s00138-024-01643-y\nunsupervised segmentation methods [2] still falls short in\nperformance compared to fully-supervised methods, ren-\ndering them impractical for widespread use. Consequently,\nweakly-supervised methods that utilize less detailed anno-\ntations have become increasingly popular. These methods\nsubstantially reduce the annotation burden while achieving\nresults nearly equivalent to those obtained with fully-super-\nvised techniques.\nFigure 1 presents several commonly-used weak annota-\ntions, including image-tag annotations [4], bounding boxes\n[1], extreme points [5], and scribbles [6â€“9]. The scribble\nannotations are very sparse, on average, around 5% of the\npixels are labeled (either foreground or background) and the\nothers are left as unknown pixels. Compared to image tags\nand bounding box annotations, scribbles provide rough posi-\ntions of Regions of Interest (ROIs) to allow for a better loca-\ntion. Compared to extreme points, annotating by scribbles\nis more flexible, especially for ROIs with irregular shapes.\nIn addition, annotators are allowed to focus on confidently\nidentifiable regions without needing to mark uncertain or\nambiguous boundaries. In practical annotation, the scribble\n1 Introduction\nDeep segmentation networks have demonstrated impres-\nsive performance in various medical applications, particu-\nlarly under fully supervised conditions [1]. These networks\ntypically require extensive annotations at the pixel or voxel\nlevel, as illustrated in Fig. 1b. However, obtaining such\ndense annotations is challenging due to the time-consuming\nnature of such manual tasks and the need for expert medi-\ncal knowledge to obtain high-quality segmentation masks.\nDespite promising outcomes, another choice of using fully\nYi Hong\nyi.hong@sjtu.edu.cn\n1 \nDepartment of Computer Science and Engineering, Shanghai\nJiao Tong University, Shanghai 200240, China\n2 \nDepartment of Radiology, Ruijin Hospital, Shanghai Jiao\nTong University of Medicine, Shanghai 200025, China\n3 \nSchool of Biomedical Engineering, Shanghai Jiao Tong\nUniversity, Shanghai 200240, China\nAbstract\nWeakly-supervised image segmentation, particularly using scribbles, has become increasingly prominent in both computer\nvision and medical image analysis. This popularity stems from the relative ease of obtaining scribble annotations com-\npared to the extensive effort required for precise pixel- or voxel-level labeling. Despite their advantages, scribble-based\nmethods often struggle with accurate boundary localization due to insufficient structural supervision of regions of interest\n(ROIs). Additionally, many existing approaches are primarily designed for 2D segmentation and fail to effectively utilize\nvolumetric data when applied directly to individual image slices. In this paper, we propose a scribble-based volumetric\nimage segmentation, Scribble2D5, which tackles 3D anisotropic image segmentation and aims to improve boundary pre-\ndiction. To achieve this, we augment a 2.5D attention UNet with a proposed label propagation module to extend semantic\ninformation from scribbles and use a combination of static and active boundary prediction to learn ROIâ€™s boundary and\nregularize its shape. Also, we propose an optional add-on component, which incorporates the shape prior information from\nunpaired segmentation masks to improve model accuracy further. Extensive experiments on three public datasets and one\nprivate dataset demonstrate our Scribble2D5 achieves state-of-the-art performance on volumetric image segmentation\nusing scribbles and shape prior if available. Our source code is available online:https://github. com/Qybc /Scribble2D5.\nKeywords Medical image segmentation Â· Weakly-supervised learning Â· Scribble annotations\nReceived: 27 May 2024 / Revised: 1 November 2024 / Accepted: 3 December 2024 / Published online: 17 December 2024\nÂ© The Author(s), under exclusive licence to Springer-Verlag GmbH Germany, part of Springer Nature 2024\nVolumetric medical image segmentation via scribble annotations and\nshape priors\nQiuhui Chen\n1 \nÂ· Haiying Lyu\n2 \nÂ· Xinyue Hu\n3 \nÂ· Yong Lu\n2 \nÂ· Yi Hong\n1\n1 3\n\nQ. Chen et al.\nmethod is simpler and requires less specialized knowledge\nfrom annotators. In contrast, extreme points require the pre-\ncise placement of six points around the target (before, after,\nleft, right, up, and down), necessitating expert knowledge\nto accurately identify the boundaries of the target. With\nbasic training, even users with no medical background can\nquickly learn how to make scribble annotations, making this\ntype of annotation useful in practice. Therefore, we choose\nscribbles as our weak annotations in this paper. Although\nusing scribble annotations for medical image segmentation\nis beneficial in many aspects, there are several challenges\nfaced by scribble-based learning methods. Firstly, scribbles\nare often sparse with no structure information of ROIs; as a\nresult, scribble-based methods have difficulty in accurately\nlocating the ROIâ€™s boundaries [7]. Moreover, existing scrib-\nble-based methods are typically designed for 2D images [6,\n9, 10], which do not fully leverage the whole image volume\nby directly applying to 3D image volumes, with missing\nconnections between slices. Preliminary work in [8] per-\nforms 3D segmentation by using transfer learning, which\nalternatively learns by using mask annotations in the source\ndomain and scribbles in the target domain. Also, in practice,\nmany clinical problems collect anisotropic medical image\nvolumes, with a much larger voxel spacing in one view than\nothers. We aim to tackle these problems and build a weakly-\nsupervised segmentation network, which suits anisotropic\nmedical volumes with improved boundary localization, and\noperates automatically at the inference stage, with no need\nof providing any scribble inputs.\nTo achieve this goal, we propose a volumetric segmenta-\ntion network called Scribble2D5. This model adopts a 2.5D\nattention UNet [1] to handle anisotropic medical volumes\nwith different voxel spacings. To amplify the influence of\nthe sparse scribbles in volumetric segmentation, we use a\nlabel propagation module based on supervoxels to generate\n3D pseudo masks from scribbles for supervision. To address\nthe boundary localization issue, we propose using the com-\nbination of learning both static and active boundaries via\npredicting edges in 3D and a proposed active boundary loss\nin 3D based on active contour model [11]. Also, we consider\nshape priors via shape descriptors [12] and skeleton context\n[13] to further improve the quality of the boundary local-\nization. This add-on component fully leverages existing\nunpaired segmentation masks while incorporating expert\nknowledge without requiring additional annotations.\nThis paper is an extension of our conference paper [14],\nby adding an optional shape prior component and perform-\ning extensive experiments, including evaluation on an addi-\ntional private dataset, comparison with more baselines and\nrecent methods, study on how to handle missing or partial\nscribble annotations, and having a better understanding of\nreal and generated scribbles. Overall, the contributions of\nthis paper are summarized as follows:\n1. We propose a scribble-based volumetric image segmen-\ntation network, Scribble2D5, which handles aniso-tropic\nmedical scans and improves boundary localization via\n3D label propagation, static and active boundary predic-\ntion and regularization, and shape priors learned from\nunpaired segmentation masks.\n2. We achieve SOTA performance compared to 10+ base-\nline models on three public datasets (i.e., ACDC [15] for\nFig. 1 Examples of different types\nof annotations used in medical\nimage segmentation. The axial\nimage slice is sampled from the\nCombined Healthy Abdominal\nOrgan Segmentation (CHAOS)\ndataset [3]. (Red: liver, yellow: left\nkidney) (color figure online)\n1 3\n22 Page 2 of 18\n\nVolumetric medical image segmentation via scribble annotations and shape priors\ncardiac segmentation, VS [16] for vestibular schwan-\nnoma tumor segmentation, CHAOS [17] for abdominal\norgan segmentation) and one private dataset for the seg-\nmentation of pituitary with tumor.\n3. We conduct comprehensive experiments to evaluate the\nperformance of our method, not only using multiple\ndatasets, including both public and private datasets to\ndemonstrate its practicality, but also studying the effect\nof using partial, real, or generated scribbles.\n2 Related work\nIn this section, we briefly review recent works on weakly-\nsupervised image segmentation using scribble annotations.\nAlso, we discuss image segmentation with shape priors,\nwhich is a useful add-on component to existing methods.\n2.1 Learning from scribble supervision\nScribbles are sparse annotations that have been successfully\nused in semantic segmentation. The segmentation accuracy\nof scribble-based methods is approaching full-supervised\nmethods in both computer vision and medical image applica-\ntions [3, 9, 18]. However, scribbles lack structure and shape\ninformation of objects or ROIs, which makes the accurate\nsegmentation of object boundaries a challenging task for\nexisting methods [6]. To address this problem, propagating\nscribble annotations to generate masks for full supervision\nis a commonly used strategy. In [6, 19], scribble annotations\nare expanded to adjacent pixels with similar intensity using\ngraph-based methods. In [20], a two-step procedure is used\nto first estimate the labels for unannotated pixels of ROIs\nbased on scribbles and then refine the predictions by using\nConditional Random Fields (CRF). The main limitation of\nthese approaches is the inaccurate relabeling step, which is\ntime-consuming and brings labeling errors for supervising\nthe learning of following-up segmentation models. Thus,\nother researchers have investigated alternatives to avoid this\nrelabeling step, such as using a CRF-based loss regularizer\n[21], a post-processing step with CRF, or a trainable CRF\nlayer [22]. Our method avoids the data relabeling step by\ndirectly learning a mapping from images to segmentation\nmasks, without using the expensive CRF-based post-pro-\ncessing. We cope with unlabelled regions of the image with\nthe help of a label propagation module based on supervoxels\nand 3D image edges.\nSimilar to our work on weakly-supervised 3D medical\nimage segmentation, researchers [23] propose an unsuper-\nvised regularization term of the loss function to constrain\nthe 3D volume size of the target region, although its network\ntakes 2D slices as inputs. Others [3] propose a dual-branch\nnetwork to dynamically mix up pseudo-labels by mixing\nthe outputs of the two branches and then use the generated\npseudo-labels to supervise the network training. In [18],\nresearchers propose a mixup augmentation of image and\nscribble supervision and a regularization term of supervi-\nsion via cycle consistency. In [24], researchers propose to\nintegrate efficient scribble supervision with the shape prior\nregularization terms into a unified framework. These meth-\nods mainly work on 2D slices when handling 3D images.\nDifferently, our scribble2D5 tackles 3D anisotropic images\nas inputs, considering 3D shapes of ROIs to treat objects as\na whole for learning.\nRecently, the Segment Anything Model (SAM) [25] has\nachieved great success in segmenting 2D natural images\nin computer vision. A couple of following works [26, 27]\nstudy its application or extension to medical images, which\nare still at an early stage and need more effort to work well\nin the domain of medical image analysis. Also, SAM takes\npoints, bounding boxes, and text as prompts, while in this\npaper we work on scribbles. Although these two methods\nare not directly comparable, the integration of SAM with\nour method is worth exploring in the future.\n2.2 Shape priors in deep medical image\nsegmentation\nIn semantic segmentation, incorporating shape prior knowl-\nedge into pixel-level segmentation is an efficient way to\naddress object occlusion or low image quality issues. A\ncommon way to incorporate shape priors into image seg-\nmentation is matching the predicted masks with those pro-\nvided in the shape priors, by using an additional module,\nsuch as the multi-scale attention gates used in adversarial\ntraining [28], a PatchGAN discriminator [29], the persistent\nhomology [30], etc. Others [31] demonstrate that a data-\ndriven shape prior can be learned through a convolutional\nautoencoder from unpaired segmentation masks and used\nas a regulariser to train a segmentation network. Similarly,\na variational autoencoder (VAE) [32] is adopted to learn\nshape priors [33], which has partial weights shared with a\nsegmentation model. Other approaches consider shape pri-\nors in the training with a regulariser [34] or a differentiable\npenalty [23], or at the inference stage via adjustment using\nVAEs [35] or denoising autoencoders [36].\nConsidering the stability issue of adversarial learning\nand the variation of masks for the same type of objects,\nwe turn to the traditional shape descriptors [37], which are\nmore robust and invariant across image modalities or sub-\nject populations. These shape descriptors are integrated into\nour main segmentation network as an optional component.\n1 3\nPage 3 of 18 22\n\nQ. Chen et al.\nknowledge and encouraging the final prediction to be more\naccurate and realistic.\n3.1 Backbone\nThe image volumes we study in the experiments are aniso-\ntropic with different voxel spacings, which is very common\nin practice. In our dataset, the in-plane resolution within a\nslice is about four times the thickness of a slice. Since 2D\nCNNs ignore the important correlations between slices and\n3D CNNs typically handle isotropic image volumes, we\nchoose a 2.5D neural network that considers the anisotro-\npic properties of an image volume. In particular, we adopt\nan attention UNet2D5 [1] as our backbone network, which\naugments UNet2D5 by adding an attention block at each\n3 Methodology\nFigure 2 illustrates the architecture of Scribble2D5, our\nproposed weakly-supervised image volume segmentation\nnetwork. This network utilizes scribble annotations and\nshape priors and is built on a 2.5D attention UNet [1] back-\nbone. It is enhanced with four additional modules, i.e., 1) a\nPseudo Label Propagation Module (PLPM) for generating\n3D pseudo masks and boundaries for supervision, 2) a Static\nBoundary Prediction Module (SBPM) for incorporating\nobject boundary information from images, 3) a Segmenta-\ntion Boosting Module (SBM) for further considering active\nboundaries via an active boundary loss, and 4) an optional\nShape Prior Module (SPM) for incorporating shape prior\nFig. 2 a Overview of our Scribble2D5 model, including five compo-\nnents: 1) pseudo label propagation module (PLPM, the yellow block),\nwhich generates image boundaries and pseudo-3D segmentation\nmasks based on scribble annotations; 2) 2.5D attention U-Net as our\nbackbone network (the blue block with details in b); 3) static boundary\nprediction model (SBPM, the green block), which uses the boundary\ninformation \ny\nB \npre-computed by PLPM for supervision; 4) segmenta-\ntion boosting module (SBM, the orange block), which further consid-\ners active boundaries via an active boundary loss; 5) shape prior model\n(the cyan block), which regularizes segmentation masks with shape\nprior. Both SBPM and SBM use residual channel attention blocks as\nshown in (c). d DenseASPP (Dense Atrous Spatial Pyramid Pooling)\nblock used between U-Net and SBM (Best viewed in color) (color fig-\nure online)\n1 3\n22 Page 4 of 18\n\nVolumetric medical image segmentation via scribble annotations and shape priors\nHere, \nd\nc\n(i, j) \nand \nd\ns\n(i, j) \nrepresent the color and spatial\ndistance between pixels \nI(x \ni \n, y\ni \n, z \ni \n, s\np\n) \nand \nI(x \nj \n, y\nj \n, z \nj \n, s\np\n)\nin the spectral band \ns\np \n, B denotes the set of spectral bands\nutilized, S defines the sampling interval of the clusters cen-\ntroids, and m controls the compactness of the superpixels.\nThe color distance, \nd\nc \n, ensures homogeneity within each\nsuperpixel, while the spatial distance, \nd\ns \n, promotes super-\npixel compactness.\nWe specifically target those supervoxels intersected by\nscribble annotations to create enhanced 3D pseudo-segmen-\ntation masks. These masks serve as improved regions of\ninterest (ROIs) for guiding the segmentation process, thus\ncompensating for the sparse nature of the scribbles and pro-\nviding a more robust framework for accurate segmentation.\nWhen generating the 3D Pseudo mask, we assume the\nscribble annotations are available on all image slices. How-\never, obtaining full annotations across all slices is labor-\nintensive, we suggest selecting specific slices for annotation\nand then propagating these annotations to neighboring\nslices. Without prior knowledge of the exact object loca-\ntion, we assume that the center slice in the region of interest\n(ROI) is likely to contain the most relevant information and\nserves as an effective starting point. While the object may\nnot always be centered, beginning annotation here provides\na practical compromise between annotation effort and accu-\nracy. We choose it as our starting point for annotation and\nlabel it â€œannotated\". Then, we gradually divide the remain-\ning slices into two groups, i.e., the annotated group and the\nunannotated group. Each time we first compute the Struc-\ntural Similarity Index Measure (SSIM) between these two\ngroups and from the unannotated group we select the one\nwith the lowest SSIM score for the annotated group. This\nprocess continues until the number of slices in the annotated\ngroup reaches its maximum value, which refers to a thresh-\nold-such as 75% of the total slices. This threshold is deter-\nmined as a proportion of the total number of slices, ensuring\nsufficient coverage for the propagation process.\nThe next step is to propagate the scribbles on the anno-\ntated slides to other unannotated ones. One choice is using\na 3D anisotropic watershed approach [39], which considers\nthe different voxel spacing when flooding to other slides.\nThen an erosion is adopted to reduce the width of gener-\nated annotations, which makes them more like scribbles\nand reduces the false positives of the generated annota-\ntions. An alternative approach is to use a random walk [40]\nmethod based on anisotropic diffusion, which has a com-\nputation time of 2 s, approximately four times slower than\nour watershed-based implementation. However, as demon-\nstrated in our experiments, the random walk-based method\nyields better results. In this way, we can handle the case of\nmissing scribble annotations on some slides and provide an\napproach to reduce the manual work of making annotations\ndeconvolutional layer, as shown in Fig. 2b. Specifically, at\nthe top two layers of both encoder and decoder branches, we\nhave 2D convolutional operations; while at other layers, the\nfeature maps are roughly isotropic, which is suitable for 3D\nconvolutions. The attention blocks are noted by purple tri-\nangles in Fig. 2b. Their attention maps are estimated via two\nlayers of convolutions, i.e., one with Peakly ReLU (PReLu)\nand the other with a Sigmoid activation function. This 2.5D\nnetwork suits all images in our experiments. In practice, the\nnumber of 2D convolution layers can be adjusted according\nto image resolution. For instance, when the slice thickness is\ntwo times that of pixel spacing, the number of 2D convolu-\ntion layers is set to 1. If the image volume is isotropic with\nan equal voxel spacing, the 2.5D attention UNet degener-\nates to 3D attention UNet.\n3.2 Pseudo label propagation module (PLPM)\nTo augment the supervision effect of weak annotations like\nscribbles and fully leverage the input image, in this pre-pro-\ncessing step, we generate a 3D Pseudo mask using scribble\npropagation and a 3D static boundary label, which are used\nlater to guide the learning of our model.\n3.2.1 3D pseudo mask generation\nScribble annotations, as noted in \nUNet\nPCE \n[7], often provide\ninsufficient supervision due to their sparse nature of cover-\ning only a limited number of pixels on each image slice,\nleading to inadequate guidance for segmentation tasks. To\novercome this limitation, we utilize supervoxels to amplify\nthe influence of scribble annotations in three dimensions.\nOur approach employs the SLIC algorithm, as introduced\nby [38], which has been shown to outperform other state-\nof-the-art superpixel methods due to its simplicity, strong\nboundary adherence, computational speed, and memory\nefficiency. SLIC requires only a single primary parameter, k,\nwhich specifies the desired number of equally sized super-\npixels. An optional parameter, m, can also be set to control\nsuperpixel compactness. The algorithm starts by initializing\nk cluster centers on a regular grid, spaced S pixels apart.\nTo prevent placement on edges or noisy pixels, each cluster\ncenter is adjusted to the lowest gradient position within a\n3 Ã— 3 \nneighborhood. The algorithm then iteratively assigns\neach pixel to the nearest cluster center, using a distance\nmeasure D that combines both color and spatial proximity\nas follows:\nD(i, j) =\nâˆš\n(\nd\nc\n(i,j)\nm\n)\n2\n+\n(\nd\ns\n(i,j)\nS\n)\n2\n,\nd\nc\n(i, j) =\nâˆš\nâˆ‘\ns\np\nâˆˆB \n(I (x \ni \n, y\ni \n, z \ni \n, s\np\n) âˆ’ I (x \nj \n, y\nj \n, z \nj \n, s\np\n))\n2\n,\nd\ns\n(i, j) =\nâˆš\n(x \nj \nâˆ’ x \ni\n)\n2 \n+ (y\nj \nâˆ’ y\ni\n)\n2 \n+ (z \nj \nâˆ’ z \ni\n)\n2\n.\n(1)\n1 3\nPage 5 of 18 22\n\nQ. Chen et al.\nreceptive fields by employing convolutional layers with\nvarying dilation rates, as depicted in Fig. 2a. The convolu-\ntional layers are densely connected to encompass a broader\nscale range without substantially increasing the modelâ€™s\nsize. Subsequently, we integrate two 3D convolutional lay-\ners followed by a \n1 Ã— 1 Ã— 1 \nconvolution, yielding the initial\nprediction \nM \ninit \n, which is guided by the generated pseudo\nmask \nM \npseudo \n. Given the tendency of supervoxels to cause\nover-segmentation, where one supervoxel might be associ-\nated with multiple classes, we address potential class con-\nfusion by exclusively considering supervoxels uniquely\nlabeled. These are marked as â€™1â€™ in the mask \nM \nvoxel \n, with\nall others set to zero. To refine the initial segmentation, we\nemploy a partial cross-entropy loss that specifically targets\nthese uniquely labeled supervoxels:\nL\nseg\n(M \ninit \n, M \npseudo \n, M \nvoxel\n)\n= âˆ’\nN\nâˆ‘\nc=1\nM \nvoxel\nc \nÂ· M \npseudo\nc \nlog(M \ninit\nc \n). \n(3)\nHere, N indicates the number of classes in the segmenta-\ntion. This loss function allows early feedback to speed up\nthe convergence of our network.\nTo enhance the quality of the initial mask and achieve a\nfinal prediction that accurately preserves boundaries, our\napproach integrates the outputs from the static boundary pre-\ndiction module with those from the initial mask prediction,\nto address different aspects of the segmentation challenge:\nboundary precision and shape coherence. The DenseASPP\nmodule captures essential low-resolution shape features,\ngenerating an initial mask prediction that provides a gen-\neral structure of the region to be segmented. However, rely-\ning solely on this low-resolution mask can result in blurred\nboundaries. To refine boundary details, we incorporate the\nSBPM, which is specifically designed to predict contour-\nlevel information. By concatenating the contour infor-\nmation from SBPM with the initial mask prediction from\nDenseASPP, we create a more comprehensive feature repre-\nsentation that includes both region-level shape and detailed\nboundary cues. This combined feature map is then processed\nby a residual channel attention block. The residual channel\nattention block selectively emphasizes salient features, par-\nticularly focusing on boundary and shape coherence, which\nare critical for accurate segmentation. Finally, a \n1 Ã— 1 Ã— 1\nconvolutional layer further refines these features to produce\nthe final segmentation mask \nM \nfinal \n. This sequential inte-\ngration of SBPM and DenseASPP is essential for achieving\nprecise boundary localization and maintaining overall shape\nfidelity in the segmentation output. To ensure the accuracy\nof \nM \nfinal \n, we employ the partial cross-entropy loss, which\nguides the prediction using the previously generated pseudo\nwhen preparing the training set. After having scribbles on\nall slices of an image, we can generate its 3D pseudo mask\nas discussed before.\n3.2.2 3D static boundary label generation\nIn addition to the pseudo mask we generate from the scrib-\nble annotations, we also generate the pseudo-static bound-\nary of ROI from an image volume by stacking 2D edges\ndetected on each image slice. This boundary is static since\nit is pre-computed from the image and remains unchanged\nduring training, which is different from the active boundary\nwe will discuss later. To obtain 2D edges, we directly adopt\nan existing method, i.e., HED [41], which is pre-trained on\nthe generic edges of BSDS500 [42]. As a result, this PLPM\ncomponent generates 3D pseudo masks from scribbles for\nROI segmentation and pre-computed boundaries for static\nboundary prediction, respectively.\n3.3 Static boundary prediction module (SBPM)\nThis module enhances the backbone networkâ€™s capability to\nextract image features that emphasize rich boundary struc-\ntures across multiple scales. Following the approach in [10],\nwe gather feature maps from various layers of the networkâ€™s\ndecoder and concatenate these 2D and 3D features at differ-\nent resolutions immediately after a convolutional layer with\na \n1 Ã— 1 Ã— 1 \nfilter. To integrate these features, we channel\nthem through a residual channel attention block (depicted\nas a pink square in Fig. 2a) followed by a \n1 Ã— 1 Ã— 1 \nconvo-\nlutional layer, which generates a 3D boundary map B. The\nnetwork is trained under the guidance of a previously gener-\nated 3D pseudo boundary \ny\nB \n, using binary cross-entropy\nloss applied to the output B:\nL\nbry\n(y\nB \n, B) = âˆ’(y\nB\nlogB + (1 âˆ’ y\nB\n)log(1 âˆ’ B)). \n(2)\nThis SBPM module only generates boundaries of images to\nsupervise the learning of our backbone network. To obtain\nthe masks of ROIs, we need the following boosting module.\n3.4 Segmentation boosting module (SBM)\nThis module conducts segmentation in two stages, including\ninitial guidance under a previously generated pseudo mask\nwith supervoxels and a follow-up regularization on the\nsegmentation output. It considers both static and dynamic\nboundaries to refine the segmentation process.\nTo generate a preliminary mask, we utilize a Dense\nAtrous Spatial Pyramid Pooling (DenseASPP, as shown in\nFig. 2d) block [43], positioned immediately following the\nbackbone networkâ€™s bottom layer. This block expands the\n1 3\n22 Page 6 of 18\n\nVolumetric medical image segmentation via scribble annotations and shape priors\n3.5 Shape prior module (SPM)\nAnother efficient way to mitigate the inaccurate bound-\nary estimation suffered by scribble-based methods is to\nincorporate shape prior knowledge into the network learn-\ning. For example, when segmenting a heart dataset with\nweak supervision, shape descriptors obtained from other\nfully annotated heart datasets offer prior knowledge that\nenhances boundary accuracy. This combination enables our\napproach to produce more precise and reliable segmenta-\ntion results by leveraging both weak supervision and shape\nregularization. As shown in Fig. 3, ACDC and M&Ms are\ncardiac image segmentation datasets collected from differ-\nent centers with different MRI scanners, but they are col-\nlected to tackle the same segmentation problem, extracting\nthe left (LV) and right ventricles (RV), as well as left ven-\ntricular myocardium (MYO) from medical scans. CHAOS\nand AbdomenCT-1K aim for the segmentation of abdominal\norgans (liver, kidneys, and spleen) from CT and MRI scans.\nThat is, we can fully leverage the masks provided by the\nM&Ms and AbdomenCT-1K datasets to help a better shape\nextraction for our task on the ACDC and CHAOS datasets.\nSince the M&Ms and AbdomenCT-1K masks are unpaired\nwith the image scans from ACDC and CHAOS, we need a\nshape descriptor that is invariant across image acquisition\ncenters and scanners. We adopt two types of shape descrip-\ntors, i.e., shape moments and skeleton descriptors to achieve\nthis goal.\n3.5.1 Shape moments\nGiven a set of M source images \nI \nm \n: Î© âˆˆ R\nn\nx\nÃ—n\ny\nÃ—n\nz \n,\nm = 1, 2, Â· Â· Â· , M \n, \nn \nx \n, n \ny \n, n \nz \nare dimensions of\nan image, we denote their ground truth K-class\nmask \nM \npseudo \n, effectively training the model to adhere\nclosely to the desired segmentation boundaries.\n3.4.1 Active boundary (AB) loss\nThe pseudo masks derived from supervoxels tend to be\ncoarse and prone to over-segmentation, leading to numer-\nous false positives. To address this limitation, we have\ndeveloped a method to enhance the accuracy of our 3D\nsegmentation. Specifically, we adapt the 2D active contour\nloss, originally introduced by [11], to a 3D context. This\nadaptation involves implementing a 3D version of the active\ncontour loss, which we refer to as the AB loss. The AB loss\neffectively regularizes both the surface and volume of the\nsegmented regions, thereby refining the segmentation by\nreducing inaccuracies and improving the delineation of the\nregions of interest (ROIs). This regularization technique\nhelps to counteract the inherent issues of over-segmentation\nand false positives associated with supervoxel-based pseudo\nmasks. We apply an AB loss as follows:\nL\nAB \n= Surface + Î»\n1 \nÂ· Volume\nIn \n+ Î»\n2 \nÂ· Volume\nOut\n, \n(4)\nwhere \nSurface = \nâˆ«\nS \n|âˆ‡u|ds \nand u is the mask prediction;\nVolume\nIn \n= \nâˆ«\nV \n(c\n1 \nâˆ’ v)\n2 \nudx \n, \nc\n1 \nis the mean image inten-\nsity inside of interested regions V, and v is the input image;\nVolume\nOut \n= \nâˆ« \nÂ¯\nV \n(c\n2 \nâˆ’ v)\n2 \nudx \nand \nc\n2 \nis the mean image\nintensity outside of the region. These items are balanced by\ntwo hyper-parameters \nÎ»\n1 \nand \nÎ»\n2\n. In the experiments, we set\nÎ»\n1 \n= 1 \nand \nÎ»\n2 \n= 0.1\n, to emphasize more on the inside region\nof the volume. This new loss function considers the shape\nand intensity of an image in 3D, which regularizes ROIâ€™s\nshapes and helps reduce false positives in segmentation.\nFig. 3 Top: Image and mask\nsamples collected from the ACDC\nand M&Ms datasets. Red: left\nventricle (LV), green: myocardium\n(MYO), blue: right ventricle (RV).\nBottom: Image and mask samples\ncollected from the CHAOS and\nAbdomenCT-1K datasets. Red:\nliver, yellow: left kidney (color\nfigure online)\n1 3\nPage 7 of 18 22\n\nQ. Chen et al.\nD\n(k) \n(s\nÎ¸\n) =\n1\nÎ©\nâˆ‘\niâˆˆÎ©\n(\n2\nâˆš\n(\nx\n(k)\n(i) \nâˆ’ Â¯x\n(k)\n)\n2\n, \n2\nâˆš\n(\ny\n(k)\n(i) \nâˆ’ Â¯y\n(k)\n)\n2\n, \n2\nâˆš\n(\nz\n(k)\n(i) \nâˆ’ Â¯z\n(k)\n)\n2\n)\n, \n(6)\nwhere \n(Â¯x\n(k)\n, Â¯y\n(k)\n, Â¯z\n(k)\n) \nis the mean coordinate of class k.\n3.5.2 Skeleton descriptors\nTo make the predicted shape close to the shape described\nby the unpaired segmentation masks, we propose to extract\nskeletons from the provided and predicted masks and match\nthem for comparison. Overall, our shape-matching model\nincludes two steps: skeleton extraction and skeleton match-\ning, as shown in Fig. 4. The extraction step takes an image\nslice and uses a skeletonization strategy to extract the skel-\neton of the region of interest; later, the matching step mea-\nsures the distance between the predicted and target masks to\nidentify if they are similar.\nSkeleton Extraction. The key point in skeletonization\nalgorithms is to preserve the topology of a shape. We adopt\nthe skeletonizing method proposed in [44], which performs\niterative morphological erosion of a segmentation mask\nto obtain the skeleton of an object. Specifically, for each\nobject in a mask, we iteratively remove the border pix-\nels of an object until a single-pixel edge, line, or point is\nsegmentation for each voxel \ni âˆˆ Î©\ns \nas a K-simplex vector\ny\nm\n(i) =\n(\ny\n(1)\nm \n(i), . . . , y\n(K)\nm \n(i)\n)\nâˆˆ {\n0, 1}\nK \n. For each voxel i,\nits coordinates in a 3D spatial domain are represented by the\ntuple \n(\nx\n(i)\n, y\n(i)\n, z\n(i)\n) \nâˆˆ R\n3\n. Our goal is to obtain a network\nN\nÎ¸ \n: I(i) \u001e â†’ s\nÎ¸\n(i) \nwith network parameters \nÎ¸ \n, for each voxel\ni âˆˆ Î©\n, where \ns\nÎ¸\n(i) =\n(\ns\n(1)\nÎ¸ \n(i), . . . , s\n(K)\nÎ¸ \n(i)\n)\nâˆˆ \n[0, 1]\nK \n, which\npredicts a softmax probability map for class \nk âˆˆ 1, 2, Â· Â· Â· , K\n. We define two shape descriptors below to obtain the compact\nrepresentation of a shape for a given input image I and a spe-\ncific class k.\nClass Ratio \nR \n. This descriptor measures the relative size\nof a shape. The ratio of class k can be computed as the per-\ncentage of the segmentation volume of this class over the\ntotal foreground volume of the input image. To calculate\nthe volume of class k, we simply use the summation of\nits prediction probability, which is a special case of shape\nmoments. As a result, we define the class ratio as\nR\n(k)\n(s\nÎ¸\n) = \n1\nÎ©\nâˆ‘\niâˆˆÎ©\ns\n(k)\nÎ¸ \n(i). \n(5)\nAverage Distance to the Centroid \nD \n. This shape descrip-\ntor measures on average how far the object spreads around\nits centroid. Here, we use the standard deviation of pixel\ncoordinates to compute this average distance for class k:\nFig. 4 Overview of our automatic skeleton matching model. In the skeletonization step, it extracts and prunes the skeleton map of each region of\ninterest; and in the matching step, it computes the skeleton context and finds the nearest match in the prototype datasets to compute the similarity\n1 3\n22 Page 8 of 18\n\nVolumetric medical image segmentation via scribble annotations and shape priors\nH\nSC \n(p\ni \n, r \nm \n, Î¸ \nn\n) = | Bin(p\ni \n, r \nm \n, Î¸ \nk\nn\n)|,\nBin(\np\ni \n, r \nm \n, Î¸ \nn\n) = {q âˆˆ S | (r \nmâˆ’1 \nâ‰¤ â€–q âˆ’ p\ni\nâ€–\n2 \n< r \nm\n)\nâˆ©\n(Î¸ \nnâˆ’1 \nâ‰¤ âˆ (q, p\ni\n) < Î¸ \nn\n)},\n(7)\nwhere \n| Â· | \nshows the number of members in a set, \nS \nis the\nset of sample points on the skeleton, and \nâˆ  (q, p\ni\n) \ncalculates\nthe angle of a vector from \np\ni \nto q with respect to the hori-\nzontal coordinate. This log-polar histogram located at each\npoint measures the distribution of other points on the skel-\neton with respect to the center point. By applying this calcu-\nlation to all sample points of a skeleton for each class k, we\nobtain the skeleton descriptor \nSC\n(k) \nfor the next matching\nstep.\nPrototype Extraction. To match with the provided\nsegmentation masks, we first summarize these masks\nwith several shape representatives, that is, extracting\nprototypes for each class, as shown in Fig. 6. We employ\nthe K-medoids algorithm [45] to find \nK \np \nprototypes for\nmatching. For each class, after initialization with \nK\np \ninitial\nmedoids, the K-medoids algorithm iterates between the\nfollowing two steps:\nâ— Assignment By treating the skeleton descriptor of each\nshapeâ€™s k-th class \nSC\n(k) \nas a whole, we assign each skel-\neton descriptor to its closest medoid \nÂ¯m\ni \n, if and only if\nthe distance between them satisfies:\nachieved. Then, we use the grayscale morphological opera-\ntor to close the generated discontinuous skeleton.\nSkeleton Context. To describe the extracted skeleton, we\nuse a shape descriptor called skeleton context, which is a\nlog-polar histogram formed for each sample point on the\nskeleton. For each sample point \np\ni \n, this log-polar histo-\ngram treats it as the center, and each bin of the histogram\ncounts the number of sample points at its specific angle\nand range of distance from the center (i.e., \np\ni \n). As shown\nin Fig. 5, the centers of the small red circles show a quite\ndifferent skeleton context, especially at the bottom right\npart of the log-polar histogram, where the corresponding\nsegments of two skeletons are different, with some missing\npoints on the second skeleton.\nIn particular, we use the notation \nH\nSC \n(p\ni \n, r \nm \n, Î¸ \nn\n) \nto pres-\nent the value of the skeleton contextâ€™s histogram centered\nat the point \np\ni \nand located in the \n(r \nm \n, Î¸ \nn\n) \nbin. For instance,\nH\nSC \n(p\ni \n, r \nm \n, Î¸ \nn\n) = 10 \nmeans that there are ten other sam-\nple points around the point \np\ni \nof the skeleton, in the dis-\ntance range of \nr \nmâˆ’1 \nâ‰¤ r < r \nm \nand in the angle range of\nÎ¸ \nnâˆ’1 \nâ‰¤ Î¸ < Î¸ \nn \n, where m is an integer within [1, 4] which\nmeans the radius of a circle and n is an integer within [1, 12]\nwhich equally divides a circle into 12 sectors, as shown in\nFig. 5. That is, the skeleton context is calculated as\nFig. 6 Four prototypes (bottom) for the\nleft ventricle (the red object), extracted\nfrom the masks provided by the M&M\ndataset (top) (color figure online)\nFig. 5 Skeleton context of two matched points on different skeletons\n1 3\nPage 9 of 18 22\n\nQ. Chen et al.\nHere, the class indicator \nk âˆˆ 1, 2, Â· Â· Â· , K \n. To reduce the\ncomputation cost, i.e., reducing the number of shapes\ninvolved in computing \nR\n(k)\n, we only consider those shapes\nthat have a similar averaged distance \nD \nto the predicted one\ns\nÎ¸ \n, e.g., their \nD \ndifference is less than 0.1:\nmin\nÎ¸ \nL\nshape\n(s\nÎ¸\n)\ns.t.\nK\nâˆ‘\nk=1\nâˆ£\nâˆ£\nâˆ£ \nË†\nD\n(k)\n(s\nÎ¸\n) âˆ’ D\n(k)\nâˆ£\nâˆ£\nâˆ£ \nâ‰¤ 0.1. \n(12)\nThis minimization is typically handled by using the Lagrang-\nian dual, which is relaxed to an unconstrained optimization\nvia a soft penalty. That is, we integrate the distance con-\nstraint via a quadratic penalty, resulting in the unconstrained\nobjective below:\nL\nshape\n(s\nÎ¸\n) = \nâˆ‘\nk\nKL\n( \nË†\nR\n(k)\n(s\nÎ¸\n), R\n(k)\n)\n+ Î» \nâˆ‘\nk\nF ( \nË†\nD\n(k)\n(s\nÎ¸\n), D\n(k)\n). \n(13)\nHere, \nÎ» \nis a weight hyper-parameter to balance these\ntwo terms and \nF \nis the quadratic penalty function, i.e.,\nF (m\n1\n, m\n2\n) = [m\n1 \nâˆ’ 0.9m\n2\n]\n2 \n+ [1.1m\n2 \nâˆ’ m\n1\n]\n2 \n.\nNext, we consider the skeleton descriptor and use the skel-\neton matching cost as a regularizer:\nL\nskeleton\n(s\nÎ¸\n) = \nâˆ‘\nk\nMC\n(\nSC(s\nÎ¸\n), {SC\nz\n}\nK\np\nÃ—K\nz\n)\n, \n(14)\nwhere \nK \np \nis the number of prototypes and K is the num-\nber of segmentation classes. Hence, the shape prior loss is\ndefined as:\nL\nSP \n= L \nshape \n+ L \nskeleton \n. \n(15)\nBy collecting all the loss terms, we have the final objective\nfunction as follows:\nL\ntotal \n= L\nseg\n(\nM \ninit \n, M \npseudo \n, M \nvoxel\n)\n+ L\nseg\n(\nM \nfinal \n, M \npseudo \n, M \nvoxel\n)\n+ Î²\n1\nL\nbry\n(b, B) + Î²\n2\nL\nAB \n+ Î²\n3\nL\nSP \n.\n(16)\nHere, \nÎ²\n1\n, \nÎ²\n2\n, and \nÎ²\n3 \nare weights for balancing these terms,\nand their default value is set as 0.3.\nD\n(\nSC\n(k)\n, Â¯m\ni\n)\nâ‰¤ \nD\n(\nSC\n(k)\n, Â¯m\nj\n)\n, âˆ€j \u001b = i, \n(8)\nwhere i and j are within \n[1, K \np\n]\n, and \nD(Â·, Â·) \nis a distance\nmetric between two vectors, which is the matching cost\ncomputed based on Eq. 9.\nâ— Update After assigning each skeleton descriptor to a\nmedoid, we have \nK\np \nupdated clusters. We update the\nmedoid of each cluster by estimating a new descriptor\nthat has the minimum sum of distances to all other skel-\neton descriptors in its cluster.\nMatching Cost. After having the skeleton context \nSC\n(k)\nfor each class k of the predicted segmentation mask and its\ncorresponding \nK \np \nskeleton prototypes. Next, we find the\nclosest prototype for the skeleton context of each predicted\nmask and follow [46] to measure how close they are.\nAssume \np\n1\ni \nand \np\n2\nj \nare two points from these two skeletons,\nrespectively, based on Eq. 7 we use the following normal-\nized difference to measure their similarity between points\non the pair of skeleton contexts:\nC \n(\np\n1\ni \n, p\n2\nj\n) \n= \n1\n2\nâˆ‘\nm,n\n(\nH\nSC\n(\np\n1\ni \n, r \nm \n, Î¸ \nn\n) \nâˆ’ H\nSC\n(\np\n2\nj \n, r \nm \n, Î¸ \nn\n))\n2\nH\nSC \n(p\n1\ni \n, r \nm \n, Î¸ \nn\n) + H\nSC\n(\np\n2\nj \n, r \nm \n, Î¸ \nn\n) \n. \n(9)\nBy summing up the difference of all sample points on two\nskeletons, we obtain the matching cost between \nSC\n(k) \nand\nits closest one among \nK\np \nprototypes \n{SC\n(k)\nz \n}\nK\np\nz=1\n, that is,\nMC\n(\nSC\n(k)\n, {SC\n(k)\nz \n}\nK\np\nz=1\n)\n= min\nz\nâˆ‘\n(p\ni\n,p\nj\n)\nC\n(\nSC\n(k)\n(p\ni\n), SC\n(k)\nz \n(p\nj \n)\n)\n. \n(10)\n3.5.3 Regularization with shape priors\nWe use the above two shape descriptors based on shape\ncontexts, i.e., the class ratio \nR \nand the average distance to\nthe centroid \nD \n, and one shape descriptor based on skeleton\ncontext \nSC \n, to incorporate the shape prior information col-\nlected from the provided segmentation masks from a differ-\nent dataset.\nGiven a prediction \ns\nÎ¸ \n, we estimate its shape descrip-\ntor \nË†\nR\n(s\nÎ¸\n) \nand \nË†\nD\n(s\nÎ¸\n)\n, and compare them with those given\nunpaired shapes. In particular, we use a KL divergence to\nmeasure the class ratio distribution:\nL\nshape\n(s\nÎ¸\n) =\nK\nâˆ‘\nk=1\nKL\n( \nË†\nR\n(k)\n(s\nÎ¸\n), R\n(k)\n)\n. \n(11)\n1 3\n22 Page 10 of 18\n\nVolumetric medical image segmentation via scribble annotations and shape priors\ndataset is subject-wisely split into 236 for training with\nscribble annotations and 20 for testing with binary masks\nof the pituitary with lesions. The scribble annotations and\nsegmentation masks are provided by experts.\nM&Ms Dataset [47]. To provide shape prior for cardiac\nsegmentation on the ACDC dataset, we choose M&Ms as\na source for learning shape knowledge about ROIs. This\ndataset is composed of 375 patients with hypertrophic and\ndilated cardiomyopathies, as well as healthy subjects. All\nsubjects were scanned in clinical centers in three different\ncountries (Spain, Germany, and Canada) using four differ-\nent magnetic resonance scanner vendors (Siemens, General\nElectric, Philips, and Canon). The slice size is \n256 Ã— 216\nwith the pixel spacing varying from 1.20 to 1.46 mm.\nAbdomenCT-1K Dataset [48]. To provide shape prior for\nthe abdominal organ segmentation on the CHAOS dataset,\nwe choose AbdomenCT-1K as a source for learning shape\nknowledge about ROIs. This dataset is composed of 1112\nCT scans from 12 medical centers. The CT scans have\nresolutions of \n512 Ã— 512 \npixels with varying pixel sizes\nand slice thicknesses between 1.25â€“5 mm.\nWe have no shape prior on VS and private datasets since\nboth involve tumor regions that likely have irregular shapes.\nScribble Generation. For the ACDC dataset, we utilize\nexpert-drawn scribbles at both end-diastolic and end-\nsystolic phases as provided in [9]. For the VS and CHAOS\ndatasets, in line with [44], we generate simulated scribbles\nthrough iterative morphological erosion and closing of\nsegmentation masks, producing a one-pixel skeleton for\neach object. Given the complex nature of the resulting\nbackground scribble, we employ the ITK-SNAP tool to\nannotate the background using 1-pixel width curves.\nTraining Details. For all public datasets, we ran-\ndomly crop an image volume and obtain patches of size\n224 Ã— 224 Ã— 32 \nas the network inputs for training. For our\nprivate dataset, due to the original small image, the patch\nsize is set to \n192 Ã— 192 Ã— 8\n. If an input image has a smaller\nsize in one or more dimensions, we pad it with zeros to\nmatch the input size. At the inference stage, we use a slid-\ning window when an image is larger than inputs, with 25%\nof patch size overlaps at the borders.\nFor all public datasets, we train our model for 200 epochs\nwith early stopping. The weights of the network are initial-\nized by following a normal distribution with a mean of\n0 and a variance of 0.01. We use Adam optimizer with a\nweight decay \n10\nâˆ’7 \nand an initial learning rate 1\neâˆ’ \n4. The\nwhole training takes about 6 h with a batch size of 4 on\none NVIDIA GeForce RTX 3090 GPU. Differently, for our\nprivate dataset, we train the models for 50 epochs with early\nstopping and an Adam optimizer with a weight decay 2\neâˆ’\n7. Since the pituitary tumors have irregular shapes, while\n4 Experiments\n4.1 Datasets and experimental settings\nACDC Dataset [15]. This dataset consists of Cine MR\nimages collected from 100 patients by using various 1.5T\nand 3T MR scanners and different temporal resolutions.\nFor each patient, manual annotations of the right ventricle\n(RV), left ventricle (LV), and myocardium (MYO) are\nprovided for both the end-diastolic (ED) and end-systolic\n(ES) phase. The slice size is \n256 Ã— 208 \nwith the pixel\nspacing varying from 1.37 to 1.68 mm. The number of\nslices is between 28 and 40, and the slice thickness is 5\nmm or 8 mm. Following [9], we subject-wisely divide the\nACDC dataset into sets of 70%, 15%, and 15% for train-\ning, validation, and test, respectively. To compare with the\nprevious state-of-the-art methods that use unpaired masks\nto incorporate shape priors, we further divide the training\nset into two halves, i.e., 35 subjects with 70 images with\nscribble labels and 35 subjects with 70 mask images with\nsegmentation labels.\nVS Dataset [16]. This dataset collects T2-weighted\nMRIs from 242 subjects with a single sporadic vestibular\nschwannoma (VS) tumor. The image slice size is \n384 Ã— 384\nor \n448 Ã— 448\n, with a pixel spacing of \n0.5 Ã— 0.5mm\n2\n. The\nnumber of slices varies from 20 to 80, with a thickness of\n1.5mm. The VS tumor masks are manually annotated by\nneurosurgeons and physicists. The dataset is subject-wisely\nsplit into 176 for training, 20 for validation, and 46 for\ntesting.\nCHAOS Dataset [17]. This dataset has abdominal\nT1-weighted MR images collected from 20 subjects and\nthe corresponding segmentation masks for the liver, kid-\nneys, and spleen. The image slice size is \n256 Ã— 256 \nwith\na resolution of 1.36\nâˆ’ \n1.89 mm (average 1.61mm). The\nnumber of slices is between 26 and 50 (average 36) with\nthe slice thickness varying from 5.5 to 9 mm (average 7.84\nmm). We also subject-wisely divide this dataset into sets of\n70%, 15%, and 15% for training, validation, and testing,\nrespectively.\nPituitary Microadenoma Dataset. To test the perfor-\nmance of our algorithm in practice, we evaluate it on a\ndataset collected from Ruijin Hospital, Shanghai for the\nsegmentation task of the pituitary with microadenoma\nlesions. This dataset includes 256 T1-weighted augmented\nMRIs collected from 86 patients with pituitary micro-\nadenoma, consisting of a sequence of coronal slices of\nbrains. The dimension of each image slice varies, including\n448 Ã— 448\n, \n512 Ã— 512\n, \n768 Ã— 768\n, \n384 Ã— 384\n, \n360 Ã— 360\n,\n256 Ã— 228\n, \n336 Ã— 336 \nor \n256 Ã— 256\n, with the pixel spacing\nranging from 0.19 to 0.70 mm. The number of slices varies\nfrom 5 to 16, and the slice thickness is 1 mm or 3 mm. The\n1 3\nPage 11 of 18 22\n\nQ. Chen et al.\nthe GT mask; and precision, which assesses the accuracy\nof our positively-segmented voxels.\n4.2 Experimental results\n4.2.1 Comparison with SOTA methods\nTables 1 and 2 present our results on three public datasets\nand one private dataset with a comparison to our baselines.\nFollowing [9, 49], we report the dice average and standard\ndeviation(STD) (subscript) obtained from each method\non the test set. For ACDC, VS, and CHAOS datasets, the\nupper bounds of the segmentation performance are mainly\nprovided by the 2.5D UNet, which are colored in blue in\nTable 1. Compared to the scribble-based SOTA method on\nACDC and CHAOS datasets, i.e., ZScribbleSeg [24], scrib-\nble2D5 with shape prior improves the Dice score by \n4.1%\nand \n9.7%\n, reduces the HD95 by 7.4 mm and 1.8 mm, and\nimproves the precision by \n1.1% \nand \n13.6%\n, respectively.\nCompared to the extreme-point-based SOTA method on the\nVS dataset, i.e., InExtremIS [49], although our method has\na lower precision and HD95, it improves the Dice score by\n0.7%\n. We do not report InExtremISâ€™ results on ACDC and\nCHAOS datasets because extreme points for these two data-\nsets are not available or easy to generate.\nFigure 7 visualizes some sample results of our method\ncompared to six baselines. Overall, we have fewer false\nthe active boundary loss smoothes out the predicted bound-\nary, we set its weight \nÎ²\n2 \nas 0.\nBaselines and Evaluation Metrics. To assess the effec-\ntiveness of our segmentation methods, we compare\nthem against three categories of baseline methods: two\nfully-supervised approaches (2D UNet [50] and 2.5D\nUNet [1]), several weakly-supervised methods utilizing\nscribbles (\nUNet\nPCE \n[7], ConstrainedCNN [23], MAAG\n[9], ScribbleSeg [3], and ZscribbleSeg [24]), and one\nweakly-supervised method that employs extreme points\n[49]. We employ three key metrics for evaluation: the Dice\nscore, which quantifies the overlap between our predicted\nsegmentation and the ground truth (GT) mask; the 95th\npercentile of the Hausdorff Distance (HD95), which mea-\nsures the boundary distance between our segmentation and\nTable 1 Quantitative comparison among baselines and our method for volumetric segmentation on three datasets\nApproach Dataset\nACDC\nVS CHAOS\nDice \nâ†‘\n(%)\nHD95 \nâ†“\n(mm)\nPreci-\nsion \nâ†‘\n(%)\nDice \nâ†‘\n(%)\nHD95 \nâ†“\n(mm)\nPreci-\nsion \nâ†‘\n(%)\nDice \nâ†‘\n(%)\nHD95 \nâ†“\n(mm)\nPreci-\nsion\nâ†‘\n(%)\nSupervi-\nsion type\nScribble \nUNet\nP CE \n[7] \n79.0\nÂ±06 \n6.9\nÂ±04 \n77.3\nÂ±06 \n44.6\nÂ±08 \n6.5\nÂ±03 \n43.8\nÂ±05 \n34.4\nÂ±06 \n9.4\nÂ±03 \n36.6\nÂ±05\nConstrainedCNN\n[23]\n80.1\nÂ±04 \n5.4\nÂ±05 \n79.8\nÂ±05 \n68.1\nÂ±04 \n7.1\nÂ±04 \n67.7\nÂ±04 \n62.1\nÂ±04 \n6.6\nÂ±04 \n65.1\nÂ±04\nMAAG [9] \n83.4\nÂ±04 \n8.6\nÂ±04 \n78.5\nÂ±05 \n69.4\nÂ±06 \n5.9\nÂ±05 \n56.8\nÂ±05 \n66.4\nÂ±05 \n3.8\nÂ±05 \n57.2\nÂ±06\nScribbleSeg [3] \n87.2\nÂ±07 \n9.3\nÂ±05 \n86.8\nÂ±05 \n80.6\nÂ±04 \n8.2\nÂ±04 \n79.0\nÂ±04 \n77.1\nÂ±04 \n4.1\nÂ±04 \n72.3\nÂ±04\nZScribbleSeg [24] \n88.1\nÂ±06 \n8.5\nÂ±04 \n87.5\nÂ±05 \n79.8\nÂ±04 \n8.5\nÂ±05 \n78.8\nÂ±05 \n77.5\nÂ±05 \n4.6\nÂ±04 \n75.5\nÂ±04\nOurs w/o PLPM \n83.2\nÂ±05 \n7.7\nÂ±03 \n84.1\nÂ±05 \n78.8\nÂ±05 \n4.6\nÂ±01 \n77.6\nÂ±05 \n81.2\nÂ±07 \n5.8\nÂ±08 \n82.0\nÂ±06\nOurs w/o SBPM \n85.6\nÂ±05 \n4.6\nÂ±04 \n85.5\nÂ±04 \n80.6\nÂ±05 \n7.1\nÂ±03 \n81.6\nÂ±04 \n84.6\nÂ±05 \n5.5\nÂ±05 \n83.1\nÂ±05\nOurs w/o ABL \n88.7\nÂ±04 \n5.1\nÂ±08 \n86.0\nÂ±05 \n81.0\nÂ±03 \n4.8\nÂ±01 \n80.1\nÂ±05 \n85.6\nÂ±04 \n4.8\nÂ±05 \n81.3\nÂ±02\nScribble2D5 (ours) \n90.6\nÂ±03 \n2.3\nÂ±05 \n84.7\nÂ±05 \n82.6\nÂ±07\n4.7\nÂ±04 \n81.5\nÂ±06 \n86.0\nÂ±04 \n2.9\nÂ±02 \n88.2\nÂ±03\nScribble2D5 w/ SP \n92.2\nÂ±04 \n1.1\nÂ±01 \n88.6\nÂ±05 \nâ€“ â€“ â€“ \n87.2\nÂ±04\n2.8\nÂ±03 \n89.1\nÂ±04\nP\nâ€  \nInExtremIS [49] â€“ â€“ â€“ \n81.9\nâˆ—\nÂ±03 \n3.7\nâˆ—\nÂ±03 \n92.9\nâˆ—\nÂ±02 \nâ€“ â€“ â€“\nMask 2D UNet [50] \n93.0\nÂ±05 \n3.5\nÂ±06 \n90.2\nÂ±07 \n80.4\nÂ±03 \n7.3\nÂ±04 \n81.2\nÂ±03 \n82.3\nÂ±04 \n3.3\nÂ±01 \n81.7\nÂ±05\n2.5D UNet [1] \n96.1\nÂ±03 \n0.3\nÂ±01 \n95.3\nÂ±04 \n87.3\nÂ±02 \n6.8\nÂ±04 \n84.7\nÂ±03 \n90.8\nÂ±03 \n1.1\nÂ±01 \n91.4\nÂ±05\nMean and standard deviation (subscript) are reported. The upper bounds are in italics, and the best results by using scribbles are marked in bold.\nâ€  \nP is short for Point, indicating the extreme points. Such annotations are only available for the VS dataset.\n*These numbers are taken from the InExtremIS paper. (Best viewed in italics).\nTable 2 Quantitative comparison among baselines and our method for\nthe pituitary and microadenoma segmentation on our private dataset\nMethod \nDice \nâ†‘ \n(%) HD95 \nâ†“ \n(mm) Precision \nâ†‘ \n(%)\nUNet\nPCE \n[7] \n63.0\nÂ±06 \n6.9\nÂ±04 \n67.3\nÂ±06\nMAAG [9] \n75.6\nÂ±04 \n7.6\nÂ±04 \n74.5\nÂ±05\nOurs w/o PLPM \n72.1\nÂ±05 \n5.5\nÂ±03 \n74.1\nÂ±05\nOurs w/o SBPM \n74.6\nÂ±05 \n3.8\nÂ±04 \n75.8\nÂ±04\nOurs w/o ABL \n76.7\nÂ±04 \n5.1\nÂ±08 \n76.0\nÂ±05\nScribble2D5 (ours) \n78.8\nÂ±03 \n2.3\nÂ±05 \n77.7\nÂ±05\nMean and standard deviation (subscript) are reported. The best\nresults are marked in bold\n1 3\n22 Page 12 of 18\n\nVolumetric medical image segmentation via scribble annotations and shape priors\n4.2.2 Ablation study\nTo evaluate the contribution of each component in our seg-\nmentation approach, we conduct an ablation study using\nfour distinct configurations:\n(a) Ours w/o PLPM Scribble2D5 without the pseudo\nlabel propagation module (PLPM);\n(b) Ours w/o SBPM Scribble2D5 without the static\nboundary prediction module (SBPM), which removes the\nstatic boundary prediction module and active boundary loss;\n(c) Ours w/o ABL Scribble2D5 without the active bound-\nary loss (ABL);\n(d) Scribble2D5 w/ SP Scribble2D5 with shape prior\n(SP) if available.\nThe ablation study results on public and private datasets\nare reported in Tables 1 and 2, respectively. For the ACDC,\nCHAOS, and our private dataset, we can observe consistent\nimprovement by adding PLPM, SBPM, and ABL modules,\none by one. The ACDC experiment in Table 3 also dem-\nonstrates the effectiveness of introducing shape prior. Due\nto the fact that we have separated from half of the training\nset as unpaired masks, the results are different from those\nobtained using the full training set in Table 1. Regarding our\npositives compared to scribble-based methods, i.e., \nUNet\nPCE\nand MAAG, and better boundary localization with more\naccurate boundary prediction for each ROI. Regarding the\ncomparison with mask-based methods, our method some-\ntimes generates even better masks than 2D UNet, while it\nstill needs improvements in details compared to 2.5D UNet.\nFor our private dataset, we save the ones with segmenta-\ntion masks for testing and the ones with scribble annotations\nfor training. Also, we have no shape prior information about\na pituitary with tumors. Therefore, we compare our method\nwith scribble-based methods only. As reported in Table 2,\nour method outperforms MAAG, by improving 3.2% dice\nscore. Figure 8 shows sample results which demonstrate\nthat our method is better at obtaining segmentation details.\nMore quantitative comparison results are included in\nTable 3, which reports the performance comparison on the\nACDC dataset between eight baselines and our methods by\nusing 35 subjects with 70 images and scribbles and by add-\ning another 70 unpaired masks as shape prior for learning.\nOur methods (with and without shape priors) outperform\nbaselines by a good margin on both individual segmentation\nregions and their average.\nFig. 7 Qualitative comparison among baseline methods and ours on\nthe ACDC, VS, and CHAOS datasets. ACDC: Red: LV, green: MYO,\nblue: RV; VS: Red: vestibular schwannoma tumor; CHAOS: Red:\nLiver, green: left kidney, blue: right kidney, yellow: spleen; for all,\nwhite indicates the background (Best viewed in color) (color figure\nonline)\n1 3\nPage 13 of 18 22\n\nQ. Chen et al.\nHD95 and precision values are just slightly lower than the\nhighest ones. We still consider that our full model performs\nthe best in the ablation study on this dataset.\nFigure 9 displays two examples from the ACDC dataset,\nshowcasing both intermediate and final prediction results\nof our segmentation method. In the absence of the Pseudo\nLabel Propagation Module (PLPM), our approach tends to\ngenerate false positives distant from the Region of Interest\n(ROI). Similarly, lacking the Supervoxel Boundary Propa-\ngation Module (SBPM) leads to over-segmentation within\nthe ROI. Incorporating a boundary map and active boundary\nregularization allows our method to refine predictions based\non image edges and texture details. Further enhancement\nis achieved by integrating a shape prior, which aligns the\nROIâ€™s shape more closely with its true form, thereby yield-\ning predictions that most closely match the ground truth.\nresults on the VS dataset, only the Dice score consistently\nincreases while adding each module gradually; however, the\nTable 3 Performance comparison in Dice score (%) on the ACDC data-\nset between our Scribble2D5 and current weakly-supervised methods\nMethod Data LV MYO RV Avg\n35 subjects with 70 scribbles\nUNet\nPCE \n[7] \nscribbles 84.2 76.4 69.3 76.6\nUNet\nWPCE \n[9] \nscribbles 78.4 67.5 56.3 67.4\nUNet\nCRF \n[22] \nscribbles 76.6 66.1 59.0 67.2\nCycleMix [18] scribbles 88.3 79.8 86.3 84.8\nScribble2D5 (ours) scribbles 92.3 82.2 89.8 88.1\n35 subjects with 70 scribbles + 70 unpaired masks\nUNet\nD \n[9] \ns+m \n40.4 59.7 75.3 58.5\nPostDAE [36] \ns+m \n80.6 66.7 55.6 67.6\nACCL [28] \ns+m \n87.8 79.7 73.5 80.3\nMAAG [9] \ns+m \n87.9 81.7 75.2 81.6\nours w/ SP \ns+m \n94.2 84.1 92.0 90.1\nWe borrow the segmentation results reported in [18] for comparison.\n(s+m: scribbles+masks)\nThe best results are marked in bold\nFig. 9 Visualization of Scribble2D5â€™s intermediate and final results\non images sampled from the ACDC dataset. The ground truth (GT)\nis colored in blue, like the blue region in the first column and the blue\ncontours overlaid on other images, and our predictions are colored in\nred. The yellow arrows show the effect produced by using the active\nboundary loss (ABL) and considering shape prior (SP) (Best viewed in\ncolor) (color figure online)\nFig. 8 Qualitative comparison between our Scribble2D5 and two baselines on our private pituitary microadenoma dataset\n1 3\n22 Page 14 of 18\n\nVolumetric medical image segmentation via scribble annotations and shape priors\nthe robustness of our model with a scarcity of scribble anno-\ntations on the ACDC dataset. In this experiment, we only\nannotate partial 2D axial slices, e.g., 25%, 50%, or 75%\nof the image slices of a volume, respectively. To gener-\nate scribbles on those slices with missing annotations, we\nexplore both watershed and random walker methods. These\ntwo methods are based on structural similarity index mea-\nsure (SSIM) sampling or equal-interval sampling. Table 4\nand Fig. 10 show the dice score of our Scribble2D5 using\nthe pseudo labels generated by these two methods with two\nkinds of sampling strategies. Choosing a good label propa-\ngation strategy, like the random walker approach with SSIM\nsampling, can reduce the annotation amount by 25% while\nachieving comparable segmentation accuracy. We do not\ntest our method using the random walker with equal-inter-\nval sampling since the watershed experiment shows SSIM\nis a better sampling choice.\n4.2.5 Comparison between real and generated scribbles\nTo further study the possibility of using generated scribbles\nto replace the real ones, we experiment on the ACDC data-\nset and compare the manual scribbles annotated by experts\nand the one generated by simulating scribbles through an\niterative morphological erosion and closing of segmentation\nmasks [44]. Firstly, we measure the size difference between\nthese two scribbles. The manual scribbles annotated for the\nforeground ROIs occupy 11.7% of a mask, while the gener-\nated ones occupy 7.2%. That is, the manual scribbles tend\nto cover more regions of interest. Then, we evaluate the\nperformance difference between them. As shown in Table 5,\nusing the manual scribbles achieve 90.7% on average in\nDice score, while only 83.5% by using the generated ones.\nThis is probably because, unlike the manual ones, the gen-\nerated scribbles locate close to the center lines of ROIs as\nshown in Fig. 12, which are far away from the boundary\nand provide less information about ROIs. Hence, if manual\nscribbles are available in the VS and CHAOS datasets, the\n4.2.3 Comparison on generated pseudo labels\nThe effectiveness of the weakly supervised scribble based\nmethod heavily depends on the quality of the pseudo-\nlabels. Figure 11 shows pseudo-labels generated by vari-\nous methods, enabling a direct comparison. Our method\nproduces pseudo-labels with higher accuracy and finer\nboundary details than InExtremIS, enhancing the modelâ€™s\nability to learn and achieve segmentation boundaries that\nclosely align with the ground truth. This visual comparison\nunderscores the effectiveness of our approach in generating\nhigh-quality pseudo-labels that contribute to more precise\nboundary delineation.\n4.2.4 Robustness to limited annotations\nSince we work on volumetric image segmentation, each\nvolume has a sequence of 2D slices that need scribble anno-\ntations for training. In practice, we probably have missing\nannotations on some slices. In this experiment, we analyze\nTable 4 The performance (Dice scores) on generated scribbles from\ndifferent label propagation methods\nType of scribbles 25% 50% 75% 100%\nRandom walker w/ SSIM 79.5\nÂ±3.1 \n83.7\nÂ±2.3 \n86.0\nÂ±1.7 \n86.1\nÂ±2.3\nWatershed w/ SSIM 77.7\nÂ±2.3 \n83.7\nÂ±2.1 \n85.8\nÂ±1.9 \n86.1\nÂ±2.3\nWatershed w/ equal-interval 75.0\nÂ±2.4 \n79.1\nÂ±1.8 \n81.6\nÂ±2.2 \n86.1\nÂ±2.3\nThe best results are marked in bold\nTable 5 The performance (Dice Scores) on generated scribbles com-\npared with real scribbles provided by experts\nType of Scribbles LV MYO RV Avg\nReal 94.3 89.6 88.2 90.7\nGenerated 87.9 84.2 78.4 83.5\nThe best results are marked in bold\nFig. 11 Pseudo labels generated by\ndifferent methods\nFig. 10 Dice score obtained on the label generation and test data by\nSSIM sampling or equal-interval sampling when changing the percent-\nage of available annotations. We consider 100% (using all the densely-\nannotated masks) as the upper bound\n1 3\nPage 15 of 18 22\n\nQ. Chen et al.\nscribbles impact segmentation accuracy; developing specific\nannotation guidelines for different regions of interest could\nenhance the practical application, which will be explored\nin future work. Furthermore, while our method narrows the\nperformance gap, it still lags behind fully-supervised seg-\nmentation approaches. This could be addressed by integrat-\ning SAM-based pertaining models. Future enhancements\ncould include interactive segmentation and incorporate user\nfeedback to boost model performance.\nAcknowledgements This research work was supported by the\nNational Natural Science Foundation of China (NSFC) 62203303,\nShanghai Municipal Science and Technology Major Project 2021SHZ-\nDZX0102, and Shanghai Jiao Tong University Trans-med Awards\nperformance of our method has the potential to be further\nimproved.\n5 Conclusion and discussion\nIn this paper, we introduce Scribble2D5, a weakly-super-\nvised volumetric image segmentation network that signifi-\ncantly surpasses existing scribble-based methods. However,\nour approach has limitations, notably that our pseudo-\nboundary labels consist of pre-computed 2D boundaries\nrather than true 3D representations, an area for future explo-\nration. Additionally, we note that the shape and location of\nFig. 12 Comparison between real\nand generated scribbles (Best\nviewed in color, GT: the ground-\ntruth mask) (color figure online)\n1 3\n22 Page 16 of 18\n\nVolumetric medical image segmentation via scribble annotations and shape priors\n16. Shapey, J., Kujawa, A., Dorent, R., Wang, G., Bisdas, S., Dimi-\ntriadis, A., Grishchuck, D., Paddick, I., Kitchen, N., Bradford, R.,\net al.: Segmentation of vestibular schwannoma from magnetic\nresonance imaging: an open annotated dataset and baseline algo-\nrithm. Cancer Imaging Arch. 8, 286 (2021)\n17. Kavur, A.E., Gezer, N.S., BarÄ±ÅŸ, M., Aslan, S., Conze, P.-H.,\nGroza, V., Pham, D.D., Chatterjee, S., Ernst, P., Ã–zkan, S., et al.:\nChaos challenge-combined (CT-MR) healthy abdominal organ\nsegmentation. Med. Image Anal. 69, 101950 (2021)\n18. Zhang, K., Zhuang, X.: Cyclemix: a holistic strategy for medical\nimage segmentation from scribble supervision. CVPR (2022)\n19. Ji, Z., Shen, Y., Ma, C., Gao, M.: Scribble-based hierarchical\nweakly supervised learning for brain tumor segmentation. In:\nInternational Conference on Medical Image Computing and\nComputer-Assisted Intervention, pp. 175â€“183 (2019). Springer\n20. Can, Y.B., Chaitanya, K., Mustafa, B., Koch, L.M., Konukoglu,\nE., Baumgartner, C.F.: Learning to segment medical images with\nscribble-supervision alone. In: Deep Learning in Medical Image\nAnalysis and Multimodal Learning for Clinical Decision Support,\npp. 236â€“244. Springer (2018)\n21. Tang, M., Perazzi, F., Djelouah, A., Ben Ayed, I., Schroers, C.,\nBoykov, Y.: On regularized losses for weakly-supervised CNN\nsegmentation. In: Proceedings of the European Conference on\nComputer Vision (ECCV), pp. 507â€“522 (2018)\n22. Zheng, S., Jayasumana, S., Romera-Paredes, B., Vineet, V., Su,\nZ., Du, D., Huang, C., Torr, P.H.: Conditional random fields as\nrecurrent neural networks. In: Proceedings of the IEEE Interna-\ntional Conference on Computer Vision, pp. 1529â€“1537 (2015)\n23. Kervadec, H., Dolz, J., Tang, M., Granger, E., Boykov, Y., Ayed,\nI.B.: Constrained-CNN losses for weakly supervised segmenta-\ntion. Med. Image Anal. 54, 88â€“99 (2019)\n24. Zhang, K., Zhuang, X.: Zscribbleseg: Zen and the art of\nscribble supervised medical image segmentation. Preprint at\narXiv:2301.04882 (2023)\n25. Kirillov, A., Mintun, E., Ravi, N., Mao, H., Rolland, C., Gus-\ntafson, L., Xiao, T., Whitehead, S., Berg, A.C., Lo, W.-Y., et al.:\nSegment anything. Preprint at arXiv:2304.02643 (2023)\n26. Wu, J., Fu, R., Fang, H., Liu, Y., Wang, Z., Xu, Y., Jin, Y., Arbel,\nT.: Medical sam adapter: adapting segment anything model for\nmedical image segmentation. Preprint at arXiv:2304.12620\n(2023)\n27. Wang, H., Guo, S., Ye, J., Deng, Z., Cheng, J., Li, T., Chen,\nJ., Su, Y., Huang, Z., Shen, Y., et al.: Sam-med3D. Preprint at\narXiv:2310.15161 (2023)\n28. Zhang, P., Zhong, Y., Li, X.: Accl: Adversarial constrained-cnn\nloss for weakly supervised medical image segmentation. Preprint\nat arXiv:2005.00328 (2020)\n29. Isola, P., Zhu, J.-Y., Zhou, T., Efros, A.A.: Image-to-image trans-\nlation with conditional adversarial networks. In: Proceedings of\nthe IEEE Conference on Computer Vision and Pattern Recogni-\ntion, pp. 1125â€“1134 (2017)\n30. Clough, J.R., Byrne, N., Oksuz, I., Zimmer, V.A., Schnabel,\nJ.A., King, A.P.: A topological loss function for deep-learning\nbased image segmentation using persistent homology. Preprint at\narXiv:1910.01877 (2019)\n31. Oktay, O., Ferrante, E., Kamnitsas, K., Heinrich, M., Bai, W.,\nCaballero, J., Cook, S.A., De Marvao, A., Dawes, T., Oâ€™Regan,\nD.P., et al.: Anatomically constrained neural networks (ACNNs):\napplication to cardiac image enhancement and segmentation.\nIEEE Trans. Med. Imaging 37(2), 384â€“395 (2017)\n32. Kingma, D.P., Welling, M.: Auto-encoding variational Bayes.\nPreprint at arXiv:1312.6114 (2013)\n33. Dalca, A.V., Guttag, J., Sabuncu, M.R.: Anatomical priors in con-\nvolutional networks for unsupervised biomedical segmentation.\nIn: Proceedings of the IEEE Conference on Computer Vision and\nPattern Recognition, pp. 9290â€“9299 (2018)\nResearch (YG2023LC02).\nReferences\n1. Shapey, J., Wang, G., Dorent, R., Dimitriadis, A., Li, W., Paddick,\nI., Kitchen, N., Bisdas, S., Saeed, S.R., Ourselin, S., et al.: An\nartificial intelligence framework for automatic segmentation and\nvolumetry of vestibular schwannomas from contrast-enhanced\nT1-weighted and high-resolution T2-weighted MRI. J. Neuro-\nsurg. 134(1), 171â€“179 (2019)\n2. Dey, R., Hong, Y.: Asc-net: adversarial-based selective network\nfor unsupervised anomaly segmentation. In: International Con-\nference on Medical Image Computing and Computer-Assisted\nIntervention, pp. 236â€“247 (2021). Springer\n3. Luo, X., Hu, M., Liao, W., Zhai, S., Song, T., Wang, G., Zhang,\nS.: Scribble-supervised medical image segmentation via dual-\nbranch network and dynamically mixed pseudo labels supervi-\nsion. Preprint at arXiv:2203.02106 (2022)\n4. Ahn, J., Kwak, S.: Learning pixel-level semantic affinity with\nimage-level supervision for weakly supervised semantic seg-\nmentation. In: Proceedings of the IEEE Conference on Computer\nVision and Pattern Recognition, pp. 4981â€“4990 (2018)\n5. Roth, H.R., Yang, D., Xu, Z., Wang, X., Xu, D.: Going to\nextremes: weakly supervised medical image segmentation. Mach.\nLearn. Knowl. Extr. 3(2), 507â€“524 (2021)\n6. Lin, D., Dai, J., Jia, J., He, K., Sun, J.: Scribblesup: scribble-\nsupervised convolutional networks for semantic segmentation.\nIn: Proceedings of the IEEE Conference on Computer Vision and\nPattern Recognition, pp. 3159â€“3167 (2016)\n7. Tang, M., Djelouah, A., Perazzi, F., Boykov, Y., Schroers, C.:\nNormalized cut loss for weakly-supervised cnn segmentation.\nIn: Proceedings of the IEEE Conference on Computer Vision and\nPattern Recognition, pp. 1818â€“1827 (2018)\n8. Dorent, R., Joutard, S., Shapey, J., Bisdas, S., Kitchen, N., Brad-\nford, R., Saeed, S., Modat, M., Ourselin, S., Vercauteren, T.:\nScribble-based domain adaptation via co-segmentation. In: Inter-\nnational Conference on Medical Image Computing and Com-\nputer-Assisted Intervention, pp. 479â€“489 (2020). Springer\n9. Valvano, G., Leo, A., Tsaftaris, S.A.: Learning to segment from\nscribbles using multi-scale adversarial attention gates. IEEE\nTrans. Med. Imaging 40(8), 1990â€“2001 (2021)\n10. Zhang, J., Yu, X., Li, A., Song, P., Liu, B., Dai, Y.: Weakly-super-\nvised salient object detection via scribble annotations. In: Pro-\nceedings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition, pp. 12546â€“12555 (2020)\n11. Chen, X., Williams, B.M., Vallabhaneni, S.R., Czanner, G., Wil-\nliams, R., Zheng, Y.: Learning active contour models for medical\nimage segmentation. In: Proceedings of the IEEE/CVF Confer-\nence on Computer Vision and Pattern Recognition, pp. 11632â€“\n11640 (2019)\n12. Fang, Y., Xie, J., Dai, G., Wang, M., Zhu, F., Xu, T., Wong, E.: 3D\ndeep shape descriptor. In: Proceedings of the IEEE Conference on\nComputer Vision and Pattern Recognition, pp. 2319â€“2328 (2015)\n13. Jiang, M., Kong, J., Bebis, G., Huo, H.: Informative joints based\nhuman action recognition using skeleton contexts. Signal Pro-\ncess. Image Commun. 33, 29â€“40 (2015)\n14. Chen, Q., Hong, Y.: Scribble2d5: weakly-supervised volumet-\nric image segmentation via scribble annotations. Preprint at\narXiv:2205.06779 (2022)\n15. Bernard, O., Lalande, A., Zotti, C., Cervenansky, F., Yang, X.,\nHeng, P.-A., Cetin, I., Lekadir, K., Camara, O., Ballester, M.A.G.,\net al.: Deep learning techniques for automatic MRI cardiac multi-\nstructures segmentation and diagnosis: is the problem solved?\nIEEE Trans. Med. Imaging 37(11), 2514â€“2525 (2018)\n1 3\nPage 17 of 18 22\n\nQ. Chen et al.\nmedical image analysis problems. Preprint at arXiv:1708.06297\n(2017)\n45. Rdusseeun, L., Kaufman, P.: Clustering by means of medoids.\nIn: Proceedings of the Statistical Data Analysis Based on the L1\nNorm Conference, Neuchatel, Switzerland, vol. 31 (1987)\n46. Belongie, S., Malik, J., Puzicha, J.: Shape matching and object\nrecognition using shape contexts. IEEE Trans. Pattern Anal.\nMach. Intell. 24(4), 509â€“522 (2002)\n47. MartÃ­n-Isla, C., Campello, V.M., Izquierdo, C., Kushibar, K.,\nSendra-Balcells, C., Gkontra, P., Sojoudi, A., Fulton, M.J., Arega,\nT.W., Punithakumar, K., et al.: Deep learning segmentation of the\nright ventricle in cardiac mri: The m &ms challenge. IEEE Jour-\nnal of Biomedical and Health Informatics (2023)\n48. Ma, J., Zhang, Y., Gu, S., Zhu, C., Ge, C., Zhang, Y., An, X.,\nWang, C., Wang, Q., Liu, X., et al.: Abdomenct-1k: Is abdominal\norgan segmentation a solved problem? IEEE Trans. Pattern Anal.\nMach. Intell. 44(10), 6695â€“6714 (2021)\n49. Dorent, R., Joutard, S., Shapey, J., Kujawa, A., Modat, M.,\nOurselin, S., Vercauteren, T.: Inter extreme points geodesics for\nend-to-end weakly supervised image segmentation. In: Interna-\ntional Conference on Medical Image Computing and Computer-\nAssisted Intervention, pp. 615â€“624 (2021). Springer\n50. Ronneberger, O., Fischer, P., Brox, T.: U-net: Convolutional net-\nworks for biomedical image segmentation. In: International Con-\nference on Medical Image Computing and Computer-assisted\nIntervention, pp. 234â€“241 (2015). Springer\nPublisherâ€™s Note Springer Nature remains neutral with regard to juris-\ndictional claims in published maps and institutional affiliations.\nSpringer Nature or its licensor (e.g. a society or other partner) holds\nexclusive rights to this article under a publishing agreement with the\nauthor(s) or other rightsholder(s); author self-archiving of the accepted\nmanuscript version of this article is solely governed by the terms of\nsuch publishing agreement and applicable law.\n34. Yue, Q., Luo, X., Ye, Q., Xu, L., Zhuang, X.: Cardiac segmen-\ntation from LGE MRI using deep neural network incorporating\nshape and spatial priors. In: International Conference on Medi-\ncal Image Computing and Computer-Assisted Intervention, pp.\n559â€“567 (2019). Springer\n35. Painchaud, N., Skandarani, Y., Judge, T., Bernard, O., Lalande,\nA., Jodoin, P.-M.: Cardiac MRI segmentation with strong ana-\ntomical guarantees. In: International Conference on Medical\nImage Computing and Computer-Assisted Intervention, pp. 632â€“\n640 (2019). Springer\n36. Larrazabal, A.J., MartÃ­nez, C., Glocker, B., Ferrante, E.: Post-\nDAE: anatomically plausible segmentation via post-processing\nwith denoising autoencoders. IEEE Trans. Med. Imaging 39(12),\n3813â€“3820 (2020)\n37. Celebi, M.E., Aslandogan, Y.A.: A comparative study of three\nmoment-based shape descriptors. In: International Conference\non Information Technology: Coding and Computing (ITCCâ€™05)-\nVolume II, 1, 788â€“793 (2005). IEEE\n38. Achanta, R., Shaji, A., Smith, K., Lucchi, A., Fua, P., SÃ¼sstrunk,\nS.: SLIC superpixels compared to state-of-the-art superpixel\nmethods. IEEE Trans. Pattern Anal. Mach. Intell. 34(11), 2274â€“\n2282 (2012)\n39. Vincent, L., Soille, P.: Watersheds in digital spaces: an efficient\nalgorithm based on immersion simulations. IEEE Trans. Pattern\nAnal. Mach. Intell. 13(06), 583â€“598 (1991)\n40. Spitzer, F.: Principles of Random Walk, vol. 34. Springer, Berlin\n(2001)\n41. Xie, S., Tu, Z.: Holistically-nested edge detection. In: Proceed-\nings of the IEEE International Conference on Computer Vision,\npp. 1395â€“1403 (2015)\n42. Arbelaez, P., Maire, M., Fowlkes, C., Malik, J.: Contour detection\nand hierarchical image segmentation. IEEE Trans. Pattern Anal.\nMach. Intell. 33(5), 898â€“916 (2010)\n43. Yang, M., Yu, K., Zhang, C., Li, Z., Yang, K.: Denseaspp for\nsemantic segmentation in street scenes. In: Proceedings of the\nIEEE Conference on Computer Vision and Pattern Recognition,\npp. 3684â€“3692 (2018)\n44. Rajchl, M., Koch, L.M., Ledig, C., Passerat-Palmbach, J., Mis-\nawa, K., Mori, K., Rueckert, D.: Employing weak annotations for\n1 3\n22 Page 18 of 18",
    "version": "5.3.31"
  },
  {
    "numpages": 12,
    "numrender": 12,
    "info": {
      "PDFFormatVersion": "1.4",
      "Language": "en-US",
      "EncryptFilterName": null,
      "IsLinearized": true,
      "IsAcroFormPresent": false,
      "IsXFAPresent": false,
      "IsCollectionPresent": false,
      "IsSignaturesPresent": false,
      "Author": "Kexin Jiang ",
      "CreationDate": "D:20250205134812+08'00'",
      "Creator": "Adobe InDesign 15.1 (Windows)",
      "Custom": {
        "CrossMarkDomains[1]": "springer.com",
        "CrossMarkDomains[2]": "springerlink.com",
        "CrossmarkDomainExclusive": "true",
        "CrossmarkMajorVersionDate": "2010-04-23",
        "doi": "10.1007/s10278-024-01198-4",
        "robots": "noindex"
      },
      "Keywords": "Meniscus;Deep learning;Segmentation;Classification;Weak supervision",
      "ModDate": "D:20250205104405Z",
      "Producer": "Adobe PDF Library 15.0",
      "Subject": "Journal of Imaging Informatics in Medicine, https://doi.org/10.1007/s10278-024-01198-4",
      "Title": "Fully and Weakly Supervised Deep Learning for Meniscal Injury Classification, and Location Based on MRI",
      "Trapped": { "name": "False" }
    },
    "metadata": {
      "xmp:createdate": "2025-02-05T13:48:12+08:00",
      "xmp:creatortool": "Adobe InDesign 15.1 (Windows)",
      "xmp:modifydate": "2025-02-05T10:44:05Z",
      "xmp:metadatadate": "2025-02-05T10:44:05Z",
      "pdf:producer": "Adobe PDF Library 15.0",
      "pdf:keywords": "Meniscus;Deep learning;Segmentation;Classification;Weak supervision",
      "pdf:trapped": "False",
      "dc:format": "application/pdf",
      "dc:identifier": "https://doi.org/10.1007/s10278-024-01198-4",
      "dc:publisher": "Springer International Publishing",
      "dc:description": "Journal of Imaging Informatics in Medicine, https://doi.org/10.1007/s10278-024-01198-4",
      "dc:subject": [
        "Meniscus",
        "Deep learning",
        "Segmentation",
        "Classification",
        "Weak supervision"
      ],
      "dc:title": "Fully and Weakly Supervised Deep Learning for Meniscal Injury Classification, and Location Based on MRI",
      "dc:creator": [
        "Kexin Jiang",
        "Yuhan Xie",
        "Xintao Zhang",
        "Xinru Zhang",
        "Beibei Zhou",
        "Mianwen Li",
        "Yanjun Chen",
        "Jiaping Hu",
        "Zhiyong Zhang",
        "Shaolong Chen",
        "Keyan Yu",
        "Changzhen Qiu",
        "Xiaodong Zhang"
      ],
      "crossmark:doi": "10.1007/s10278-024-01198-4",
      "crossmark:majorversiondate": "2010-04-23",
      "crossmark:crossmarkdomainexclusive": "true",
      "crossmark:crossmarkdomains": "springer.comspringerlink.com",
      "prism:url": "https://doi.org/10.1007/s10278-024-01198-4",
      "prism:doi": "10.1007/s10278-024-01198-4",
      "prism:issn": "2948-2933",
      "prism:volume": "38",
      "prism:number": "1",
      "prism:startingpage": "191",
      "prism:endingpage": "202",
      "prism:aggregationtype": "journal",
      "prism:publicationname": "Journal of Imaging Informatics in Medicine",
      "prism:copyright": "The Author(s) under exclusive licence to Society for Imaging Informatics in Medicine",
      "pdfx:crossmarkmajorversiondate": "2010-04-23",
      "pdfx:crossmarkdomainexclusive": "true",
      "pdfx:doi": "10.1007/s10278-024-01198-4",
      "pdfx:robots": "noindex",
      "pdfx:crossmarkdomains": "springer.comspringerlink.com",
      "xmpmm:documentid": "uuid:e3bf5506-31b4-4caa-a228-302ab72f25e4",
      "xmpmm:instanceid": "uuid:4f242c44-5afd-4e69-8ff4-a57da1e2b564",
      "xmpmm:renditionclass": "default",
      "xmpmm:versionid": "1",
      "xmpmm:history": "converteduuid:919ad0c2-e956-4ef7-be92-0ceeb7f2c7a5converted to PDF/A-2bpdfToolbox2025-02-05T10:43:50Z",
      "pdfaid:part": "2",
      "pdfaid:conformance": "B",
      "sn:authorinfo": "Xiaodong Zhang http://orcid.org/0000-0002-4789-7324",
      "pdfaextension:schemas": "http://ns.adobe.com/pdfx/1.3/pdfxAdobe Document Info PDF eXtension SchemaexternalMirrors crossmark:MajorVersionDateCrossmarkMajorVersionDateTextexternalMirrors crossmark:CrossmarkDomainExclusiveCrossmarkDomainExclusiveTextinternalMirrors crossmark:DOIdoiTextexternalMirrors crossmark:CrosMarkDomainsCrossMarkDomainsseq TextinternalA name object indicating whether the document has been modified to include trapping informationrobotsTextinternalID of PDF/X standardGTS_PDFXVersionTextinternalConformance level of PDF/X standardGTS_PDFXConformanceTextinternalCompany creating the PDFCompanyTextinternalDate when document was last modifiedSourceModifiedTexthttp://crossref.org/crossmark/1.0/crossmarkCrossmark SchemainternalUsual same as prism:doiDOITextexternalThe date when a publication was publishe.MajorVersionDateTextinternalCrossmarkDomainExclusiveCrossmarkDomainExclusiveTextinternalCrossMarkDomainsCrossMarkDomainsseq Texthttp://prismstandard.org/namespaces/basic/2.0/prismPrism SchemaexternalThis element provides the url for an article or unit of content. The attribute platform is optionally allowed for situations in which multiple URLs must be specified. PRISM recommends that a subset of the PCV platform values, namely â€œmobileâ€ and â€œwebâ€, be used in conjunction with this element. NOTE: PRISM recommends against the use of the #other value allowed in the PRISM Platform controlled vocabulary. In lieu of using #other please reach out to the PRISM group at prism-wg@yahoogroups.com to request addition of your term to the Platform Controlled Vocabulary.urlURIexternalThe Digital Object Identifier for the article.\nThe DOI may also be used as the dc:identifier. If used as a dc:identifier, the URI form should be captured, and the bare identifier should also be captured using prism:doi. If an alternate unique identifier is used as the required dc:identifier, then the DOI should be specified as a bare identifier within prism:doi only. If the URL associated with a DOI is to be specified, then prism:url may be used in conjunction with prism:doi in order to provide the service endpoint (i.e. the URL). doiTextexternalISSN for an electronic version of the issue in which the resource occurs. Permits publishers to include a second ISSN, identifying an electronic version of the issue in which the resource occurs (therefore e(lectronic)Issn. If used, prism:eIssn MUST contain the ISSN of the electronic version.issnTextinternalVolume numbervolumeTextinternalIssue numbernumberTextinternalStarting pagestartingPageTextinternalEnding pageendingPageTextexternalThe aggregation type specifies the unit of aggregation for a content collection. Comment PRISM recommends that the PRISM Aggregation Type Controlled Vocabulary be used to provide values for this element. Note: PRISM recommends against the use of the #other value currently allowed in this controlled vocabulary. In lieu of using #other please reach out to the PRISM group at info@prismstandard.org to request addition of your term to the Aggregation Type Controlled Vocabulary. aggregationTypeTextexternalTitle of the magazine, or other publication, in which a resource was/will be published. Typically this will be used to provide the name of the magazine an article appeared in as metadata for the article, along with information such as the article title, the publisher, volume, number, and cover date. Note: Publication name can be used to differentiate between a print magazine and the online version if the names are different such as â€œmagazineâ€ and â€œmagazine.com.â€publicationNameTextexternalCopyrightcopyrightTexthttp://ns.adobe.com/pdf/1.3/pdfAdobe PDF SchemainternalA name object indicating whether the document has been modified to include trapping informationTrappedTexthttp://ns.adobe.com/xap/1.0/mm/xmpMMXMP Media Management SchemainternalUUID based identifier for specific incarnation of a documentInstanceIDURIinternalThe common identifier for all versions and renditions of a document.DocumentIDURIinternalThe common identifier for all versions and renditions of a document.OriginalDocumentIDURIhttp://www.aiim.org/pdfa/ns/id/pdfaidPDF/A ID SchemainternalPart of PDF/A standardpartIntegerinternalAmendment of PDF/A standardamdTextinternalConformance level of PDF/A standardconformanceTextSpringer Nature ORCID Schemahttp://springernature.com/ns/xmpExtensions/2.0/snauthorInfoBag AuthorInformationexternalAuthor information: contains the name of each author and his/her ORCiD (ORCiD: Open Researcher and Contributor ID). An ORCiD is a persistent identifier (a non-proprietary alphanumeric code) to uniquely identify scientific and other academic authors.AuthorInformationhttp://springernature.com/ns/xmpExtensions/2.0/authorInfo/authorSpecifies the types of author information: name and ORCID of an author.nameTextGives the name of an author.orcidURIGives the ORCID of an author."
    },
    "text": "Vol.:(0123456789)\nJournal of Imaging Informatics in Medicine (2025) 38:191â€“202\nhttps://doi.org/10.1007/s10278-024-01198-4\nFully and Weakly Supervised Deep Learning for Meniscal Injury\nClassification, and Location Based on MRI\nKexin Jiang\n1 \nÂ· Yuhan Xie \n2 \nÂ· Xintao Zhang\n1 \nÂ· Xinru Zhang\n1 \nÂ· Beibei Zhou\n1 \nÂ· Mianwen Li \n1 \nÂ· Yanjun Chen \n1 \nÂ· Jiaping Hu\n1 \nÂ·\nZhiyong Zhang \n2 \nÂ· Shaolong Chen\n2 \nÂ· Keyan Yu \n1 \nÂ· Changzhen Qiu\n2 \nÂ· Xiaodong Zhang\n1\nReceived: 17 February 2024 / Revised: 14 June 2024 / Accepted: 8 July 2024 / Published online: 17 July 2024\nÂ© The Author(s) under exclusive licence to Society for Imaging Informatics in Medicine 2024\nAbstract\nMeniscal injury is a common cause of knee joint pain and a precursor to knee osteoarthritis (KOA). The purpose of this study\nis to develop an automatic pipeline for meniscal injury classification and localization using fully and weakly supervised net-\nworks based on MRI images. In this retrospective study, data were from the osteoarthritis initiative (OAI). The MR images\nwere reconstructed using a sagittal intermediate-weighted fat-suppressed turbo spin-echo sequence. (1) We used 130 knees\nfrom the OAI to develop the LGSA-UNet model which fuses the features of adjacent slices and adjusts the blocks in Siam to\nenable the central slice to obtain rich contextual information. (2) One thousand seven hundred and fifty-six knees from the\nOAI were included to establish segmentation and classification models. The segmentation model achieved a DICE coefficient\nranging from 0.84 to 0.93. The AUC values ranged from 0.85 to 0.95 in the binary models. The accuracy for the three types\nof menisci (normal, tear, and maceration) ranged from 0.60 to 0.88. Furthermore, 206 knees from the orthopedic hospital\nwere used as an external validation data set to evaluate the performance of the model. The segmentation and classification\nmodels still performed well on the external validation set. To compare the diagnostic performances between the deep learning (DL) models\nand radiologists, the external validation sets were sent to two radiologists. The binary classification model outperformed the\ndiagnostic performance of the junior radiologist (0.82â€“0.87 versus 0.74â€“0.88). This study highlights the potential of DL in\nknee meniscus segmentation and injury classification which can help improve diagnostic efficiency.\nKeywords Meniscus Â· Deep learning Â· Segmentation Â· Classification Â· Weak supervision\nIntroduction\nComposed of fibrocartilage within the knee joint, the\nmenisci are essential for shock absorption, weight distri-\nbution, and friction reduction [1, 2]. Orthopedic surgeons\nfrequently encounter meniscal injuries, a prevalent cause\nof knee pain and a precursor to knee osteoarthritis (KOA)\n[3], with an average annual incidence of 60â€“70 cases per\n100,000 [4]. Therefore, timely diagnosis of meniscal injury\nis essential to minimize patient morbidity and plan effective\ntreatment strategies.\nVarious examination methods are available to identify\nmeniscus tears, including the history, physical examina-\ntion, and magnetic resonance imaging (MRI) [5]. The\nmeniscus is mainly composed of type I collagen fibers, and\ndue to this structural characteristic, MRI is currently the\nmost important imaging method for detecting and grading\nmeniscus lesions [6]. Additionally, it provides comprehen-\nsive information for treatment planning. MRI demonstrates\nhigh sensitivity and specificity in detecting meniscal tears,\nachieving rates of 96% and 88% for medial meniscal tears,\nand 82% and 98% for lateral meniscal tears at 3.0 T, respec-\ntively [7]. They are categorized based on their injury type\nand location on MRI [8, 9], providing an accurate descrip-\ntion classification and localization of these injuries can aid\nKexin Jiang and Yuhan Xie contributed equally to this work.\n* Changzhen Qiu\nqiuchzh@mail.sysu.edu.cn\n* Xiaodong Zhang\nddautumn@126.com\n1 \nDepartment of Medical Imaging, The Third Affiliated\nHospital, Southern Medical University (Academy\nof Orthopedics Guangdong Province), 183 Zhongshan Ave\nW, Guangzhou 510630, China\n2 \nSchool of Electronics and Communication Engineering, Sun\nYat-sen University, Guangzhou, China\n\n192 Journal of Imaging Informatics in Medicine (2025) 38:191â€“202\nphysicians in selecting the most suitable treatment approach\n[10]. The semi-quantitative score, such as MRI osteoarthritis\nknee score (MOAKS) [11], can provide more in-depth and\ndetailed information, and help to understand the relationship\nbetween meniscal lesions and KOA, especially for experi-\nenced radiologists who have undergone musculoskeletal\ntraining. Nonetheless, accurate diagnosis poses challenges\nfor radiologists without musculoskeletal specialization,\ninterns, or those in rural settings lacking specialized train-\ning. Moreover, traditional diagnostic approaches that employ\nsemi-quantitative scoring systems require significant effort\nand are prone to variability due to reader subjectivity.\nDeep learning (DL) offers significant advantages in\naddressing these inaccuracies and shortcomings by analyz-\ning vast datasets generated from clinically widespread MRI\nscans, thereby streamlining the labor-intensive diagnostic\nreporting process [12]. DL has been widely employed in\nrecent years for preprocessing and analyzing MR images\n[13]. It cannot only automatically segment and diagnose\nhuman tissues, but also optimize the quality of magnetic\nresonance images [14â€“16]. In contrast to tasks involving\nnon-medical image interpretation, which primarily depend\non the analysis of global features, the identification of\npathology in MRI images is often confined to smaller areas\nof the image. Especially in the case of MRI assessments\nfor musculoskeletal injuries, this holds significant relevance\ndue to the relatively small size or slender shapes of many\nassociated anatomical parts, such as ligaments, tendons, and\nthe meniscus.\nAlthough DL is a powerful approach for recognizing\nsubtle imaging patterns, the need for more comprehensive\ntraining data grows as it aims to detect finer details within\nimages. The previous models used for the segmentation of\nthe medial meniscus (MM) and lateral meniscus (LM) have\nshown good performance [17â€“19]. The DL model devel-\noped by Astuto et al. [20] segmented the anterior and pos-\nterior horns of the meniscus, covering both the MM and\nLM. However, their 3D Intersection over Union (IoU) values\nranged only from 0.49 to 0.61, indicating that the segmen-\ntation accuracy in these areas requires improvement [20].\nFor the classification model, most of the current research\nis focused on the injury classification of the global menis-\ncus [21â€“23] or MM and LM [24]. Tack et al. established\na binary classification model based on the six sub-regions\nof the meniscus [25]. However, these models only achieve\nthree classifications based on the medial and lateral of the\nmeniscus or binary classifications based on six subregions.\nFurthermore, most of the training of segmentation models\nin current research requires manual delineation of the region\nof interest (ROIs) on each image as input data for the model,\nwhich is time-consuming and labor-intensive.\nTherefore, this study aimed to develop a fully automated\nmethod to segment the knee meniscus into six sub-regions:\nanterior horn, body, and posterior horn in both MM and LM,\nusing both fully and weakly supervised networks, and com-\npare the differences between these models. Following this,\nconstruct binary and multi-classification models to classify\nmeniscal injuries based on the automatically segmented sub-\nregions. This study constructed a fully automated pipeline\nfor meniscus segmentation and multi classification. Our\nexperimental results indicate that compared with previous\nordinary U-NET models, our LGSA model is more accurate\nin meniscus segmentation and suitable for weakly super-\nvised networks. In addition, the classification model con-\nstructed based on deep learning methods performs better\nthan that of junior radiologists.\nOur study has the following contributions that is sum-\nmarized as below:\nâ€¢ \nOur LGSA model is more accurate than previous models\nin meniscus segmentation and suitable for weakly super-\nvised networks.\nâ€¢ \nThe segmentation and classification models have been exter-\nnally validated and still perform well on external datasets.\nâ€¢ \nThe classification model constructed based on deep\nlearning methods performs better than that of junior\nradiologists.\nThe remainder of the paper is arranged as follows:\nâ€œMethodsâ€ section explains the sources of the dataset and\nthe model structure. Our experimental evaluation results\nare presented in â€œResultsâ€ section. â€œDiscussionâ€ section\ndiscusses the comparison of our work with other current\nmodels. Finally, â€œLimitationâ€ and â€œConclusionâ€ sections\nrespectively discuss limitations and conclusions.\nMethods\nPatients\nParticipants were selected from the osteoarthritis initiative\n(OAI) database, a multi-center, longitudinal study that col-\nlected clinical and imaging data during 9 years of follow-up\nin 4796 subjects (9592 knees) with or at high risk for KOA.\nOAI was approved by the Human Research Committee and\nthe Internal Review Committee. All data from the study are\navailable at https://nda.nih.gov/oai/.\nFor this study, 1760 knees were identified within the\nframework of the Pivotal OAI MR Imaging Analyses\n(POMA) project and the Foundation for the National Insti-\ntutes of Health (FNIH) project, both of which are nested\ncase-control studies within the OAI. After excluding those\nwithout MRI data or MOAKS measurement from these, 130\nknees were randomly selected to develop the segmentation\nmodel. And we included all available baseline MRIs from\n\n193Journal of Imaging Informatics in Medicine (2025) 38:191â€“202\nthe two projects (n = 1756) to construct the classification\nmodel. During the two experimental phases, datasets for\ntraining, validation, and testing were randomly divided in a\n7:1:2 ratio. Figure 1 presents the participant inclusion and\nexclusion process for both models.\nAdditionally, 206 MRIs of knees from an orthopedic hospital\nas an external dataset were used to further validate the robust-\nness of the segmentation and classification models. The internal\nand external data sets demographics are shown in Table 1.\nMRI Protocol and Assessment\nFor the patients from the OAI, all MR images were obtained\nusing the same type of 3.0 Tesla MRI scanner (Magnetom\nTrio, Siemens Healthcare) from four clinical centers,\nemploying a sagittal intermediate-weighted fat-suppressed\nturbo spin-echo sequence. However, the external dataset uti-\nlized proton density weightedâ€“spectral attenuated inversion\nrecovery sequence scanning. These sequences were chosen\nfor their clinical routine application and their excellent sen-\nsitivity to fluid-filled structures, aiding in the detection of\nmeniscal abnormal signals such as effusion or tears. Details\nof the MRI sequence for internal and external datasets are\nprovided in Table 2.\nThe MOAKS system, assessed by experienced musculo-\nskeletal radiologists, served as the gold standard for labeling.\nThis system scores the anterior horns, body, and posterior\nhorns of the MM and LM in MR images, with scores rang-\ning from 0 to 8 (0 = normal meniscus, 1 = signal abnormality,\n2 = radial tear, 3 = horizontal tear, 4 = vertical tear, 5 = com-\nplex tear, 6 = partial maceration, 7 = progressive partial mac-\neration, 8 = complete maceration) reflecting the severity of\nmeniscal pathology. All semi-quantitative scores in the inter-\nnal data set were derived from publicly available records of\nFig. 1 Flowchart illustrates the participant selection process for the segmentation and classification models. OAI, osteoarthritis initiative;\nPOMA, pivotal OAI M imaging analyses; FNIH, Foundation for the National Institutes of Health; MOAKS, MRI osteoarthritis knee score\nTable 1 Characteristics of internal and external data sets\nNormal\n(MOAKS 0â€“1)\nTear\n(MOAKS 2â€“5)\nMaceration\n(MOAKS 6â€“8)\nInternal data set\nNo. of knees 820 473 463\nAge (y) 59 (53, 66) 63 (56, 70) 65 (57, 71)\nNo. of women 630 258 199\nExternal data set\nNo. of knees 65 92 49\nAge (y) 32 (19, 25.5) 47.5 (29.3, 60.8) 62 (51.5, 69.5)\nNo. of women 28 41 31\n\n194 Journal of Imaging Informatics in Medicine (2025) 38:191â€“202\nimaging markers in the OAI database. For the external data\nset, the labels evaluated by two radiologists with over 10\nyears of diagnostic experience were used as the ground truth.\nIn addition, to compare the performance of the classifica-\ntion model with that of the radiologist, the external dataset\nunderwent independent review by another two radiologists\nfor injury classification, who were blinded to the patientsâ€™\npathological and other imaging results.\nMRI Annotations\nThe ROIs of the meniscus were manually delineated on sagit-\ntal fat-suppressed MRI sequences by a junior radiologist with\n2 years of diagnostic experience, under the guidance of a sen-\nior radiologist with 10 years of musculoskeletal imaging diag-\nnostic experience, using ITK-SNAP software (http://www.\nitksnap.org/). Subsequently, the delineated ROIs received\nfinal validation from an expert musculoskeletal radiologist\nwith more than 15 years of diagnostic experience.\nTable 2 MR Image acquisition parameters in two data sets\nIW TSE intermediate-weighted turbo spin-echo, PDW-SPAIR proton\ndensity weightedâ€“spectral attenuated inversion recovery\nSource OAI External dataset\nManufacture Siemens Philips Philips\nSequence IW TSE PDW-SPAIR PDW-SPAIR\nField strength (T) 3.0 3.0 1.5\nPlane Sagittal Sagittal Sagittal\nRepetition time (msec) 3200 3000 3000\nEcho time (msec) 30 30 30\nSlice thickness (mm) 3.0 3.5 3.5\nFlip angle (Â°) 180 90 90\nFig. 2 The network structure of the segmentation model based on full and weak supervision\n\n195Journal of Imaging Informatics in Medicine (2025) 38:191â€“202\nExperimental Details\nAll our experiments were completed using two RTX3090\ngraphics cards, within an experimental environment powered\nby Python3.7 and Pytorch1.0. For both meniscus segmenta-\ntion and meniscus injury grading tasks, we normalized all\ninputs to ensure image values were scaled between 0 and 1.\nDuring the meniscus segmentation training, we employed\na batch size of 8 and utilized Adam as the optimizer with a\nlearning rate of 1e-4, momentum set to 0.9, and weight decay\nalso at 1e-4. The training was spread over 200 epochs. Given\nthat the meniscus occupies a relatively small area within the\nknee MR imagesâ€”surrounded by bones, muscles, and other\nsoft tissues which introduce a significant amount of irrelevant\ninformation â€”it was crucial to enhance the modelâ€™s feature\nextraction capability specific to the meniscus. Therefore, in\nthe preprocessing step, we applied the center cropping to crop\nthe two-dimensional MRI image slices to a size of 384 Ã— 384,\nsubsequently resizing them to 224 Ã— 224. This approach was\nbased on statistical analysis indicating that the meniscus does\nnot extend beyond the central area of 384 Ã— 384.\nFor the training of the meniscus injury classification\nmodel, we adjusted the parameters to a batch size of 16,\nusing Adam as the optimizer with a learning rate of 5e-5,\nmomentum of 0.9, and a weight decay of 1e-4. After seg-\nmenting and localizing the meniscus images, they were\ncropped to the specified dimensions and input into the model\nin a three-dimensional format.\nThe Development of the Meniscus Segmentation Model\nUNet++ and DenseUNet did not show significant advantages in\noptimizing model far jump connections in meniscus MR images,\nand their segmentation performance was similar to that of the\nmain benchmark model UNet [26, 27]. The basic component of\nthe Swin UNet segmentation model uses a transformer instead\nof CNN, and its segmentation performance on meniscus MR\nimages is very poor, which fully demonstrates that the trans-\nformer is not suitable for meniscus MR images [28]. TransU-\nNet uses a combination of transformer and CNN components\nto design a model, which has slightly higher segmentation per-\nformance than UNet. However, the model architecture is very\nlarge, resulting in high computational resource consumption\n[29]. Compared with other models proposed in previous stud-\nies, our models used a structurally simple and robust UNet as\nthe backbone network, introduces positional prior guidance and\ntwin context interaction, and fully combines the characteristics of\nmeniscus images to design, greatly improving the performance of\nthe meniscus segmentation model. The methodology behind our\nmeniscus segmentation model, incorporating both fully super-\nvised and weakly supervised networks, is depicted in Fig. 2. In\nthe first part, our proposed segmentation network LGSA-UNet\nadopted a dual-phase method to leverage the location guidance\nfor precise segmentation. This is achieved by integrating the\nfeatures of adjacent slices through a Siamese adjustment block,\nenriching the central slices with contextual information for supe-\nrior segmentation outcomes. Furthermore, to categorize the\nmeniscus into six sub-regions, we introduced a segmentation\nalgorithm that relies on sequential features and domain connec-\ntivity for delineation. Meanwhile, we devised a method for creat-\ning weak labels that combine key points and lines, facilitating the\nannotation of meniscus segmentation. This approach employs\ncurves to outline the meniscus edges and points to identify its\ninterior regions (Fig. 2). Utilizing these weak labels, regional\ngrowth techniques were applied to generate pseudo labels, which\nthen serve as training data for the segmentation model. The weak\nlabels significantly reduce the annotation time, while achieving\na segmentation model performance that closely mirrors that of a\nfully supervised approach.\nMeniscus sub-regions segmentation algorithm\nAlgorithm Meniscus subregion segmentation\nInput: Whole Meniscus Segmentation Results\nOutput: Meniscal Subregion Segmentation Results\nInitialization: Sagittalize the image into a sequence of slices S = s\n1 \n,\ns\n2 \n,..s\nN\n. Set sequence features: F = [blank, connect, split, blank].\nWherein, the sequence feature indicates that the slice consists of no\ntarget, body connected state, anterior and posterior hole separation\nstate, and no target\n1. Slice-by-slice connected domain extraction: Extract the top\ntwo connected domains slice by slice, remove the impurity area,\nand identify the slice as a target/non-target (blank) state, and a\ntarget state is identified as connect/split based on the number of\nconnected domains\n2. Subregion Segmentation Based on Sequence Features:\nDetermine the lateral/ medial type at slice \ns\n1 \naccording to the foot\ninformation (left/right)\nstate = 0, n = 1;\nwhile n < N and state < len(F):\nGet slice s\nn\n;\nif F(s \nn\n) == blank:\nif F(s\nn\n) != F(s\nnâ€“1\n):\nstate += 1; move the sequence state;\nelse:\nKeep the state and mark the slice output as blank type;\nelse:\nif F(s \nn\n) == connect:\nif F(s\nnâ€“1\n) == split:\nF(s \nn\n) = split; correct the mistake in slice-by-slice connected\ndomain extraction and mark the slice output as anterior/posterior\nhorn type;\nif F(s\nn\n) ! = F(s\nnâ€“1\n):\nstate += 1; move the sequence state;\nMark the slice output as body type;\nif F(s \nn\n) == split:\nif F(s\nn\n) ! = F(s\nnâ€“1\n):\nstate += 1; move the sequence state;\nMark the slice output as anterior/posterior type according\nto the position;\nChange the direction and conduct the same algorithm above to\nsegment the subregion in the other part (lateral/ medial)\n\n196 Journal of Imaging Informatics in Medicine (2025) 38:191â€“202\nThe Development of the Meniscus Classification Model\nAccording to the segmentation results of the six sub-regions,\nwe used bounding boxes to cut out the target area to obtain\nthe positioning results, which are then input into six differ-\nent classification networks to grade the meniscus damage\nin this region. As shown in Fig. 3, the two-level classifiers\nused the 3D ResNet26 network for classification. Specifi-\ncally, we adopted a tree-like hierarchical structure, in which\nthe first-level classifier was to classify the damage results\ninto normal (score 0â€“1) and abnormal (score 2â€“8) cases. The\nsecond-level classifier was to classify the cases with abnor-\nmality into tear (score 2â€“5) and maceration (score 6â€“8). In\nthe training process, we performed class balance processing\non the training set, and through data enhancement opera-\ntions of random translation and grayscale transformation, the\ntraining data of different classes can be balanced. We adopt\nthe balanced cross-entropy function as the loss function. Its\nexpression is as follows:\nwhere the weight size was set according to the distribution\nof positive and negative samples, that is \nğ›¼\n1âˆ’ğ›¼ \n= \nn\nm\n.\nThe 3D ResNet26 has performed a total of 32 times\ndownsampling in the length and width dimensions, and eight\ntimes downsampling in the depth dimension. The feature\nmap was subjected to global average pooling after multi-\nple dimension reductions to obtain feature vectors and then\ninput into a fully connected layer for classification output.\nL = \n1\nN\n\u001f\u001e\nm\ny\ni\n=0 \nâˆ’ ğ›¼log\n\u001d\n\u001ep\n\u001c \n+ \n\u001e\nn\ny\ni\n=1 \nâˆ’ (1 âˆ’ ğ›¼\n)log\n\u001d\n1 âˆ’ \u001ep\n\u001c\n\u001b\nRegarding the external dataset, certain MRI parameters\ndiffered from the internal dataset. As a result, we fine-tuned\n(equivalent to retrained) the segmentation and classification\nmodels mentioned earlier using the training set from the\nexternal dataset to ensure compatibility.\nStatistical Analysis\nThe median (upper and lower quartiles) is used to repre-\nsent patients whose age does not meet the normal distri-\nbution. For the segmentation model, the Dice coefficient\nis used as an evaluation metric. This coefficient quanti-\nfies the accuracy of image segmentation by measuring\nthe extent of overlap between the model-generated seg-\nmentation results and the true, or ground truth, segmenta-\ntion. It is calculated as twice the intersection of the two\nsets divided by the sum of their sizes, resulting in a value\nthat ranges from 0 (no overlap) to 1 (perfect overlap). The\ncloser the Dice coefficient is to 1, the better the overlap\nbetween the segmentation results and the true segmenta-\ntion, that is, the more accurate the segmentation effect.\nFor the injury classification model, we employed the area\nunder the receiver operating characteristic curve (AUC),\nsensitivity, specificity, and accuracy as evaluation indica-\ntors. The receiver operating characteristic (ROC) curve\nwas used to demonstrate the performance of the models.\nTo compare the performance of our deep learning (DL)\nmodels on external validation sets against the diagnostic\ncapabilities of radiologists, we utilized the Delong test.\nP < 0.05 indicated a significant difference.\nFig. 3 The network structure of the classification model\n\n197Journal of Imaging Informatics in Medicine (2025) 38:191â€“202\nResults\nSegmentation Model Performance on Internal\nand External Test Sets\nOur segmentation models, employing both fully and weakly\nsupervised methods, demonstrated high Dice coefficients,\nas summarized in Table 3. Specifically, for the fully super-\nvised approach, Dice coefficients ranged from 0.84 to 0.93\nacross different sub-regions of the MM and LM. The weakly\nsupervised approach also showed strong performance, with\nDice coefficients between 0.84 and 0.92. It is worth not-\ning that there were no significant differences observed\nbetween fully supervised and weakly supervised methods\nfor the anterior horn of MM, as well as the anterior horn and\nbody of LM. These results underscore the modelsâ€™ accurate\nsegmentation capabilities, with Fig. 4 illustrating the close\nagreement between manual and automated segmentations.\nOn the external test set, all Dice coefficient values exceeded\n0.75, affirming the robustness of the models despite a slight\ndecrease in performance compared to the internal test set.\nTable 4 summarizes the comparison results between the\ndeep learning segmentation model of this study and other\nresearch models.\nClassification Model Performance on the Internal\nTest Set\nIn binary classification tasks, based on the fully supervised\napproach, the AUC values achieved 0.85 to 0.95 for the six\nsub-regions; based on the weakly supervised method, the\nAUC values were 0.83 to 0.94, slightly lower than the fully\nsupervised models in certain areas, the differences were not\nsignificant. As for multi-classification tasks, AUC values\nTable 3 Dice coefficient of\nsegmentation models\nThe P values represent the comparison between fully supervision and weakly supervision in the internal\ntest set\nMM medial meniscus, LM lateral meniscus\nMM LM\nMethods Anterior Body Posterior Anterior Body Posterior\nInternal test set\nFull supervision 0.93 0.90 0.87 0.86 0.84 0.87\nWeak supervision 0.92 0.86 0.85 0.84 0.84 0.85\nP values 0.23 < 0.01 0.01 0.15 0.86 0.01\nExternal test set 0.85 0.80 0.84 0.86 0.77 0.83\nFig. 4 Examples illustrate\nthe results of the manual and\nautomatic segmentations. The\nfirst column shows manual\nsegmentation, while the second\nand third columns respectively\nshow schematic diagrams of\nautomatic segmentation based\non fully supervised and weakly\nsupervised network models. a\nand b show results for which\na large level of agreement was\nobtained for each method. In c,\ncompared to manual segmenta-\ntion, the weakly supervised\nnetwork model did not correctly\noutline the posterior horn of the\nmeniscus\n\n198 Journal of Imaging Informatics in Medicine (2025) 38:191â€“202\nranged from 0.6 to 0.9 under fully supervised learning and\n0.5 to 0.8 with weakly supervised learning. Table 5 lists the\nspecific performance of the classification models.\nExternal Test Set Evaluation\nDue to the better performance of models based on fully\nsupervised networks compared to those based on weakly\nsupervised networks, we only conducted external validation\non fully supervised networks. Performances of the classifi-\ncation model, junior radiologist, and senior radiologist on\nthe external test data set reached 0.82 to 0.87, 0.75 to 0.88,\nand 0.81 to 0.91 (Fig. 5). Figure 6 shows the time spent by\nthe DL model and the radiologists for manual meniscus seg-\nmentation and classification on the external dataset, respec-\ntively. The DL models significantly expedited the segmenta-\ntion and classification processes compared to radiologists,\nemphasizing the efficiency of automated methods.\nDiscussion\nDeep learning (DL) has significantly advanced medical\nimaging analysis, offering unparalleled feature extraction\ncapabilities compared to traditional methods. Yet, the appli-\ncation of DL in generating structured reports, such as those\naligned with the MRI osteoarthritis knee score (MOAKS)\nsystem, remains underexplored. In this work, we proposed a\nTable 4 Comparison of segmentation efficiency between this research\nmodel and other models\nSegmentation models Dice (%)\nLGSA-UNet 90.32 Â± 0.32\nUnet 87.12 Â± 0.35\nUnet++ 87.37 Â± 0.43\nDenseunet 86.89 Â± 0.55\nSwinunet 81.10 Â± 0.87\nTransunet 88.16 Â± 0.38\nTable 5 Evaluation of classification model performance with various metrics\nAUC area under the receiver operating characteristic curve, MM medial menisci, LM lateral menisci\nFull supervision Weak supervision\nAUC Specificity Sensitivity AUC Specificity Sensitivity\nMM Anterior 0.91 0.88 0.89 0.91 0.91 0.78\nBody 0.91 0.88 0.84 0.91 0.86 0.84\nPosterior 0.95 0.88 0.91 0.94 0.86 0.87\nAnterior 0.85 0.87 0.82 0.83 0.84 0.75\nLM Body 0.91 0.88 0.87 0.91 0.81 0.87\nPosterior 0.90 0.88 0.92 0.88 0.82 0.81\nMulti-classification (accuracy)\nFull supervision Weak supervision\nNormal Tear Maceration Normal Tear Maceration\nMM Anterior 0.88 0.60 0.74 0.91 0.52 0.63\nBody 0.88 0.71 0.68 0.86 0.64 0.64\nPosterior 0.88 0.79 0.75 0.86 0.77 0.73\nAnterior 0.87 0.64 0.67 0.84 0.53 0.63\nLM \nBody 0.88 0.72 0.74 0.81 0.68 0.74\nPosterior 0.88 0.73 0.79 0.83 0.56 0.70\nFig. 5 Comparison of the binary classification model based on fully\nsupervised networks and radiologistsâ€™ area under the receiver oper-\nating characteristic curve on the external test set. MB, body on the\nmedial meniscus; MA, anterior horn on the medial meniscus; MP,\nposterior horn on the medial meniscus; LB, body on the medial\nmeniscus; LA, anterior horn on the lateral meniscus; LP, posterior\nhorn on the lateral meniscus\n\n199Journal of Imaging Informatics in Medicine (2025) 38:191â€“202\ncomprehensive DL pipeline for automatic meniscus segmen-\ntation and injury classification from MR images.\nThe proposed pipeline is divided into two different parts.\nFirst, we established meniscal segmentation models based\non fully and weakly supervised networks. Secondly, we built\nbinary class and multi-class DL models to classify damage\nin the six sub-regions of the meniscus. The entire pipeline\nrequires no manual interaction, and the segmentation and\nclassification processes are notably fast. The efficiency of\nour models, with segmentation times averaging 0.53 s and\nclassification times 0.03 s, presents a substantial improve-\nment over manual processing, underscoring the potential of\nDL to streamline radiological assessments (Fig. 6).\nThe precise segmentation of the meniscus is an important\nfoundation for the classification of meniscus injuries. Divid-\ning the meniscus into inner and outer sides is currently the\nmost commonly used segmentation method. Norman et al.\nused a 2D U-Net convolutional network structure DL model\nto automatically segment the meniscus in MR images, with\na medial meniscus DSC of 0.75 and a lateral meniscus DSC\nof 0.81 [30]. Although 2D methods may be appropriate for\nsome simple applications, musculoskeletal injuries typically\nrely on the synthesis of 3D contextual information. The same\napplies to identifying tears in the meniscus. Identifying fiber\ndiscontinuities requires evaluating the trajectory of multiple\nconsecutive sections of the upper meniscus. Astuto et al. [20]\nfurther divided the meniscus into four horns by calculating\ntwo consecutive 3D V-Nets. This segmentation method has\ngreater clinical significance, but its segmentation accuracy\nis poor. The mean Â± standard deviation intersection over\nunion values computed for the 3D bounding boxes of four\nhorns of the meniscus is 0.49 Â± 6.15 to 0.61 Â± 0.15. The\nauthors speculate that this may be due to the small volume\nof the meniscus [20]. Our LGSA-UNet model overcomes\nthese problems by fusing the features of adjacent slices and\nadjusting the blocks in Siam to enable the central slice to\nobtain rich contextual information. The Dice coefficients\nof our segmentation model achieved 0.84â€“0.93 for the six\nsub-regions. The model demonstrates high performance with\nfiner segmentation of sub-regions compared to previous\nstudies, enhancing its clinical significance. Moreover, we\nexplored whether the weakly supervised network model can\neffectively perform automatic segmentation of the menis-\ncus and compared them with fully supervised network mod-\nels. The results indicate that both the fully supervised and\nweakly supervised methods performed well. Specifically,\nwhile the performance of the fully supervised method was\nslightly higher than that of the weakly supervised method,\nthis difference was statistically significant only in the body\nand posterior horn of MM, as well as the posterior horn of\nLM. Due to the introduction of the weakly supervised labe-\nling method, the complexity and time of manual labeling\nare greatly reduced under the premise of ensuring the per-\nformance of the segmenter. Our DL-based models obtained\na remarkably superior segmentation and classification per-\nformance. We illustrate the enhanced accuracy in network\nclassification achieved through leveraging the contextual\ninformation from adjacent image slices.\nVarious studies have been conducted to establish\nmeniscus classification models. For example, Bien et al.â€™s\nuse of the MRNet framework demonstrated the utility\nof a 2D DL approach across multiple imaging planes for\ndetecting meniscus injuries [21]. We used only sagittal\nimages to segment the meniscus into six detailed anatomical\nsub-regions for binary and multi-classification. This\nrefinement enables a more precise anatomical analysis,\nbetter meeting clinical requirements. Our proposed LGSA-\nUNet and post-processing algorithm based on the clinical\ndefinition of sub-regions help to improve the spatial fluency\nof the entire meniscus and the positioning accuracy of\nsub-regions. For binary classification, based on the fully\nsupervised approach, the area under the receiver operating\ncharacteristic curve (AUC) values were 0.85â€“0.91; based\non the weakly supervised method, the AUC value is similar\nto that of the fully supervised method. Although our\nresults are marginally lower than those reported by Tack et al.\n[25], we speculate that this difference may be due to the\nuse of different MR sequences. Tack et al. [25] adopted the\nFig. 6 Comparison of speed\nof meniscus segmentation and\nmulti-classification by deep\nlearning model and junior radi-\nologist on the external test set\n\n200 Journal of Imaging Informatics in Medicine (2025) 38:191â€“202\nthree-dimensional double-echo steady state sequence, which\nhas 160 slices and more detailed diagnostic information.\nHowever, the long acquisition times associated with this\nsequence limit its routine clinical application. In contrast,\nour model employs more commonly used sequences (only\n35â€“40 slices), balancing detailed diagnostic information\nwith practicality for broader clinical usage. Despite the\nhigh diagnostic performance of DL models, challenges\npersist in accurately evaluating knee MR images in the\nsagittal plane. Misdiagnoses can occur, for instance, with\nthe transverse ligament being mistaken for a tear [31] or\nthe anterior cruciate ligamentâ€™s fiber bundle causing high\nsignal shadows that mimic tears [32, 33]. Therefore, in the\nbinary classification of meniscus injury, the most common\nmistake is to diagnose the lateral anterior horn of the normal\nmeniscus as a tear. Such pitfalls underscore the necessity for\ncontinuous refinement of DL models to minimize errors.\nIn the task of multi-classification of meniscal damage,\nour model demonstrated excellent performance in detect-\ning normal meniscus conditions, achieving AUC values\nbetween 0.87 and 0.88. However, the model faced challenges\nwith more complex cases, such as meniscal tears and mac-\nerations, which have traditionally been difficult to classify\naccurately. This indicates that precise multi-classification\nof injuries within the meniscusâ€™s six sub-regions remains a\nformidable task. Specifically, the modelâ€™s ability to differ-\nentiate between tearing and what might maceration showed\nvariability, with performance metrics ranging from 0.60 to\n0.79. Notably, the detection of tears in the anterior horn of\nthe meniscus was identified as the most challenging aspect\nin our multi-classification evaluation. We attribute this dif-\nficulty partly to the limited data available on tears in the\nmedial anterior horn of the meniscus within our dataset.\nDespite implementing strategies to mitigate the effects of\nthis data imbalance, such as data augmentation and category\nbalancing, these measures could not fully compensate for\nthe inherent data scarcity. Additionally, the division of the\nmeniscus into six distinct subregions introduces challenges\ndue to the small volume of specific injury types within each\nsubregion. This segmentation, while clinically relevant for\ndetailed anatomical analysis, complicates the modelâ€™s learn-\ning process and impacts its ability to generalize across all\ntypes of meniscal damage.\nFurthermore, while the majority of previous studies have\nachieved performance above 0.8, their validations were pre-\ndominantly limited to internal datasets, with only a hand-\nful extending their analysis to external datasets for model\neffectiveness verification. In this study, in addition to using\ninternal datasets for validation, we also included MRI scans\nof different brands and field strengths from an orthopedic\nhospital for external validation. This approach not only\ntests the modelsâ€™ adaptability but also their practical util-\nity across different clinical settings. Our models exhibited\nstrong performance on the external dataset, with Dice coef-\nficients for the segmentation model ranging from 0.77 to\n0.86, and AUC values for the classification model between\n0.82 and 0.87. We have observed in external datasets that the\nbinary classification models outperformed primary radiolo-\ngists in detecting injuries across five of the six sub-regions,\nwith significant differences (p < 0.05). Remarkably, in the\nanterior horn of the MM, the diagnostic performance of the\nmodel even surpassed that of senior radiologists, achieving\nan AUC of 0.87 compared to 0.81. The results revealed that\nour models achieved a high degree of robustness and were\neffective in assisting in diagnosing meniscal tears.\nLimitation\nSome potential limitations need consideration. Firstly, in\nthe multi-classification analysis, the number of meniscus\ntear and maceration cases was less than that of normal\nmeniscus cases, despite our efforts to address this issue\nby utilizing measures such as data augmentation, data cat-\negory balancing, and the introduction of a weighted cross-\nentropy loss function. Secondly, although multiple datasets\nwere used for validation in the study, in deep learning, the\nlarger the sample size, the better the performance of the\nmodel. Therefore, a larger dataset may improve the robust-\nness and generalization ability of the model.\nConclusion\nThis study proposes a customized LGSA UNet based\non the anatomical characteristics of the meniscus. The\nselected backbone network is UNet, which consists of five\nlayers of encoders and decoders. Each layer of encoder\ncompletes one downsampling, and each layer of decoder\ncompletes one upsampling. The single-layer encoder con-\nsists of basic components: convolution, activation func-\ntion, and bn layer. The Siamese adjustment module of\nLGSANet collects features from a three-layer backbone\nnetwork, subtracts adjacent slice features to obtain edge\nregions, multiplies adjacent slice features to obtain over-\nlapping regions, and inputs edge and overlapping informa-\ntion into the middle layer through fusion convolutional\nlayers to adjust the features of the middle layer, so that the\nmiddle layer features perceive changes in edge structure in\nspace, making the segmentation results more accurate for\nedge segmentation. Our LGSA model is more accurate in\nmeniscus segmentation than previous models and is suit-\nable for weakly supervised networks.\nTo the authorâ€™s knowledge, this is the first study\nto propose the use of weakly supervised methods for\n\n201Journal of Imaging Informatics in Medicine (2025) 38:191â€“202\nsegmentation and classification of meniscus MRI images,\nwhich can greatly reduce manual annotation time and\nachieve performance similar to fully supervised methods.\nOur DL model, based on the multi-classification models\nof six sub regions, is more in line with clinical needs. It\nprovides a promising tool to assist radiologists in effec-\ntively evaluating meniscus injuries and generating struc-\ntured reports.\nFuture research should focus on expanding dataset\ndiversity and exploring the integration of these models\ninto clinical workflows, potentially incorporating multi-\nplanar and sequence analyses to further improve diagnostic\naccuracy and robustness.\nAuthor Contributions All authors contributed to the study conception\nand design. Material preparation, data collection, and analysis were\nperformed by Kexin Jiang, Yuhan Xie, Xintao Zhang, Xinru Zhang,\nBeibei Zhou, Mianwen Li, Yanjun Chen, Jiaping Hu, Zhiyong Zhang,\nShaolong Chen, and Keyan Yu. The first draft of the manuscript\nwas written by Kexin Jiang, and Yuhan Xie. Changzhen Qiu, and\nXiaodong Zhang commented and reviewed the previous versions of\nthe manuscript. All authors read and approved the final version of\nthe manuscript.\nFunding This work was mainly supported by the Guang Dong Basic\nand Applied Basic Research Foundation (2024A1515010352), and the\nPresident Foundation of the Third Affiliated Hospital of Southern\nMedical University (YM2021012).\nData Availability The internal dataset of this study is sourced from the\npublicly available dataset Osteoarthritis Initiative. Data from the study\nare available at https://nda.nih.gov/oai/.\nDeclarations\nEthics Approval and Consent to Participate Osteoarthritis Initiative was\napproved by the Human Research Committee and the Internal Review\nCommittee. The patients/participants provided their written informed\nconsent to participate in this study. The external data set of this study\nwas approved by the clinical research ethics committee of The Third\nAffiliated Hospital, Southern Medical University (2024-ER-017) and\nwas conducted in accordance with the Declaration of Helsinki.\nCompeting Interests The authors declare no competing interests.\nReferences\n1. Markes AR, Hodax JD, Ma CB: Meniscus Form and Function.\nClin Sports Med 39:1-12, 2020\n2. Makris EA, Hadidi P, Athanasiou KA: The knee meniscus: struc-\nture-function, pathophysiology, current repair techniques, and\nprospects for regeneration. Biomaterials 32:7411-7431, 2011\n3. Englund M, Guermazi A, Lohmander SL: The role of the menis-\ncus in knee osteoarthritis: a cause or consequence? Radiol Clin\nNorth Am 47:703-712, 2009\n4. Adams BG, Houston MN, Cameron KL: The Epidemiology of\nMeniscus Injury. Sports Med Arthrosc Rev 29:e24-e33, 2021\n5. Greis PE, Bardana DD, Holmstrom MC, Burks RT: Meniscal\ninjury: I. Basic science and evaluation. J Am Acad Orthop Surg\n10:168â€“176, 2002\n6. Chang CY, Wu HT, Huang TF, Ma HL, Hung SC: Imaging evalua-\ntion of meniscal injury of the knee joint: a comparative MR imag-\ning and arthroscopic study. Clin Imaging 28:372-376, 2004\n7. Van Dyck P, et al.: Prospective comparison of 1.5 and 3.0-T MRI\nfor evaluating the knee menisci and ACL. J Bone Joint Surg Am\n95:916â€“924, 2013\n8. Englund M, Roemer FW, Hayashi D, Crema MD, Guermazi A:\nMeniscus pathology, osteoarthritis and the treatment controversy.\nNat Rev Rheumatol 8:412-419, 2012\n9. Fox AJ, Wanivenhaus F, Burge AJ, Warren RF, Rodeo SA: The\nhuman meniscus: a review of anatomy, function, injury, and\nadvances in treatment. Clin Anat 28:269-287, 2015\n10. Mordecai SC, Al-Hadithy N, Ware HE, Gupte CM: Treatment of\nmeniscal tears: An evidence-based approach. World journal of\northopedics 5:233-241, 2014\n11. Hunter DJ, et al.: Evolution of semi-quantitative whole joint\nassessment of knee OA: MOAKS (MRI Osteoarthritis Knee\nScore). Osteoarthritis Cartilage 19:990-1002, 2011\n12. Chartrand G, et al.: Deep Learning: A Primer for Radiologists.\nRadiographics 37:2113-2131, 2017\n13. McBee MP, et al.: Deep Learning in Radiology. Acad Radiol\n25:1472-1480, 2018\n14. Awan MJ, Rahim MSM, Salim N, Mohammed MA, Garcia-Zapirain\nB, Abdulkareem KH: Efficient Detection of Knee Anterior Cruciate\nLigament from Magnetic Resonance Imaging Using Deep Learning\nApproach. Diagnostics (Basel, Switzerland) 11, 2021\n15. Venkatesan Rajinikanth, et al.: Glioma/glioblastoma detec-\ntion in brain MRI using pre-trained deep-learning scheme.\n2022 Third International Conference on Intelligent Computing\nInstrumentation\n16. Jan Nedoma, et al.: Fiber-Optic Breathing Mask: An Alternative\nSolution for MRI Respiratory Triggering. IEEE Transactions on\nInstrumentation and Measurement\n17. Jeon U, Kim H, Hong H, Wang J: Automatic Meniscus Segmenta-\ntion Using Adversarial Learning-Based Segmentation Network with\nObject-Aware Map in Knee MR Images. Diagnostics (Basel) 11, 2021\n18. Gaj S, Yang M, Nakamura K, Li X: Automated cartilage and\nmeniscus segmentation of knee MRI with conditional generative\nadversarial networks. Magn Reson Med 84:437-449, 2020\n19. Byra M, et al.: Knee menisci segmentation and relaxometry of\n3D ultrashort echo time cones MR imaging using attention U-Net\nwith transfer learning. Magn Reson Med 83:1109-1122, 2020\n20. Astuto B, et al.: Automatic Deep Learning-assisted Detection\nand Grading of Abnormalities in Knee MRI Studies. Radiol Artif\nIntell 3:e200165, 2021\n21. Bien N, et al.: Deep-learning-assisted diagnosis for knee magnetic\nresonance imaging: Development and retrospective validation of\nMRNet. PLoS Med 15:e1002699, 2018\n22. Dai Y, Gao Y, Liu F: TransMed: Transformers Advance Multi-\nModal Medical Image Classification. Diagnostics (Basel) 11, 2021\n23. Qiu X, Liu Z, Zhuang M, Cheng D, Zhu C, Zhang X: Fusion\nof CNN1 and CNN2-based magnetic resonance image diagno-\nsis of knee meniscus injury and a comparative analysis with\ncomputed tomography. Comput Methods Programs Biomed\n211:106297, 2021\n24. Fritz B, Marbach G, Civardi F, Fucentese SF, Pfirrmann CWA:\nDeep convolutional neural network-based detection of meniscus\ntears: comparison with radiologists and surgery as standard of\nreference. Skeletal Radiol 49:1207-1217, 2020\n25. Tack A, Shestakov A, Ludke D, Zachow S: A Multi-Task Deep\nLearning Method for Detection of Meniscal Tears in MRI Data\nfrom the Osteoarthritis Initiative Database. Front Bioeng Biotech-\nnol 9:747217, 2021\n\n202 Journal of Imaging Informatics in Medicine (2025) 38:191â€“202\n26. Zhou Z, Zhao G, Kijowski R, Liu F: Deep convolutional neural\nnetwork for segmentation of knee joint anatomy. Magnetic reso-\nnance in medicine 80:2759-2770, 2018\n27. Guan S, Khan AA, Sikdar S, Chitnis PV: Fully Dense UNet for\n2-D Sparse Photoacoustic Tomography Artifact Removal. IEEE\njournal of biomedical and health informatics 24:568-576, 2020\n28. Cao Y, et al.: Swin-unet: Unet-like pure transformer for medical\nimage segmentation. European conference on computer vision\nCham: Springer Nature Switzerland, 2022, 2022: 205-218.\n29. Chen Y, et al.: TransUNet: transformers make strong encoders for\nmedical image segmentation. Available at https://arxiv.org/abs/\n2102.04306. Accessed 16 January 2023.\n30. Norman B, Pedoia V, Majumdar S: Use of 2D U-Net Convolu-\ntional Neural Networks for Automated Cartilage and Meniscus\nSegmentation of Knee MR Imaging Data to Determine Relaxom-\netry and Morphometry. Radiology 288:177-185, 2018\n31. Nelson EW, LaPrade RF: The anterior intermeniscal ligament of\nthe knee. An anatomic study. Am J Sports Med 28:74â€“76, 2000\n32. Johnson DL, Swenson TM, Livesay GA, Aizawa H, Fu FH, Harner\nCD: Insertion-site anatomy of the human menisci: gross, arthro-\nscopic, and topographical anatomy as a basis for meniscal trans-\nplantation. Arthroscopy 11:386-394, 1995\n33. Ziegler CG, et al.: Arthroscopically pertinent landmarks for tunnel\npositioning in single-bundle and double-bundle anterior cruciate\nligament reconstructions. Am J Sports Med 39:743-752, 2011\nPublisher's Note Springer Nature remains neutral with regard to\njurisdictional claims in published maps and institutional affiliations.\nSpringer Nature or its licensor (e.g. a society or other partner) holds\nexclusive rights to this article under a publishing agreement with the\nauthor(s) or other rightsholder(s); author self-archiving of the accepted\nmanuscript version of this article is solely governed by the terms of\nsuch publishing agreement and applicable law.",
    "version": "5.3.31"
  }
]
